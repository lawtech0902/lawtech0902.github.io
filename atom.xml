<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LawTech&#39;s Blog</title>
  <subtitle>不破不立</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-05-09T06:43:11.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LawTech.</name>
    <email>584563542@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——理解Session和Cookie机制</title>
    <link href="http://yoursite.com/2017/05/09/scrapy-session-cookie/"/>
    <id>http://yoursite.com/2017/05/09/scrapy-session-cookie/</id>
    <published>2017-05-09T06:18:54.000Z</published>
    <updated>2017-05-09T06:43:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Cookie-机制"><a href="#Cookie-机制" class="headerlink" title="Cookie 机制"></a><strong>Cookie 机制</strong></h2><p>Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。</p>
<a id="more"></a>
<p>具体来说cookie机制采用的是在客户端保持状态的方案。它是在用户端的会话状态的存贮机制，他需要用户打开客户端的cookie支持。cookie的作用就是为了解决HTTP协议无状态的缺陷所作的努力。</p>
<p>正统的cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示浏览器按照指示生成相应的cookie。然而纯粹的客户端脚本如JavaScript也可以生成cookie。而cookie的使用是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的cookie，如果某个cookie所声明的作用范围大于等于将要请求的资源所在的位置，则把该cookie附在请求资源的HTTP请求头上发送给服务器。</p>
<p>cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。若不设置过期时间，则表示这个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式。</p>
<p>而session机制采用的是一种在服务器端保持状态的解决方案。同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的。而session提供了方便管理全局变量的方式 。</p>
<p>session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，这个值也可能设置为由get来返回给服务器。</p>
<p>就安全性来说：当你访问一个使用session 的站点，同时在自己机子上建立一个cookie，建立在服务器端的session机制更安全些，因为它不会任意读取客户存储的信息。 </p>
<h2 id="Session-机制"><a href="#Session-机制" class="headerlink" title="Session 机制"></a><strong>Session 机制</strong></h2><p>session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。</p>
<p>当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个session标识（称为session id），如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来使用（检索不到，会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个session id将被在本次响应中返回给客户端保存。</p>
<p>保存这个session id的方式可以采用cookie，这样在交互过程中浏览器可以自动的按照规则把这个标识发挥给服务器。一般这个cookie的名字都是类似于SEEESIONID。但cookie可以被人为的禁止，则必须有其他机制以便在cookie被禁止时仍然能够把session id传递回服务器。</p>
<p>经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路径的后面。还有一种技术叫做表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器。</p>
<p>Cookie与Session都能够进行会话跟踪，但是完成的原理不太一样。普通状况下二者均能够满足需求，但有时分不能够运用Cookie，有时分不能够运用Session。</p>
<h2 id="两者比较"><a href="#两者比较" class="headerlink" title="两者比较"></a><strong>两者比较</strong></h2><h3 id="存取方式不同"><a href="#存取方式不同" class="headerlink" title="存取方式不同"></a>存取方式不同</h3><p>Cookie中只能保管ASCII字符串，假如需求存取Unicode字符或者二进制数据，需求先进行编码。Cookie中也不能直接存取Java对象。若要存储略微复杂的信息，运用Cookie是比拟艰难的。</p>
<p>而Session中能够存取任何类型的数据，包括而不限于String、Integer、List、Map等。Session中也能够直接保管Java Bean乃至任何Java类，对象等，运用起来十分便当。能够把Session看做是一个Java容器类。</p>
<h3 id="隐私策略不同"><a href="#隐私策略不同" class="headerlink" title="隐私策略不同"></a>隐私策略不同</h3><p>Cookie存储在客户端阅读器中，对客户端是可见的，客户端的一些程序可能会窥探、复制以至修正Cookie中的内容。而Session存储在服务器上，对客户端是透明的，不存在敏感信息泄露的风险。</p>
<p>假如选用Cookie，比较好的方法是，敏感的信息如账号密码等尽量不要写到Cookie中。最好是像Google、Baidu那样将Cookie信息加密，提交到服务器后再进行解密，保证Cookie中的信息只要本人能读得懂。而假如选择Session就省事多了，反正是放在服务器上，Session里任何隐私都能够有效的保护。</p>
<h3 id="服务器压力不同"><a href="#服务器压力不同" class="headerlink" title="服务器压力不同"></a>服务器压力不同</h3><p>Session是保管在服务器端的，每个用户都会产生一个Session。假如并发访问的用户十分多，会产生十分多的Session，耗费大量的内存。因而像Google、Baidu、Sina这样并发访问量极高的网站，是不太可能运用Session来追踪客户会话的。</p>
<p>而Cookie保管在客户端，不占用服务器资源。假如并发阅读的用户十分多，Cookie是很好的选择。关于Google、Baidu、Sina来说，Cookie或许是唯一的选择。</p>
<h3 id="浏览器支持不同"><a href="#浏览器支持不同" class="headerlink" title="浏览器支持不同"></a>浏览器支持不同</h3><p>Cookie是需要客户端浏览器支持的。假如客户端禁用了Cookie，或者不支持Cookie，则会话跟踪会失效。关于WAP上的应用，常规的Cookie就派不上用场了。</p>
<p>假如客户端浏览器不支持Cookie，需要运用Session以及URL地址重写。需要注意的是一切的用到Session程序的URL都要进行URL地址重写，否则Session会话跟踪还会失效。关于WAP应用来说，Session+URL地址重写或许是它唯一的选择。</p>
<p>假如客户端支持Cookie，则Cookie既能够设为本浏览器窗口以及子窗口内有效（把过期时间设为–1），也能够设为一切阅读器窗口内有效（把过期时间设为某个大于0的整数）。但Session只能在本阅读器窗口以及其子窗口内有效。假如两个浏览器窗口互不相干，它们将运用两个不同的Session。（IE8下不同窗口Session相干）</p>
<h3 id="跨域支持不同"><a href="#跨域支持不同" class="headerlink" title="跨域支持不同"></a>跨域支持不同</h3><p>Cookie支持跨域名访问，例如将domain属性设置为“.biaodianfu.com”，则以“.biaodianfu.com”为后缀的一切域名均能够访问该Cookie。跨域名Cookie如今被普遍用在网络中，例如Google、Baidu、Sina等。而Session则不会支持跨域名访问。Session仅在他所在的域名内有效。</p>
<h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><p>仅运用Cookie或者仅运用Session可能完成不了理想的效果。这时应该尝试一下同时运用Cookie与Session。Cookie与Session的搭配运用在实践项目中会完成很多意想不到的效果。</p>
<h2 id="Python-Django-中实现两种机制"><a href="#Python-Django-中实现两种机制" class="headerlink" title="Python Django 中实现两种机制"></a>Python Django 中实现两种机制</h2><h3 id="Cookie-设置"><a href="#Cookie-设置" class="headerlink" title="Cookie 设置"></a>Cookie 设置</h3><p>以下是Cookie设置的详细流程：</p>
<ol>
<li>客户端发起一个请求连接（如HTTP GET）</li>
<li>服务器在http响应头上加上Set-Cookie，里面存放字符串的键值对</li>
<li>客户端随后的http请求头加上Cookie首部，它包含了之前服务器响应中设置cookie的信息。</li>
</ol>
<p>根据这个Cookie首部的信息，服务器便能“记住”当前用户的信息。</p>
<p>下面就来看看Python中如何设置Cookie：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> BaseHTTPServer <span class="keyword">import</span> HTTPServer</div><div class="line"><span class="keyword">from</span> SimpleHTTPServer <span class="keyword">import</span> SimpleHTTPRequestHandler</div><div class="line"><span class="keyword">import</span> Cookie</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRequestHandler</span><span class="params">(SimpleHTTPRequestHandler)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_GET</span><span class="params">(self)</span>:</span></div><div class="line">        content = <span class="string">"&lt;html&gt;&lt;body&gt;Path is: %s&lt;/body&gt;&lt;/html&gt;"</span> % self.path</div><div class="line">        self.send_response(<span class="number">200</span>)</div><div class="line">        self.send_header(<span class="string">'Content-type'</span>, <span class="string">'text/html'</span>)</div><div class="line">        self.send_header(<span class="string">'Content-length'</span>, str(len(content)))</div><div class="line"></div><div class="line">        cookie = Cookie.SimpleCookie()</div><div class="line">        cookie[<span class="string">'id'</span>] = <span class="string">'some_value_42'</span></div><div class="line"></div><div class="line">        self.wfile.write(cookie.output())</div><div class="line">        self.wfile.write(<span class="string">'\r\n'</span>)</div><div class="line"></div><div class="line">        self.end_headers()</div><div class="line">        self.wfile.write(content)</div><div class="line"></div><div class="line">server = HTTPServer((<span class="string">''</span>, <span class="number">59900</span>), MyRequestHandler)</div><div class="line">server.serve_forever()</div></pre></td></tr></table></figure>
<p>查看服务器端的http响应头，会发现以下字段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Set-Cookie: id=some_value_42</div></pre></td></tr></table></figure>
<p>在Django中，可以用如下的方式获取或设置Cookie：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_cookie</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'id'</span> <span class="keyword">in</span> request.COOKIES:</div><div class="line">        cookie_id = request.COOKIES[<span class="string">'id'</span>]</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'Got cookie with id=%s'</span> % cookie_id)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        resp = HttpResponse(<span class="string">'No id cookie! Sending cookie to client'</span>)</div><div class="line">        resp.set_cookie(<span class="string">'id'</span>, <span class="string">'some_value_99'</span>)</div><div class="line">        <span class="keyword">return</span> resp</div></pre></td></tr></table></figure>
<p>Django通过一系列的包装使得封装Cookie的操作变得更加简单，那么它在其中是怎么实现cookie的读取的呢，下面来窥探原理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_cookies</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'_cookies'</span>):</div><div class="line">        self._cookies = http.parse_cookie(self.environ.get(<span class="string">'HTTP_COOKIE'</span>, <span class="string">''</span>))</div><div class="line">    <span class="keyword">return</span> self._cookies</div></pre></td></tr></table></figure>
<p>可以看出，获取cookie的操作用了Lazy initialization（延迟加载）的技术，因为如果客户端不需要用到cookie，这个过程只会浪费不必要的操作。</p>
<p>再来看parse_cookie的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_cookie</span><span class="params">(cookie)</span>:</span></div><div class="line">    <span class="keyword">if</span> cookie == <span class="string">''</span>:</div><div class="line">        <span class="keyword">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(cookie, Cookie.BaseCookie):</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            c = SimpleCookie()</div><div class="line">            c.load(cookie, ignore_parse_errors=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">except</span> Cookie.CookieError:</div><div class="line">            <span class="comment"># 无效cookie</span></div><div class="line">            <span class="keyword">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        c = cookie</div><div class="line">    cookiedict = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> c.keys():</div><div class="line">        cookiedict[key] = c.get(key).value</div><div class="line">    <span class="keyword">return</span> cookiedict</div></pre></td></tr></table></figure>
<p>它负责解析Cookie并把结果集成到一个dict（字典）对象中，并返回字典。而设置cookie的操作则会被WSGIHandler执行。</p>
<p>注：Django的底层实现了WSGI的接口（如WSGIRequest，WSGIServer等）。</p>
<h3 id="Session-应用"><a href="#Session-应用" class="headerlink" title="Session 应用"></a>Session 应用</h3><p>下面看一个简单的session应用例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_count_session</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'count'</span> <span class="keyword">in</span> request.session:</div><div class="line">        request.session[<span class="string">'count'</span>] += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'new count=%s'</span> % request.session[<span class="string">'count'</span>])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        request.session[<span class="string">'count'</span>] = <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'No count in session. Setting to 1'</span>)</div></pre></td></tr></table></figure>
<p>它用session实现了一个计数器，当每一个请求到来时，就为计数器加一，把新的结果更新到session中。</p>
<p>查看http的响应头，会得到类似下面的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Set-Cookie:sessionid=a92d67e44a9b92d7dafca67e507985c0;</div><div class="line">           expires=Thu, <span class="number">07</span>-Jul<span class="number">-2011</span> <span class="number">04</span>:<span class="number">16</span>:<span class="number">28</span> GMT;</div><div class="line">           Max-Age=<span class="number">1209600</span>;</div><div class="line">           Path=/</div></pre></td></tr></table></figure>
<p>里面包含了session_id以及过期时间等信息。</p>
<p>那么服务器端是如何保存session的呢？</p>
<p>在django中，默认会把session保存在setting指定的数据库中，除此之外，也可以通过指定session engine，使session保存在文件(file)，内存(cache)中。</p>
<p>如果保存在数据库中，django会在数据库中创建一个如下的session表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE &quot;django_session&quot; (</div><div class="line">    &quot;session_key&quot; varchar(40) NOT NULL PRIMARY KEY,</div><div class="line">    &quot;session_data&quot; text NOT NULL,</div><div class="line">    &quot;expire_date&quot; datetime NOT NULL</div><div class="line">);</div></pre></td></tr></table></figure>
<p>session_key是放置在cookie中的id，它是唯一的，而session_data则存放序列化后的session数据字符串。</p>
<p>通过session_key可以在数据库中取得这条session的信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.contrib.sessions.models <span class="keyword">import</span> Session</div><div class="line"><span class="comment">#...</span></div><div class="line">sess = Session.objects.get(pk=<span class="string">'a92d67e44a9b92d7dafca67e507985c0'</span>)</div><div class="line">print(sess.session_data)</div><div class="line">print(sess.get_decoded())</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ZmEyNDVhNTBhMTk2ZmRjNzVlYzQ4NTFjZDk2Y2UwODc3YmVjNWVjZjqAAn1xAVUFY291bnRxAksG</div><div class="line">cy4=&#123;<span class="string">'count'</span>: <span class="number">6</span>&#125;</div></pre></td></tr></table></figure>
<p>回看第一个例子，我们是通过request.session来获取session的，为什么请求对象会附带一个session对象呢，这其中做了什么呢？</p>
<p>这就引出了下面要说的django里的中间件技术 <code>Session middleware</code>。</p>
<p>关于中间件，<code>&lt;&lt;the Django Book&gt;&gt;</code>是这样解释的：</p>
<p>Django的中间件框架，是django处理请求和响应的一套钩子函数的集合。</p>
<p>我们看传统的django视图模式一般是这样的：http请求-&gt;view-&gt;http响应，而加入中间件框架后，则变为：http请求-&gt;中间件处理-&gt;app-&gt;中间件处理-&gt;http响应。而在django中这两个处理分别对应process_request和process_response函数，这两个钩子函数将会在特定的时候被触发。</p>
<p>直接看SessionMiddleware可能更清晰一些：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SessionMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        engine = import_module(settings.SESSION_ENGINE)</div><div class="line">        self.SessionStore = engine.SessionStore</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request)</span>:</span></div><div class="line">        session_key = request.COOKIES.get(settings.SESSION_COOKIE_NAME)</div><div class="line">        request.session = self.SessionStore(session_key)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        If request.session was modified, or if the configuration is to save the</div><div class="line">        session every time, save the changes and set a session cookie or delete</div><div class="line">        the session cookie if the session has been emptied.</div><div class="line">        """</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            accessed = request.session.accessed</div><div class="line">            modified = request.session.modified</div><div class="line">            empty = request.session.is_empty()</div><div class="line">        <span class="keyword">except</span> AttributeError:</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># First check if we need to delete this cookie.</span></div><div class="line">            <span class="comment"># The session should be deleted only if the session is entirely empty</span></div><div class="line">            <span class="keyword">if</span> settings.SESSION_COOKIE_NAME <span class="keyword">in</span> request.COOKIES <span class="keyword">and</span> empty:</div><div class="line">                response.delete_cookie(settings.SESSION_COOKIE_NAME,</div><div class="line">                    domain=settings.SESSION_COOKIE_DOMAIN)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">if</span> accessed:</div><div class="line">                    patch_vary_headers(response, (<span class="string">'Cookie'</span>,))</div><div class="line">                <span class="keyword">if</span> (modified <span class="keyword">or</span> settings.SESSION_SAVE_EVERY_REQUEST) <span class="keyword">and</span> <span class="keyword">not</span> empty:</div><div class="line">                    <span class="keyword">if</span> request.session.get_expire_at_browser_close():</div><div class="line">                        max_age = <span class="keyword">None</span></div><div class="line">                        expires = <span class="keyword">None</span></div><div class="line">                    <span class="keyword">else</span>:</div><div class="line">                        max_age = request.session.get_expiry_age()</div><div class="line">                        expires_time = time.time() + max_age</div><div class="line">                        expires = cookie_date(expires_time)</div><div class="line">                    <span class="comment"># Save the session data and refresh the client cookie.</span></div><div class="line">                    <span class="comment"># Skip session save for 500 responses, refs #3881.</span></div><div class="line">                    <span class="keyword">if</span> response.status_code != <span class="number">500</span>:</div><div class="line">                        <span class="keyword">try</span>:</div><div class="line">                            request.session.save()</div><div class="line">                        <span class="keyword">except</span> UpdateError:</div><div class="line">                            <span class="comment"># The user is now logged out; redirecting to same</span></div><div class="line">                            <span class="comment"># page will result in a redirect to the login page</span></div><div class="line">                            <span class="comment"># if required.</span></div><div class="line">                            <span class="keyword">return</span> redirect(request.path)</div><div class="line">                        response.set_cookie(settings.SESSION_COOKIE_NAME,</div><div class="line">                                request.session.session_key, max_age=max_age,</div><div class="line">                                expires=expires, domain=settings.SESSION_COOKIE_DOMAIN,</div><div class="line">                                path=settings.SESSION_COOKIE_PATH,</div><div class="line">                                secure=settings.SESSION_COOKIE_SECURE <span class="keyword">or</span> <span class="keyword">None</span>,</div><div class="line">                                httponly=settings.SESSION_COOKIE_HTTPONLY <span class="keyword">or</span> <span class="keyword">None</span>)</div><div class="line">        <span class="keyword">return</span> response</div></pre></td></tr></table></figure>
<p>在请求到来后，SessionMiddleware的process_request在请求取出session_key，并把一个新的session对象赋给request.session，而在返回响应时，process_response则判断session是否被修改或过期，来更新session的信息。</p>
<h3 id="Django-用户认证中的-Session"><a href="#Django-用户认证中的-Session" class="headerlink" title="Django 用户认证中的 Session"></a>Django 用户认证中的 Session</h3><p>在django中，用下面的方法来验证用户是否登录是常见的事情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_user</span><span class="params">(request)</span>:</span></div><div class="line">    user_str = str(request.user)</div><div class="line">    <span class="keyword">if</span> request.user.is_authenticated():</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'%s is logged in'</span> % user_str)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'%s is not logged in'</span> % user_str)</div></pre></td></tr></table></figure>
<p>其实request.user的实现也借助到了session。</p>
<p>在这个例子中，成功登录后，session表会保存类似下面的信息，里面记录了用户的id，以后进行验证时，便会到这个表中获取用户的信息。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;'_auth_user_id': 1, '_auth_user_backend': 'django.contrib.auth.backends.ModelBackend'&#125;</div></pre></td></tr></table></figure>
<p>跟上面提到的Session中间件相似，用户验证也有一个中间件：AuthenticationMiddleware，在process_request中，通过request.<strong>class</strong>.user = LazyUser()在request设置了一个全局的可缓存的用户对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyUser</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, request, obj_type=None)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(request, <span class="string">'_cached_user'</span>):</div><div class="line">            <span class="keyword">from</span> django.contrib.auth <span class="keyword">import</span> get_user</div><div class="line">            request._cached_user = get_user(request)</div><div class="line">        <span class="keyword">return</span> request._cached_user</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuthenticationMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request)</span>:</span></div><div class="line">        request.__class__.user = LazyUser()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div></pre></td></tr></table></figure>
<p>在get_user里，会在检查session中是否存放了当前用户对应的user_id，如果有，则通过id在model查找相应的用户返回，否则返回一个匿名的用户对象(AnonymousUser)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">from</span> django.contrib.auth.models <span class="keyword">import</span> AnonymousUser</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        user_id = request.session[SESSION_KEY]</div><div class="line">        backend_path = request.session[BACKEND_SESSION_KEY]</div><div class="line">        backend = load_backend(backend_path)</div><div class="line">        user = backend.get_user(user_id) <span class="keyword">or</span> AnonymousUser()</div><div class="line">    <span class="keyword">except</span> KeyError:</div><div class="line">        user = AnonymousUser()</div><div class="line">    <span class="keyword">return</span> user</div></pre></td></tr></table></figure>
<h3 id="Django中的Session实现"><a href="#Django中的Session实现" class="headerlink" title="Django中的Session实现"></a>Django中的Session实现</h3><p>Django使用的Session默认都继承于SessionBase类里，这个类实现了一些session操作方法，以及hash，decode，encode等方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SessionBase</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Base class for all Session classes.</div><div class="line">    """</div><div class="line">    TEST_COOKIE_NAME = <span class="string">'testcookie'</span></div><div class="line">    TEST_COOKIE_VALUE = <span class="string">'worked'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, session_key=None)</span>:</span></div><div class="line">        self._session_key = session_key</div><div class="line">        self.accessed = <span class="keyword">False</span></div><div class="line">        self.modified = <span class="keyword">False</span></div><div class="line">        self.serializer = import_string(settings.SESSION_SERIALIZER)</div></pre></td></tr></table></figure>
<p>说的更直白一些，其实django中的session就是一个模拟dict的对象，并实现了一系列的hash和序列化方法，默认持久化在数据库中（有时候也可能由于为了提高性能，用redis之类的内存数据库来缓存session）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Cookie-机制&quot;&gt;&lt;a href=&quot;#Cookie-机制&quot; class=&quot;headerlink&quot; title=&quot;Cookie 机制&quot;&gt;&lt;/a&gt;&lt;strong&gt;Cookie 机制&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Django，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CDjango%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Loaders机制介绍与实例</title>
    <link href="http://yoursite.com/2017/05/08/scrapy-item-loader/"/>
    <id>http://yoursite.com/2017/05/08/scrapy-item-loader/</id>
    <published>2017-05-08T06:18:54.000Z</published>
    <updated>2017-05-08T07:02:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 </p>
<p>从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。</p>
<p>Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。</p>
<a id="more"></a>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="使用-Item-Loaders-来填充-Items"><a href="#使用-Item-Loaders-来填充-Items" class="headerlink" title="使用 Item Loaders 来填充 Items"></a><strong>使用 Item Loaders 来填充 Items</strong></h3><p>要使用 Item Loader, 你必须先将它实例化。你可以使用类似字典的对象(例如: Item or dict)来进行实例化，或者不使用对象也可以，当不用对象进行实例化的时候，Item 会自动使用 <code>ItemLoader.default\_item_class</code> 属性中指定的 Item 类在 <code>Item Loader constructor</code> 中实例化。 </p>
<p>然后，你开始收集数值到 Item Loader 时，通常使用 Selectors。你可以在同一个 item field 里面添加多个数 值；Item Loader 将知道如何用合适的处理函数来“添加”这些数值。 下面是在 Spider 中典型的 Item Loader 的用法，使用 <code>Items chapter</code> 中声明的 <code>Product item</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader </div><div class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> Product</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span> </div><div class="line">    l = ItemLoader(item=Product(), response=response) </div><div class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_name"]'</span>) </div><div class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_title"]'</span>) </div><div class="line">    l.add_xpath(<span class="string">'price'</span>, <span class="string">'//p[@id="price"]'</span>) </div><div class="line">    l.add_css(<span class="string">'stock'</span>, <span class="string">'p#stock]'</span>) </div><div class="line">    l.add_value(<span class="string">'last_updated'</span>, <span class="string">'today'</span>) <span class="comment"># you can also use literal values </span></div><div class="line">    <span class="keyword">return</span> l.load_item()</div></pre></td></tr></table></figure>
<p>快速查看这些代码之后，我们可以看到发现 name 字段被从页面中两个不同的 XPath 位置提取：</p>
<ol>
<li><code>//div[@class=&quot;product_name&quot;]</code></li>
<li><code>//div[@class=&quot;product_title&quot;]</code></li>
</ol>
<p>换言之,数据通过用 <code>add_xpath()</code> 的方法，把从两个不同的 XPath 位置提取的数据收集起来。这是将在以后分配给 <code>name</code> 字段中的数据?</p>
<p>之后，类似的请求被用于 price 和 stock 字段 （后者使用 <code>CSS selector</code> 和 <code>add_css()</code> 方法）， 最后使用不同的方法 <code>add_value()</code> 对 <code>last_update</code> 填充文本值( today )。 最终，当所有数据被收集起来之后，调用 <code>ItemLoader.load_item()</code> 方法，实际上填充并且返回了之前通过调用 <code>add_xpath()</code>，<code>add_css()</code> ，<code>add_value()</code> 所提取和收集到的数据的 Item。</p>
<h3 id="输入和输出处理器"><a href="#输入和输出处理器" class="headerlink" title="输入和输出处理器"></a><strong>输入和输出处理器</strong></h3><p>Item Loader 在每个（Item）字段中都包含了一个输入处理器和一个输出处理器。输入处理器收到数据时立刻提取数据 （通过  <code>add_xpath()</code>， <code>add_css()</code>  或者  <code>add_value()</code>方法）之后输入处理器的结果被收集起来并且保存在ItemLoader内。收集到所有的数据后，调用 <code>ItemLoader.load_item()</code> 方法来填充，并得到填充后的 Item 对象。这是当输出处理器被和之前收集到的数据（和用输入处理器处理的）被调用。输出处理器的结果是被分配到Item的最终值。</p>
<p>让我们看一个例子来说明如何输入和输出处理器被一个特定的字段调用（同样适用于其他field）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">l = ItemLoader(Product(), some_selector)</div><div class="line">l.add_xpath(<span class="string">'name'</span>, xpath1) <span class="comment"># (1)</span></div><div class="line">l.add_xpath(<span class="string">'name'</span>, xpath2) <span class="comment"># (2)</span></div><div class="line">l.add_css(<span class="string">'name'</span>, css) <span class="comment"># (3)</span></div><div class="line">l.add_value(<span class="string">'name'</span>, <span class="string">'test'</span>) <span class="comment"># (4)</span></div><div class="line"><span class="keyword">return</span> l.load_item() <span class="comment"># (5)</span></div></pre></td></tr></table></figure>
<p>发生了这些事情:</p>
<ol>
<li>从 <code>xpath1</code> 提取出的数据,传递给 <em>输入处理器</em> 的 <code>name</code> 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡</li>
<li>从 <code>xpath2</code> 提取出来的数据,传递给(1)中使用的相同的 <em>输入处理器</em> .输入处理器的结果被附加到在(1)中收集的数据(如果有的话) ｡</li>
<li>和之前相似，只不过这里的数据是通过 <code>css</code> CSS selector抽取，之后传输到在(1)和(2)使用 的<em>input processor</em> 中。最终输入处理器的结果被附加到在(1)和(2)中收集的数据之后 (如果存在数据的话)。</li>
<li>这里的处理方式也和之前相似，但是此处的值是通过add_value直接赋予的， 而不是利用XPath表达式或CSS selector获取。得到的值仍然是被传送到输入处理器。 在这里例程中，因为得到的值并非可迭代，所以在传输到输入处理器之前需要将其 转化为可迭代的单个元素，这才是它所接受的形式。</li>
<li>在之前步骤中所收集到的数据被传送到 <em>output processor</em> 的 <code>name</code> field中。 输出处理器的结果就是赋到item中 <code>name</code> field的值。</li>
</ol>
<p><strong>理解：</strong></p>
<p>就是在使用Item Loader 时候，会有一个输入处理器，一个输出处理器，首先是收集好同一个字段的结果，传入到<strong>输入处理器</strong>当中，然后收集完后，会传递给<strong>输出处理器</strong>进行处理。输出处理器的处理结果，就是填充到item的结果。</p>
<p>需要注意的是：输入处理器的返回值会是内部收集的，然后被传递给输出处理器，来填充fields。</p>
<h4 id="Scrapy-内部的处理器"><a href="#Scrapy-内部的处理器" class="headerlink" title="Scrapy 内部的处理器"></a><strong>Scrapy 内部的处理器</strong></h4><p>Scrapy内部，已经有一些设置好的<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#topics-loaders-available-processors" target="_blank" rel="external">内置处理器</a></p>
<ul>
<li>Identity<br>这是最简单的一个处理器，实际上就是什么都不做，传入多少个字段，就存储多少个字段，以list形式。<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import Identity
\&gt;&gt;&gt; proc = Identity()
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</code> </li>
<li>TakeFirst<br>从接受到的list中返回第一个非null/非空的值，<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import TakeFirst
\&gt;&gt;&gt; proc = TakeFirst()
\&gt;&gt;&gt; proc([&#39;&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
&#39;one&#39;</code> </li>
<li>Join<br>返回用分隔符（separator）作为间隔的连接形成的字符串。若不传入separator，则默认使用’ ‘（空格）。<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import Join
\&gt;&gt;&gt; proc = Join()
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
u&#39;one two three&#39;
\&gt;&gt;&gt; proc = Join(&#39;&lt;br&gt;&#39;)
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
u&#39;one&lt;br&gt;two&lt;br&gt;three&#39;</code> </li>
<li>另外还有Compose以及MapCompose，这里不一一详述。</li>
</ul>
<hr>
<h3 id="声明-Item-Loaders"><a href="#声明-Item-Loaders" class="headerlink" title="声明 Item Loaders"></a><strong>声明 Item Loaders</strong></h3><p>声明ItemLoaders 和声明Item类似，使用Class语法，例子：</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="title">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader</div><div class="line"><span class="title">from</span> scrapy.contrib.loader.processor <span class="keyword">import</span> TakeFirst, MapCompose, Join</div><div class="line"><span class="class"></span></div><div class="line"><span class="keyword">class</span> <span class="type">ProductLoader</span>(<span class="type">ItemLoader</span>):</div><div class="line"></div><div class="line">    default_output_processor = <span class="type">TakeFirst</span>()</div><div class="line"></div><div class="line">    name_in = <span class="type">MapCompose</span>(<span class="title">unicode</span>.<span class="title">title</span>)</div><div class="line">    name_out = <span class="type">Join</span>()</div><div class="line"></div><div class="line">    price_in = <span class="type">MapCompose</span>(<span class="title">unicode</span>.<span class="title">strip</span>)</div><div class="line"></div><div class="line">    # ...</div></pre></td></tr></table></figure>
<p>上述代码中:</p>
<ul>
<li>输出处理器，被声明为 <code>_in</code> 前缀，而输出处理器被声明为 <code>_out</code> 前缀。</li>
<li>设置默认处理器 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_input_processor" target="_blank" rel="external"><code>ItemLoader.default_input_processor</code></a>  and  <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_output_processor" target="_blank" rel="external"><code>ItemLoader.default_output_processor</code></a></li>
</ul>
<h3 id="声明输入、输出处理器"><a href="#声明输入、输出处理器" class="headerlink" title="声明输入、输出处理器"></a><strong>声明输入、输出处理器</strong></h3><p>输入、输出可以被如上方那样被声明，这也是最正常的方式。另外，我们也可以在另外的一个地方去声明输入和输出处理器：在item Field，元数据中。</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="title">from</span> scrapy.contrib.loader.processor <span class="keyword">import</span> Join, MapCompose, TakeFirst</div><div class="line"><span class="title">from</span> w3lib.html <span class="keyword">import</span> remove_tags</div><div class="line"></div><div class="line"><span class="title">def</span> filter_price(value):</div><div class="line">    <span class="keyword">if</span> value.isdigit():</div><div class="line">        return value</div><div class="line"><span class="class"></span></div><div class="line"><span class="keyword">class</span> <span class="type">Product</span>(<span class="title">scrapy</span>.<span class="type">Item</span>):</div><div class="line">    name = scrapy.<span class="type">Field</span>(</div><div class="line">        <span class="title">input_processor</span>=<span class="type">MapCompose(remove_tags)</span>,</div><div class="line">        <span class="title">output_processor</span>=<span class="type">Join</span>(),</div><div class="line">    )</div><div class="line">    price = scrapy.<span class="type">Field</span>(</div><div class="line">        <span class="title">input_processor</span>=<span class="type">MapCompose</span>(<span class="title">remove_tags</span>, <span class="title">filter_price</span>),</div><div class="line">        output_processor=<span class="type">TakeFirst</span>(),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>输出和输出处理器的优先级如下：</p>
<ol>
<li>Item Loader field 指定的<code>field_in</code> 和 <code>field_out</code>（最优先）</li>
<li>Field 元数据中(input_processor 和 output_processor key)</li>
<li>item loader 默认。 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_input_processor" target="_blank" rel="external"><code>ItemLoader.default_input_processor()</code></a> and<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_output_processor" target="_blank" rel="external"><code>ItemLoader.default_output_processor()</code></a> (least precedence)</li>
</ol>
<hr>
<p>最后，给出Item Loader的官方说明API：</p>
<p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#itemloader-objects" target="_blank" rel="external">ItemLoader objects</a></p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a><strong>实例</strong></h2><h3 id="通过-Item-loader-加载-Item"><a href="#通过-Item-loader-加载-Item" class="headerlink" title="通过 Item loader 加载 Item"></a><strong>通过 Item loader 加载 Item</strong></h3><p>首先在 <code>jobbole.py</code> 中引入 <code>from scrapy.loader import ItemLoader</code></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</div><div class="line">item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</div><div class="line">item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</div><div class="line">item_loader.add_css(<span class="string">"create_date"</span>, <span class="string">"p.entry-meta-hide-on-mobile::text"</span>)</div><div class="line">front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></div><div class="line">item_loader.add_value(<span class="string">"front_image_url"</span>, [front_image_url])</div><div class="line">item_loader.add_css(<span class="string">"praise_nums"</span>, <span class="string">".vote-post-up h10::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"comment_nums"</span>, <span class="string">"a[href='#article-comment'] span::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"fav_nums"</span>, <span class="string">".bookmark-btn::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"tags"</span>, <span class="string">"p.entry-meta-hide-on-mobile a::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"content"</span>, <span class="string">"div.entry"</span>)</div><div class="line"></div><div class="line">article_item = item_loader.load_item()</div></pre></td></tr></table></figure>
<p>其中第一行中 <code>JobBoleArticleItem()</code> 为在 <code>items.py</code> 中声明的实例，<code>response</code> 为返回的响应。这属于固定写法。<br><code>add_css()</code>中第一个值为 <code>items.py</code> 中定义的值，第二个值为css选择器规则，类似的方法还有 <code>add_xpath()</code>，根据场景进行选择。</p>
<p>同理，<code>add_value()</code>为添加确定值的方法。这里通过值传递附给 <code>front_image_url</code> 再通过add_value的方法，加入到最终的item中。</p>
<p>最后通过调用 <code>load_item()</code> 方法对结果进行解析，所有的结果都是一个list并保存到 <code>article_item</code> 中。</p>
<p>断点调试结果如图：</p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1ffdwrwwdu2j31kw1ax1a5.jpg" alt=""></p>
<p>发现获取到的所有值都是一个list，这样很不方便，但使得代码可读性更高，可维护性更强。</p>
<h3 id="通过-items-py-处理数据"><a href="#通过-items-py-处理数据" class="headerlink" title="通过 items.py 处理数据"></a><strong>通过 items.py 处理数据</strong></h3><p>在 <code>items.py</code> 中引入 <code>from scrapy.loader.processors import MapCompose</code> ，然后可以在定义 <code>scrapy.Field()</code> 时可以加入处理函数（可以使匿名函数），例如：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdx4319hjj30x009kq4b.jpg" alt=""></p>
<p>在 <code>MapCompose()</code> 中可以加入多个函数，在 <code>jobbole.py</code> 中断点调试结果如图：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdx56xexcj31kw0c37aq.jpg" alt=""></p>
<p>在title的结果后面出现了我们想要的后缀。</p>
<p>另外，可以看到，结果都是 <code>list</code>，我们每次都需要提取第一个值。Scrapy给我们提供了 <code>TakeFirst</code> 方法。</p>
<p>同样引入 <code>from scrapy.loader.processors import MapCompose,TakeFirst</code> ，修改代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">title = scrapy.Field(</div><div class="line">        input_processor = MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"-jobbole"</span>, add_jobbole),</div><div class="line">        output_processor = TakeFirst()</div><div class="line">	)</div></pre></td></tr></table></figure>
<p>即可以得到第一个值。由于每一个结果都是取第一个值，每个值全部调用这个方法重复代码过多，可以通过自定义Item loader重载的方法解决。引入 <code>from scrapy.loader import ItemLoader</code> ，这个类提供了以下方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItemLoader</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    default_item_class = Item</div><div class="line">    default_input_processor = Identity()</div><div class="line">    default_output_processor = Identity()</div><div class="line">    default_selector_class = Selector</div></pre></td></tr></table></figure>
<p>我们自定义的Item loader需要继承这个类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></div><div class="line">    default_output_processor = TakeFirst()</div></pre></td></tr></table></figure>
<p>然后在 <code>jobbole.py</code> 文件中，把<br> <code>item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</code> 中的 <code>ItemLoader</code> 变为 <code>ArticleItemLoader</code>，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response)</div></pre></td></tr></table></figure>
<p>这样得到的结果就是一个str而不是list了。</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffdyhajhdvj31kw0bz445.jpg" alt=""></p>
<p>不过在上图是可以看到，它的tags也取了第一个值，但实际上它的值是三个，不满足我们的需要。<br>引入Join方法 <code>from scrapy.loader.processors import MapCompose, TakeFirst, Join</code>，同时不使用自定义的item loader即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tags = scrapy.Field(</div><div class="line">        output_processor=Join(<span class="string">','</span>),</div><div class="line">	)</div></pre></td></tr></table></figure>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdyj0tk9lj30oe00sdg0.jpg" alt=""></p>
<p>和前面一样，有时候tags会有 <code>评论</code> 的不符合要求的tags，还需要自定义函数把相应的字段去掉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_comment</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'评论'</span> <span class="keyword">in</span> value:</div><div class="line">        <span class="keyword">return</span> <span class="string">''</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> value</div></pre></td></tr></table></figure>
<p>在处理图片时，使用pipelines需要传递的是一个列表，这里经过处理后，变成了str。可以通过一个默认函数不让默认的TakeFirst处理即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_value</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="keyword">return</span> value</div></pre></td></tr></table></figure>
<p>调用方法是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">front_image_url = scrapy.Field(</div><div class="line">        output_processor=MapCompose(return_value),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>最后，我们在之前还用正则表达式来清洗点赞数，收藏数，评论数这些数据，在item loader中我们也可以用函数处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_nums</span><span class="params">(value)</span>:</span></div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, value)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        nums = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> nums</div><div class="line"></div><div class="line">praise_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div><div class="line">    comment_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div><div class="line">    fav_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>调试结果中str就变成int类型了：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffdynb5xtsj31kw0c9afc.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 &lt;/p&gt;
&lt;p&gt;从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。&lt;/p&gt;
&lt;p&gt;Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——保存item到json文件</title>
    <link href="http://yoursite.com/2017/05/07/scrapy-item-json/"/>
    <id>http://yoursite.com/2017/05/07/scrapy-item-json/</id>
    <published>2017-05-07T06:18:54.000Z</published>
    <updated>2017-05-07T05:05:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。</p>
<a id="more"></a>
<h2 id="使用系统-exporter-导出为-JSON-文件"><a href="#使用系统-exporter-导出为-JSON-文件" class="headerlink" title="使用系统 exporter 导出为 JSON 文件"></a><strong>使用系统 exporter 导出为 JSON 文件</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 调用Scrapy提供的json exporter导出json文件</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = open(<span class="string">'article.json'</span>, <span class="string">'wb'</span>)</div><div class="line">        self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</div><div class="line">        self.exporter.start_exporting()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.exporter.finish_exporting()</div><div class="line">        self.file.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        self.exporter.export_item(item)</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h2 id="自定义-Pipeline-导出为-JSON-文件"><a href="#自定义-Pipeline-导出为-JSON-文件" class="headerlink" title="自定义 Pipeline 导出为 JSON 文件"></a><strong>自定义 Pipeline 导出为 JSON 文件</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWithEncodingPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 自定义json文件的导出</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></div><div class="line">        self.file.write(lines)</div><div class="line">        <span class="keyword">return</span> item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.file.close()</div></pre></td></tr></table></figure>
<h2 id="函数说明"><a href="#函数说明" class="headerlink" title="函数说明"></a><strong>函数说明</strong></h2><p><code>codecs</code> ：避免打开文件时出现编码错误。<br><code>json.dumps</code> ：dict转成str<br><code>json.loads</code> ：str转成dict<br><code>ensure_ascii=False</code> ：避免处理英文以外语言时出错<br><code>return item</code> ：交给下一个pipeline处理</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——通过Pipeline保存数据到MySQL</title>
    <link href="http://yoursite.com/2017/05/07/scrapy-item-mysql/"/>
    <id>http://yoursite.com/2017/05/07/scrapy-item-mysql/</id>
    <published>2017-05-07T06:18:54.000Z</published>
    <updated>2017-05-07T11:36:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>将数据保存到MySQL数据库，需要用到 <code>mysqlclient</code> 模块，需要在我们的虚拟环境中用 <code>pip</code> 进行安装。</p>
<a id="more"></a>
<h2 id="设计数据表"><a href="#设计数据表" class="headerlink" title="设计数据表"></a><strong>设计数据表</strong></h2><p>需要根据之前Item来设计我们的数据表 <code>jobbole_article</code> ，数据库取名为 <code>article_spider</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    title = scrapy.Field()</div><div class="line">    create_date = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    front_image_url = scrapy.Field()</div><div class="line">    front_image_path = scrapy.Field()</div><div class="line">    praise_nums = scrapy.Field()</div><div class="line">    comment_nums = scrapy.Field()</div><div class="line">    fav_nums = scrapy.Field()</div><div class="line">    tags = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure>
<p>初步设计的数据表如下，在后面使用时还会进行必要的改动：</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1ffcueufjwhj317e0kwwib.jpg" alt=""></p>
<h2 id="采用同步机制写入MySQL"><a href="#采用同步机制写入MySQL" class="headerlink" title="采用同步机制写入MySQL"></a><strong>采用同步机制写入MySQL</strong></h2><p>首先在 <code>pipelines.py</code> 中引入数据库连接模块 <code>import MySQLdb</code> ，然后完善 <code>MysqlPipeline</code> 类的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 采用同步的机制写入mysql</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.conn = MySQLdb.connect(<span class="string">'127.0.0.1'</span>, <span class="string">'root'</span>, <span class="string">'12'</span>, <span class="string">'article_spider'</span>,</div><div class="line">                                    charset=<span class="string">'utf8'</span>, use_unicode=<span class="keyword">True</span>)</div><div class="line">        self.cursor = self.conn.cursor()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                    insert into jobbole_article(title, url, create_date, fav_nums)</div><div class="line">                    VALUES (%s, %s, %s, %s)</div><div class="line">        """</div><div class="line">        self.cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</div><div class="line">        self.conn.commit()</div></pre></td></tr></table></figure>
<p><code>__init__</code> 方法是对数据进行初始化，定义连接信息如host，数据库用户名、密码、数据库名称、数据库编码<br>在 <code>process_item</code> 方法中进行插入数据操作，格式都是固定的。</p>
<p>最后在 <code>settings.py</code> 中把 <code>MysqlPipeline()</code> 加入到 <code>ITEM_PIPELINES</code> 的配置中。</p>
<h2 id="采用异步机制写入MySQL"><a href="#采用异步机制写入MySQL" class="headerlink" title="采用异步机制写入MySQL"></a><strong>采用异步机制写入MySQL</strong></h2><p>在上面的同步机制写入数据库中，我们把连接信息 <code>MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;12&#39;, &#39;article_spider&#39;, charset=&#39;utf8&#39;, use_unicode=True)</code> 直接定义在函数中，如果不经常改动的话，可以把相关信息放到 <code>settings.py</code> 中进行调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MYSQL_HOST = <span class="string">'127.0.0.1'</span></div><div class="line">MYSQL_DBNAME = <span class="string">'article_spider'</span></div><div class="line">MYSQL_USER = <span class="string">'root'</span></div><div class="line">MYSQL_PASSWORD = <span class="string">'12'</span></div></pre></td></tr></table></figure>
<p>在 <code>pipelines.py</code> 中新建 <code>MysqlTwistedPipeline</code> ，写入如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        host = settings[<span class="string">'MYSQL_HOST'</span>]</div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>在 <code>from_settings</code> 这个类方法中，我们获取到了settings配置中的 <code>MYSQL_HOST</code> ，这个方法在Scrapy初始化的时候就会被调用，会将Scrapy的settings对象传递进来，我们在这里进行断点调试，查看是否获取到了这个对象：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffcv0p0d3xj31kw0ejqcr.jpg" alt=""></p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1ffcv1i1po9j319w04kgmy.jpg" alt=""></p>
<p>发现在settings的attributes这个字典中，确实有我们定义的各种属性。</p>
<p>我们的异步操作需要引入twisted，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</div><div class="line"><span class="keyword">import</span> MySQLdb</div><div class="line"><span class="keyword">import</span> MySQLdb.cursors</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></div><div class="line">        self.dbpool = dbpool</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        dbparams = dict(</div><div class="line">            host=settings[<span class="string">'MYSQL_HOST'</span>],</div><div class="line">            db=settings[<span class="string">'MYSQL_DBNAME'</span>],</div><div class="line">            user=settings[<span class="string">'MYSQL_USER'</span>],</div><div class="line">            passwd=settings[<span class="string">'MYSQL_PASSWORD'</span>],</div><div class="line">            charset=<span class="string">'utf8'</span>,</div><div class="line">            cursorclass=MySQLdb.cursors.DictCursor,</div><div class="line">            use_unicode=<span class="keyword">True</span>,</div><div class="line">        )</div><div class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparams)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> cls(dbpool)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="comment"># 使用twisted将mysql插入变成异步执行</span></div><div class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</div><div class="line">        query.addErrback(self.handle_error)  <span class="comment"># 处理异常</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure)</span>:</span></div><div class="line">        <span class="comment"># 处理异步插入异常</span></div><div class="line">        print(failure)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></div><div class="line">        <span class="comment"># 执行具体的插入</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                            insert into jobbole_article(title, url, create_date, fav_nums)</div><div class="line">                            VALUES (%s, %s, %s, %s)</div><div class="line">                """</div><div class="line">        cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</div></pre></td></tr></table></figure>
<p>我们在使用时，绝大部分代码无须变动，只要修改 <code>do_insert</code> 方法中的插入内容，以及自己的信息即可。</p>
<p>在数据量不大时，用同步插入即可。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将数据保存到MySQL数据库，需要用到 &lt;code&gt;mysqlclient&lt;/code&gt; 模块，需要在我们的虚拟环境中用 &lt;code&gt;pip&lt;/code&gt; 进行安装。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Pipeline</title>
    <link href="http://yoursite.com/2017/05/06/scrapy-item-pipeline/"/>
    <id>http://yoursite.com/2017/05/06/scrapy-item-pipeline/</id>
    <published>2017-05-06T06:18:54.000Z</published>
    <updated>2017-05-06T13:35:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。</p>
<p>每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。</p>
<a id="more"></a>
<p>以下是 item pipeline 的一些典型应用：</p>
<ul>
<li>清理 HTML 数据</li>
<li>验证爬取的数据（检查 item 包含某些字段）</li>
<li>查重（并丢弃）</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<h2 id="编写自定义的-Pipeline"><a href="#编写自定义的-Pipeline" class="headerlink" title="编写自定义的 Pipeline"></a><strong>编写自定义的 Pipeline</strong></h2><p>定义一个Python类，然后实现方法 <code>process_item(self, item, spider)</code> 即可，返回一个字典或Item，或者抛出 <code>DropItem</code> 异常丢弃这个Item。</p>
<p>除此之外，还可以实现以下几个方法：</p>
<ul>
<li><code>open_spider(self, spider)</code> ：当spider被开启时，这个方法被调用</li>
<li><code>close_spider(self, spider)</code> ：当spider被关闭时，这个方法被调用</li>
<li><code>from_crawler(cls, crawler)</code> ： 可访问核心组件比如配置和信号，并注册钩子函数到Scrapy中</li>
</ul>
<h2 id="Item-Pipeline示例"><a href="#Item-Pipeline示例" class="headerlink" title="Item Pipeline示例"></a><strong>Item Pipeline示例</strong></h2><h3 id="价格验证"><a href="#价格验证" class="headerlink" title="价格验证"></a><strong>价格验证</strong></h3><p>让我们来看一下以下这个假设的 pipeline，它为那些不含税（<code>price_excludes_vat</code> 属性）的item调整了price属性，同时丢弃了那些没有价格item：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">	vat_factor = <span class="number">1.15</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span> </div><div class="line">        <span class="keyword">if</span> item[<span class="string">'price'</span>]: </div><div class="line">            <span class="keyword">if</span> item[<span class="string">'price_excludes_vat'</span>]:</div><div class="line">				item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor </div><div class="line">            <span class="keyword">return</span> item </div><div class="line">        <span class="keyword">else</span>: </div><div class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</div></pre></td></tr></table></figure>
<h3 id="将item写入Json文件"><a href="#将item写入Json文件" class="headerlink" title="将item写入Json文件"></a><strong>将item写入Json文件</strong></h3><p>下面的这个Pipeline将所有的item写入到一个单独的json文件，，每行包含一个序列化 为 JSON 格式的 item:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'wb'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></div><div class="line">        self.file.write(line)</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>JsonWriterPipeline 的目的只是为了介绍怎样编写 item pipeline，如果你想要将所有爬取的 item 都保存到同 一个 JSON 文件， 你需要使用 Feed exports 。</p>
<h3 id="将item存储到MongoDB中"><a href="#将item存储到MongoDB中" class="headerlink" title="将item存储到MongoDB中"></a>将item存储到MongoDB中</h3><p>这个例子使用<a href="http://api.mongodb.org/python/current/" target="_blank" rel="external">pymongo</a>来演示怎样讲item保存到MongoDB中。 MongoDB的地址和数据库名在配置 <code>settings.py</code> 中指定，这个例子主要是向你展示怎样使用<code>from_crawler()</code>方法，以及如何清理资源。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    collection_name = <span class="string">'scrapy_items'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></div><div class="line">        self.mongo_uri = mongo_uri</div><div class="line">        self.mongo_db = mongo_db</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></div><div class="line">        <span class="keyword">return</span> cls(</div><div class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</div><div class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>, <span class="string">'items'</span>)</div><div class="line">        )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</div><div class="line">        self.db = self.client[self.mongo_db]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        self.db[self.collection_name].insert(dict(item))</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a><strong>去重</strong></h3><p>一个用于去重的过滤器，丢弃那些已经被处理过的 item。让我们假设我们的 item 有一个唯一的 id，但是我们 sp ider 返回的多个 item 中包含有相同的 id:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span> </div><div class="line">        self.ids_seen = set()</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span> </div><div class="line">        <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">in</span> self.ids_seen:</div><div class="line">			<span class="keyword">raise</span> DropItem(<span class="string">"Duplicate item found: %s"</span> % item) </div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.ids_seen.add(item[<span class="string">'id'</span>]) </div><div class="line">            <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h2 id="启用一个-Item-Pipeline-组件"><a href="#启用一个-Item-Pipeline-组件" class="headerlink" title="启用一个 Item Pipeline 组件"></a>启用一个 Item Pipeline 组件</h2><p>为了启用一个 Item Pipeline 组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123; </div><div class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>, </div><div class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>, </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分配给每个类的整型值，确定了他们运行的顺序，item 按数字从低到高的顺序，通过 pipeline，通常将这些数字 定义在 0-1000 范围内。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。&lt;/p&gt;
&lt;p&gt;每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Feed Exports</title>
    <link href="http://yoursite.com/2017/05/06/scrapy-feed-exports/"/>
    <id>http://yoursite.com/2017/05/06/scrapy-feed-exports/</id>
    <published>2017-05-06T06:18:54.000Z</published>
    <updated>2017-05-06T14:46:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。</p>
<p>Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。</p>
<a id="more"></a>
<h2 id="序列化方式（serialization-format）"><a href="#序列化方式（serialization-format）" class="headerlink" title="序列化方式（serialization format）"></a><strong>序列化方式（serialization format）</strong></h2><p>feed 输出使用到了 Item exporters 。其自带支持的类型有:</p>
<ul>
<li>JSON</li>
<li>JSON lines</li>
<li>CSV</li>
<li>XML</li>
</ul>
<p>也可以通过 FEED_EXPORTERS 设置扩展支持的属性。</p>
<p>在 <code>exporters.py</code> 中可以看到所有的 Item exporters：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffbz5887gzj30x603qdge.jpg" alt=""></p>
<p>下表对主要的 Item exporters进行简要的介绍：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>FEED_FORMAT</th>
<th>使用的 exporter</th>
</tr>
</thead>
<tbody>
<tr>
<td>JSON</td>
<td>json</td>
<td>JsonItemExporter</td>
</tr>
<tr>
<td>JSON lines</td>
<td>jsonlines</td>
<td>JsonLinesItemExporter</td>
</tr>
<tr>
<td>CSV</td>
<td>csv</td>
<td>CsvItemExporter</td>
</tr>
<tr>
<td>XML</td>
<td>xml</td>
<td>XmlItemExporter</td>
</tr>
<tr>
<td>Pickle</td>
<td>pickle</td>
<td>PickleItemExporter</td>
</tr>
<tr>
<td>Marshal</td>
<td>marshal</td>
<td>MarshalItemExporter</td>
</tr>
</tbody>
</table>
<h2 id="存储（Storages）"><a href="#存储（Storages）" class="headerlink" title="存储（Storages）"></a><strong>存储（Storages）</strong></h2><p>使用 feed 输出时您可以通过使用 <a href="http://en.wikipedia.org/wiki/Uniform_Resource_Identifier" target="_blank" rel="external">URI</a>（通过 FEED_URI 设置）来定义存储端。feed 输出支持 URI 方式支持的多种存储后端类型。</p>
<p>自带支持的存储后端有：</p>
<ul>
<li>本地文件系统</li>
<li>FTP</li>
<li>S3（需要 boto）</li>
<li>标准输出</li>
</ul>
<p>有些存储后端会因所需的外部库未安装而不可用。例如，S3 只有在 boto 库安装的情况下才可使用。</p>
<h2 id="存储-URI-参数"><a href="#存储-URI-参数" class="headerlink" title="存储 URI 参数"></a><strong>存储 URI 参数</strong></h2><p>存储 URI 也包含参数。当 feed 被创建时这些参数可以被覆盖：</p>
<ul>
<li><code>%(time)s</code> - 当 feed 被创建时被 timestamp 覆盖</li>
<li><code>%(name)s</code> - 被 spider 的名字覆盖</li>
</ul>
<p>其他命名的参数会被 spider 同名的属性所覆盖。例如， 当 feed 被创建时，<code>%(site_id)s</code> 将会被 <code>spider.site_id</code> 属性所覆盖。</p>
<p>下面用一些例子来说明:</p>
<ul>
<li>存储在 FTP，每个 spider 一个目录: <ul>
<li><code>ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</code></li>
</ul>
</li>
<li>存储在 S3，每一个 spider 一个目录:<ul>
<li><code>s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</code></li>
</ul>
</li>
</ul>
<h2 id="存储后端（Storage-backends）"><a href="#存储后端（Storage-backends）" class="headerlink" title="存储后端（Storage backends）"></a><strong>存储后端（Storage backends）</strong></h2><h3 id="本地文件系统"><a href="#本地文件系统" class="headerlink" title="本地文件系统"></a><strong>本地文件系统</strong></h3><p>将 feed 存储在本地系统。</p>
<ul>
<li>URI scheme: <code>file</code></li>
<li>URI 样例: <code>file:///tmp/export.csv</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<p>注意: (只有)存储在本地文件系统时，您可以指定一个绝对路径 /tmp/export.csv 并忽略协议(scheme)。不过这 仅仅只能在 Unix 系统中工作。</p>
<h3 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a><strong>FTP</strong></h3><p>将 feed 存储在 FTP 服务器。</p>
<ul>
<li>URI scheme: <code>ftp</code></li>
<li>URI 样例: <code>ftp://user:pass@ftp.example.com/path/to/export.csv</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<h3 id="S3"><a href="#S3" class="headerlink" title="S3"></a><strong>S3</strong></h3><p>将 feed 存储在 Amazon S3 。 </p>
<ul>
<li>URI scheme: s3 </li>
<li>URI 样例: <ul>
<li>s3://mybucket/path/to/export.csv </li>
<li>s3://aws_key:aws_secret@mybucket/path/to/export.csv </li>
</ul>
</li>
<li>需要的外部依赖库: <code>boto</code></li>
</ul>
<p>您可以通过在 URI 中传递 user/pass 来完成 AWS 认证，或者也可以通过下列的设置来完成: </p>
<p>AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY</p>
<h3 id="标准输出"><a href="#标准输出" class="headerlink" title="标准输出"></a><strong>标准输出</strong></h3><p>feed 输出到 Scrapy 进程的标准输出。</p>
<ul>
<li>URI scheme: <code>stdout</code></li>
<li>URI 样例: <code>stdout</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<h2 id="设定（Settings）"><a href="#设定（Settings）" class="headerlink" title="设定（Settings）"></a><strong>设定（Settings）</strong></h2><p>这些是配置 feed 输出的设定:</p>
<ul>
<li>FEED_URI (必须)</li>
<li>FEED_FORMAT</li>
<li>FEED_STORAGES</li>
<li>FEED_EXPORTERS</li>
<li>FEED_STORE_EMPTY</li>
</ul>
<h3 id="FEED-URI"><a href="#FEED-URI" class="headerlink" title="FEED_URI"></a><strong>FEED_URI</strong></h3><p>Default: <code>None</code> </p>
<p>输出 feed 的 URI。支持的 URI 协议请参见存储后端。</p>
<p> 为了启用 feed 输出，该设定是必须的。</p>
<h3 id="FEED-FORMAT"><a href="#FEED-FORMAT" class="headerlink" title="FEED_FORMAT"></a><strong>FEED_FORMAT</strong></h3><p>输出 feed 的序列化格式。可用的值请参见序列化方式（Serialization formats）。</p>
<h3 id="FEED-STORE-EMPTY"><a href="#FEED-STORE-EMPTY" class="headerlink" title="FEED_STORE_EMPTY"></a><strong>FEED_STORE_EMPTY</strong></h3><p>Default: <code>False</code> </p>
<p>是否输出空 feed（没有 item 的 feed）。</p>
<h3 id="FEED-STORAGES"><a href="#FEED-STORAGES" class="headerlink" title="FEED_STORAGES"></a><strong>FEED_STORAGES</strong></h3><p>Default: <code>{}</code> </p>
<p>包含项目支持的额外 feed 存储端的字典。 字典的键（key）是 URI 协议（scheme），值是存储类（storage class）的路径。</p>
<h3 id="FEED-STORAGES-BASE"><a href="#FEED-STORAGES-BASE" class="headerlink" title="FEED_STORAGES_BASE"></a><strong>FEED_STORAGES_BASE</strong></h3><p>Default:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line"><span class="string">''</span>: <span class="string">'scrapy.contrib.feedexport.FileFeedStorage'</span>, </div><div class="line"><span class="string">'file'</span>: <span class="string">'scrapy.contrib.feedexport.FileFeedStorage'</span>, </div><div class="line"><span class="string">'stdout'</span>: <span class="string">'scrapy.contrib.feedexport.StdoutFeedStorage'</span>, </div><div class="line"><span class="string">'s3'</span>: <span class="string">'scrapy.contrib.feedexport.S3FeedStorage'</span>, </div><div class="line"><span class="string">'ftp'</span>: <span class="string">'scrapy.contrib.feedexport.FTPFeedStorage'</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>包含 Scrapy 内置支持的 feed 存储端的字典。</p>
<h3 id="FEED-EXPORTERS"><a href="#FEED-EXPORTERS" class="headerlink" title="FEED_EXPORTERS"></a><strong>FEED_EXPORTERS</strong></h3><p>Default: <code>{}</code> </p>
<p>包含项目支持的额外输出器（exporter）的字典。 该字典的键（key）是 URI 协议（scheme），值是 Item 输出器（exp orter）类的路径。</p>
<h3 id="FEED-EXPORTERS-BASE"><a href="#FEED-EXPORTERS-BASE" class="headerlink" title="FEED_EXPORTERS_BASE"></a><strong>FEED_EXPORTERS_BASE</strong></h3><p>Default:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">FEED_EXPORTERS_BASE = &#123; </div><div class="line">    <span class="string">'json'</span>: <span class="string">'scrapy.contrib.exporter.JsonItemExporter'</span>, </div><div class="line">    <span class="string">'jsonlines'</span>: <span class="string">'scrapy.contrib.exporter.JsonLinesItemExporter'</span>, </div><div class="line">    <span class="string">'csv'</span>: <span class="string">'scrapy.contrib.exporter.CsvItemExporter'</span>, </div><div class="line">    <span class="string">'xml'</span>: <span class="string">'scrapy.contrib.exporter.XmlItemExporter'</span>, </div><div class="line">    <span class="string">'marshal'</span>: <span class="string">'scrapy.contrib.exporter.MarshalItemExporter'</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>包含 Scrapy 内置支持的 feed 输出器（exporter）的字典。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。&lt;/p&gt;
&lt;p&gt;Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——items设计</title>
    <link href="http://yoursite.com/2017/05/05/scrapy-items-design/"/>
    <id>http://yoursite.com/2017/05/05/scrapy-items-design/</id>
    <published>2017-05-05T06:18:54.000Z</published>
    <updated>2017-05-06T10:36:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 <code>scrapy.Field()</code> 。</p>
<a id="more"></a>
<h2 id="定义-items"><a href="#定义-items" class="headerlink" title="定义 items"></a><strong>定义 items</strong></h2><p>由于需要添加一个封面图，对上面的爬虫添加一个 <code>front_image_url</code> 字段对 <code>parse</code> 函数进行修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析</div><div class="line">    2. 获取下一页的url并交给scrapy进行下载</div><div class="line">    :param response: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># 解析列表页中的所有文章url并交给解析函数进行具体字段的解析</span></div><div class="line">    post_nodes = response.css(<span class="string">"#archive .floated-thumb .post-thumb a"</span>)</div><div class="line">    <span class="keyword">for</span> post_node <span class="keyword">in</span> post_nodes:</div><div class="line">        image_url = post_node.css(<span class="string">"img::attr(src)"</span>).extract_first(<span class="string">""</span>)</div><div class="line">        post_url = post_node.css(<span class="string">"::attr(href)"</span>).extract_first(<span class="string">""</span>)</div><div class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), meta=&#123;<span class="string">"front_image_url"</span>: image_url&#125;, callback=self.parse_detail)</div></pre></td></tr></table></figure>
<p>其中的 <code>meta</code> 字段是传递值的方法。在调试时返回的 <code>response</code> 中会出现 <code>meta</code> 的内容，它是一个字典，故在传递时可以直接通过 <code>response.meta[&#39;front_image_url&#39;]</code> 进行引用（也可以使用get的方法，附默认值防止出现异常）：</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1ffaslwq4mqj30l1058t9b.jpg" alt=""></p>
<p>在 <code>items.py</code> 文件中，定义一个item并声明其字段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    title = scrapy.Field()</div><div class="line">    create_date = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    front_image_url = scrapy.Field()</div><div class="line">    front_image_path = scrapy.Field()</div><div class="line">    praise_nums = scrapy.Field()</div><div class="line">    comment_nums = scrapy.Field()</div><div class="line">    fav_nums = scrapy.Field()</div><div class="line">    tags = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure>
<p>在 <code>jobbole.py</code> 中添加 <code>from ArticleSpider.items import JobBoleArticleItem</code> 对item进行引用，然后在 <code>parse_detail</code> 中进行初始化 <code>article_item = JobBoleArticleItem()</code> ，之后将获取到的字段内容存入初始化的item中，最终代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></div><div class="line">    article_item = JobBoleArticleItem()</div><div class="line"></div><div class="line">    <span class="comment"># 通过css选择器提取字段</span></div><div class="line">    front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></div><div class="line">    title = response.css(<span class="string">".entry-header h1::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</div><div class="line">    praise_nums = int(response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>])</div><div class="line">    fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        fav_nums = <span class="number">0</span></div><div class="line">    comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        comment_nums = <span class="number">0</span></div><div class="line">    content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</div><div class="line">    tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</div><div class="line">    tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">    tags = <span class="string">","</span>.join(tag_list)</div><div class="line"></div><div class="line">    article_item[<span class="string">"title"</span>] = title</div><div class="line">    article_item[<span class="string">"url"</span>] = response.url</div><div class="line">    article_item[<span class="string">"create_date"</span>] = create_date</div><div class="line">    article_item[<span class="string">"front_image_url"</span>] = front_image_url</div><div class="line">    article_item[<span class="string">"praise_nums"</span>] = praise_nums</div><div class="line">    article_item[<span class="string">"comment_nums"</span>] = comment_nums</div><div class="line">    article_item[<span class="string">"fav_nums"</span>] = fav_nums</div><div class="line">    article_item[<span class="string">"content"</span>] = content</div><div class="line">    article_item[<span class="string">"tags"</span>] = tags</div><div class="line">    </div><div class="line">    <span class="keyword">yield</span> article_item</div></pre></td></tr></table></figure>
<p>其中 <code>yield article_item</code> 会自动提交到 <code>settings</code> 中的 <code>ITEM_PIPELINES</code> 进行处理。<br>此时在 <code>pipelines.py</code> 中设置断点调试，可以看到 <code>article_item</code> 中的值已经传递到这里了。</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffasxu8jauj31ge0my46y.jpg" alt=""></p>
<h2 id="自定义-Pipelines"><a href="#自定义-Pipelines" class="headerlink" title="自定义 Pipelines"></a><strong>自定义 Pipelines</strong></h2><p>在 <code>settings.py</code> 中有一个ITEM_PIPELINES的选项，把它的注释去掉增加下载图片的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Configure item pipelines</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    <span class="string">'ArticleSpider.pipelines.ArticlespiderPipeline'</span>: <span class="number">300</span>,</div><div class="line">    <span class="string">'scrapy.pipelines.images.ImagesPipeline'</span>: <span class="number">1</span>,</div><div class="line">&#125;</div><div class="line">IMAGES_URLS_FIELD = <span class="string">"front_image_url"</span></div><div class="line">project_dir = os.path.abspath(os.path.dirname(__file__))</div><div class="line">IMAGES_STORE = os.path.join(project_dir, <span class="string">'images'</span>)</div></pre></td></tr></table></figure>
<p>上面的代码启用了下载图片piplines，并定义了存储地址及想要存储的图片地址。<br>在settings.py同级目录下建立文件夹 <code>images</code> 用来保存图片。当运行爬虫时，图片就会自动下载图片并保存到本地。<br>如果想要得到存储的图片路径的话，需要自定义pipelines。</p>
<p>首先，在 <code>pipeines.py</code> 中引入 <code>from scrapy.pipelines.images import ImagesPipeline</code> ， 然后自定义一个pipeline对ImagesPipeline进行重载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>进行断点调试，查看results中的信息：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffblwnn93tj31aq0t2wmq.jpg" alt=""></p>
<p>调试结果中，results是一个list，第一个值是一个bool值表示图片是否获取成功，第二个值是一个字典，保存了图片路径，图片地址等信息。</p>
<p>最终自定义的pipeline代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></div><div class="line">        <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</div><div class="line">            image_file_path = value[<span class="string">"path"</span>]</div><div class="line">        item[<span class="string">"front_image_path"</span>] = image_file_path</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>上面代码得到image_path保存到 <code>item[&quot;front_image_path&quot;]</code> 中并返回，这时会根据pipelines的顺序进行下一个pipelines进行处理。通过断点调试可以得到想要的结果。</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffbpccuulfj31kw0w7dqk.jpg" alt=""></p>
<h2 id="完善-items-获取"><a href="#完善-items-获取" class="headerlink" title="完善 items 获取"></a><strong>完善 items 获取</strong></h2><p>对之前定义的items中的 <code>url_object_id</code> 字段，需要对url进行md5处理，因此在 <code>items.py</code> 同级目录下新建一个名为 <code>utils</code> 的 <code>python package</code>，新建 <code>common.py</code> ，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> hashlib</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></div><div class="line">    <span class="keyword">if</span> isinstance(url, str):</div><div class="line">        url = url.encode(<span class="string">"utf-8"</span>)</div><div class="line">    m = hashlib.md5()</div><div class="line">    m.update(url)</div><div class="line">    <span class="keyword">return</span> m.hexdigest()</div></pre></td></tr></table></figure>
<p>然后在 <code>jobbole.py</code> 下引入 <code>from ArticleSpider.utils.common import get_md5</code> ，在item内容填充时加上 <code>article_item[&quot;url_object_id&quot;] = get_md5(response.url)</code> 即可。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 &lt;code&gt;scrapy.Field()&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Selectors</title>
    <link href="http://yoursite.com/2017/05/05/scrapy-selectors/"/>
    <id>http://yoursite.com/2017/05/05/scrapy-selectors/</id>
    <published>2017-05-05T06:18:54.000Z</published>
    <updated>2017-05-05T12:51:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： </p>
<ul>
<li><a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">BeautifulSoup</a> 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 </li>
<li><a href="http://lxml.de/index.html" target="_blank" rel="external">lxml</a> 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。</li>
</ul>
<p>Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。</p>
<a id="more"></a>
<p>XPath 是一门用来在 XML 文件中选择节点的语言，也可以用在 HTML 上。</p>
<p>CSS 是一门将 HTML 文档样式化的语言。选择器由它定义，并与特定的 HTML 元素的样式相关连。 </p>
<p>Scrapy 选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。 </p>
<p>本文解释了选择器如何工作，并描述了相应的 API。不同于 lxml API 的臃肿，该 API 短小而简洁。这是因为 lxml 库除了用来选择标记化文档外，还可以用到许多任务上。</p>
<h2 id="使用选择器"><a href="#使用选择器" class="headerlink" title="使用选择器"></a><strong>使用选择器</strong></h2><h3 id="构造选择器"><a href="#构造选择器" class="headerlink" title="构造选择器"></a><strong>构造选择器</strong></h3><p>Scrapy selectors是 <code>Selector</code> 类的实例，通过传入 text 或 <code>TextResponse</code> 来创建，它自动根据传入的类型选择解析规则（XML or HTML）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from scrapy.selector import Selector </div><div class="line">&gt;&gt;&gt; from scrapy.http import HtmlResponse</div></pre></td></tr></table></figure>
<p>以文字构造（都以 xpath 和 css 两种方法解析字段内容，加深理解）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; body = &apos;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&apos;</div><div class="line">&gt;&gt;&gt; Selector(text=body).xpath(&quot;//span/text()&quot;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; Selector(text=body).css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<p>以 response 构造：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response = HtmlResponse(url=&apos;http://example.com&apos;, body=body, encoding=&apos;utf-8&apos;)</div><div class="line">&gt;&gt;&gt; Selector(response=response).xpath(&apos;//span/text()&apos;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; Selector(response=response).css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<p>response 对象以 <em>.selector</em> 属性提供了一个 selector ， 可以随时使用该快捷方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.selector.xpath(&apos;//span/text()&apos;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; response.selector.css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<h3 id="使用选择器-1"><a href="#使用选择器-1" class="headerlink" title="使用选择器"></a><strong>使用选择器</strong></h3><p>我们将使用 Scrapy shell （提供交互测试）和位于 Scrapy 文档服务器的一个样例页面，来解释如何使用选择器：</p>
<p><code>http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</code></p>
<p>该页面源码如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></div><div class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">base</span> <span class="attr">href</span>=<span class="string">'http://example.com/'</span> /&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>Example website<span class="tag">&lt;/<span class="name">title</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">'images'</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image1.html'</span>&gt;</span>Name: My image 1 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image1_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image2.html'</span>&gt;</span>Name: My image 2 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image2_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image3.html'</span>&gt;</span>Name: My image 3 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image3_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image4.html'</span>&gt;</span>Name: My image 4 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image4_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image5.html'</span>&gt;</span>Name: My image 5 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image5_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<p>首先，打开 scrapy shell，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</div></pre></td></tr></table></figure>
<p>当 shell 载入后，您将获得名为 <code>response</code> 的 shell 变量，其为响应的 response，并且在其 <code>response.selector</code> 属性上绑定了一个 selector。</p>
<p>因为我们处理的是 HTML，选择器将自动使用 HTML 语法分析。 那么，通过查看该页面的源码，我们构建一个 XPath 来选择 title 标签内的文字:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.selector.xpath(&quot;//title/text()&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</div></pre></td></tr></table></figure>
<p>由于在 response 中使用 XPath、CSS 查询十分普遍，因此，Scrapy 提供了两个实用的快捷方式：response.xpath() 及 response.css() ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(&quot;title::text&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;descendant-or-self::title/text()&apos; data=&apos;Example website&apos;&gt;]</div></pre></td></tr></table></figure>
<p>现在我们将得到根 URL（base URL）和一些图片链接:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//base/@href'</span>).extract() </div><div class="line">[<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'base::attr(href)'</span>).extract() </div><div class="line">[<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//a[contains(@href, "image")]/@href'</span>).extract() </div><div class="line">[<span class="string">'image1.html'</span>, <span class="string">'image2.html'</span>, <span class="string">'image3.html'</span>, <span class="string">'image4.html'</span>, <span class="string">'image5.html'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'a[href*=image]::attr(href)'</span>).extract() </div><div class="line">[<span class="string">'image1.html'</span>, <span class="string">'image2.html'</span>, <span class="string">'image3.html'</span>, <span class="string">'image4.html'</span>, <span class="string">'image5.html'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//a[contains(@href, "image")]/img/@src'</span>).extract() </div><div class="line">[<span class="string">'image1_thumb.jpg'</span>, <span class="string">'image2_thumb.jpg'</span>, <span class="string">'image3_thumb.jpg'</span>, <span class="string">'image4_thumb.jpg'</span>, <span class="string">'image5_thumb.jpg'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'a[href*=image] img::attr(src)'</span>).extract() </div><div class="line">[<span class="string">'image1_thumb.jpg'</span>, <span class="string">'image2_thumb.jpg'</span>, <span class="string">'image3_thumb.jpg'</span>, <span class="string">'image4_thumb.jpg'</span>, <span class="string">'image5_thumb.jpg'</span>]</div></pre></td></tr></table></figure>
<h3 id="嵌套选择器"><a href="#嵌套选择器" class="headerlink" title="嵌套选择器"></a><strong>嵌套选择器</strong></h3><p>选择器方法（ .xpath() or .css() ）返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; links = response.xpath(&quot;//a[contains(@href,&apos;image&apos;)]&quot;)</div><div class="line"></div><div class="line">&gt;&gt;&gt; links.extract()</div><div class="line">[&apos;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;]</div><div class="line"></div><div class="line">&gt;&gt;&gt; for index, link in enumerate(links):</div><div class="line">...:     args = (index, link.xpath(&apos;@href&apos;).extract(), link.xpath(&apos;img/@src&apos;).extract())</div><div class="line">...:     print(&apos;Link number %d points to url %s and image %s&apos; % args)</div><div class="line">...:</div><div class="line">Link number 0 points to url [&apos;image1.html&apos;] and image [&apos;image1_thumb.jpg&apos;]</div><div class="line">Link number 1 points to url [&apos;image2.html&apos;] and image [&apos;image2_thumb.jpg&apos;]</div><div class="line">Link number 2 points to url [&apos;image3.html&apos;] and image [&apos;image3_thumb.jpg&apos;]</div><div class="line">Link number 3 points to url [&apos;image4.html&apos;] and image [&apos;image4_thumb.jpg&apos;]</div><div class="line">Link number 4 points to url [&apos;image5.html&apos;] and image [&apos;image5_thumb.jpg&apos;]</div></pre></td></tr></table></figure>
<h3 id="结合正则表达式使用选择器"><a href="#结合正则表达式使用选择器" class="headerlink" title="结合正则表达式使用选择器"></a><strong>结合正则表达式使用选择器</strong></h3><p>Selector 也有一个 .re() 方法，用来通过正则表达式来提取数据。然而，不同于使用 .xpath() 或者 .css() 方法，.re()方法返回 unicode 字符串的列表。所以你无法构造嵌套式的 .re() 调用。 </p>
<p>下面是一个例子，从上面的 html 源码中提取图像名字：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//a[contains(@href, &apos;image&apos;)]/text()&quot;).re(r&apos;Name:\s*(.*)&apos;)</div><div class="line">[&apos;My image 1 &apos;, &apos;My image 2 &apos;, &apos;My image 3 &apos;, &apos;My image 4 &apos;, &apos;My image 5 &apos;]</div></pre></td></tr></table></figure>
<h3 id="使用相对-XPaths"><a href="#使用相对-XPaths" class="headerlink" title="使用相对 XPaths"></a><strong>使用相对 XPaths</strong></h3><p>记住如果你使用嵌套的选择器，并使用起始为 <code>/</code> 的 XPath，那么该 XPath 将对文档使用绝对路径，而且对于你调用的 <code>Selector</code> 不是相对路径。</p>
<p>比如，假设你想提取在 <code>&lt;div&gt;</code> 元素中的所有 <code>&lt;p&gt;</code> 元素。首先，你将先得到所有的 <code>&lt;div&gt;</code> 元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; divs = response.xpath(&quot;//div&quot;)</div></pre></td></tr></table></figure>
<p>开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 <code>&lt;div&gt;</code> 元素内部提取所有的 <code>&lt;p&gt;</code> 元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;//p&apos;): # this is wrong - gets all &lt;p&gt; from the whole document </div><div class="line">...		print p.extract()</div></pre></td></tr></table></figure>
<p>下面是比较合适的处理方法(注意 .//p XPath 的点前缀)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;.//p&apos;): # extracts all &lt;p&gt; inside </div><div class="line">... 	print p.extract()</div></pre></td></tr></table></figure>
<p>另一种常见的情况将是提取所有直系 <code>&lt;p&gt;</code> 的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;p&apos;): </div><div class="line">... 	print p.extract()</div></pre></td></tr></table></figure>
<h3 id="使用-EXSLT-扩展"><a href="#使用-EXSLT-扩展" class="headerlink" title="使用 EXSLT 扩展"></a><strong>使用 EXSLT 扩展</strong></h3><p>因建于 lxml 之上，Scrapy 选择器也支持一些 EXSLT 扩展，可以在 XPath 表达式中使用这些预先制定的命名空间：</p>
<table>
<thead>
<tr>
<th>前缀</th>
<th>命名空间</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>re</td>
<td><a href="http://exslt.org/regular-expressions" target="_blank" rel="external">http://exslt.org/regular-expressions</a></td>
<td>正则表达式</td>
</tr>
<tr>
<td>set</td>
<td><a href="http://exslt.org/sets" target="_blank" rel="external">http://exslt.org/sets</a></td>
<td>集合操作</td>
</tr>
</tbody>
</table>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a><strong>正则表达式</strong></h3><p>例如在XPath的 <code>starts-with()</code> 或 <code>contains()</code> 无法满足需求时， <code>test()</code> 函数可以非常有用。</p>
<p>例如在列表中选择有”class”元素且结尾为一个数字的链接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from scrapy import Selector</div><div class="line">&gt;&gt;&gt; doc = &quot;&quot;&quot;</div><div class="line">... &lt;div&gt;</div><div class="line">...      &lt;ul&gt;</div><div class="line">...          &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...       &lt;/ul&gt;</div><div class="line">... &lt;/div&gt;</div><div class="line">...  &quot;&quot;&quot;</div><div class="line">&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)</div><div class="line">&gt;&gt;&gt; sel.xpath(&quot;//li//@href&quot;).extract()</div><div class="line">[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link3.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]</div><div class="line">&gt;&gt;&gt; sel.xpath(&quot;//li[re:test(@class, &apos;item-\d$&apos;)]//@href&quot;).extract()</div><div class="line">[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]</div></pre></td></tr></table></figure>
<p>注意：C语言库 <code>libxslt</code> 不原生支持EXSLT正则表达式，因此 lxml 在实现时使用了Python  <code>re</code>  模块的钩子。 因此，在 XPath 表达式中使用 regexp 函数可能会牺牲少量的性能。</p>
<h3 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a><strong>集合操作</strong></h3><p>集合操作可以方便地用于在提取文字元素前从文档树中去除一些部分。 </p>
<p>例如使用 itemscopes 组和对应的 itemprops 来提取微数据（来自 <a href="http://schema.org/Product" target="_blank" rel="external">http://schema.org/Product</a> 的样本内容）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; doc = &quot;&quot;&quot;</div><div class="line">... &lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</div><div class="line">...   &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</div><div class="line">...   ![](kenmore-microwave-17in.jpg)</div><div class="line">...   &lt;div itemprop=&quot;aggregateRating&quot;</div><div class="line">...     itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</div><div class="line">...    Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</div><div class="line">...    based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</div><div class="line">...     &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   Product description:</div><div class="line">...   &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</div><div class="line">...   Has six preset cooking categories and convenience features like</div><div class="line">...   Add-A-Minute and Child Lock.&lt;/span&gt;</div><div class="line">...</div><div class="line">...   Customer reviews:</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</div><div class="line">...     by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</div><div class="line">...     &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</div><div class="line">...     &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</div><div class="line">...       &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</div><div class="line">...       &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</div><div class="line">...       &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</div><div class="line">...     &lt;/div&gt;</div><div class="line">...     &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</div><div class="line">...     it. &lt;/span&gt;</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</div><div class="line">...     by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</div><div class="line">...     &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</div><div class="line">...     &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</div><div class="line">...       &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</div><div class="line">...       &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</div><div class="line">...       &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</div><div class="line">...     &lt;/div&gt;</div><div class="line">...     &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</div><div class="line">...     fits in my apartment.&lt;/span&gt;</div><div class="line">...   &lt;/div&gt;</div><div class="line">...   ...</div><div class="line">... &lt;/div&gt;</div><div class="line">... &quot;&quot;&quot;</div><div class="line">&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)</div><div class="line">&gt;&gt;&gt; for scope in sel.xpath(&apos;//div[@itemscope]&apos;):</div><div class="line">...     print &quot;current scope:&quot;, scope.xpath(&apos;@itemtype&apos;).extract()</div><div class="line">...     props = scope.xpath(&apos;&apos;&apos;</div><div class="line">...                 set:difference(./descendant::*/@itemprop,</div><div class="line">...                                .//*[@itemscope]/*/@itemprop)&apos;&apos;&apos;)</div><div class="line">...     print &quot;    properties:&quot;, props.extract()</div><div class="line">...     print</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Product&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;aggregateRating&apos;, u&apos;offers&apos;, u&apos;description&apos;, u&apos;review&apos;, u&apos;review&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/AggregateRating&apos;]</div><div class="line">    properties: [u&apos;ratingValue&apos;, u&apos;reviewCount&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Offer&apos;]</div><div class="line">    properties: [u&apos;price&apos;, u&apos;availability&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Review&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Rating&apos;]</div><div class="line">    properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Review&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Rating&apos;]</div><div class="line">    properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]</div></pre></td></tr></table></figure>
<p>这里我们先迭代 <code>itemscope</code> 元素，对于每一个元素，我们寻找所有 <code>itemprops</code> 元素，并排除那些在另一个元素内部的元素 <code>itemscope</code> 。</p>
<h2 id="内置选择器参考"><a href="#内置选择器参考" class="headerlink" title="内置选择器参考"></a>内置选择器参考</h2><h3 id="Selector-实例"><a href="#Selector-实例" class="headerlink" title="Selector 实例"></a><strong>Selector 实例</strong></h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> scrapy.selector.Selector(response=<span class="keyword">None</span>, text=<span class="keyword">None</span>, <span class="keyword">type</span>=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>一个实例<code>Selector</code>是一个包装器响应来选择其内容的某些部分。</p>
<p>response是一个<code>HtmlResponse</code>或一个<code>XmlResponse</code>将被用于选择和提取的数据对象。</p>
<p><code>text</code>是一个<code>unicode</code>字符串或<code>utf-8</code>编码的文本，当一个 <code>response</code>不可用时。使用<code>text</code>和<code>response</code>一起是未定义的行为。</p>
<p><code>type</code>定义选择器类型，它可以是<code>&quot;html&quot;</code>，<code>&quot;xml&quot;</code>或<code>None（默认）</code>。</p>
<p>如果<code>type</code>是<code>None</code>，选择器将根据<code>response</code>类型（见下文）自动选择最佳类型，或者默认<code>&quot;html&quot;</code>情况下与选项一起使用<code>text</code>。</p>
<p>如果<code>type</code>是<code>None</code>和<code>response</code>传递，选择器类型从响应类型推断如下：</p>
<ul>
<li><code>&quot;html&quot;</code>对于HtmlResponse类型</li>
<li><code>&quot;xml&quot;</code>对于XmlResponse类型</li>
<li><code>&quot;html&quot;</code>为任何其他</li>
</ul>
<p>否则，如果<code>type</code>设置，选择器类型将被强制，并且不会发生检测。</p>
<h4 id="xpath（查询）"><a href="#xpath（查询）" class="headerlink" title="xpath（查询）"></a><strong>xpath（查询）</strong></h4><p>查找与<code>xpath</code>匹配的节点<code>query</code>，并将结果作为 <code>SelectorList</code>实例将所有元素展平。列表元素也实现<code>Selector</code>接口。</p>
<p><code>query</code> 是一个包含要应用的XPATH查询的字符串。</p>
<p><strong>注意</strong></p>
<blockquote>
<p>为了方便起见，这种方法可以称为 response.xpath()</p>
</blockquote>
<h4 id="css（查询）"><a href="#css（查询）" class="headerlink" title="css（查询）"></a><strong>css（查询）</strong></h4><p>应用给定的CSS选择器并返回一个SelectorList实例。</p>
<p>query 是一个包含要应用的CSS选择器的字符串。</p>
<p>在后台，CSS查询使用cssselect库和run .xpath()方法转换为XPath查询 。</p>
<p><strong>注意</strong></p>
<blockquote>
<p>为了方便起见，该方法可以称为 response.css()</p>
</blockquote>
<h4 id="extract（）"><a href="#extract（）" class="headerlink" title="extract（）"></a><strong>extract（）</strong></h4><p>序列化并返回匹配的节点作为unicode字符串列表。编码内容的百分比未引用。</p>
<h4 id="re（regex）"><a href="#re（regex）" class="headerlink" title="re（regex）"></a><strong>re（regex）</strong></h4><p>应用给定的正则表达式并返回一个包含匹配项的unicode字符串的列表。</p>
<p><code>regex</code>可以是编译的正则表达式或将被编译为正则表达式的字符串 <code>re.compile(regex)</code></p>
<p><strong>注意</strong></p>
<blockquote>
<p>注意，re()和re_first()解码HTML实体（除\&lt;和\&amp;）。</p>
</blockquote>
<h4 id="register-namespace（prefix，uri）"><a href="#register-namespace（prefix，uri）" class="headerlink" title="register_namespace（prefix，uri）"></a><strong>register_namespace（prefix，uri）</strong></h4><p>注册在此使用的给定命名空间Selector。如果不注册命名空间，则无法从非标准命名空间中选择或提取数据。参见下面的例子。</p>
<h4 id="remove-namespaces（）"><a href="#remove-namespaces（）" class="headerlink" title="remove_namespaces（）"></a><strong>remove_namespaces（）</strong></h4><p>删除所有命名空间，允许使用无命名空间的xpaths遍历文档。参见下面的例子。</p>
<h4 id="nonzero（）"><a href="#nonzero（）" class="headerlink" title="nonzero（）"></a><strong>nonzero（）</strong></h4><p>返回True如果有选择或任何实际的内容False 除外。换句话说，a的布尔值Selector由它选择的内容给出。</p>
<h3 id="SelectorList对象"><a href="#SelectorList对象" class="headerlink" title="SelectorList对象"></a>SelectorList对象</h3><p><code>class scrapy.selector.SelectorList</code></p>
<p>本SelectorList类是内置的一个子list 类，它提供了几个方法。</p>
<h4 id="xpath（查询）-1"><a href="#xpath（查询）-1" class="headerlink" title="xpath（查询）"></a><strong>xpath（查询）</strong></h4><p>调用.xpath()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。</p>
<p>query 是同一个参数 Selector.xpath()</p>
<h4 id="css（查询）-1"><a href="#css（查询）-1" class="headerlink" title="css（查询）"></a><strong>css（查询）</strong></h4><p>调用.css()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。</p>
<p>query 是同一个参数 Selector.css()</p>
<h4 id="extract（）-1"><a href="#extract（）-1" class="headerlink" title="extract（）"></a><strong>extract（）</strong></h4><p>调用.extract()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。</p>
<h4 id="re（）"><a href="#re（）" class="headerlink" title="re（）"></a><strong>re（）</strong></h4><p>调用.re()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。</p>
<h4 id="nonzero（）-1"><a href="#nonzero（）-1" class="headerlink" title="nonzero（）"></a><strong>nonzero（）</strong></h4><p>如果列表不为空，则返回True，否则返回False。</p>
<h3 id="HTML响应的选择器示例"><a href="#HTML响应的选择器示例" class="headerlink" title="HTML响应的选择器示例"></a><strong>HTML响应的选择器示例</strong></h3><p>这里有几个Selector例子来说明几个概念。在所有情况下，我们假设已经Selector实例化了一个HtmlResponse对象，如下：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sel</span> = Selector(html_response)</div></pre></td></tr></table></figure>
<ol>
<li><code>&lt;h1&gt;</code>从HTML响应主体中选择所有元素，返回Selector对象列表 （即SelectorList对象）：</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1"</span>)</div></pre></td></tr></table></figure>
<ol>
<li><code>&lt;h1&gt;</code>从HTML响应正文中提取所有元素的文本，返回unicode字符串</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1"</span>).extract()         <span class="comment"># this includes the h1 tag</span></div><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1/text()"</span>).extract()  <span class="comment"># this excludes the h1 tag</span></div></pre></td></tr></table></figure>
<ol>
<li>迭代所有<code>&lt;p&gt;</code>标签并打印其类属性：</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">for <span class="keyword">node</span> <span class="title">in</span> sel.<span class="keyword">xpath</span>(<span class="string">"//p"</span>):</div><div class="line">    print <span class="keyword">node</span>.<span class="title">xpath</span>(<span class="string">"@class"</span>).extract()</div></pre></td></tr></table></figure>
<h3 id="XML响应的选择器示例"><a href="#XML响应的选择器示例" class="headerlink" title="XML响应的选择器示例"></a><strong>XML响应的选择器示例</strong></h3><p>这里有几个例子来说明几个概念。在这两种情况下，我们假设已经Selector实例化了一个 XmlResponse对象，像这样：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sel</span> = Selector(xml_response)</div></pre></td></tr></table></figure>
<ol>
<li><product>从XML响应主体中选择所有元素，返回Selector对象列表（即SelectorList对象）：</product></li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//product"</span>)</div></pre></td></tr></table></figure>
<ol>
<li>从需要注册命名空间的<a href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799" target="_blank" rel="external">Google Base XML Feed</a>中提取所有价格：</li>
</ol>
<figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">sel</span><span class="selector-class">.register_namespace</span>(<span class="string">"g"</span>, <span class="string">"http://base.google.com/ns/1.0"</span>)</div><div class="line"><span class="selector-tag">sel</span><span class="selector-class">.xpath</span>(<span class="string">"//g:price"</span>)<span class="selector-class">.extract</span>()</div></pre></td></tr></table></figure>
<h3 id="删除名称空间"><a href="#删除名称空间" class="headerlink" title="删除名称空间"></a><strong>删除名称空间</strong></h3><p>当处理抓取项目时，通常很方便地完全删除命名空间，只需处理元素名称，编写更简单/方便的XPath。你可以使用的 Selector.remove_namespaces()方法。</p>
<p>让我们展示一个例子，用GitHub博客atom feed来说明这一点。</p>
<p>首先，我们打开shell和我们想要抓取的url：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy <span class="keyword">shell</span> http<span class="variable">s:</span>//github.<span class="keyword">com</span>/blog.atom</div></pre></td></tr></table></figure>
<p>一旦在shell中，我们可以尝试选择所有<link>对象，并看到它不工作（因为Atom XML命名空间模糊了这些节点）：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;</span>&gt; response.xpath(<span class="string">"//link"</span>)</div><div class="line">[]</div></pre></td></tr></table></figure>
<p>但是一旦我们调用该Selector.remove_namespaces()方法，所有节点都可以直接通过他们的名字访问：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>response.selector.remove_namespaces()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">"//link"</span>)</div><div class="line">[&lt;Selector xpath=<span class="string">'//link'</span> data=<span class="string">u'&lt;link xmlns="http://www.w3.org/2005/Atom'</span>&gt;,</div><div class="line"> &lt;Selector xpath=<span class="string">'//link'</span> data=<span class="string">u'&lt;link xmlns="http://www.w3.org/2005/Atom'</span>&gt;,</div><div class="line"> ...</div></pre></td></tr></table></figure>
<p>如果你想知道为什么默认情况下不调用命名空间删除过程，而不是手动调用它，这是因为两个原因，按照相关性的顺序：</p>
<ol>
<li>删除命名空间需要迭代和修改文档中的所有节点，这对于Scrapy爬取的所有文档来说是一个相当昂贵的操作</li>
<li>可能有一些情况下，实际上需要使用命名空间，以防某些元素名称在命名空间之间冲突。这些情况非常罕见。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://beautifulsoup.readthedocs.io/zh_CN/latest/&quot;&gt;BeautifulSoup&lt;/a&gt; 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://lxml.de/index.html&quot;&gt;lxml&lt;/a&gt; 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——编写Spider爬取伯乐在线所有文章</title>
    <link href="http://yoursite.com/2017/04/26/scrapy-jobbole-spider/"/>
    <id>http://yoursite.com/2017/04/26/scrapy-jobbole-spider/</id>
    <published>2017-04-26T06:18:54.000Z</published>
    <updated>2017-04-26T07:58:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>仍然是以 <code>http://blog.jobbole.com/all-posts/</code> 页面为例</p>
<a id="more"></a>
<h2 id="提取文章列表页"><a href="#提取文章列表页" class="headerlink" title="提取文章列表页"></a><strong>提取文章列表页</strong></h2><p>首页使用CSS选择器获取页面中的文章url列表：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff04stoom0j30gw0n4te3.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;#archive .floated-thumb .post-thumb a::attr(href)&quot;).extract()</div><div class="line">&gt;&gt;&gt;[&apos;http://blog.jobbole.com/111005/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/108468/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/110975/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/110986/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110957/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110976/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110923/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110962/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110958/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110140/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110939/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110941/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110931/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110934/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110929/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110835/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110906/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110916/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110913/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110903/&apos;]</div></pre></td></tr></table></figure>
<p>先在Spider头部引入<code>from scrapy.http import Request</code>，使用Request进行对文章url列表获取函数的调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析</div><div class="line">    2. 获取下一页的url并交给scrapy进行下载</div><div class="line">    :param response: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># 解析列表页中的所有文章url并交给解析函数进行具体字段的解析</span></div><div class="line">    post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</div><div class="line">    <span class="keyword">for</span> post_url <span class="keyword">in</span> post_urls:</div><div class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</div></pre></td></tr></table></figure>
<p>其中，最后 <code>yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</code> 是对每个url调用parse_detail方法进行字段解析，这里url的参数是带有完整域名的格式，如果不是完整域名，则需要对域名进行拼接成完成域名进行解析。首先要引入 <code>from urllib import parse</code> ，通过parse自带的 <code>parse.urljoin()</code> 进行拼接，代码为： <code>yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</code> 。</p>
<h2 id="循环获取下一个列表页"><a href="#循环获取下一个列表页" class="headerlink" title="循环获取下一个列表页"></a><strong>循环获取下一个列表页</strong></h2><p><img src="http://ww2.sinaimg.cn/large/006tNbRwgy1ff04orjgoej30yc02g74a.jpg" alt=""></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff04p002aoj311800qdg7.jpg" alt=""></p>
<p>每一个列表页都有“下一页”链接，我们通过CSS选择器来获取下一页的链接，然后交给parse函数进行循环解析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 提取下一页并交给scrapy进行下载</span></div><div class="line">next_url = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first()</div><div class="line"><span class="keyword">if</span> next_url:</div><div class="line">	<span class="keyword">yield</span> Request(url=parse.urljoin(response.url, next_url), callback=self.parse)</div></pre></td></tr></table></figure>
<p>其中，extract_first()方法与extract()[0]用法相同，都是提取第一个字符串元素。</p>
<h2 id="解析函数"><a href="#解析函数" class="headerlink" title="解析函数"></a><strong>解析函数</strong></h2><p>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="comment"># 通过css选择器提取字段</span></div><div class="line">    title = response.css(<span class="string">".entry-header h1::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</div><div class="line">    praise_nums = int(response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>])</div><div class="line">    fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">    	fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">    	fav_nums = <span class="number">0</span></div><div class="line">    comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">    	comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">    	comment_nums = <span class="number">0</span></div><div class="line">    content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</div><div class="line">    tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</div><div class="line">    tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">    tags = <span class="string">","</span>.join(tag_list)</div><div class="line">    print(title, create_date, fav_nums, comment_nums, tags)</div></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a><strong>运行结果</strong></h2><p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff04zpbvtzj31g20bkte9.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;仍然是以 &lt;code&gt;http://blog.jobbole.com/all-posts/&lt;/code&gt; 页面为例&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——CSS选择器</title>
    <link href="http://yoursite.com/2017/04/25/css-selector/"/>
    <id>http://yoursite.com/2017/04/25/css-selector/</id>
    <published>2017-04-25T12:18:54.000Z</published>
    <updated>2017-04-25T13:13:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CSS选择器的用法"><a href="#CSS选择器的用法" class="headerlink" title="CSS选择器的用法"></a><strong>CSS选择器的用法</strong></h2><h3 id="CSS选择器简介"><a href="#CSS选择器简介" class="headerlink" title="CSS选择器简介"></a><strong>CSS选择器简介</strong></h3><p>在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。</p>
<a id="more"></a>
<h3 id="常用CSS选择器介绍"><a href="#常用CSS选择器介绍" class="headerlink" title="常用CSS选择器介绍"></a><strong>常用CSS选择器介绍</strong></h3><table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>*</td>
<td>选择所有节点</td>
</tr>
<tr>
<td>#container</td>
<td>选择id为container的节点</td>
</tr>
<tr>
<td>.container</td>
<td>选择所有class包含container的节点</td>
</tr>
<tr>
<td>li a</td>
<td>选取所有li下的所有a节点</td>
</tr>
<tr>
<td>ul + p</td>
<td>选择ul后面的第一个p元素</td>
</tr>
<tr>
<td>div#container &gt; ul</td>
<td>选取id为container的div的第一个ul子元素</td>
</tr>
<tr>
<td>ul ~ p</td>
<td>选取与ul相邻的所有p元素</td>
</tr>
<tr>
<td>a[title]</td>
<td>选取所有有title属性的a元素</td>
</tr>
<tr>
<td>a[href=”<code>http://163.com</code>“]</td>
<td>选取所有href属性为163的a元素</td>
</tr>
<tr>
<td>a[href*=”163”]</td>
<td>选取所有href属性包含163的a元素</td>
</tr>
<tr>
<td>a[href^=”http”]</td>
<td>选取所有href属性以http开头的a元素</td>
</tr>
<tr>
<td>a[href$=”.jpg”]</td>
<td>选取所有href以.jpg结尾的a元素</td>
</tr>
<tr>
<td>input[type=radio]:checked</td>
<td>选择选中的radio的元素</td>
</tr>
<tr>
<td>div:not(#container)</td>
<td>选取所有id非container的div属性</td>
</tr>
<tr>
<td>li:nth-child(3)</td>
<td>选取第三个li元素</td>
</tr>
<tr>
<td>tr:nth-child(2n)</td>
<td>第偶数个tr</td>
</tr>
</tbody>
</table>
<h3 id="Scrapy中CSS选择器用法示例"><a href="#Scrapy中CSS选择器用法示例" class="headerlink" title="Scrapy中CSS选择器用法示例"></a><strong>Scrapy中CSS选择器用法示例</strong></h3><p>仍然是用<a href="http://lawtech0902.com/2017/04/16/xpath-example/" target="_blank" rel="external">Xpath用法示例</a>中的例子来进行测试</p>
<h4 id="获取标题"><a href="#获取标题" class="headerlink" title="获取标题"></a>获取标题</h4><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1fez7spfa23j31aw03ytay.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;.entry-header h1::text&quot;).extract()[0]</div><div class="line">&apos;2016 腾讯软件开发面试题（部分）&apos;</div></pre></td></tr></table></figure>
<p>注意：这里获取文字内容的方法为::text，而不是text()。</p>
<h4 id="获取文章发布时间"><a href="#获取文章发布时间" class="headerlink" title="获取文章发布时间"></a>获取文章发布时间</h4><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1fez7svm5gdj30hs01g74p.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile::text&quot;).extract()[0].strip().replace(&quot;·&quot;, &quot;&quot;).strip()</div><div class="line">&apos;2017/02/18&apos;</div></pre></td></tr></table></figure>
<h4 id="获取点赞数、收藏数、评论数"><a href="#获取点赞数、收藏数、评论数" class="headerlink" title="获取点赞数、收藏数、评论数"></a><strong>获取点赞数</strong>、收藏数、评论数</h4><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1fez7terph3j30gi03amxc.jpg" alt=""></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1fez7tk0m0mj31eg0cq7ag.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 点赞数</div><div class="line">&gt;&gt;&gt; response.css(&quot;.vote-post-up h10::text&quot;).extract()[0]</div><div class="line">&apos;2&apos;</div><div class="line"></div><div class="line"># 收藏数，获取之后需要用正则表达式进行清洗</div><div class="line">&gt;&gt;&gt; response.css(&quot;.bookmark-btn::text&quot;).extract()[0]</div><div class="line">&apos; 23 收藏&apos;</div><div class="line"></div><div class="line"># 评论数，获取之后需要用正则表达式进行清洗</div><div class="line">&gt;&gt;&gt; response.css(&quot;a[href=&apos;#article-comment&apos;] span::text&quot;).extract()[0]</div><div class="line">&apos; 7 评论&apos;</div></pre></td></tr></table></figure>
<p>正则表达式清洗收藏数，评论数的逻辑如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    </div><div class="line">comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	comment_nums = int(match_re.group(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h4 id="获取正文"><a href="#获取正文" class="headerlink" title="获取正文"></a>获取正文</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;div.entry&quot;).extract()[0]</div></pre></td></tr></table></figure>
<h4 id="获取tags"><a href="#获取tags" class="headerlink" title="获取tags"></a>获取tags</h4><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1fez85kutnaj30bi01ydfs.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()</div><div class="line">[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;]</div></pre></td></tr></table></figure>
<p>然后需要对数据进行清洗</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text()"</span>).extract()</div><div class="line">tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">tags = <span class="string">","</span>.join(tag_list)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;CSS选择器的用法&quot;&gt;&lt;a href=&quot;#CSS选择器的用法&quot; class=&quot;headerlink&quot; title=&quot;CSS选择器的用法&quot;&gt;&lt;/a&gt;&lt;strong&gt;CSS选择器的用法&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;CSS选择器简介&quot;&gt;&lt;a href=&quot;#CSS选择器简介&quot; class=&quot;headerlink&quot; title=&quot;CSS选择器简介&quot;&gt;&lt;/a&gt;&lt;strong&gt;CSS选择器简介&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。&lt;/p&gt;
    
    </summary>
    
      <category term="CSS" scheme="http://yoursite.com/categories/CSS/"/>
    
    
      <category term="Scrapy，CSS，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CCSS%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Spiders</title>
    <link href="http://yoursite.com/2017/04/22/scrapy-spiders/"/>
    <id>http://yoursite.com/2017/04/22/scrapy-spiders/</id>
    <published>2017-04-22T06:18:54.000Z</published>
    <updated>2017-04-22T11:34:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p>
<a id="more"></a>
<p>对 spider 来说，爬取的流程如下：</p>
<ol>
<li>先初始化请求URL列表，并指定下载后处理response的回调函数。初次请求URL通过 <code>start_urls</code> 指定，调用 <code>start_requests()</code> 产生 <code>Request</code> 对象，然后注册 <code>parse</code> 方法作为回调</li>
<li>在parse回调中解析response并返回字典, <code>Item</code> 对象, <code>Request</code> 对象或它们的迭代对象。 <code>Request</code> 对象还会包含回调函数，之后Scrapy下载完后会被这里注册的回调函数处理。</li>
<li>在回调函数里面，你通过使用选择器（同样可以使用BeautifulSoup,lxml或其他工具）解析页面内容，并生成解析后的结果Item。</li>
<li>最后返回的这些Item通常会被持久化到数据库中(使用<a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="external">Item Pipeline</a>)或者使用<a href="http://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports" target="_blank" rel="external">Feed exports</a>将其保存到文件中。</li>
</ol>
<p>虽然该循环对任何类型的 spider 都（多少）适用，但 Scrapy 仍然为了不同的需求提供了多种默认 spider。 之后将讨论这些 spider。</p>
<h2 id="Spider-参数"><a href="#Spider-参数" class="headerlink" title="Spider 参数"></a><strong>Spider 参数</strong></h2><p>Spider 可以通过接受参数来修改其功能。 spider 参数一般用来定义初始 URL 或者指定限制爬取网站的部分。 您也可以使用其来配置 spider 的任何功能。 </p>
<p>在运行 <code>crawl</code> 时添加 <code>-a</code> 可以传递 Spider 参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl myspider -a category=electronics</div></pre></td></tr></table></figure>
<p>Spider 在构造器（constructor）中获取参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'myspider'</span></div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, category=None, *args, **kwargs)</span>:</span></div><div class="line">        super(MySpider, self).__init__(*args, **kwargs) </div><div class="line">        self.start_urls = [<span class="string">'http://www.example.com/categories/%s'</span> % category] </div><div class="line">        <span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p>Spider 参数也可以通过 <code>Scrapyd</code> 的 <code>schedule.json API</code> 来传递。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Items</title>
    <link href="http://yoursite.com/2017/04/21/scrapy-items/"/>
    <id>http://yoursite.com/2017/04/21/scrapy-items/</id>
    <published>2017-04-21T06:18:54.000Z</published>
    <updated>2017-04-21T13:14:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。</p>
<p>Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。</p>
<a id="more"></a>
<h2 id="声明-Item"><a href="#声明-Item" class="headerlink" title="声明 Item"></a><strong>声明 Item</strong></h2><p>Item 使用简单的 class 定义语法以及 Field 对象来声明。例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductItem</span><span class="params">(scrapy.Item)</span>:</span> </div><div class="line">    name = scrapy.Field() </div><div class="line">    price = scrapy.Field() </div><div class="line">    stock = scrapy.Field() </div><div class="line">    last_updated = scrapy.Field(serializer=str)</div></pre></td></tr></table></figure>
<p>Scrapy Item 的定义方式与 Django Models 很类似，但是没有 Django 那么多不同的字段类型（Field Type）。</p>
<h2 id="Item-字段（Item-Fields）"><a href="#Item-字段（Item-Fields）" class="headerlink" title="Item 字段（Item Fields）"></a><strong>Item 字段（Item Fields）</strong></h2><p>Field 对象指明了每个字段的元数据（metadata）。例如上面例子中 last_updated 中指明了该字段的序列化函数。</p>
<p>您可以为每个字段指明任何类型的元数据。Field 对象对接受的值没有任何限制。也正是因为这个原因，文档也无法提供所有可用的元数据的键（key）参考列表。Field 对象中保存的每个键可以由多个组件使用，并且只有这些组件知道这个键的存在。您可以根据自己的需求，定义使用其他的 Field 键。 设置 Field 对象的主要目的就是在一个地方定义好所有的元数据。一般来说，那些依赖某个字段的组件肯定使用了特定的键（key）。您必须查看组件相关的文档，查看其用了哪些元数据键（metadata key）。</p>
<p>需要注意的是，用来声明 item 的 Field 对象并没有被赋值为 class 的属性。不过您可以通过 Item.fields 属性进 行访问。</p>
<h2 id="与-Item-配合"><a href="#与-Item-配合" class="headerlink" title="与 Item 配合"></a><strong>与 Item 配合</strong></h2><p>在API这里的操作，和 <code>dict API</code> 非常的相似。</p>
<h3 id="创建-item"><a href="#创建-item" class="headerlink" title="创建 item"></a><strong>创建 item</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> scrapy</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">ProductItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line"><span class="meta">... </span>    name = scrapy.Field()</div><div class="line"><span class="meta">... </span>    price = scrapy.Field()</div><div class="line"><span class="meta">... </span>    stock = scrapy.Field()</div><div class="line"><span class="meta">... </span>    last_updated = scrapy.Field(serializer=str)</div><div class="line">...</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product = ProductItem(name=<span class="string">'Desktop'</span>, price=<span class="number">1000</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(product)</div><div class="line">&#123;<span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="获取字段的值"><a href="#获取字段的值" class="headerlink" title="获取字段的值"></a><strong>获取字段的值</strong></h3><p>两种方式：</p>
<ul>
<li>键值对</li>
<li>get()</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 已设定key和value值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'name'</span>]</div><div class="line"><span class="string">'Desktop'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'name'</span>)</div><div class="line"><span class="string">'Desktop'</span></div><div class="line"></div><div class="line"><span class="comment"># 未设定key和value值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>]</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">59</span>, <span class="keyword">in</span> __getitem__</div><div class="line">    <span class="keyword">return</span> self._values[key]</div><div class="line">KeyError: <span class="string">'last_updated'</span></div><div class="line">    </div><div class="line"><span class="comment"># 默认返回空值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'last_updated'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设定返回值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'last_updated'</span>, <span class="string">'not set value'</span>)</div><div class="line"><span class="string">'not set value'</span></div><div class="line"></div><div class="line"><span class="comment"># 未声明的字段</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'lala'</span>]</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">59</span>, <span class="keyword">in</span> __getitem__</div><div class="line">    <span class="keyword">return</span> self._values[key]</div><div class="line">KeyError: <span class="string">'lala'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'lala'</span>, <span class="string">'not exist'</span>)</div><div class="line"><span class="string">'not exist'</span></div><div class="line"></div><div class="line"><span class="comment"># 字段是否被赋值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'name'</span> <span class="keyword">in</span> product</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'last_updated'</span> <span class="keyword">in</span> product</div><div class="line"><span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 字段是否被声明</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'last_updated'</span> <span class="keyword">in</span> product.fields</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'lala'</span> <span class="keyword">in</span> product.fields</div><div class="line"><span class="keyword">False</span></div></pre></td></tr></table></figure>
<h3 id="设置字段的值"><a href="#设置字段的值" class="headerlink" title="设置字段的值"></a><strong>设置字段的值</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 已经声明的字段</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>] = <span class="string">'today'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>]</div><div class="line"><span class="string">'today'</span></div><div class="line"></div><div class="line"><span class="comment"># 未声明的字段无法赋值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'lala'</span>] = <span class="string">'test'</span></div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">66</span>, <span class="keyword">in</span> __setitem__</div><div class="line">    (self.__class__.__name__, key))</div><div class="line">KeyError: <span class="string">'ProductItem does not support field: lala'</span></div></pre></td></tr></table></figure>
<h3 id="获取所有能够获取到的值"><a href="#获取所有能够获取到的值" class="headerlink" title="获取所有能够获取到的值"></a><strong>获取所有能够获取到的值</strong></h3><p>可以使用 dict API 来获取所有的值:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.keys()</div><div class="line">dict_keys([<span class="string">'last_updated'</span>, <span class="string">'price'</span>, <span class="string">'name'</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.items()</div><div class="line">ItemsView(&#123;<span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;)</div></pre></td></tr></table></figure>
<h3 id="复制-item"><a href="#复制-item" class="headerlink" title="复制 item"></a><strong>复制 item</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>product2 = ProductItem(product)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(product2)</div><div class="line">&#123;<span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;</div><div class="line"></div><div class="line"><span class="comment"># 推荐使用第二种方法</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product3 = product2.copy()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(product3)</div><div class="line">&#123;<span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="根据-item-创建字典-dict"><a href="#根据-item-创建字典-dict" class="headerlink" title="根据 item 创建字典(dict)"></a><strong>根据 item 创建字典(dict)</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>dict(product)</div><div class="line">&#123;<span class="string">'price'</span>: <span class="number">1000</span>, <span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="根据字典-dict-创建-item"><a href="#根据字典-dict-创建-item" class="headerlink" title="根据字典(dict)创建 item"></a><strong>根据字典(dict)创建 item</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>ProductItem(&#123;<span class="string">'name'</span>:<span class="string">'laptop pc'</span>, <span class="string">'price'</span>:<span class="number">1500</span>&#125;)</div><div class="line">&#123;<span class="string">'name'</span>: <span class="string">'laptop pc'</span>, <span class="string">'price'</span>: <span class="number">1500</span>&#125;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>ProductItem(&#123;<span class="string">'name'</span>:<span class="string">'laptop pc'</span>, <span class="string">'lala'</span>:<span class="number">1500</span>&#125;)</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">56</span>, <span class="keyword">in</span> __init__</div><div class="line">    self[k] = v</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">66</span>, <span class="keyword">in</span> __setitem__</div><div class="line">    (self.__class__.__name__, key))</div><div class="line">KeyError: <span class="string">'ProductItem does not support field: lala'</span></div></pre></td></tr></table></figure>
<h2 id="扩展-Item"><a href="#扩展-Item" class="headerlink" title="扩展 Item"></a><strong>扩展 Item</strong></h2><p>可以通过继承原始的 Item 来扩展 item(添加更多的字段或者修改某些字段的元数据)。</p>
<ul>
<li>添加新的字段</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiscountedProductItem</span><span class="params">(Product)</span>:</span></div><div class="line">    discount_percent = scrapy.Field(serializer=str)</div><div class="line">    discount_expiration_date = scrapy.Field()</div></pre></td></tr></table></figure>
<ul>
<li>使用原字段的元数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpecificProduct</span><span class="params">(Product)</span>:</span></div><div class="line">    name = scrapy.Field(Product.fields[<span class="string">'name'</span>], serializer=my_serializer)</div><div class="line"><span class="comment">#my_serializer 指序列化的类型</span></div></pre></td></tr></table></figure>
<p>上述代码，在保留了原始的元数据值的情况下，添加（或覆盖）了 <code>name</code> 字段的  <code>serializer</code> 。 存在及覆盖，不存在即添加。</p>
<h2 id="Item-对象"><a href="#Item-对象" class="headerlink" title="Item 对象"></a><strong>Item 对象</strong></h2><p><code>class scrapy.item.Item([arg])</code></p>
<p>返回一个根据给定的参数可选初始化的 <code>item</code> 。</p>
<p><code>Item</code> 复制了标准化的 <code>dict API</code> ，包括初始化函数也是一样。除此之外，唯一添加的额外属性就是 <code>fields</code> 。</p>
<p><code>fields</code> 是一个包含了 item 所有声明的字段的字典，而不仅仅是获取到的字段。该字典的 key 是字段（field）的名字，值是 Item 声明中使用到的 Field 对象。</p>
<h2 id="字段（Field）对象"><a href="#字段（Field）对象" class="headerlink" title="字段（Field）对象"></a>字段（Field）对象</h2><p><code>class scrapy.item.Field([arg])</code></p>
<p><code>Field</code>仅仅是内置的 <code>dict</code> 类的一个别名（继承于 <code>dict</code> ），并没有提供额外的方法或属性。说白了，<code>Field</code>就是完完全全的Python字典，被用来基于类属性的方法支持 <code>Item</code> 声明语法。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。&lt;/p&gt;
&lt;p&gt;Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Redis学习笔记(六)：数据安全与性能保障——处理系统故障</title>
    <link href="http://yoursite.com/2017/04/20/Redis-6/"/>
    <id>http://yoursite.com/2017/04/20/Redis-6/</id>
    <published>2017-04-20T12:18:54.000Z</published>
    <updated>2017-04-20T13:46:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。</p>
<a id="more"></a>
<h2 id="验证快照文件和-AOF-文件"><a href="#验证快照文件和-AOF-文件" class="headerlink" title="验证快照文件和 AOF 文件"></a><strong>验证快照文件和 AOF 文件</strong></h2><p>无论时快照持久化还是AOF持久化，都提供了在遇到系统故障时进行数据回复的工具。Redis提供了两个命令行程序 <code>redis-check-aof</code> 和 <code>redis-check-rdb(redis-check-dump was renamed to redis-check-rdb in redis version 3.2)</code> ，它们可以在系统故障发生之后，检查AOF文件和快照文件的状态，并在有需要的情况下对文件进行修复。</p>
<p>在不给定任何参数的情况下运行这两个程序，就可以看见它们的基本使用方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ redis-check-rdb</div><div class="line">Usage: redis-check-rdb &lt;rdb-file-name&gt;</div><div class="line">$ redis-check-dump</div><div class="line">Usage: redis-check-dump &lt;dump.rdb&gt;</div></pre></td></tr></table></figure>
<p>如果运行 <code>redis-check-aof</code> 程序时给了 <code>--fix</code> 参数，那么会对AOF文件进行修复。修复方法非常简单：扫描给定的 AOF 文件，寻找不正确或不完整的命令，当发现第一个出错命令的时候，程序会删除出错的命令以及位于出错命令之后的所有命令。在大多数情况下，被删除的都是 AOF 文件末尾的不完整的写命令。<br>遗憾的是，目前没有办法修复出错的快照文件。尽管发现快照文件首个出现错误的地方是有可能的，但因为快照文件本身经过了压缩，而出现在快照文件中间的错误有可能会导致快照文件的剩余部分无法读取。因此，最好为重要的快照文件保留多个备份，并在进行数据恢复时，通过计算快照文件的 SHA1 散列值和 SHA256 散列值来对内容进行验证。</p>
<h2 id="更换故障主服务器"><a href="#更换故障主服务器" class="headerlink" title="更换故障主服务器"></a><strong>更换故障主服务器</strong></h2><p>我们来看一下在拥有一个主服务器和一个从服务器的情况下，更换主服务器的具体步骤。假设A、B两台机器都运行着 Redis ，机器A为 master ，机器B为 slave 。机器A因为暂时无法修复的故障而断开了连接，因此决定将同样安装了 Redis 的机器 C 用作新的主服务器。</p>
<p>更换服务器的计划非常简单：首先向机器B发送一个 SAVE 命令，让它创建一个新的快照文件，接着将这个快照文件发送给机器C，并在机器 C 上面启动 Redis 。最后，让B成为机器C的从服务器。由于环境有限，就在同一台机器上用不同的端口进行测试，下面进行演示：</p>
<ol>
<li><p>先进入 Redis 安装位置，再安装两个 Redis 服务并分别修改配置文件 redis.conf 中的 port 为6380和6381</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ cd /usr/local</div><div class="line">$ sudo cp -r redis redis6380</div><div class="line">Password:</div><div class="line">$ sudo chmod -R 777 redis6380</div><div class="line">$ vim redis6380/redis.conf</div><div class="line">$ sudo cp -r redis redis6381</div><div class="line">$ sudo chmod -R 777 redis6381</div><div class="line">$ vim redis6381/redis.conf</div></pre></td></tr></table></figure>
</li>
<li><p>启动机器A端口为6379，机器B端口为6380，并让B成为A的从服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 启动A</div><div class="line">$ cd redis</div><div class="line">$ ./src/redis-server redis.conf</div><div class="line"></div><div class="line"># 启动B</div><div class="line">$ cd redis6380</div><div class="line">$ ./src/redis-server redis.conf</div><div class="line"></div><div class="line"># 让B成为A的从服务器</div><div class="line">$ $ redis-cli -h localhost -p 6380</div><div class="line">localhost:6380&gt; SLAVEOF localhost 6379</div><div class="line">OK</div></pre></td></tr></table></figure>
</li>
<li><p>停止机器A的 Redis 服务，此时只剩 Redis 从服务器B在运行</p>
</li>
<li><p>向机器B发送 SAVE 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">localhost:6380&gt; SAVE</div><div class="line">OK</div></pre></td></tr></table></figure>
</li>
<li><p>将机器B的快照文件复制到机器C的对应目录，并启动 Redis 服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cp -f /usr/local/redis6380/dump.rdb /usr/local/redis6381</div><div class="line">$ cd /usr/local/redis6381</div><div class="line">$ ./src/redis-server redis.conf</div></pre></td></tr></table></figure>
</li>
<li><p>让机器B成为机器C的从服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">localhost:6380&gt; SLAVEOF localhost 6381</div><div class="line">OK</div></pre></td></tr></table></figure>
</li>
<li><p>测试机器B是否能从机器C同步数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ redis-cli -h localhost -p 6381</div><div class="line">localhost:6381&gt; set key new-master</div><div class="line">OK</div><div class="line">$ redis-cli -h localhost -p 6380</div><div class="line">localhost:6380&gt; get key</div><div class="line">&apos;new-master&apos;</div></pre></td></tr></table></figure>
</li>
</ol>
<p>另一种创建新的主服务器的方法，就是将从服务器升级（turn）为主服务器，并为升级后的主服务器创建从服务器。</p>
<p>以上两种方法都可以让 Redis 回到之前的一个主服务器和一个从服务器的状态，而用户接下来需要做的就是更新客户端的配置，让它们去读写正确的服务器。除此之外，如果用户需要重启 Redis 的话，那么可能还需要对服务器的持久化配置进行更新。</p>
<p>Redis Sentinel可以监视指定的Redis主服务器及其下属的从服务器，并在主服务器下线时自动进行故障转移(failover)。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。&lt;/p&gt;
    
    </summary>
    
      <category term="Redis" scheme="http://yoursite.com/categories/Redis/"/>
    
    
      <category term="Redis, Python" scheme="http://yoursite.com/tags/Redis-Python/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy命令行工具</title>
    <link href="http://yoursite.com/2017/04/20/scrapy-command-line-tools/"/>
    <id>http://yoursite.com/2017/04/20/scrapy-command-line-tools/</id>
    <published>2017-04-20T06:18:54.000Z</published>
    <updated>2017-04-20T07:53:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。</p>
<a id="more"></a>
<h2 id="使用-scrapy-工具"><a href="#使用-scrapy-工具" class="headerlink" title="使用 scrapy 工具"></a><strong>使用 scrapy 工具</strong></h2><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a><strong>创建项目</strong></h3><p>一般来说，使用 <code>scrapy</code> 工具的第一件事就是创建 Scrapy 项目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject myproject</div></pre></td></tr></table></figure>
<p>该命令将会在 myproject 目录中创建一个 Scrapy 项目。 接下来，进入到项目目录中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd myproject</div></pre></td></tr></table></figure>
<p>这时候就可以使用 scrapy 命令来管理和控制项目了。</p>
<h3 id="控制项目"><a href="#控制项目" class="headerlink" title="控制项目"></a><strong>控制项目</strong></h3><p>创建一个新的 spider：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy genspider mydomain mydomain.com</div></pre></td></tr></table></figure>
<p>Scrapy 提供了两种类型的命令。一种必须在 Scrapy 项目中运行(针对项目(Project-specific)的命令)，另外一种则不需要(全局命令)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"># 全局命令(不需要在项目中运行)</div><div class="line">startproject</div><div class="line">settings</div><div class="line">runspider</div><div class="line">shell</div><div class="line">fetch</div><div class="line">view</div><div class="line">version</div><div class="line"></div><div class="line"># 项目(Project-only)命令(必须在项目中运行)</div><div class="line">crawl</div><div class="line">check</div><div class="line">list</div><div class="line">edit</div><div class="line">parse</div><div class="line">genspider</div><div class="line">deploy</div><div class="line">bench</div></pre></td></tr></table></figure>
<h2 id="工具命令介绍"><a href="#工具命令介绍" class="headerlink" title="工具命令介绍"></a><strong>工具命令介绍</strong></h2><p>我们可以通过运行命令来获取关于每个命令的详细内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy &lt;command&gt; -h</div></pre></td></tr></table></figure>
<p>也可以查看所有的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy -h</div></pre></td></tr></table></figure>
<p>下面就对这些命令进行介绍。</p>
<h3 id="startproject"><a href="#startproject" class="headerlink" title="startproject"></a>startproject</h3><ul>
<li>语法：<code>scrapy startproject &lt;project_name&gt;</code></li>
<li>全局命令</li>
</ul>
<p>在 project_name 文件夹下创建一个名为 project_name 的 Scrapy 项目。</p>
<h3 id="genspider"><a href="#genspider" class="headerlink" title="genspider"></a>genspider</h3><ul>
<li>语法：<code>scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</code></li>
<li>项目命令</li>
</ul>
<p>在当前项目中创建 spider。这仅仅是创建 spider 的一种快捷方法。该方法可以使用提前定义好的模板来生成 spider。您也可以自己创建 spider 的源码文件。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"># 查看模板</div><div class="line">$ scrapy genspider -l</div><div class="line">Available templates:</div><div class="line">  basic</div><div class="line">  crawl</div><div class="line">  csvfeed</div><div class="line">  xmlfeed</div><div class="line"></div><div class="line"># 编辑模板</div><div class="line">$ scrapy genspider -d basic</div><div class="line"># -*- coding: utf-8 -*-</div><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class $classname(scrapy.Spider):</div><div class="line">    name = &quot;$name&quot;</div><div class="line">    allowed_domains = [&quot;$domain&quot;]</div><div class="line">    start_urls = [&apos;http://$domain/&apos;]</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line">        pass</div><div class="line">    </div><div class="line"># 根据模板来生成spider</div><div class="line">$ scrapy genspider -t basic example example.com </div><div class="line">Created spider &apos;example&apos; using template &apos;basic&apos; in module: tutorial.spiders.example</div></pre></td></tr></table></figure>
<h3 id="crawl"><a href="#crawl" class="headerlink" title="crawl"></a>crawl</h3><ul>
<li>语法：<code>scrapy crawl myspider</code></li>
<li>项目命令</li>
</ul>
<p>使用 spider 进行爬取。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy crawl myspider </div><div class="line">[ ... myspider starts crawling ... ]</div></pre></td></tr></table></figure>
<h3 id="check"><a href="#check" class="headerlink" title="check"></a>check</h3><ul>
<li>语法：<code>scrapy check [-l] &lt;spider&gt;</code></li>
<li>项目命令</li>
</ul>
<p>运行 contract 检查。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ scrapy check -l </div><div class="line">first_spider </div><div class="line">* parse </div><div class="line">* parse_item </div><div class="line">second_spider </div><div class="line">* parse </div><div class="line">* parse_item</div><div class="line"></div><div class="line">$ scrapy check </div><div class="line">[FAILED] first_spider:parse_item </div><div class="line">&gt;&gt;&gt; &apos;RetailPricex&apos; field is missing</div><div class="line"></div><div class="line">[FAILED] first_spider:parse </div><div class="line">&gt;&gt;&gt; Returned 92 requests, expected 0..4</div></pre></td></tr></table></figure>
<h3 id="list"><a href="#list" class="headerlink" title="list"></a>list</h3><ul>
<li>语法：<code>scrapy list</code></li>
<li>项目命令</li>
</ul>
<p>列出当前项目中所有可用的 spider。每行输出一个 spider。 </p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ scrapy list </div><div class="line">spider1 </div><div class="line">spider2</div></pre></td></tr></table></figure>
<h3 id="edit"><a href="#edit" class="headerlink" title="edit"></a>edit</h3><ul>
<li>语法：<code>scrapy edit &lt;spider&gt;</code></li>
<li>项目命令</li>
</ul>
<p>使用 EDITOR 中设定的编辑器编辑给定的 spider </p>
<p>该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者 IDE 来编写调试 spider。 </p>
<h3 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h3><ul>
<li>语法：<code>scrapy fetch &lt;url&gt;</code></li>
<li>全局命令</li>
</ul>
<p>使用 Scrapy 下载器(downloader)下载给定的 URL，并将获取到的内容送到标准输出。 </p>
<p>该命令以 spider 下载页面的方式获取页面。例如，如果 spider 有 USER_AGENT 属性修改了 User Agen t，该命令将会使用该属性。</p>
<p>因此，您可以使用该命令来查看 spider 如何获取某个特定页面。 该命令如果非项目中运行则会使用默认 Scrapy downloader 设定。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ scrapy fetch --nolog http://www.example.com/some/page.html </div><div class="line">[ ... html content here ... ]</div><div class="line"></div><div class="line">$ scrapy fetch --nolog --headers http://www.example.com/ </div><div class="line">&#123;&apos;Accept-Ranges&apos;: [&apos;bytes&apos;],</div><div class="line">&apos;Age&apos;: [&apos;1263 &apos;], </div><div class="line">&apos;Connection&apos;: [&apos;close &apos;], </div><div class="line">&apos;Content-Length&apos;: [&apos;596&apos;], </div><div class="line">&apos;Content-Type&apos;: [&apos;text/html; charset=UTF-8&apos;], </div><div class="line">&apos;Date&apos;: [&apos;Wed, 18 Aug 2010 23:59:46 GMT&apos;], </div><div class="line">&apos;Etag&apos;: [&apos;&quot;573c1-254-48c9c87349680&quot;&apos;], </div><div class="line">&apos;Last-Modified&apos;: [&apos;Fri, 30 Jul 2010 15:30:18 GMT&apos;], </div><div class="line">&apos;Server&apos;: [&apos;Apache/2.2.3 (CentOS)&apos;]&#125;</div></pre></td></tr></table></figure>
<h3 id="view"><a href="#view" class="headerlink" title="view"></a>view</h3><ul>
<li>语法：<code>scrapy view &lt;url&gt;</code></li>
<li>全局命令</li>
</ul>
<p>在浏览器中打开给定的 URL，并以 Scrapy spider 获取到的形式展现。 有些时候 spider 获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查 spider 所获取到的页面，并确认这是您所期望的。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy view http://www.example.com/some/page.html </div><div class="line">[ ... browser starts ... ]</div></pre></td></tr></table></figure>
<h3 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h3><ul>
<li>语法：<code>scrapy shell [url]</code></li>
<li>全局命令</li>
</ul>
<p>以给定的 URL(如果给出)或者空(没有给出 URL)启动 Scrapy shell。查看 Scrapy 终端(Scrapy shell) 获取更多信息。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy shell http://www.example.com/some/page.html </div><div class="line">[ ... scrapy shell starts ... ]</div></pre></td></tr></table></figure>
<h3 id="parse"><a href="#parse" class="headerlink" title="parse"></a>parse</h3><ul>
<li>语法：<code>scrapy parse &lt;url&gt; [options]</code></li>
<li>项目命令</li>
</ul>
<p>获取给定的 URL 并使用相应的 spider 分析处理。如果提供 <code>--callback</code> 选项，则使用 spider 的该方法处理，否则使用 <code>parse</code> 。</p>
<h3 id="settings"><a href="#settings" class="headerlink" title="settings"></a>settings</h3><ul>
<li>语法：<code>scrapy settings [option]</code></li>
<li>全局命令</li>
</ul>
<p>获取 Scrapy 的设定 </p>
<p>在项目中运行时，该命令将会输出项目的设定值，否则输出 Scrapy 默认设定。 </p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ scrapy settings --get BOT_NAME </div><div class="line">scrapybot </div><div class="line">$ scrapy settings --get DOWNLOAD_DELAY </div><div class="line">0</div></pre></td></tr></table></figure>
<h3 id="runspider"><a href="#runspider" class="headerlink" title="runspider"></a>runspider</h3><ul>
<li>语法：<code>scrapy runspider &lt;spider_file.py&gt;</code></li>
<li>全局命令</li>
</ul>
<p>在未创建项目的情况下，运行一个编写在 Python 文件中的 spider。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy runspider myspider.py </div><div class="line">[ ... spider starts crawling ... ]</div></pre></td></tr></table></figure>
<h3 id="version"><a href="#version" class="headerlink" title="version"></a>version</h3><ul>
<li>语法：<code>scrapy version [-v]</code></li>
<li>全局命令</li>
</ul>
<p>输出 Scrapy 版本。配合 -v 运行时，该命令同时输出 Python，Twisted 以及平台的信息，方便 bug 提交。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ scrapy version -v</div><div class="line">Scrapy    : 1.3.3</div><div class="line">lxml      : 3.7.3.0</div><div class="line">libxml2   : 2.9.4</div><div class="line">cssselect : 1.0.1</div><div class="line">parsel    : 1.1.0</div><div class="line">w3lib     : 1.17.0</div><div class="line">Twisted   : 17.1.0</div><div class="line">Python    : 3.5.2 (default, Oct 11 2016, 04:59:56) - [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)]</div><div class="line">pyOpenSSL : 16.2.0 (OpenSSL 1.1.0e  16 Feb 2017)</div><div class="line">Platform  : Darwin-16.6.0-x86_64-i386-64bit</div></pre></td></tr></table></figure>
<h3 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h3><ul>
<li>语法：<code>scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ]</code></li>
<li>项目命令</li>
</ul>
<p>将项目部署到 Scrapyd 服务。</p>
<h3 id="bench"><a href="#bench" class="headerlink" title="bench"></a>bench</h3><ul>
<li>语法：<code>scrapy bench</code></li>
<li>全局命令</li>
</ul>
<p>运行 benchmark 测试。Benchmarking。</p>
<h2 id="自定义项目命令"><a href="#自定义项目命令" class="headerlink" title="自定义项目命令"></a>自定义项目命令</h2><p>您也可以通过 COMMANDS_MODULE 来添加您自己的项目命令。您可以以 scrapy/commands 中 Scrapy commands 为例来了解如何实现您的命令。</p>
<h2 id="COMMANDS-MODULE"><a href="#COMMANDS-MODULE" class="headerlink" title="COMMANDS_MODULE"></a>COMMANDS_MODULE</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Default: &apos;&apos; (empty string)</div></pre></td></tr></table></figure>
<p>用于查找添加自定义 Scrapy 命令的模块。 例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">COMMANDS_MODULE = &apos;mybot.commands&apos;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法示例</title>
    <link href="http://yoursite.com/2017/04/16/xpath-example/"/>
    <id>http://yoursite.com/2017/04/16/xpath-example/</id>
    <published>2017-04-16T12:18:54.000Z</published>
    <updated>2017-04-16T10:26:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scrapy-中-XPath-获取相应内容"><a href="#Scrapy-中-XPath-获取相应内容" class="headerlink" title="Scrapy 中 XPath 获取相应内容"></a><strong>Scrapy 中 XPath 获取相应内容</strong></h2><p>为了方便调试，在终端下输入以下命令进入Scrapy shell：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy shell &apos;http://blog.jobbole.com/110287&apos;</div></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="获取标题"><a href="#获取标题" class="headerlink" title="获取标题"></a><strong>获取标题</strong></h3><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1feomdj05qkj31aw03yq47.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//div[@class=&apos;entry-header&apos;]/h1/text()&quot;).extract()[0]</div><div class="line">&apos;2016 腾讯软件开发面试题（部分）&apos;</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//*[@id=&quot;post-110287&quot;]/div[1]/h1/text()&apos;).extract()[0]</div><div class="line">&apos;2016 腾讯软件开发面试题（部分）&apos;</div></pre></td></tr></table></figure>
<p>以上两种方法都可以得到文章标题，第一种方法通过标题class的属性得到，第二种方法通过确定id，然后通过列表切片得到标题字符串。</p>
<h3 id="获得文章发布时间"><a href="#获得文章发布时间" class="headerlink" title="获得文章发布时间"></a><strong>获得文章发布时间</strong></h3><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1feomdvqxpxj30hs01g3yo.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0]</div><div class="line">&apos;\r\n\r\n            2017/02/18 ·  &apos;</div><div class="line">&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0].strip().replace(&apos;·&apos;, &apos;&apos;).strip()</div><div class="line">&apos;2017/02/18&apos;</div></pre></td></tr></table></figure>
<p>第一条命令只能获取p标签中的内容，还需要对获取的数据用 <code>strip()</code> 和 <code>replace()</code> 方法进行清洗。</p>
<h3 id="获取点赞数、收藏数、评论数"><a href="#获取点赞数、收藏数、评论数" class="headerlink" title="获取点赞数、收藏数、评论数"></a><strong>获取点赞数</strong>、收藏数、评论数</h3><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1feomjmf5h8j30gi03at8w.jpg" alt=""></p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1feomjq25ixj31eg0cq77q.jpg" alt=""></p>
<p>对于含有多个属性的class如：<code>class=&quot; btn-bluet-bigger href-style vote-post-up   register-user-only &quot;</code>，若只使用其中的一个属性得到值，可以使用<code>contains</code>。</p>
<h4 id="获取点赞数"><a href="#获取点赞数" class="headerlink" title="获取点赞数"></a><strong>获取点赞数</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; int(response.xpath(&quot;//span[contains(@class, &apos;vote-post-up&apos;)]/h10/text()&quot;).extract()[0])</div><div class="line">2</div></pre></td></tr></table></figure>
<h4 id="获取收藏数、评论数"><a href="#获取收藏数、评论数" class="headerlink" title="获取收藏数、评论数"></a><strong>获取收藏数、评论数</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//span[contains(@class, &apos;bookmark-btn&apos;)]/text()&quot;).extract()[0]</div><div class="line">&apos; 23 收藏&apos;</div></pre></td></tr></table></figure>
<p>得到的内容为’ 23 收藏’，需要使用正则表达式进行数据清洗。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fav_nums = response.xpath(<span class="string">"//span[contains(@class, 'bookmark-btn')]/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	fav_nums = int(match_re.group(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>同样的，评论数的获取也需要正则表达式的帮忙。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">comment_nums = response.xpath(<span class="string">"//a[@href='#article-comment']/span/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	comment_nums = int(match_re.group(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h3 id="获取正文"><a href="#获取正文" class="headerlink" title="获取正文"></a><strong>获取正文</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">content = response.xpath(<span class="string">"//div[@class='entry']"</span>).extract()[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<h3 id="获取tags"><a href="#获取tags" class="headerlink" title="获取tags"></a><strong>获取tags</strong></h3><p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1feooy0mzgpj30bi01ydfw.jpg" alt=""></p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1feooya6zapj30x2072jt4.jpg" alt=""></p>
<p>所有的tag都在a标签下，类似获得日期的方式，增加一个a标签路径即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/a/text()&quot;).extract()</div><div class="line">[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;]</div></pre></td></tr></table></figure>
<p>现在需要对数据进行清洗，去除评论标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tag_list = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/a/text()"</span>).extract()</div><div class="line">tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">tags = <span class="string">","</span>.join(tag_list)</div></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>最后我们构造的spider文件如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"jobbole"</span></div><div class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</div><div class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287'</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        title = response.xpath(<span class="string">"//div[@class='entry-header']/h1/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">        create_date = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/text()"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,</div><div class="line">                                                                                                                          <span class="string">""</span>).strip()</div><div class="line">        praise_nums = int(response.xpath(<span class="string">"//span[contains(@class, 'vote-post-up')]/h10/text()"</span>).extract()[<span class="number">0</span>])</div><div class="line">        fav_nums = response.xpath(<span class="string">"//span[contains(@class, 'bookmark-btn')]/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">        match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">        <span class="keyword">if</span> match_re:</div><div class="line">            fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">        comment_nums = response.xpath(<span class="string">"//a[@href='#article-comment']/span/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">        match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">        <span class="keyword">if</span> match_re:</div><div class="line">            comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">        content = response.xpath(<span class="string">"//div[@class='entry']"</span>).extract()[<span class="number">0</span>]</div><div class="line">        tag_list = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/a/text()"</span>).extract()</div><div class="line">        tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">        tags = <span class="string">","</span>.join(tag_list)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Scrapy-中-XPath-获取相应内容&quot;&gt;&lt;a href=&quot;#Scrapy-中-XPath-获取相应内容&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 中 XPath 获取相应内容&quot;&gt;&lt;/a&gt;&lt;strong&gt;Scrapy 中 XPath 获取相应内容&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;为了方便调试，在终端下输入以下命令进入Scrapy shell：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;scrapy shell &amp;apos;http://blog.jobbole.com/110287&amp;apos;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="XPath" scheme="http://yoursite.com/categories/XPath/"/>
    
    
      <category term="Scrapy，XPath，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CXPath%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy简单入门</title>
    <link href="http://yoursite.com/2017/04/16/scrapy-simple-intro/"/>
    <id>http://yoursite.com/2017/04/16/scrapy-simple-intro/</id>
    <published>2017-04-16T06:18:54.000Z</published>
    <updated>2017-04-16T07:13:21.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a><strong>创建项目</strong></h2><p>开始爬取前，首先需要创建一个新的Scrapy项目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject tutorial</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>该命令将会创建包含下列内容的 tutorial 目录:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">tutorial/</div><div class="line">  scrapy.cfg </div><div class="line">  tutorial/ </div><div class="line">    __init__.py </div><div class="line">    items.py </div><div class="line">    pipelines.py</div><div class="line">    settings.py </div><div class="line">    spiders/ </div><div class="line">      __init__.py </div><div class="line">      ...</div></pre></td></tr></table></figure>
<p>这些文件分别是:</p>
<ul>
<li>scrapy.cfg：项目的配置文件</li>
<li>tutorial/：该项目的 python 模块，之后我们将在此加入代码。 </li>
<li>tutorial/items.py：项目中的 item 文件。 </li>
<li>tutorial/pipelines.py：项目中的 pipelines 文件。 </li>
<li>tutorial/settings.py：项目的设置文件。 </li>
<li>tutorial/spiders/：放置 spider 代码的目录。</li>
</ul>
<h2 id="定义-Item"><a href="#定义-Item" class="headerlink" title="定义 Item"></a><strong>定义 Item</strong></h2><p>Item 是保存爬取到的数据的容器；其使用方法和 python 字典类似， 并且提供了额外保护机制来避免拼写错误导 致的未定义字段错误。</p>
<p>类似在 ORM 中做的一样，您可以通过创建一个<code>scrapy.Item</code>类， 并且定义类型为<code>scrapy.Field</code>的类属性来定义一个 Item。 (如果不了解 ORM, 不用担心，您会发现这个步骤非常简单)</p>
<p>首先根据需要从 dmoz.org 获取到的数据对 item 进行建模。 我们需要从 dmoz 中获取名字，url，以及网站的描 述。 对此，在 item 中定义相应的字段。编辑<code>tutorial</code>目录中的<code>items.py</code>文件:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozItem</span><span class="params">(scrapy.Item)</span>:</span> </div><div class="line">    title = scrapy.Field() </div><div class="line">    link = scrapy.Field() </div><div class="line">    desc = scrapy.Field()</div></pre></td></tr></table></figure>
<p>可能一开始这有些复杂，但是通过定义 item， 我们可以很方便的使用 Scrapy 的其他方法，而这些方法需要知道我们的 item 的定义。</p>
<h2 id="编写第一个爬虫"><a href="#编写第一个爬虫" class="headerlink" title="编写第一个爬虫"></a><strong>编写第一个爬虫</strong></h2><p>Spider 是用户编写用于从单个网站(或者一些网站)爬取数据的类。 </p>
<p>其包含了一个用于下载的初始 URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方 法。 </p>
<p>为了创建一个 Spider，我们必须继承<code>scrapy.Spider</code>类， 且定义以下三个属性: </p>
<ul>
<li><code>name</code> : 用于区别 Spider。 该名字必须是唯一的，您不可以为不同的 Spider 设定相同的名字。</li>
<li><code>start_urls</code> : 包含了 Spider 在启动时进行爬取的 url 列表。 因此，第一个被获取到的页面将是其中之一。 后续的 URL 则从初始的 URL 获取到的数据中提取。 </li>
<li><code>parse()</code> : spider 的一个方法。 被调用时，每个初始 URL 完成下载后生成的<code>Response</code>对象将会作为 唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成 item)以及生成需 要进一步处理的 URL 的<code>Request</code>对象。 </li>
</ul>
<p>以下为我们的第一个 Spider 代码，保存在<code>tutorial/spiders</code>目录下的<code>dmoz_spider.py</code>文件中:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">      <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">      <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span>  </div><div class="line">    ]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        filename = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>] </div><div class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f: </div><div class="line">            f.write(response.body)</div></pre></td></tr></table></figure>
<p>其中，allow_domains是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页。</p>
<h3 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a><strong>爬取</strong></h3><p>进入项目的根目录，执行以下命令启动spider</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl dmoz</div></pre></td></tr></table></figure>
<p><code>crawl dmoz</code>启动用于爬取<code>dmoz.org</code>的 spider，可以得到如下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">2017-04-15 21:51:39 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)</div><div class="line">2017-04-15 21:51:39 [scrapy.utils.log] INFO: Overridden settings: &#123;&apos;BOT_NAME&apos;: &apos;tutorial&apos;, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;]&#125;</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled extensions:</div><div class="line">[&apos;scrapy.extensions.corestats.CoreStats&apos;,</div><div class="line"> &apos;scrapy.extensions.logstats.LogStats&apos;,</div><div class="line"> &apos;scrapy.extensions.telnet.TelnetConsole&apos;]</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:</div><div class="line">[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:</div><div class="line">[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled item pipelines:</div><div class="line">[]</div><div class="line">2017-04-15 21:51:39 [scrapy.core.engine] INFO: Spider opened</div><div class="line">2017-04-15 21:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</div><div class="line">2017-04-15 21:51:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)</div><div class="line">2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;: HTTP status code is not handled or not allowed</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)</div><div class="line">2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt;: HTTP status code is not handled or not allowed</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] INFO: Closing spider (finished)</div><div class="line">2017-04-15 21:51:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</div><div class="line">&#123;&apos;downloader/request_bytes&apos;: 734,</div><div class="line"> &apos;downloader/request_count&apos;: 3,</div><div class="line"> &apos;downloader/request_method_count/GET&apos;: 3,</div><div class="line"> &apos;downloader/response_bytes&apos;: 3525,</div><div class="line"> &apos;downloader/response_count&apos;: 3,</div><div class="line"> &apos;downloader/response_status_count/403&apos;: 3,</div><div class="line"> &apos;finish_reason&apos;: &apos;finished&apos;,</div><div class="line"> &apos;finish_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 41, 968931),</div><div class="line"> &apos;log_count/DEBUG&apos;: 4,</div><div class="line"> &apos;log_count/INFO&apos;: 9,</div><div class="line"> &apos;response_received_count&apos;: 3,</div><div class="line"> &apos;scheduler/dequeued&apos;: 2,</div><div class="line"> &apos;scheduler/dequeued/memory&apos;: 2,</div><div class="line"> &apos;scheduler/enqueued&apos;: 2,</div><div class="line"> &apos;scheduler/enqueued/memory&apos;: 2,</div><div class="line"> &apos;start_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 39, 764494)&#125;</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] INFO: Spider closed (finished)</div></pre></td></tr></table></figure>
<p>查看包含<code>[dmoz]</code>的输出，可以看到输出的 log 中包含定义在<code>start_urls</code>的初始 URL，并且与 spider 中是一 一对应的。在 log 中可以看到其没有指向其他页面( (<code>referer:None</code>) )。 除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含 url 所对应的内容的文件被创建 了: Book，Resources 。</p>
<h3 id="发生了什么？"><a href="#发生了什么？" class="headerlink" title="发生了什么？"></a><strong>发生了什么？</strong></h3><p>Scrapy 为 Spider 的<code>start_urls</code>属性中的每个 URL 创建了<code>scrapy.Request</code>对象，并将<code>parse</code>方法作为回调函数(callback)赋值给了<code>Request</code>。 <code>Request</code>对象经过调度，执行生成<code>scrapy.http.Response</code>对象并送回给<code>spider parse()</code>方法。</p>
<h3 id="提取-Item"><a href="#提取-Item" class="headerlink" title="提取 Item"></a><strong>提取 Item</strong></h3><h4 id="Selectors-选择器简介"><a href="#Selectors-选择器简介" class="headerlink" title="Selectors 选择器简介"></a><strong>Selectors 选择器简介</strong></h4><p>从网页中提取数据有很多方法。Scrapy 使用了一种基于 XPath 和 CSS 表达式机制: <code>Scrapy Selectors</code>。关于 selector 和其他提取机制的信息请参考<code>Selector</code>文档。 </p>
<p>关于Xpath的简单使用方法，可以查看之前的一篇博客<a href="http://lawtech0902.com/2017/04/11/xpath-usage/" target="_blank" rel="external">Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法</a></p>
<p>为了配合 XPath，Scrapy 除了提供了<code>Selector</code>之外，还提供了方法来避免每次从 response 中提取数据时生成 selector 的麻烦。</p>
<p>Selector 有四个基本的方法：</p>
<ul>
<li><code>xpath()</code>：传入 xpath 表达式，返回该表达式所对应的所有节点的 selector list 列表 。</li>
<li><code>css()</code>：传入 CSS 表达式，返回该表达式所对应的所有节点的 selector list 列表。</li>
<li><code>extract()</code>：序列化该节点为 unicode 字符串并返回 list。</li>
<li><code>re()</code>：根据传入的正则表达式对数据进行提取，返回 unicode 字符串 list 列表。</li>
</ul>
<h4 id="在-Shell-中尝试-Selector-选择器"><a href="#在-Shell-中尝试-Selector-选择器" class="headerlink" title="在 Shell 中尝试 Selector 选择器"></a><strong>在 Shell 中尝试 Selector 选择器</strong></h4><p>为了介绍 Selector 的使用方法，接下来我们将要使用内置的 Scrapy shell。Scrapy Shell 需要我们预装好 IPython(一个扩展的 Python 终端)。 我们需要进入项目的根目录，执行下列命令来启动 shell:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</div></pre></td></tr></table></figure>
<p>Shell 的输出类似于：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2017-04-15 22:04:22 [scrapy.core.engine] INFO: Spider opened</div><div class="line">2017-04-15 22:04:23 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)</div><div class="line">2017-04-15 22:04:24 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)</div><div class="line">[s] Available Scrapy objects:</div><div class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</div><div class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x109728ac8&gt;</div><div class="line">[s]   item       &#123;&#125;</div><div class="line">[s]   request    &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;</div><div class="line">[s]   response   &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;</div><div class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x10a2a0a58&gt;</div><div class="line">[s]   spider     &lt;DefaultSpider &apos;default&apos; at 0x10a4dc3c8&gt;</div><div class="line">[s] Useful shortcuts:</div><div class="line">[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)</div><div class="line">[s]   fetch(req)                  Fetch a scrapy.Request and update local objects</div><div class="line">[s]   shelp()           Shell help (print this help)</div><div class="line">[s]   view(response)    View response in a browser</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>当 shell 载入后，我们将得到一个包含 response 数据的本地 response 变量。输入 response.body 将输出 resp onse 的包体，输出 response.headers 可以看到 response 的包头。 </p>
<p>更为重要的是，当输入 response.selector 时， 我们将获取到一个可以用于查询返回数据的 selector(选择器)， 以及映射到 response.selector.xpath() 、response.selector.css() 的 快捷方法(shortcut): response.xpat h() 和 response.css() 。 </p>
<p>下面就来试试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title&apos;)</div><div class="line">[&lt;Selector xpath=&apos;//title&apos; data=&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;&gt;]</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title&apos;).extract()</div><div class="line">[&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;]</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;)</div><div class="line">[&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;DMOZ&apos;&gt;]</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;).extract()</div><div class="line">[&apos;DMOZ&apos;]</div></pre></td></tr></table></figure>
<h4 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a><strong>提取数据</strong></h4><p>现在，我们来尝试从这些页面中提取些有用的数据。 </p>
<p>我们可以在终端中输入 response.body 来观察 HTML 源码并确定合适的 XPath 表达式。不过，这任务非常无聊且不易。您可以考虑使用 Firefox 的 Firebug 扩展来使得工作更为轻松。</p>
<p>在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。 </ul></p>
<p>我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:</li></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li&apos;)</div></pre></td></tr></table></figure>
<p>网站的描述：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li/text()&apos;).extract()</div></pre></td></tr></table></figure>
<p>网站的标题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li/a/text()&apos;).extract()</div></pre></td></tr></table></figure>
<p>以及网站的链接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li/a/@href&apos;).extract()</div></pre></td></tr></table></figure>
<p>之前提到过，每个 <code>.xpath()</code> 调用返回 selector 组成的 list，因此我们可以拼接更多的  <code>.xpath()</code> 来进一步获取某个节点。我们将在下边使用这样的特性:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> response <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</div><div class="line">	title = response.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">    link = response.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">    desc = response.xpath(<span class="string">'text()'</span>).extract()</div><div class="line">    print(title, link, desc)</div></pre></td></tr></table></figure>
<p>在我们的 spider 中加入如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [      	     <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> response <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</div><div class="line">            title = response.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">            link = response.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">            desc = response.xpath(<span class="string">'text()'</span>).extract()</div><div class="line">            print(title, link, desc)</div></pre></td></tr></table></figure>
<p>现在尝试再次爬取 dmoz.org，您将看到爬取到的网站信息被成功输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl dmoz</div></pre></td></tr></table></figure>
<h3 id="使用-Item"><a href="#使用-Item" class="headerlink" title="使用 Item"></a><strong>使用 Item</strong></h3><p>Item 对象是自定义的 python 字典。您可以使用标准的字典语法来获取到其每个字段的值。(字段就是我们之前用 Field 赋值的属性):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; item = DmozItem() </div><div class="line">&gt;&gt;&gt; item[&apos;title&apos;] = &apos;Example title&apos; </div><div class="line">&gt;&gt;&gt; item[&apos;title&apos;] </div><div class="line">&apos;Example title&apos;</div></pre></td></tr></table></figure>
<p>一般来说，Spider 将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> response <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</div><div class="line">            item = DmozItem()</div><div class="line">            item[<span class="string">'title'</span>] = response.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">            item[<span class="string">'link'</span>] = response.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">            item[<span class="string">'desc'</span>] = response.xpath(<span class="string">'text()'</span>).extract()</div><div class="line">            <span class="keyword">yield</span> item</div></pre></td></tr></table></figure>
<p>现在对 dmoz.org 进行爬取将会产生 <code>DmozItem</code> 对象:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author webs &apos;link&apos;: [u&apos;http://gnosis.cx/TPiP/&apos;], &apos;title&apos;: [u&apos;Text Processing in Python&apos;]&#125; [dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applic &apos;link&apos;: [u&apos;http://www.informit.com/store/product.aspx?isbn=0130211192&apos;], &apos;title&apos;: [u&apos;XML Processing with Python&apos;]&#125;</div></pre></td></tr></table></figure>
<h2 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a><strong>保存爬取到的数据</strong></h2><p>最简单存储爬取的数据的方式是使用 <code>Feed exports</code> :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl dmoz -o items.json</div></pre></td></tr></table></figure>
<p>该命令将采用 JSON 格式对爬取的数据进行序列化，生成 <code>items.json</code> 文件。</p>
<p>在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的 item 做更多更为复杂的 操作，您可以编写 <code>Item Pipeline</code> 。 类似于我们在创建项目时对 Item 做的，用于您编写自己的 <code>tutorial/pipelines.py</code> 也被创建。 不过如果您仅仅想要保存 item，您不需要实现任何的 pipeline。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;创建项目&quot;&gt;&lt;a href=&quot;#创建项目&quot; class=&quot;headerlink&quot; title=&quot;创建项目&quot;&gt;&lt;/a&gt;&lt;strong&gt;创建项目&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;开始爬取前，首先需要创建一个新的Scrapy项目&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;scrapy startproject tutorial&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>python3+Django配置mysql连接</title>
    <link href="http://yoursite.com/2017/04/14/django-py3-mysql/"/>
    <id>http://yoursite.com/2017/04/14/django-py3-mysql/</id>
    <published>2017-04-14T07:18:54.000Z</published>
    <updated>2017-04-14T07:24:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前一直使用的是Django1.9和Python2.7，现在使用Python3和Django1.10，发现Mysql-Python一直无法安装</p>
<p><img src="http://ww4.sinaimg.cn/large/006tKfTcgy1fem85h0fdlj30wi0p4gqc.jpg" alt=""></p>
<p>这是因为mysql官网上的版本只支持Python3.4的数据库驱动，所以Python3.5是安装不上相应的驱动的，可以使用pymysql。在虚拟环境下pip install pymysql就可以了，然后在项目目录下的<code>__init__.py</code>文件中添加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymysql</div><div class="line">pymysql.install_as_mysqldb()</div></pre></td></tr></table></figure>
<p>就可以代替Django默认使用的<code>MySQLdb</code>了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前一直使用的是Django1.9和Python2.7，现在使用Python3和Django1.10，发现Mysql-Python一直无法安装&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://ww4.sinaimg.cn/large/006tKfTcgy1fem85h0fd
    
    </summary>
    
      <category term="Django" scheme="http://yoursite.com/categories/Django/"/>
    
    
      <category term="Django，Mysql，Python" scheme="http://yoursite.com/tags/Django%EF%BC%8CMysql%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Python字符编码</title>
    <link href="http://yoursite.com/2017/04/12/python-str-encode/"/>
    <id>http://yoursite.com/2017/04/12/python-str-encode/</id>
    <published>2017-04-12T12:18:54.000Z</published>
    <updated>2017-04-12T07:05:23.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python字符编码"><a href="#Python字符编码" class="headerlink" title="Python字符编码"></a><strong>Python字符编码</strong></h2><p>字符编码是计算机编程中不可回避的问题，不管你用 Python2 还是 Python3，亦或是 C++, Java 等，我都觉得非常有必要理清计算机中的字符编码概念。</p>
<a id="more"></a>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a><strong>基本概念</strong></h3><ul>
<li><p>字符（Character）</p>
<p>在电脑和电信领域中，<strong>字符是一个信息单位，它是各种文字和符号的总称</strong>，包括各国家文字、标点符号、图形符号、数字等。比如，一个汉字，一个英文字母，一个标点符号等都是一个字符。</p>
</li>
<li><p>字符集（Character set）</p>
<p><strong>字符集是字符的集合</strong>。字符集的种类较多，每个字符集包含的字符个数也不同。比如，常见的字符集有 ASCII 字符集、GB2312 字符集、Unicode 字符集等，其中，ASCII 字符集共有 128 个字符，包含可显示字符（比如英文大小写字符、阿拉伯数字）和控制字符（比如空格键、回车键）；GB2312 字符集是中国国家标准的简体中文字符集，包含简化汉字、一般符号、数字等；Unicode 字符集则包含了世界各国语言中使用到的所有字符。</p>
</li>
<li><p>字符编码（Character encoding）</p>
<p><strong>字符编码，是指对于字符集中的字符，将其编码为特定的二进制数</strong>，以便计算机处理。常见的字符编码有 ASCII 编码，UTF-8 编码，GBK 编码等。一般而言，<strong>字符集</strong>和<strong>字符编码</strong>往往被认为是同义的概念，比如，对于字符集 ASCII，它除了有「字符的集合」这层含义外，同时也包含了「编码」的含义，也就是说，<strong>ASCII 既表示了字符集也表示了对应的字符编码</strong>。</p>
</li>
</ul>
<p>下面我们用一个表格做下总结：</p>
<table>
<thead>
<tr>
<th style="text-align:left">概念</th>
<th style="text-align:left">描述</th>
<th style="text-align:left">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">字符</td>
<td style="text-align:left">一个信息单位，各种文字和符号的总称</td>
<td style="text-align:left">‘中’, ‘a’, ‘1’, ‘$’, ‘￥’, …</td>
</tr>
<tr>
<td style="text-align:left">字符集</td>
<td style="text-align:left">字符的集合</td>
<td style="text-align:left">ASCII 字符集, GB2312 字符集, Unicode 字符集</td>
</tr>
<tr>
<td style="text-align:left">字符编码</td>
<td style="text-align:left">将字符集中的字符，编码为特定的二进制数</td>
<td style="text-align:left">ASCII 编码，GB2312 编码，Unicode 编码</td>
</tr>
<tr>
<td style="text-align:left">字节</td>
<td style="text-align:left">计算机中存储数据的单元，一个 8 位（bit）的二进制数</td>
<td style="text-align:left">0x01, 0x45, …</td>
</tr>
</tbody>
</table>
<h3 id="常见字符编码简介"><a href="#常见字符编码简介" class="headerlink" title="常见字符编码简介"></a><strong>常见字符编码简介</strong></h3><p>常见的字符编码有 ASCII 编码，GBK 编码，Unicode 编码和 UTF-8 编码等等。这里，我们主要介绍 ASCII、Unicode 和 UTF-8。</p>
<ul>
<li><p>ASCII</p>
<p>计算机是在美国诞生的，人家用的是英语，而在英语的世界里，不过就是英文字母，数字和一些普通符号的组合而已。</p>
<p>在 20 世纪 60 年代，美国制定了一套字符编码方案，规定了英文字母，数字和一些普通符号跟二进制的转换关系，被称为 ASCII (American Standard Code for Information Interchange，美国信息互换标准编码) 码。</p>
<p>比如，大写英文字母 A 的二进制表示是 01000001（十进制 65），小写英文字母 a 的二进制表示是 01100001 （十进制 97），空格 SPACE 的二进制表示是 00100000（十进制 32）。</p>
</li>
<li><p>Unicode</p>
<p>ASCII 码只规定了 128 个字符的编码，这在美国是够用的。可是，计算机后来传到了欧洲，亚洲，乃至世界各地，而世界各国的语言几乎是完全不一样的，用 ASCII 码来表示其他语言是远远不够的，所以，不同的国家和地区又制定了自己的编码方案，比如中国大陆的 GB2312 编码 和 GBK 编码等，日本的 Shift_JIS 编码等等。</p>
<p>虽然各个国家和地区可以制定自己的编码方案，但不同国家和地区的计算机在数据传输的过程中就会出现各种各样的乱码（mojibake），这无疑是个灾难。</p>
<p>怎么办？想法也很简单，就是将全世界所有的语言统一成一套编码方案，这套编码方案就叫 Unicode，<strong>它为每种语言的每个字符设定了独一无二的二进制编码</strong>，这样就可以跨语言，跨平台进行文本处理了，是不是很棒！</p>
<p>Unicode 1.0 版诞生于 1991 年 10 月，至今它仍在不断增修，每个新版本都会加入更多新的字符，目前最新的版本为 2016 年 6 月 21 日公布的 9.0.0。</p>
<p>Unicode 标准使用十六进制数字，而且在数字前面加上前缀 U+，比如，大写字母「A」的 unicode 编码为 U+0041，汉字「严」的 unicode 编码为 U+4E25。更多的符号对应表，可以查询 unicode.org，或者专门的汉字对应表。</p>
</li>
<li><p>UTF-8</p>
<p>Unicode 看起来已经很完美了，实现了大一统。但是，Unicode 却存在一个很大的问题：资源浪费。</p>
<p>为什么这么说呢？原来，Unicode 为了能表示世界各国所有文字，一开始用两个字节，后来发现两个字节不够用，又用了四个字节。比如，汉字「严」的 unicode 编码是十六进制数 <code>4E25</code>，转换成二进制有十五位，即 100111000100101，因此至少需要两个字节才能表示这个汉字，但是对于其他的字符，就可能需要三个或四个字节，甚至更多。</p>
<p>这时，问题就来了，如果以前的 ASCII 字符集也用这种方式来表示，那岂不是很浪费存储空间。比如，大写字母「A」的二进制编码为 01000001，它只需要一个字节就够了，如果 unicode 统一使用三个字节或四个字节来表示字符，那「A」的二进制编码的前面几个字节就都是 <code>0</code>，这是很浪费存储空间的。</p>
<p>为了解决这个问题，在 Unicode 的基础上，人们实现了 UTF-16, UTF-32 和 UTF-8。下面只说一下 UTF-8。</p>
<p>UTF-8 (8-bit Unicode Transformation Format) 是一种针对 Unicode 的可变长度字符编码，它使用一到四个字节来表示字符，例如，ASCII 字符继续使用一个字节编码，阿拉伯文、希腊文等使用两个字节编码，常用汉字使用三个字节编码，等等。</p>
<p>因此，我们说，UTF-8 是 Unicode 的实现方式之一，其他实现方式还包括 UTF-16（字符用两个或四个字节表示）和 UTF-32（字符用四个字节表示）。</p>
</li>
</ul>
<h3 id="Python的默认编码"><a href="#Python的默认编码" class="headerlink" title="Python的默认编码"></a><strong>Python的默认编码</strong></h3><p>Python2 的默认编码是 ascii，Python3 的默认编码是 utf-8，可以通过下面的方式获取：</p>
<ul>
<li><p>Python2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Python 2.7.12 (default, Oct 11 2016, 05:20:59)</div><div class="line">[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwin</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; import sys</div><div class="line">&gt;&gt;&gt; sys.getdefaultencoding()</div><div class="line">&apos;ascii&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>Python3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Python 3.5.2 (default, Oct 11 2016, 04:59:56)</div><div class="line">[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwin</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; import sys</div><div class="line">&gt;&gt;&gt; sys.getdefaultencoding()</div><div class="line">&apos;utf-8&apos;</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Python3编码问题"><a href="#Python3编码问题" class="headerlink" title="Python3编码问题"></a><strong>Python3编码问题</strong></h3><p>Python3 最重要的一项改进之一就是解决了 Python2 中字符串与字符编码遗留下来的这个大坑。</p>
<p>Python2 字符串设计上的一些缺陷：</p>
<ul>
<li>使用 ASCII 码作为默认编码方式，对中文处理很不友好。 </li>
<li>把字符串的牵强地分为 unicode 和 str 两种类型，误导开发者</li>
</ul>
<p>首先，Python3 把系统默认编码设置为 UTF-8，然后，文本字符和二进制数据区分得更清晰，分别用 str 和 bytes 表示。文本字符全部用 str 类型表示，str 能表示 Unicode 字符集中所有字符，而二进制字节数据用一种全新的数据类型，用 bytes 来表示。</p>
<h4 id="str"><a href="#str" class="headerlink" title="str"></a><strong>str</strong></h4><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = "a"</div><div class="line">&gt;&gt;&gt; a</div><div class="line">'a'</div><div class="line">&gt;&gt;&gt; type(a)</div><div class="line">&lt;class 'str'&gt;</div><div class="line">&gt;&gt;&gt; b = "禅"</div><div class="line">&gt;&gt;&gt; b</div><div class="line">'禅'</div><div class="line">&gt;&gt;&gt; type(b)</div><div class="line">&lt;class 'str'&gt;</div></pre></td></tr></table></figure>
<h4 id="bytes"><a href="#bytes" class="headerlink" title="bytes"></a><strong>bytes</strong></h4><p>Python3 中，在字符引号前加‘b’，明确表示这是一个 bytes 类型的对象，实际上它就是一组二进制字节序列组成的数据，bytes 类型可以是 ASCII范围内的字符和其它十六进制形式的字符数据，但不能用中文等非ASCII字符表示。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; c = b'a'</div><div class="line">&gt;&gt;&gt; c</div><div class="line">b'a'</div><div class="line">&gt;&gt;&gt; type(c)</div><div class="line">&lt;class 'bytes'&gt;</div><div class="line">&gt;&gt;&gt; d = b'\xe7\xa6\x85'</div><div class="line">&gt;&gt;&gt; d</div><div class="line">b'\xe7\xa6\x85'</div><div class="line">&gt;&gt;&gt; type(d)</div><div class="line">&lt;class 'bytes'&gt;</div><div class="line">&gt;&gt;&gt; e = b'禅'</div><div class="line">  File "&lt;stdin&gt;", line 1</div><div class="line">SyntaxError: bytes can only contain ASCII literal characters.</div></pre></td></tr></table></figure>
<p>bytes 类型提供的操作和 str 一样，支持分片、索引、基本数值运算等操作。但是 str 与 bytes 类型的数据不能执行 <code>+</code> 操作，尽管在py2中是可行的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'a'</span>+<span class="string">b'c'</span></div><div class="line"><span class="string">b'ac'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'a'</span>*<span class="number">2</span></div><div class="line"><span class="string">b'aa'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b"abcdef\xd6"</span>[<span class="number">1</span>:]</div><div class="line"><span class="string">b'bcdef\xd6'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b"abcdef\xd6"</span>[<span class="number">-1</span>]</div><div class="line"><span class="number">214</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b"a"</span> + <span class="string">"b"</span></div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">TypeError: can<span class="string">'t concat bytes to str</span></div></pre></td></tr></table></figure>
<p>python2 与 python3 字节与字符的对应关系</p>
<table>
<thead>
<tr>
<th>python2</th>
<th>python3</th>
<th>表现</th>
<th>转换</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>str</td>
<td>bytes</td>
<td>字节</td>
<td>encode</td>
<td>存储</td>
</tr>
<tr>
<td>unicode</td>
<td>str</td>
<td>字符</td>
<td>decode</td>
<td>显示</td>
</tr>
</tbody>
</table>
<h4 id="encode与decode"><a href="#encode与decode" class="headerlink" title="encode与decode"></a><strong>encode与decode</strong></h4><p>str 与 bytes 之间的转换可以用 encode 和 decode 方法。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tNc79gy1fejwfglr5uj316o0b4go4.jpg" alt=""></p>
<p>encode 负责字符到字节的编码转换。默认使用 UTF-8 编码转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">"Python之禅"</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s.encode()</div><div class="line"><span class="string">b'Python\xe4\xb9\x8b\xe7\xa6\x85'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s.encode(<span class="string">'gbk'</span>)</div><div class="line"><span class="string">b'Python\xd6\xae\xec\xf8'</span></div></pre></td></tr></table></figure>
<p>decode 负责字节到字符的解码转换，通用使用 UTF-8 编码格式进行转换。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; b&apos;Python\xe4\xb9\x8b\xe7\xa6\x85&apos;.decode()</div><div class="line">&apos;Python之禅&apos;</div><div class="line">&gt;&gt;&gt; b&apos;Python\xd6\xae\xec\xf8&apos;.decode(&quot;gbk&quot;)</div><div class="line">&apos;Python之禅&apos;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Python字符编码&quot;&gt;&lt;a href=&quot;#Python字符编码&quot; class=&quot;headerlink&quot; title=&quot;Python字符编码&quot;&gt;&lt;/a&gt;&lt;strong&gt;Python字符编码&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;字符编码是计算机编程中不可回避的问题，不管你用 Python2 还是 Python3，亦或是 C++, Java 等，我都觉得非常有必要理清计算机中的字符编码概念。&lt;/p&gt;
    
    </summary>
    
      <category term="Unicode" scheme="http://yoursite.com/categories/Unicode/"/>
    
    
      <category term="Scrapy，Unicode，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CUnicode%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Python正则表达式</title>
    <link href="http://yoursite.com/2017/04/11/regex/"/>
    <id>http://yoursite.com/2017/04/11/regex/</id>
    <published>2017-04-11T12:18:54.000Z</published>
    <updated>2017-04-11T12:10:21.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python正则表达式"><a href="#Python正则表达式" class="headerlink" title="Python正则表达式"></a><strong>Python正则表达式</strong></h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h3><p>正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。</p>
<p>Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。</p>
<p>re 模块使 Python 语言拥有全部的正则表达式功能。<br><a id="more"></a></p>
<p>compile 函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。</p>
<p>re 模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串做为它们的第一个参数。</p>
<h3 id="正则表达式模式（常用）"><a href="#正则表达式模式（常用）" class="headerlink" title="正则表达式模式（常用）"></a><strong>正则表达式模式（常用）</strong></h3><p>模式字符串使用特殊的语法来表示一个正则表达式：字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。多数字母和数字前加一个反斜杠时会拥有不同的含义。标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。反斜杠本身需要使用反斜杠转义。由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’/t’，等价于’//t’)匹配相应的特殊字符。</p>
<p>下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>^</td>
<td>匹配字符串的开头</td>
</tr>
<tr>
<td>$</td>
<td>匹配字符串的末尾</td>
</tr>
<tr>
<td>.</td>
<td>匹配任意字符，除了换行符”\n”，当re.DOTALL标记被指定时，可以匹配包含换行符的任意字符</td>
</tr>
<tr>
<td>[…]</td>
<td>用来表示一组字符，单独列出：[amk]匹配’a’，’m’或’k’</td>
</tr>
<tr>
<td>[^…]</td>
<td>不在[]中的字符： [ ^abc ]匹配除了a,b,c之外的字符</td>
</tr>
<tr>
<td>re*</td>
<td>匹配0个或多个的表达式</td>
</tr>
<tr>
<td>re+</td>
<td>匹配1个或多个的表达式</td>
</tr>
<tr>
<td>re?</td>
<td>匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式</td>
</tr>
<tr>
<td>re{ n,}</td>
<td>精确匹配n个前面表达式</td>
</tr>
<tr>
<td>re{n,m}</td>
<td>匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式</td>
</tr>
<tr>
<td>a &#124; b</td>
<td>匹配a或b</td>
</tr>
<tr>
<td>(re)</td>
<td>匹配括号内的表达式，也表示一个组</td>
</tr>
<tr>
<td>(?imx)</td>
<td>正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域</td>
</tr>
<tr>
<td>(?-imx)</td>
<td>正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域</td>
</tr>
<tr>
<td>\w</td>
<td>匹配字母数字及下划线</td>
</tr>
<tr>
<td>\W</td>
<td>匹配非字母数字及下划线</td>
</tr>
<tr>
<td>\s</td>
<td>匹配任意空白字符，等价于 [\t\n\r\f].</td>
</tr>
<tr>
<td>\S</td>
<td>匹配任意非空字符</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任意数字，等价于 [0-9].</td>
</tr>
<tr>
<td>\D</td>
<td>匹配任意非数字</td>
</tr>
<tr>
<td>\A</td>
<td>匹配字符串开始</td>
</tr>
<tr>
<td>\Z</td>
<td>匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串</td>
</tr>
<tr>
<td>\z</td>
<td>匹配字符串结束</td>
</tr>
<tr>
<td>\G</td>
<td>匹配最后匹配完成的位置</td>
</tr>
<tr>
<td>\b</td>
<td>匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’</td>
</tr>
<tr>
<td>\B</td>
<td>匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’</td>
</tr>
<tr>
<td>\n,\t,等</td>
<td>匹配一个换行符。匹配一个制表符。等</td>
</tr>
<tr>
<td>\1…\9</td>
<td>匹配第n个分组的子表达式。</td>
</tr>
<tr>
<td>\10</td>
<td>匹配第n个分组的子表达式，如果它经匹配。否则指的是八进制字符码的表达式</td>
</tr>
</tbody>
</table>
<h3 id="正则表达式实例"><a href="#正则表达式实例" class="headerlink" title="正则表达式实例"></a><strong>正则表达式实例</strong></h3><p>字符匹配</p>
<table>
<thead>
<tr>
<th>实例</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>python</td>
<td>匹配”python”</td>
</tr>
</tbody>
</table>
<p>字符类</p>
<table>
<thead>
<tr>
<th>实例</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>[Pp]ython</td>
<td>匹配 “Python” 或 “python”</td>
</tr>
<tr>
<td>rub[ye]</td>
<td>匹配 “ruby” 或 “rube”</td>
</tr>
<tr>
<td>[lawtech]</td>
<td>匹配中括号内的任意一个字母</td>
</tr>
<tr>
<td>[0-9]</td>
<td>匹配任何数字。类似于 [0123456789]</td>
</tr>
<tr>
<td>[a-z]</td>
<td>匹配任何小写字母</td>
</tr>
<tr>
<td>[A-Z]</td>
<td>匹配任何大写字母</td>
</tr>
<tr>
<td>[a-zA-Z0-9]</td>
<td>匹配任何字母及数字</td>
</tr>
<tr>
<td>[^lawtech]</td>
<td>除了lawtech字母以外的所有字符</td>
</tr>
<tr>
<td>[^0-9]</td>
<td>匹配除了数字外的字符</td>
</tr>
</tbody>
</table>
<p>特殊字符类</p>
<table>
<thead>
<tr>
<th>实例</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>.</td>
<td>匹配除 “\n” 之外的任何单个字符，要匹配包括 ‘\n’ 在内的任何字符，请使用像’[.\n]’ 的模式</td>
</tr>
<tr>
<td>\d</td>
<td>匹配一个数字字符，等价于 [0-9]</td>
</tr>
<tr>
<td>\D</td>
<td>匹配一个非数字字符，等价于 [ ^0-9 ]</td>
</tr>
<tr>
<td>\s</td>
<td>匹配任何空白字符，包括空格、制表符、换页符等等，等价于[\f\n\r\t\v]</td>
</tr>
<tr>
<td>\S</td>
<td>匹配任何非空白字符。等价于 [ ^\f\n\r\t\v ]</td>
</tr>
<tr>
<td>\w</td>
<td>匹配包括下划线的任何单词字符，等价于[A-Za-z0-9_]</td>
</tr>
<tr>
<td>\W</td>
<td>匹配任何非单词字符等价于 [ ^A-Za-z0-9_ ]</td>
</tr>
</tbody>
</table>
<h3 id="正则表达式修饰符-可选标志"><a href="#正则表达式修饰符-可选标志" class="headerlink" title="正则表达式修饰符 - 可选标志"></a>正则表达式修饰符 - 可选标志</h3><p>正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志：</p>
<table>
<thead>
<tr>
<th>修饰符</th>
<th style="text-align:left">描述</th>
<th>全拼</th>
</tr>
</thead>
<tbody>
<tr>
<td>re.I</td>
<td style="text-align:left">使匹配对大小写不敏感</td>
<td>IGNORECASE</td>
</tr>
<tr>
<td>re.L</td>
<td style="text-align:left">做本地化识别（locale-aware）匹配</td>
<td>LOCALE</td>
</tr>
<tr>
<td>re.M</td>
<td style="text-align:left">多行匹配，影响 ^ 和 $</td>
<td>MULTILINE</td>
</tr>
<tr>
<td>re.S</td>
<td style="text-align:left">使 . 匹配包括换行在内的所有字符</td>
<td>DOTALL</td>
</tr>
<tr>
<td>re.U</td>
<td style="text-align:left">根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.</td>
<td>UNICODE</td>
</tr>
<tr>
<td>re.X</td>
<td style="text-align:left">该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解</td>
<td>VERBOSE</td>
</tr>
</tbody>
</table>
<h3 id="re模块能够处理正则表达式的操作生成正则表达式对象"><a href="#re模块能够处理正则表达式的操作生成正则表达式对象" class="headerlink" title="re模块能够处理正则表达式的操作生成正则表达式对象"></a><strong>re模块能够处理正则表达式的操作生成正则表达式对象</strong></h3><ul>
<li><p>生成正则表达式对象 compile(pattern, flags=0)构建一个正则表达式，返回该正则表达式对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">pattern = re.compile(<span class="string">'re'</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>进行匹配</p>
<ul>
<li>match() 确定正则表达式是否匹配字符串的开头</li>
<li>search() 扫描字符串以查找匹配</li>
<li>findall() 找到所有正则表达式匹配的子字符串，并把它们作为一个列表返回</li>
<li>finditer() 找到所有正则表达式匹配的子字符串，并把它们以迭代器的形式返回</li>
<li>group() 返回通过正则表达式匹配到的字符串</li>
<li>start() 返回成功匹配开始位置</li>
<li>end()   返回成功匹配结束位置</li>
<li>span()  返回包含成功匹配开始和结束位置的元组</li>
</ul>
</li>
</ul>
<h4 id="re-match函数"><a href="#re-match函数" class="headerlink" title="re.match函数"></a><strong>re.match函数</strong></h4><p>re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回None。</p>
<p>函数语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.match(pattern, string, flags=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p>函数参数说明：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>pattern</td>
<td>匹配的正则表达式</td>
</tr>
<tr>
<td>string</td>
<td>要匹配的字符串</td>
</tr>
<tr>
<td>flags</td>
<td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等</td>
</tr>
</tbody>
</table>
<p>匹配成功re.match方法返回一个匹配的对象（match object），否则返回None。</p>
<p>我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。</p>
<table>
<thead>
<tr>
<th>匹配对象方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>group(num=0)</td>
<td>匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组</td>
</tr>
<tr>
<td>groups()</td>
<td>返回一个包含所有小组字符串的元组，从1到所含的小组号</td>
</tr>
</tbody>
</table>
<p>实例1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*- </span></div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">print(re.match(<span class="string">'www'</span>, <span class="string">'www.lawtech0902.com'</span>).span())  <span class="comment"># 在起始位置匹配</span></div><div class="line">print(re.match(<span class="string">'com'</span>, <span class="string">'www.lawtech0902.com'</span>)) <span class="comment"># 不在起始位置匹配</span></div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(<span class="number">0</span>, <span class="number">3</span>)</div><div class="line"><span class="keyword">None</span></div></pre></td></tr></table></figure>
<p>实例2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*- </span></div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">line = <span class="string">"Cats are smarter than dogs"</span></div><div class="line"></div><div class="line">matchObj = re.match( <span class="string">r'(.*) are (.*?) .*'</span>, line, re.M|re.I)</div><div class="line"></div><div class="line"><span class="keyword">if</span> matchObj:</div><div class="line">   print(<span class="string">"matchObj.group() : "</span>, matchObj.group())</div><div class="line">   print(<span class="string">"matchObj.group(1) : "</span>, matchObj.group(<span class="number">1</span>))</div><div class="line">   print(<span class="string">"matchObj.group(2) : "</span>, matchObj.group(<span class="number">2</span>))</div><div class="line"><span class="keyword">else</span>:</div><div class="line">   print(<span class="string">"No match!!"</span>)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">matchObj.group() :  Cats are smarter than dogs</div><div class="line">matchObj.group(<span class="number">1</span>) :  Cats</div><div class="line">matchObj.group(<span class="number">2</span>) :  smarter</div></pre></td></tr></table></figure>
<h4 id="re-search方法"><a href="#re-search方法" class="headerlink" title="re.search方法"></a><strong>re.search方法</strong></h4><p>re.search 扫描整个字符串并返回第一个成功的匹配。</p>
<p>函数语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.search(pattern, string, flags=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p>函数参数说明与re.match方法相同。</p>
<p>匹配成功re.search方法返回一个匹配的对象，否则返回None。</p>
<p>实例1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*- </span></div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">print(re.search(<span class="string">'www'</span>, <span class="string">'www.lawtech0902.com'</span>).span())  <span class="comment"># 在起始位置匹配</span></div><div class="line">print(re.search(<span class="string">'com'</span>, <span class="string">'www.lawtech0902.com'</span>).span())  <span class="comment"># 不在起始位置匹配</span></div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(<span class="number">0</span>, <span class="number">3</span>)</div><div class="line">(<span class="number">16</span>, <span class="number">19</span>)</div></pre></td></tr></table></figure>
<p>实例2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*- </span></div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">line = <span class="string">"Cats are smarter than dogs"</span></div><div class="line"></div><div class="line">searchObj = re.search( <span class="string">r'(.*) are (.*?) .*'</span>, line, re.M|re.I)</div><div class="line"></div><div class="line"><span class="keyword">if</span> searchObj:</div><div class="line">   print(<span class="string">"searchObj.group() : "</span>, searchObj.group())</div><div class="line">   print(<span class="string">"searchObj.group(1) : "</span>, searchObj.group(<span class="number">1</span>))</div><div class="line">   print(<span class="string">"searchObj.group(2) : "</span>, searchObj.group(<span class="number">2</span>))</div><div class="line"><span class="keyword">else</span>:</div><div class="line">   print(<span class="string">"Nothing found!!"</span>)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">searchObj.group() :  Cats are smarter than dogs</div><div class="line">searchObj.group(<span class="number">1</span>) :  Cats</div><div class="line">searchObj.group(<span class="number">2</span>) :  smarter</div></pre></td></tr></table></figure>
<h4 id="re-match与re-search的区别"><a href="#re-match与re-search的区别" class="headerlink" title="re.match与re.search的区别"></a>re.match与re.search的区别</h4><p>re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。</p>
<p>实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*- </span></div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">line = <span class="string">"Cats are smarter than dogs"</span></div><div class="line"></div><div class="line">matchObj = re.match( <span class="string">r'dogs'</span>, line, re.M|re.I)</div><div class="line"><span class="keyword">if</span> matchObj:</div><div class="line">   print(<span class="string">"match --&gt; matchObj.group() : "</span>, matchObj.group())</div><div class="line"><span class="keyword">else</span>:</div><div class="line">   print(<span class="string">"No match!!"</span>)</div><div class="line"></div><div class="line">matchObj = re.search( <span class="string">r'dogs'</span>, line, re.M|re.I)</div><div class="line"><span class="keyword">if</span> matchObj:</div><div class="line">   print(<span class="string">"search --&gt; matchObj.group() : "</span>, matchObj.group())</div><div class="line"><span class="keyword">else</span>:</div><div class="line">   print(<span class="string">"No match!!"</span>)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">No match!!</div><div class="line">search --&gt; matchObj.group() :  dogs</div></pre></td></tr></table></figure>
<h4 id="检索和替换"><a href="#检索和替换" class="headerlink" title="检索和替换"></a><strong>检索和替换</strong></h4><p>Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。</p>
<p>语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.sub(pattern, repl, string, count=<span class="number">0</span>, flags=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p>参数：</p>
<ul>
<li>pattern : 正则中的模式字符串。</li>
<li>repl : 替换的字符串，也可为一个函数。</li>
<li>string : 要被查找替换的原始字符串。</li>
<li>count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。</li>
</ul>
<p>实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line">phone = <span class="string">"2004-959-559 # 这是一个国外电话号码"</span></div><div class="line"></div><div class="line"><span class="comment"># 删除字符串中的 Python注释 </span></div><div class="line">num = re.sub(<span class="string">r'#.*$'</span>, <span class="string">""</span>, phone)</div><div class="line">print(<span class="string">"电话号码是: "</span>, num)</div><div class="line"></div><div class="line"><span class="comment"># 删除非数字(-)的字符串 </span></div><div class="line">num = re.sub(<span class="string">r'\D'</span>, <span class="string">""</span>, phone)</div><div class="line">print(<span class="string">"电话号码是 : "</span>, num)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">电话号码是:  <span class="number">2004</span><span class="number">-959</span><span class="number">-559</span></div><div class="line">电话号码是 :  <span class="number">2004959559</span></div></pre></td></tr></table></figure>
<h5 id="repl参数是一个函数"><a href="#repl参数是一个函数" class="headerlink" title="repl参数是一个函数"></a><strong>repl参数是一个函数</strong></h5><p>以下实例中将字符串中的匹配的数字乘于 2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="comment"># 将匹配的数字乘于 2</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">double</span><span class="params">(matched)</span>:</span></div><div class="line">    value = int(matched.group(<span class="string">'value'</span>))</div><div class="line">    <span class="keyword">return</span> str(value * <span class="number">2</span>)</div><div class="line"></div><div class="line">s = <span class="string">'A23G4HFD567'</span></div><div class="line">print(re.sub(<span class="string">'(?P&lt;value&gt;\d+)'</span>, double, s))</div></pre></td></tr></table></figure>
<p>运行结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">A46G8HFD1134</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Python正则表达式&quot;&gt;&lt;a href=&quot;#Python正则表达式&quot; class=&quot;headerlink&quot; title=&quot;Python正则表达式&quot;&gt;&lt;/a&gt;&lt;strong&gt;Python正则表达式&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。&lt;/p&gt;
&lt;p&gt;Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。&lt;/p&gt;
&lt;p&gt;re 模块使 Python 语言拥有全部的正则表达式功能。&lt;br&gt;
    
    </summary>
    
      <category term="Regular Expression" scheme="http://yoursite.com/categories/Regular-Expression/"/>
    
    
      <category term="Scrapy，Regular Expression，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRegular-Expression%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy安装及调试</title>
    <link href="http://yoursite.com/2017/04/11/scrapy-install/"/>
    <id>http://yoursite.com/2017/04/11/scrapy-install/</id>
    <published>2017-04-11T12:18:54.000Z</published>
    <updated>2017-04-11T07:48:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a><strong>环境搭建</strong></h2><a id="more"></a>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ mkvirtualenv article_spider</div><div class="line">$ pip install -i https://pypi.douban.com/simple/ scrapy</div></pre></td></tr></table></figure>
<h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a><strong>创建项目</strong></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ workon article_spider</div><div class="line">$ scrapy startproject ArticleSpider</div><div class="line">New Scrapy project &apos;ArticleSpider&apos;, using template directory &apos;/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/templates/project&apos;, created in:</div><div class="line">    /Users/lawtech/PycharmProjects/ArticleSpider</div><div class="line"></div><div class="line">You can start your first spider with:</div><div class="line">    cd ArticleSpider</div><div class="line">    scrapy genspider example example.com</div><div class="line">    </div><div class="line">$ cd ArticleSpider</div><div class="line">$ scrapy genspider jobbole blog.jobbole.com</div><div class="line">Created spider &apos;jobbole&apos; using template &apos;basic&apos; in module:</div><div class="line">  ArticleSpider.spiders.jobbole</div></pre></td></tr></table></figure>
<h2 id="项目目录介绍"><a href="#项目目录介绍" class="headerlink" title="项目目录介绍"></a><strong>项目目录介绍</strong></h2><p>首先先要回答一个问题。</p>
<p>问：把网站装进爬虫里，总共分几步？<br>答案很简单，四步：</p>
<ul>
<li>新建项目 (Project)：新建一个新的爬虫项目</li>
<li>明确目标（Items）：明确你想要抓取的目标</li>
<li>制作爬虫（Spider）：制作爬虫开始爬取网页</li>
<li>存储内容（Pipeline）：设计管道存储爬取内容</li>
</ul>
<p>创建好AticleSpider项目之后，可以看到将会创建一个AticleSpider文件夹，目录结构如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ArticleSpider/  </div><div class="line">    scrapy.cfg  </div><div class="line">    ArticleSpider/  </div><div class="line">        __init__.py  </div><div class="line">        items.py  </div><div class="line">        pipelines.py  </div><div class="line">        settings.py  </div><div class="line">        spiders/  </div><div class="line">            __init__.py  </div><div class="line">            jobbole.py</div></pre></td></tr></table></figure>
<p>下面来简单介绍一下各个文件的作用：</p>
<ul>
<li>scrapy.cfg：项目的配置文件</li>
<li>ArticleSpider/：项目的 Python 模块，将会从这里引用代码</li>
<li>ArticleSpider/items.py：项目的 items 文件</li>
<li>ArticleSpider/pipelines.py：项目的 pipelines 文件</li>
<li>ArticleSpider/settings.py：项目的设置文件</li>
<li>ArticleSpider/spiders/：存储爬虫的目录</li>
</ul>
<h2 id="pycharm-调试-scrapy-执行流程"><a href="#pycharm-调试-scrapy-执行流程" class="headerlink" title="pycharm 调试 scrapy 执行流程"></a><strong>pycharm 调试 scrapy 执行流程</strong></h2><p>在项目根目录下新建 main.py 文件，添加如下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line"><span class="comment"># __file__ 表示当前py文件</span></div><div class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</div><div class="line"><span class="comment"># 将实际命令拆分</span></div><div class="line">execute([<span class="string">"scrapy"</span>, <span class="string">"crawl"</span>, <span class="string">"spiders文件夹下的py文件名称"</span>])</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;环境搭建&quot;&gt;&lt;a href=&quot;#环境搭建&quot; class=&quot;headerlink&quot; title=&quot;环境搭建&quot;&gt;&lt;/a&gt;&lt;strong&gt;环境搭建&lt;/strong&gt;&lt;/h2&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
</feed>
