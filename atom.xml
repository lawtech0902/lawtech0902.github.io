<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LawTech&#39;s Blog</title>
  <subtitle>不破不立</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-06-09T08:31:25.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LawTech.</name>
    <email>584563542@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取拉勾网</title>
    <link href="http://yoursite.com/2017/06/09/scrapy-crawlspider-lagou/"/>
    <id>http://yoursite.com/2017/06/09/scrapy-crawlspider-lagou/</id>
    <published>2017-06-09T06:18:54.000Z</published>
    <updated>2017-06-09T08:31:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前的Spider都是默认根据basic的templates创建，现在我们要用crawl的方式创建Spider，以爬取拉勾网整站信息。</p>
<a id="more"></a>
<h2 id="CrawlSpider源码解析"><a href="#CrawlSpider源码解析" class="headerlink" title="CrawlSpider源码解析"></a><strong>CrawlSpider源码解析</strong></h2><p>Spider基本上能做很多事情了，但是如果你想爬取知乎或者是简书全站的话，你可能需要一个更强大的武器。<br>CrawlSpider基于Spider，但是可以说是为全站爬取而生。</p>
<h3 id="简要说明"><a href="#简要说明" class="headerlink" title="简要说明"></a><strong>简要说明</strong></h3><p>CrawlSpider是爬取那些具有一定规则网站的常用的爬虫，它基于Spider并有一些独特属性</p>
<ul>
<li>rules: 是<em>Rule</em>对象的集合，用于匹配目标网站并排除干扰</li>
<li>parse_start_url: 用于爬取起始响应，必须要返回<em>Item</em>，<em>Request</em>中的一个。</li>
</ul>
<p>因为rules是Rule对象的集合，所以这里也要介绍一下Rule。它有几个参数：link_extractor、callback=None、cb_kwargs=None、follow=None、process_links=None、process_request=None<br>其中的link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为：</p>
<ul>
<li>allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</li>
<li>deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。</li>
<li>allow_domains：会被提取的链接的domains。</li>
<li>deny_domains：一定不会被提取链接的domains。</li>
<li><strong>restrict_xpaths</strong>：使用<strong>xpath</strong>表达式，和<strong>allow</strong>共同作用过滤链接。还有一个类似的<strong>restrict_css</strong></li>
</ul>
<p>下面是官方提供的例子，我将从源代码的角度开始解读一些常见问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'example.com'</span></div><div class="line">    allowed_domains = [<span class="string">'example.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</div><div class="line"></div><div class="line">    rules = (</div><div class="line">        <span class="comment"># Extract links matching 'category.php' (but not matching 'subsection.php')</span></div><div class="line">        <span class="comment"># and follow links from them (since no callback means follow=True by default).</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</div><div class="line"></div><div class="line">        <span class="comment"># Extract links matching 'item.php' and parse them with the spider's method parse_item</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        self.logger.info(<span class="string">'Hi, this is an item page! %s'</span>, response.url)</div><div class="line">        item = scrapy.Item()</div><div class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</div><div class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h3 id="问题：CrawlSpider如何工作的？"><a href="#问题：CrawlSpider如何工作的？" class="headerlink" title="问题：CrawlSpider如何工作的？"></a><strong>问题：CrawlSpider如何工作的？</strong></h3><p><strong>因为CrawlSpider继承了Spider，所以具有Spider的所有函数。</strong><br>首先由 <code>start_requests</code> 对 <code>start_urls</code> 中的每一个url发起请求（ <code>make_requests_from_url</code> )，这个请求会被parse接收。在Spider里面的parse需要我们定义，但CrawlSpider定义 <code>parse</code> 去解析响应（ <code>self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)</code> ）<br><strong>_parse_response</strong>根据有无 <code>callback</code> ， <code>follow</code> 和 <code>self.follow_links</code> 执行不同的操作</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(<span class="keyword">self</span>, response, callback, cb_kwargs, follow=True)</span></span>:</div><div class="line"><span class="comment">##如果传入了callback，使用这个callback解析页面并获取解析得到的reques或item</span></div><div class="line">    <span class="keyword">if</span> <span class="symbol">callback:</span></div><div class="line">        cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</div><div class="line">        cb_res = <span class="keyword">self</span>.process_results(response, cb_res)</div><div class="line">        <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</div><div class="line">            <span class="keyword">yield</span> requests_or_item</div><div class="line"><span class="comment">## 其次判断有无follow，用_requests_to_follow解析响应是否有符合要求的link。</span></div><div class="line">    <span class="keyword">if</span> follow <span class="keyword">and</span> <span class="keyword">self</span>.<span class="symbol">_follow_links:</span></div><div class="line">        <span class="keyword">for</span> request_or_item <span class="keyword">in</span> <span class="keyword">self</span>._requests_to_follow(response):</div><div class="line">            <span class="keyword">yield</span> request_or_item</div></pre></td></tr></table></figure>
<p>其中<code>_requests_to_follow</code>又会获取<code>link_extractor</code>（这个是我们传入的LinkExtractor）解析页面得到的link<code>（link_extractor.extract_links(response)）</code>,对url进行加工（process_links，需要自定义），对符合的link发起Request。使用<code>.process_request</code>(需要自定义）处理响应。</p>
<h3 id="问题：CrawlSpider如何获取rules？"><a href="#问题：CrawlSpider如何获取rules？" class="headerlink" title="问题：CrawlSpider如何获取rules？"></a><strong>问题：CrawlSpider如何获取rules？</strong></h3><p>CrawlSpider类会在<code>__init__</code>方法中调用<code>_compile_rules</code>方法，然后在其中浅拷贝<code>rules</code>中的各个<code>Rule</code>获取要用于回调(callback)，要进行处理的链接（process_links）和要进行的处理请求（process_request)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></div><div class="line">        <span class="keyword">if</span> callable(method):</div><div class="line">            <span class="keyword">return</span> method</div><div class="line">        <span class="keyword">elif</span> isinstance(method, six.string_types):</div><div class="line">            <span class="keyword">return</span> getattr(self, method, <span class="keyword">None</span>)</div><div class="line"></div><div class="line">    self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</div><div class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</div><div class="line">        rule.callback = get_method(rule.callback)</div><div class="line">        rule.process_links = get_method(rule.process_links)</div><div class="line">        rule.process_request = get_method(rule.process_request)</div></pre></td></tr></table></figure>
<p>那么<code>Rule</code>是怎么样定义的呢？</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class Rule(object):</div><div class="line"></div><div class="line">    def __init__(<span class="built_in">self</span>, link_extractor, callback=<span class="literal">None</span>, cb_kwargs=<span class="literal">None</span>, follow=<span class="literal">None</span>, process_links=<span class="literal">None</span>, process_request=identity):</div><div class="line">        <span class="built_in">self</span>.link_extractor = link_extractor</div><div class="line">        <span class="built_in">self</span>.callback = callback</div><div class="line">        <span class="built_in">self</span>.cb_kwargs = cb_kwargs <span class="literal">or</span> &#123;&#125;</div><div class="line">        <span class="built_in">self</span>.process_links = process_links</div><div class="line">        <span class="built_in">self</span>.process_request = process_request</div><div class="line">        <span class="keyword">if</span> follow is <span class="literal">None</span>:</div><div class="line">            <span class="built_in">self</span>.follow = <span class="literal">False</span> <span class="keyword">if</span> callback <span class="keyword">else</span> <span class="literal">True</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="built_in">self</span>.follow = follow</div></pre></td></tr></table></figure>
<p>因此LinkExtractor会传给link_extractor。</p>
<h3 id="有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？"><a href="#有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？" class="headerlink" title="有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？"></a><strong>有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？</strong></h3><p>由上面的讲解可以发现<code>_parse_response</code>会处理有<code>callback</code>的（响应）respons。<br>cb_res = callback(response, **cb_kwargs) or ()<br>而<code>_requests_to_follow</code>会将<code>self._response_downloaded</code>传给<code>callback</code>用于对页面中匹配的url发起请求（request）。<br>r = Request(url=link.url, callback=self._response_downloaded) </p>
<h3 id="如何在CrawlSpider进行模拟登陆"><a href="#如何在CrawlSpider进行模拟登陆" class="headerlink" title="如何在CrawlSpider进行模拟登陆"></a><strong>如何在CrawlSpider进行模拟登陆</strong></h3><p>因为CrawlSpider和Spider一样，都要使用start_requests发起请求，用从<a href="http://www.jianshu.com/users/4ee453b72aff" target="_blank" rel="external">Andrew_liu</a>大神借鉴的代码说明如何模拟登陆：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">##替换原来的start_requests，callback为</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> [Request(<span class="string">"http://www.zhihu.com/#signin"</span>, meta = &#123;<span class="string">'cookiejar'</span> : <span class="number">1</span>&#125;, callback = self.post_login)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Preparing login'</span></div><div class="line">    <span class="comment">#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单</span></div><div class="line">    xsrf = Selector(response).xpath(<span class="string">'//input[@name="_xsrf"]/@value'</span>).extract()[<span class="number">0</span>]</div><div class="line">    <span class="keyword">print</span> xsrf</div><div class="line">    <span class="comment">#FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单</span></div><div class="line">    <span class="comment">#登陆成功后, 会调用after_login回调函数</span></div><div class="line">    <span class="keyword">return</span> [FormRequest.from_response(response,   <span class="comment">#"http://www.zhihu.com/login",</span></div><div class="line">                        meta = &#123;<span class="string">'cookiejar'</span> : response.meta[<span class="string">'cookiejar'</span>]&#125;,</div><div class="line">                        headers = self.headers,</div><div class="line">                        formdata = &#123;</div><div class="line">                        <span class="string">'_xsrf'</span>: xsrf,</div><div class="line">                        <span class="string">'email'</span>: <span class="string">'1527927373@qq.com'</span>,</div><div class="line">                        <span class="string">'password'</span>: <span class="string">'321324jia'</span></div><div class="line">                        &#125;,</div><div class="line">                        callback = self.after_login,</div><div class="line">                        dont_filter = <span class="keyword">True</span></div><div class="line">                        )]</div><div class="line"></div><div class="line"><span class="comment">#make_requests_from_url会调用parse，就可以与CrawlSpider的parse进行衔接了</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">after_login</span><span class="params">(self, response)</span> :</span></div><div class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls :</div><div class="line">        <span class="keyword">yield</span> self.make_requests_from_url(url)</div></pre></td></tr></table></figure>
<h3 id="源码及注释"><a href="#源码及注释" class="headerlink" title="源码及注释"></a><strong>源码及注释</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrawlSpider</span><span class="params">(Spider)</span>:</span></div><div class="line"></div><div class="line">    rules = ()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *a, **kw)</span>:</span></div><div class="line">        super(CrawlSpider, self).__init__(*a, **kw)</div><div class="line">        self._compile_rules()</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        首先调用parse()来处理start_urls中返回的response对象  </div><div class="line">    	parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url()  </div><div class="line">    	设置了跟进标志位True  </div><div class="line">    	parse将返回item和跟进了的Request对象 </div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_start_url</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理start_url中返回的response，需要重写</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> []</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_results</span><span class="params">(self, response, results)</span>:</span></div><div class="line">        <span class="keyword">return</span> results</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回 </div><div class="line">        """</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</div><div class="line">            <span class="keyword">return</span></div><div class="line">        seen = set()</div><div class="line">        <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</div><div class="line">            <span class="comment"># 抽取之内的所有链接，只要通过任意一个'规则'，即表示合法 </span></div><div class="line">            links = [lnk <span class="keyword">for</span> lnk <span class="keyword">in</span> rule.link_extractor.extract_links(response)</div><div class="line">                     <span class="keyword">if</span> lnk <span class="keyword">not</span> <span class="keyword">in</span> seen] </div><div class="line">            <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</div><div class="line">                <span class="comment"># 使用用户指定的process_links处理每个连接 </span></div><div class="line">                links = rule.process_links(links)</div><div class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> links:</div><div class="line">                <span class="comment"># 将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded()  </span></div><div class="line">                seen.add(link)</div><div class="line">                <span class="comment"># 构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数  </span></div><div class="line">                r = Request(url=link.url, callback=self._response_downloaded)</div><div class="line">                r.meta.update(rule=n, link_text=link.text)</div><div class="line">                <span class="comment"># 对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request.</span></div><div class="line">                <span class="keyword">yield</span> rule.process_request(r)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_response_downloaded</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理通过rule提取出的连接，并返回item以及request</div><div class="line">        """</div><div class="line">        rule = self._rules[response.meta[<span class="string">'rule'</span>]]</div><div class="line">        <span class="keyword">return</span> self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(self, response, callback, cb_kwargs, follow=True)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        解析response对象，会用callback解析处理他，并返回request或Item对象  </div><div class="line">        首先判断是否设置了回调函数。（该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数）  </div><div class="line">        如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象，  </div><div class="line">        然后再交给process_results处理。返回cb_res的一个列表  </div><div class="line">        """</div><div class="line">        <span class="keyword">if</span> callback:</div><div class="line">            <span class="comment"># 如果是parse调用的，则会解析成Request对象  </span></div><div class="line">            <span class="comment"># 如果是rule callback，则会解析成Item </span></div><div class="line">            cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</div><div class="line">            cb_res = self.process_results(response, cb_res)</div><div class="line">            <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</div><div class="line">                <span class="keyword">yield</span> requests_or_item</div><div class="line"></div><div class="line">        <span class="keyword">if</span> follow <span class="keyword">and</span> self._follow_links:</div><div class="line">            <span class="comment">#如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象 </span></div><div class="line">            <span class="keyword">for</span> request_or_item <span class="keyword">in</span> self._requests_to_follow(response):</div><div class="line">                <span class="keyword">yield</span> request_or_item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></div><div class="line">            <span class="keyword">if</span> callable(method):</div><div class="line">                <span class="keyword">return</span> method</div><div class="line">            <span class="keyword">elif</span> isinstance(method, six.string_types):</div><div class="line">                <span class="keyword">return</span> getattr(self, method, <span class="keyword">None</span>)</div><div class="line"></div><div class="line">        self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</div><div class="line">        <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</div><div class="line">            rule.callback = get_method(rule.callback)</div><div class="line">            rule.process_links = get_method(rule.process_links)</div><div class="line">            rule.process_request = get_method(rule.process_request)</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, *args, **kwargs)</span>:</span></div><div class="line">        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)</div><div class="line">        spider._follow_links = crawler.settings.getbool(</div><div class="line">            <span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> spider</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_crawler</span><span class="params">(self, crawler)</span>:</span></div><div class="line">        super(CrawlSpider, self).set_crawler(crawler)</div><div class="line">        self._follow_links = crawler.settings.getbool(<span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<h2 id="创建CrawlSpider"><a href="#创建CrawlSpider" class="headerlink" title="创建CrawlSpider"></a><strong>创建CrawlSpider</strong></h2><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fgf0mz0pcuj31fu0vawon.jpg" alt=""></p>
<p>出现问题：<code>ImportError: No module named &#39;utils&#39;</code></p>
<p>原因：我们之前将项目目录下的 <code>ArticleSpider</code> 文件夹Mark为 <code>Sources Root</code> 导致。</p>
<p>解决办法：自己在 <code>Settings.py</code> 中设置搜索路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置搜索路径</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line">BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))</div><div class="line">sys.path.insert(<span class="number">0</span>, os.path.join(BASE_DIR, <span class="string">'ArticleSpider'</span>))</div></pre></td></tr></table></figure>
<p>设置之后重新创建spider</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy genspider -t crawl lagou www.lagou.com</div></pre></td></tr></table></figure>
<h2 id="数据表结构及items设计"><a href="#数据表结构及items设计" class="headerlink" title="数据表结构及items设计"></a><strong>数据表结构及items设计</strong></h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fgf10fvkqvj31400qe79r.jpg" alt=""></p>
<h2 id="完整代码逻辑"><a href="#完整代码逻辑" class="headerlink" title="完整代码逻辑"></a><strong>完整代码逻辑</strong></h2><h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a><strong>items.py</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_splash</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    去除工作城市的斜杠</div><div class="line">    """</div><div class="line">    <span class="keyword">return</span> value.replace(<span class="string">"/"</span>, <span class="string">""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_jobaddr</span><span class="params">(value)</span>:</span></div><div class="line">    addr_list = value.split(<span class="string">"\n"</span>)</div><div class="line">    addr_list = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> addr_list <span class="keyword">if</span> item.strip() != <span class="string">"查看地图"</span>]</div><div class="line">    <span class="keyword">return</span> <span class="string">""</span>.join(addr_list)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItemLoader</span><span class="params">(ItemLoader)</span>:</span></div><div class="line">    default_output_processor = TakeFirst()</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    拉勾网职位信息</div><div class="line">    """</div><div class="line">    title = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    salary = scrapy.Field()</div><div class="line">    job_city = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_splash),</div><div class="line">    )</div><div class="line">    work_years = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_splash),</div><div class="line">    )</div><div class="line">    degree_need = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_splash),</div><div class="line">    )</div><div class="line">    job_type = scrapy.Field()</div><div class="line">    publish_time = scrapy.Field()</div><div class="line">    job_advantage = scrapy.Field()</div><div class="line">    job_desc = scrapy.Field()</div><div class="line">    job_addr = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_tags, handle_jobaddr),</div><div class="line">    )</div><div class="line">    company_name = scrapy.Field()</div><div class="line">    company_url = scrapy.Field()</div><div class="line">    tags = scrapy.Field(</div><div class="line">        input_processor=Join(<span class="string">","</span>)</div><div class="line">    )</div><div class="line">    crawl_time = scrapy.Field()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">            insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need,</div><div class="line">            job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url,</div><div class="line">            tags, crawl_time)</div><div class="line">            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</div><div class="line">            ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc)</div><div class="line">        """</div><div class="line"></div><div class="line">        params = (</div><div class="line">            self[<span class="string">"title"</span>], self[<span class="string">"url"</span>], self[<span class="string">"url_object_id"</span>], self[<span class="string">"salary"</span>], self[<span class="string">"job_city"</span>],</div><div class="line">            self[<span class="string">"work_years"</span>], self[<span class="string">"degree_need"</span>], self[<span class="string">"job_type"</span>],</div><div class="line">            self[<span class="string">"publish_time"</span>], self[<span class="string">"job_advantage"</span>], self[<span class="string">"job_desc"</span>],</div><div class="line">            self[<span class="string">"job_addr"</span>], self[<span class="string">"company_name"</span>], self[<span class="string">"company_url"</span>],</div><div class="line">            self[<span class="string">"tags"</span>], self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="keyword">return</span> insert_sql, params</div></pre></td></tr></table></figure>
<h3 id="lagou-py"><a href="#lagou-py" class="headerlink" title="lagou.py"></a><strong>lagou.py</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"></div><div class="line"><span class="keyword">from</span> items <span class="keyword">import</span> LagouJobItemLoader, LagouJobItem</div><div class="line"><span class="keyword">from</span> utils.common <span class="keyword">import</span> get_md5</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouSpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'lagou'</span></div><div class="line">    allowed_domains = [<span class="string">'www.lagou.com'</span>]</div><div class="line">    start_urls = [<span class="string">'https://www.lagou.com/'</span>]</div><div class="line"></div><div class="line">    rules = (</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'zhaopin/.*'</span>,)), ),</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'gongsi/j\d+.html'</span>,)), ),</div><div class="line">        Rule(LinkExtractor(allow=<span class="string">r'jobs/\d+.html'</span>), callback=<span class="string">'parse_job'</span>, follow=<span class="keyword">True</span>),</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_job</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        解析拉勾网的职位</div><div class="line">        """</div><div class="line">        item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response)</div><div class="line">        item_loader.add_css(<span class="string">"title"</span>, <span class="string">".job-name::attr(title)"</span>)</div><div class="line">        item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">        item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</div><div class="line">        item_loader.add_css(<span class="string">"salary"</span>, <span class="string">".job_request .salary::text"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"job_city"</span>, <span class="string">"//*[@class='job_request']/p/span[2]/text()"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"work_years"</span>, <span class="string">"//*[@class='job_request']/p/span[3]/text()"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"degree_need"</span>, <span class="string">"//*[@class='job_request']/p/span[4]/text()"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"job_type"</span>, <span class="string">"//*[@class='job_request']/p/span[5]/text()"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"publish_time"</span>, <span class="string">".publish_time::text"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"job_advantage"</span>, <span class="string">".job-advantage p::text"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"job_desc"</span>, <span class="string">".job_bt div"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"job_addr"</span>, <span class="string">".work_addr"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"company_name"</span>, <span class="string">"#job_company dt a img::attr(alt)"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"company_url"</span>, <span class="string">"#job_company dt a::attr(href)"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"tags"</span>, <span class="string">".position-label li::text"</span>)</div><div class="line">        item_loader.add_value(<span class="string">"crawl_time"</span>, datetime.now())</div><div class="line"></div><div class="line">        job_item = item_loader.load_item()</div><div class="line"></div><div class="line">        <span class="keyword">return</span> job_item</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前的Spider都是默认根据basic的templates创建，现在我们要用crawl的方式创建Spider，以爬取拉勾网整站信息。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Requests，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRequests%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取知乎</title>
    <link href="http://yoursite.com/2017/06/07/scrapy-zhihu/"/>
    <id>http://yoursite.com/2017/06/07/scrapy-zhihu/</id>
    <published>2017-06-07T06:18:54.000Z</published>
    <updated>2017-06-07T10:41:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>在完成了Scrapy模拟登录知乎后，下一步要进行的就是进行对知乎页面，问题以及答案等内容的爬取工作了。</p>
<a id="more"></a>
<h2 id="通过Scrapy-Shell进行调试"><a href="#通过Scrapy-Shell进行调试" class="headerlink" title="通过Scrapy Shell进行调试"></a><strong>通过Scrapy Shell进行调试</strong></h2><p>在使用shell调试时，直接通过 <code>scrapy shell https://www.zhihu.com/question/58765535</code> 会出现error 500错误。这是因为没有加headers的原因。<br>正确添加headers的方法是：<code>scrapy -s USER_AGENT=&quot;任意的User Agent&quot;</code> 。此时，就可以在shell中进行分析了。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fgct2bwtn7j30vo0n8gsb.jpg" alt=""></p>
<h2 id="获得要分析的链接"><a href="#获得要分析的链接" class="headerlink" title="获得要分析的链接"></a>获得要分析的链接</h2><p>在登录完成进入首页之后，通过深度优先算法获得首页需要的链接，然后打开这些链接再次获得里面的链接，不断重复，获得所有内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="comment"># 因为没有具体的入口，采用深度优先的算法</span></div><div class="line">        all_urls = response.css(<span class="string">"a::attr(href)"</span>).extract()</div><div class="line">        all_urls = [parse.urljoin(response.url, url) <span class="keyword">for</span> url <span class="keyword">in</span> all_urls]</div><div class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> all_urls:</div><div class="line">            <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>在分析页面内容之后，设计我们所需的数据表zhihu_question和zhihu_answer</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fgctdm536rj317a0k4jvj.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fgctdu7ub8j317k0kwaek.jpg" alt=""></p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a><strong>完整代码</strong></h2><ul>
<li>zhihu.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</div><div class="line"><span class="keyword">from</span> items <span class="keyword">import</span> ZhihuAnswerItem, ZhihuQuestionItem</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">import</span> urlparse <span class="keyword">as</span> parse</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    <span class="keyword">from</span> urllib <span class="keyword">import</span> parse</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"zhihu"</span></div><div class="line">    allowed_domains = [<span class="string">"www.zhihu.com"</span>]</div><div class="line">    start_urls = [<span class="string">'https://www.zhihu.com/'</span>]</div><div class="line"></div><div class="line">    <span class="comment"># question的第一页answer的请求url</span></div><div class="line">    start_answer_url = <span class="string">"https://www.zhihu.com/api/v4/questions/&#123;0&#125;/answers?sort_by=default&amp;include=data%5B%2A%5D.is_normal%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccollapsed_counts%2Creviewing_comments_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Crelationship.is_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.is_blocking%2Cis_blocked%2Cis_followed%2Cvoteup_count%2Cmessage_thread_token%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=&#123;1&#125;&amp;offset=&#123;2&#125;"</span></div><div class="line"></div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">"Host"</span>: <span class="string">"www.zhihu.com"</span>,</div><div class="line">        <span class="string">"Referer"</span>: <span class="string">"https://www.zhihu.com/"</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4"</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        提取出html页面中的所有url 并跟踪这些url进行一步爬取</div><div class="line">        如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数</div><div class="line">        """</div><div class="line">        all_urls = response.css(<span class="string">"a::attr(href)"</span>).extract()</div><div class="line">        all_urls = [parse.urljoin(response.url, url) <span class="keyword">for</span> url <span class="keyword">in</span> all_urls]</div><div class="line">        all_urls = filter(<span class="keyword">lambda</span> x: <span class="keyword">True</span> <span class="keyword">if</span> x.startswith(<span class="string">"https"</span>) <span class="keyword">else</span> <span class="keyword">False</span>, all_urls)</div><div class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> all_urls:</div><div class="line">            match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, url)</div><div class="line">            <span class="keyword">if</span> match_obj:</div><div class="line">                <span class="comment"># 如果提取到question相关的页面则下载后交由提取函数进行提取</span></div><div class="line">                request_url = match_obj.group(<span class="number">1</span>)</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(request_url, headers=self.headers, callback=self.parse_question)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="comment"># 如果不是question页面则直接进一步跟踪</span></div><div class="line">                <span class="keyword">yield</span> scrapy.Request(url, headers=self.headers, callback=self.parse)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_question</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理question页面， 从页面中提取出具体的question item</div><div class="line">        """</div><div class="line">        <span class="comment"># 处理question页面， 从页面中提取出具体的question item</span></div><div class="line">        <span class="keyword">if</span> <span class="string">"QuestionHeader-title"</span> <span class="keyword">in</span> response.text:</div><div class="line">            <span class="comment"># 处理新版本</span></div><div class="line">            match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, response.url)</div><div class="line">            <span class="keyword">if</span> match_obj:</div><div class="line">                question_id = int(match_obj.group(<span class="number">2</span>))</div><div class="line"></div><div class="line">            item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response)</div><div class="line">            item_loader.add_css(<span class="string">"title"</span>, <span class="string">"h1.QuestionHeader-title::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"content"</span>, <span class="string">".QuestionHeader-detail"</span>)</div><div class="line">            item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">            item_loader.add_value(<span class="string">"zhihu_id"</span>, question_id)</div><div class="line">            item_loader.add_css(<span class="string">"answer_num"</span>, <span class="string">".List-headerText span::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"comments_num"</span>, <span class="string">".QuestionHeader-Comment button::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"watch_user_num"</span>, <span class="string">".NumberBoard-value::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"topics"</span>, <span class="string">".QuestionHeader-topics .Popover div::text"</span>)</div><div class="line"></div><div class="line">            question_item = item_loader.load_item()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># 处理老版本页面的item提取</span></div><div class="line">            match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, response.url)</div><div class="line">            <span class="keyword">if</span> match_obj:</div><div class="line">                question_id = int(match_obj.group(<span class="number">2</span>))</div><div class="line"></div><div class="line">            item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response)</div><div class="line">            <span class="comment"># item_loader.add_css("title", ".zh-question-title h2 a::text")</span></div><div class="line">            item_loader.add_xpath(<span class="string">"title"</span>,</div><div class="line">                                  <span class="string">"//*[@id='zh-question-title']/h2/a/text()|//*[@id='zh-question-title']/h2/span/text()"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"content"</span>, <span class="string">"#zh-question-detail"</span>)</div><div class="line">            item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">            item_loader.add_value(<span class="string">"zhihu_id"</span>, question_id)</div><div class="line">            item_loader.add_css(<span class="string">"answer_num"</span>, <span class="string">"#zh-question-answer-num::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"comments_num"</span>, <span class="string">"#zh-question-meta-wrap a[name='addcomment']::text"</span>)</div><div class="line">            <span class="comment"># item_loader.add_css("watch_user_num", "#zh-question-side-header-wrap::text")</span></div><div class="line">            item_loader.add_xpath(<span class="string">"watch_user_num"</span>,</div><div class="line">                                  <span class="string">"//*[@id='zh-question-side-header-wrap']/text()|//*[@class='zh-question-followers-sidebar']/div/a/strong/text()"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"topics"</span>, <span class="string">".zm-tag-editor-labels a::text"</span>)</div><div class="line"></div><div class="line">            question_item = item_loader.load_item()</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> scrapy.Request(self.start_answer_url.format(question_id, <span class="number">20</span>, <span class="number">0</span>), headers=self.headers, callback=self.parse_answer)</div><div class="line">        <span class="keyword">yield</span> question_item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_answer</span><span class="params">(self, reponse)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理question的answer</div><div class="line">        """</div><div class="line">        ans_json = json.loads(reponse.text)</div><div class="line">        is_end = ans_json[<span class="string">"paging"</span>][<span class="string">"is_end"</span>]</div><div class="line">        next_url = ans_json[<span class="string">"paging"</span>][<span class="string">"next"</span>]</div><div class="line"></div><div class="line">        <span class="comment"># 提取answer的具体字段</span></div><div class="line">        <span class="keyword">for</span> answer <span class="keyword">in</span> ans_json[<span class="string">"data"</span>]:</div><div class="line">            answer_item = ZhihuAnswerItem()</div><div class="line">            answer_item[<span class="string">"zhihu_id"</span>] = answer[<span class="string">"id"</span>]</div><div class="line">            answer_item[<span class="string">"url"</span>] = answer[<span class="string">"url"</span>]</div><div class="line">            answer_item[<span class="string">"question_id"</span>] = answer[<span class="string">"question"</span>][<span class="string">"id"</span>]</div><div class="line">            answer_item[<span class="string">"author_id"</span>] = answer[<span class="string">"author"</span>][<span class="string">"id"</span>] <span class="keyword">if</span> <span class="string">"id"</span> <span class="keyword">in</span> answer[<span class="string">"author"</span>] <span class="keyword">else</span> <span class="keyword">None</span></div><div class="line">            answer_item[<span class="string">"content"</span>] = answer[<span class="string">"content"</span>] <span class="keyword">if</span> <span class="string">"content"</span> <span class="keyword">in</span> answer <span class="keyword">else</span> <span class="keyword">None</span></div><div class="line">            answer_item[<span class="string">"praise_num"</span>] = answer[<span class="string">"voteup_count"</span>]</div><div class="line">            answer_item[<span class="string">"comments_num"</span>] = answer[<span class="string">"comment_count"</span>]</div><div class="line">            answer_item[<span class="string">"create_time"</span>] = answer[<span class="string">"created_time"</span>]</div><div class="line">            answer_item[<span class="string">"update_time"</span>] = answer[<span class="string">"updated_time"</span>]</div><div class="line">            answer_item[<span class="string">"crawl_time"</span>] = datetime.datetime.now()</div><div class="line"></div><div class="line">            <span class="keyword">yield</span> answer_item</div><div class="line"></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_end:</div><div class="line">            <span class="keyword">yield</span> scrapy.Request(next_url, headers=self.headers, callback=self.parse_answer)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> [scrapy.Request(<span class="string">'https://www.zhihu.com/#signin'</span>, headers=self.headers, callback=self.login)]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></div><div class="line">        response_text = response.text</div><div class="line">        match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response_text, re.DOTALL)</div><div class="line">        xsrf = <span class="string">''</span></div><div class="line">        <span class="keyword">if</span> match_obj:</div><div class="line">            xsrf = (match_obj.group(<span class="number">1</span>))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> xsrf:</div><div class="line">            post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">            post_data = &#123;</div><div class="line">                <span class="string">"_xsrf"</span>: xsrf,</div><div class="line">                <span class="string">"phone_num"</span>: <span class="string">"18251556927"</span>,</div><div class="line">                <span class="string">"password"</span>: <span class="string">"lawtech0301520"</span>,</div><div class="line">                <span class="string">"captcha"</span>: <span class="string">""</span></div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">import</span> time</div><div class="line">            t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">            captcha_url = <span class="string">"https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login"</span>.format(t)</div><div class="line">            <span class="keyword">yield</span> scrapy.Request(captcha_url, headers=self.headers, meta=&#123;<span class="string">"post_data"</span>: post_data&#125;,</div><div class="line">                                 callback=self.login_after_captcha)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login_after_captcha</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">with</span> open(<span class="string">"captcha.jpg"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(response.body)</div><div class="line">            f.close()</div><div class="line"></div><div class="line">        <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">            im.show()</div><div class="line">            im.close()</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line">        captcha = input(<span class="string">"输入验证码\n&gt;"</span>)</div><div class="line"></div><div class="line">        post_data = response.meta.get(<span class="string">"post_data"</span>, &#123;&#125;)</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        post_data[<span class="string">"captcha"</span>] = captcha</div><div class="line">        <span class="keyword">return</span> [scrapy.FormRequest(</div><div class="line">            url=post_url,</div><div class="line">            formdata=post_data,</div><div class="line">            headers=self.headers,</div><div class="line">            callback=self.check_login</div><div class="line">        )]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        验证服务器的返回数据判断是否成功</div><div class="line">        """</div><div class="line">        text_json = json.loads(response.text)</div><div class="line">        <span class="keyword">if</span> <span class="string">"msg"</span> <span class="keyword">in</span> text_json <span class="keyword">and</span> text_json[<span class="string">"msg"</span>] == <span class="string">"登录成功"</span>:</div><div class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(url, dont_filter=<span class="keyword">True</span>, headers=self.headers)</div></pre></td></tr></table></figure>
<p>为了用同一个Pipeline处理所有的数据库存储操作，因此将操作都放入items中，再有Pipeline进行统一处理。</p>
<ul>
<li>items.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuQuestionItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎问题Item</div><div class="line">    """</div><div class="line">    zhihu_id = scrapy.Field()</div><div class="line">    topics = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    title = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div><div class="line">    answer_num = scrapy.Field()</div><div class="line">    comments_num = scrapy.Field()</div><div class="line">    watch_user_num = scrapy.Field()</div><div class="line">    click_num = scrapy.Field()</div><div class="line">    crawl_time = scrapy.Field()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 插入知乎question表的sql语句</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                    insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num,</div><div class="line">                      watch_user_num, click_num, crawl_time</div><div class="line">                      )</div><div class="line">                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) </div><div class="line">                    ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num),</div><div class="line">              watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num)</div><div class="line">                """</div><div class="line"></div><div class="line">        zhihu_id = self[<span class="string">"zhihu_id"</span>][<span class="number">0</span>]</div><div class="line">        topics = <span class="string">","</span>.join(self[<span class="string">"topics"</span>])</div><div class="line">        url = self[<span class="string">"url"</span>][<span class="number">0</span>]</div><div class="line">        title = <span class="string">""</span>.join(self[<span class="string">"title"</span>])</div><div class="line">        content = <span class="string">""</span>.join(self[<span class="string">"content"</span>])</div><div class="line">        answer_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"answer_num"</span>]))</div><div class="line">        comments_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"comments_num"</span>]))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> len(self[<span class="string">"watch_user_num"</span>]) == <span class="number">2</span>:</div><div class="line">            watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</div><div class="line">            click_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">1</span>])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</div><div class="line">            click_num = <span class="number">0</span></div><div class="line"></div><div class="line">        crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT)</div><div class="line"></div><div class="line">        params = (zhihu_id, topics, url, title, content, answer_num, comments_num,</div><div class="line">                  watch_user_num, click_num, crawl_time)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> insert_sql, params</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuAnswerItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎回答Item</div><div class="line">    """</div><div class="line">    zhihu_id = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    question_id = scrapy.Field()</div><div class="line">    author_id = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div><div class="line">    praise_num = scrapy.Field()</div><div class="line">    comments_num = scrapy.Field()</div><div class="line">    create_time = scrapy.Field()</div><div class="line">    update_time = scrapy.Field()</div><div class="line">    crawl_time = scrapy.Field()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 插入知乎question表的sql语句</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                    insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, praise_num, comments_num,</div><div class="line">              create_time, update_time, crawl_time</div><div class="line">              ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</div><div class="line">              ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), praise_num=VALUES(praise_num),</div><div class="line">              update_time=VALUES(update_time)</div><div class="line">                """</div><div class="line"></div><div class="line">        create_time = datetime.datetime.fromtimestamp(self[<span class="string">'create_time'</span>]).strftime(SQL_DATETIME_FORMAT)</div><div class="line">        update_time = datetime.datetime.fromtimestamp(self[<span class="string">'update_time'</span>]).strftime(SQL_DATETIME_FORMAT)</div><div class="line"></div><div class="line">        params = (</div><div class="line">            self[<span class="string">"zhihu_id"</span>], self[<span class="string">"url"</span>], self[<span class="string">"question_id"</span>],</div><div class="line">            self[<span class="string">"author_id"</span>], self[<span class="string">"content"</span>], self[<span class="string">"praise_num"</span>],</div><div class="line">            self[<span class="string">"comments_num"</span>], create_time, update_time,</div><div class="line">            self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="keyword">return</span> insert_sql, params</div></pre></td></tr></table></figure>
<ul>
<li>Pipelines.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 采用异步的机制写入mysql</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></div><div class="line">        self.dbpool = dbpool</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        dbparms = dict(</div><div class="line">            host=settings[<span class="string">"MYSQL_HOST"</span>],</div><div class="line">            db=settings[<span class="string">"MYSQL_DBNAME"</span>],</div><div class="line">            user=settings[<span class="string">"MYSQL_USER"</span>],</div><div class="line">            passwd=settings[<span class="string">"MYSQL_PASSWORD"</span>],</div><div class="line">            charset=<span class="string">'utf8'</span>,</div><div class="line">            cursorclass=MySQLdb.cursors.DictCursor,</div><div class="line">            use_unicode=<span class="keyword">True</span>,</div><div class="line">        )</div><div class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparms)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> cls(dbpool)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="comment"># 使用twisted将mysql插入变成异步执行</span></div><div class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</div><div class="line">        query.addErrback(self.handle_error, item, spider)  <span class="comment"># 处理异常</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure, item, spider)</span>:</span></div><div class="line">        <span class="comment"># 处理异步插入的异常</span></div><div class="line">        print(failure)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></div><div class="line">        <span class="comment"># 执行具体的插入</span></div><div class="line">        <span class="comment"># 根据不同的item 构建不同的sql语句并插入到mysql中</span></div><div class="line">        insert_sql, params = item.get_insert_sql()</div><div class="line">        cursor.execute(insert_sql, params)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在完成了Scrapy模拟登录知乎后，下一步要进行的就是进行对知乎页面，问题以及答案等内容的爬取工作了。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Requests，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRequests%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Requests模拟登录知乎</title>
    <link href="http://yoursite.com/2017/05/11/scrapy-requests-zhihu-login/"/>
    <id>http://yoursite.com/2017/05/11/scrapy-requests-zhihu-login/</id>
    <published>2017-05-11T06:18:54.000Z</published>
    <updated>2017-05-12T06:55:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="external">Requests</a> 是以 <a href="http://www.python.org/dev/peps/pep-0020" target="_blank" rel="external">PEP 20</a> 的箴言为中心开发的</p>
<ol>
<li>Beautiful is better than ugly.(美丽优于丑陋)</li>
<li>Explicit is better than implicit.(直白优于含蓄)</li>
<li>Simple is better than complex.(简单优于复杂)</li>
<li>Complex is better than complicated.(复杂优于繁琐)</li>
<li>Readability counts.(可读性很重要)</li>
</ol>
<a id="more"></a>
<h2 id="常见状态码"><a href="#常见状态码" class="headerlink" title="常见状态码"></a><strong>常见状态码</strong></h2><table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>200</td>
<td>请求被正确执行</td>
</tr>
<tr>
<td>301/302</td>
<td>永久性重定向/临时性重定向</td>
</tr>
<tr>
<td>403</td>
<td>没有权限访问</td>
</tr>
<tr>
<td>404</td>
<td>没有资源访问</td>
</tr>
<tr>
<td>500</td>
<td>服务器错误</td>
</tr>
<tr>
<td>503</td>
<td>服务器停机或正在维护</td>
</tr>
</tbody>
</table>
<h2 id="登录分析"><a href="#登录分析" class="headerlink" title="登录分析"></a><strong>登录分析</strong></h2><p><img src="http://ww2.sinaimg.cn/large/006tNbRwgy1ffikxe4s70j30lo05sjsg.jpg" alt="">在登录界面，输入手机号和密码，返回的地址为 <code>Request URL:https://www.zhihu.com/login/phone_num</code><br>当输入email地址后返回的地址为 <code>Request URL:https://www.zhihu.com/login/email</code><br>并且在formdata中出现 <code>_xsrf:a71f46d549979fa192c09e11e4a463b5</code> 这样的字符串。</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNbRwgy1ffiky17f7cj30uk05mabd.jpg" alt=""></p>
<h2 id="抓取xsrf的值"><a href="#抓取xsrf的值" class="headerlink" title="抓取xsrf的值"></a><strong>抓取xsrf的值</strong></h2><p>正则匹配抓取xsrf需要使用header头来进行源代码的获取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_xsrf</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    获取xsrf code</div><div class="line">    :return: xsrf code</div><div class="line">    """</div><div class="line">    response = requests.get(<span class="string">"https://www.zhihu.com"</span>, headers=headers)</div><div class="line">    match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response.text)</div><div class="line">    <span class="keyword">if</span> match_obj:</div><div class="line">        print(match_obj.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">""</span></div></pre></td></tr></table></figure>
<h2 id="验证码获取"><a href="#验证码获取" class="headerlink" title="验证码获取"></a><strong>验证码获取</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_captcha</span><span class="params">()</span>:</span></div><div class="line">    t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">    captcha_url = <span class="string">'https://www.zhihu.com/captcha.gif?r='</span> + t + <span class="string">"&amp;type=login"</span></div><div class="line">    r = session.get(captcha_url, headers=headers)</div><div class="line">    <span class="keyword">with</span> open(<span class="string">'captcha.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(r.content)</div><div class="line">        f.close()</div><div class="line">    im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">    im.show()</div><div class="line">    im.close()</div><div class="line">    captcha = input(<span class="string">"请输入验证码：\n"</span>)</div><div class="line">    <span class="keyword">return</span> captcha</div></pre></td></tr></table></figure>
<h2 id="登录逻辑"><a href="#登录逻辑" class="headerlink" title="登录逻辑"></a><strong>登录逻辑</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zhihu_login</span><span class="params">(account, password)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎登录</div><div class="line">    :param account: </div><div class="line">    :param password: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> re.match(<span class="string">"^1\d&#123;10&#125;$"</span>, account):</div><div class="line">        print(<span class="string">"手机号码登录 \n"</span>)</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">"_xsrf"</span>: get_xsrf(),</div><div class="line">            <span class="string">"phone_num"</span>: account,</div><div class="line">            <span class="string">"password"</span>: password</div><div class="line">        &#125;</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">if</span> <span class="string">"@"</span> <span class="keyword">in</span> account:</div><div class="line">            print(<span class="string">"邮箱登录 \n"</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">"你的账号输入有问题，请重新登录"</span>)</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        post_url = <span class="string">'https://www.zhihu.com/login/email'</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">'_xsrf'</span>: get_xsrf(),</div><div class="line">            <span class="string">'password'</span>: password,</div><div class="line">            <span class="string">'email'</span>: account</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="comment"># 不需要验证码直接登录成功</span></div><div class="line">    login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">    login_code = login_page.json()</div><div class="line">    <span class="keyword">if</span> login_code[<span class="string">'r'</span>] == <span class="number">1</span>:</div><div class="line">        <span class="comment"># 不输入验证码登录失败</span></div><div class="line">        <span class="comment"># 使用需要输入验证码的方式登录</span></div><div class="line">        post_data[<span class="string">"captcha"</span>] = get_captcha()</div><div class="line">        login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">        login_code = login_page.json()</div><div class="line">        print(login_code[<span class="string">'msg'</span>])</div><div class="line">    <span class="comment"># 保存 cookies 到文件，</span></div><div class="line">    <span class="comment"># 下次可以使用 cookie 直接登录，不需要输入账号和密码</span></div><div class="line">    session.cookies.save()</div></pre></td></tr></table></figure>
<p>以上代码是通过引入requests库，使用它的session方法，进行连接，构造post_data，把自己的用户名密码等信息发送到网站，并通过正则判断发送的是邮箱或是手机进行登录。<br>引入<code>import http.cookiejar as cookielib</code>，通过session.cookies.save()，对cookie进行保存。</p>
<h2 id="通过Cookie登录"><a href="#通过Cookie登录" class="headerlink" title="通过Cookie登录"></a><strong>通过Cookie登录</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 使用登录cookie信息</span></div><div class="line">session = requests.session()</div><div class="line">session.cookies = cookielib.LWPCookieJar(filename=<span class="string">"cookies.txt"</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    session.cookies.load(ignore_discard=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"Cookie未能加载"</span>)</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_login</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    通过查看用户个人信息来判断是否已经登录</div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    url = <span class="string">"https://www.zhihu.com/settings/profile"</span></div><div class="line">    response = session.get(url, headers=headers, allow_redirects=<span class="keyword">False</span>)</div><div class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure>
<p>登录只能一次，如果再次登录，可以直接通过查看cookie来判断是否为登录状态。</p>
<p>首先把cookie通过session.cookies.load装载进来，执行is_login()函数，如果成功可以访问inbox_url页面，则状态码为200表示成功。这里一定要注意allow_redirects=False，当不允许且登录时会自动跳转到登录页面，则状态码是301或者302。</p>
<h2 id="完整代码示例"><a href="#完整代码示例" class="headerlink" title="完整代码示例"></a><strong>完整代码示例</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"><span class="string">"""</span></div><div class="line">__author__ = 'lawtech'</div><div class="line">__date__ = '2017/5/9 下午3:18'</div><div class="line">"""</div><div class="line"></div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">import</span> cookielib</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    <span class="keyword">import</span> http.cookiejar <span class="keyword">as</span> cookielib</div><div class="line"></div><div class="line"><span class="comment"># 构造requests headers</span></div><div class="line">agent = <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36'</span></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">"Host"</span>: <span class="string">"www.zhihu.com"</span>,</div><div class="line">    <span class="string">"Referer"</span>: <span class="string">"https://www.zhihu.com/"</span>,</div><div class="line">    <span class="string">'User-Agent'</span>: agent</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># 使用登录cookie信息</span></div><div class="line">session = requests.session()</div><div class="line">session.cookies = cookielib.LWPCookieJar(filename=<span class="string">"cookies.txt"</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    session.cookies.load(ignore_discard=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"Cookie未能加载"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_xsrf</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    获取xsrf code</div><div class="line">    :return: xsrf code</div><div class="line">    """</div><div class="line">    response = requests.get(<span class="string">"https://www.zhihu.com"</span>, headers=headers)</div><div class="line">    match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response.text)</div><div class="line">    <span class="keyword">if</span> match_obj:</div><div class="line">        print(match_obj.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">""</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_captcha</span><span class="params">()</span>:</span></div><div class="line">    t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">    captcha_url = <span class="string">'https://www.zhihu.com/captcha.gif?r='</span> + t + <span class="string">"&amp;type=login"</span></div><div class="line">    r = session.get(captcha_url, headers=headers)</div><div class="line">    <span class="keyword">with</span> open(<span class="string">'captcha.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(r.content)</div><div class="line">        f.close()</div><div class="line">    im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">    im.show()</div><div class="line">    im.close()</div><div class="line">    captcha = input(<span class="string">"请输入验证码：\n"</span>)</div><div class="line">    <span class="keyword">return</span> captcha</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zhihu_login</span><span class="params">(account, password)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎登录</div><div class="line">    :param account: </div><div class="line">    :param password: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> re.match(<span class="string">"^1\d&#123;10&#125;$"</span>, account):</div><div class="line">        print(<span class="string">"手机号码登录 \n"</span>)</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">"_xsrf"</span>: get_xsrf(),</div><div class="line">            <span class="string">"phone_num"</span>: account,</div><div class="line">            <span class="string">"password"</span>: password</div><div class="line">        &#125;</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">if</span> <span class="string">"@"</span> <span class="keyword">in</span> account:</div><div class="line">            print(<span class="string">"邮箱登录 \n"</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">"你的账号输入有问题，请重新登录"</span>)</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        post_url = <span class="string">'https://www.zhihu.com/login/email'</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">'_xsrf'</span>: get_xsrf(),</div><div class="line">            <span class="string">'password'</span>: password,</div><div class="line">            <span class="string">'email'</span>: account</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="comment"># 不需要验证码直接登录成功</span></div><div class="line">    login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">    login_code = login_page.json()</div><div class="line">    <span class="keyword">if</span> login_code[<span class="string">'r'</span>] == <span class="number">1</span>:</div><div class="line">        <span class="comment"># 不输入验证码登录失败</span></div><div class="line">        <span class="comment"># 使用需要输入验证码的方式登录</span></div><div class="line">        post_data[<span class="string">"captcha"</span>] = get_captcha()</div><div class="line">        login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">        login_code = login_page.json()</div><div class="line">        print(login_code[<span class="string">'msg'</span>])</div><div class="line">    <span class="comment"># 保存 cookies 到文件，</span></div><div class="line">    <span class="comment"># 下次可以使用 cookie 直接登录，不需要输入账号和密码</span></div><div class="line">    session.cookies.save()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_login</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    通过查看用户个人信息来判断是否已经登录</div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    url = <span class="string">"https://www.zhihu.com/settings/profile"</span></div><div class="line">    response = session.get(url, headers=headers, allow_redirects=<span class="keyword">False</span>)</div><div class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">if</span> is_login():</div><div class="line">        print(<span class="string">"您已经登录！"</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        account = input(<span class="string">"请输入用户名：\n"</span>)</div><div class="line">        password = input(<span class="string">"请输入密码：\n"</span>)</div><div class="line">        zhihu_login(account, password)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://docs.python-requests.org/zh_CN/latest/index.html&quot;&gt;Requests&lt;/a&gt; 是以 &lt;a href=&quot;http://www.python.org/dev/peps/pep-0020&quot;&gt;PEP 20&lt;/a&gt; 的箴言为中心开发的&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Beautiful is better than ugly.(美丽优于丑陋)&lt;/li&gt;
&lt;li&gt;Explicit is better than implicit.(直白优于含蓄)&lt;/li&gt;
&lt;li&gt;Simple is better than complex.(简单优于复杂)&lt;/li&gt;
&lt;li&gt;Complex is better than complicated.(复杂优于繁琐)&lt;/li&gt;
&lt;li&gt;Readability counts.(可读性很重要)&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Requests，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRequests%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy模拟登录知乎</title>
    <link href="http://yoursite.com/2017/05/11/scrapy-login-zhihu/"/>
    <id>http://yoursite.com/2017/05/11/scrapy-login-zhihu/</id>
    <published>2017-05-11T06:18:54.000Z</published>
    <updated>2017-05-12T08:12:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy登录知乎要解决两个问题</p>
<ol>
<li>session的传递，保证处理登录是同一个状态。</li>
<li>首个登录页面的改变，由直接爬取的页面变为登录页面，再去爬取页面。</li>
</ol>
<a id="more"></a>
<p>话不多说，直接上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"zhihu"</span></div><div class="line">    allowed_domains = [<span class="string">"www.zhihu.com"</span>]</div><div class="line">    start_urls = [<span class="string">'http://www.zhihu.com/'</span>]</div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">"Host"</span>: <span class="string">"www.zhihu.com"</span>,</div><div class="line">        <span class="string">"Referer"</span>: <span class="string">"https://www.zhihu.com/"</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36'</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> [scrapy.Request(<span class="string">"https://www.zhihu.com/#signin"</span>, headers=self.headers, callback=self.login)]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        登录</div><div class="line">        :param response: </div><div class="line">        :return: </div><div class="line">        """</div><div class="line">        response_text = response.text</div><div class="line">        match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response_text, re.DOTALL)</div><div class="line">        xsrf = <span class="string">''</span></div><div class="line">        <span class="keyword">if</span> match_obj:</div><div class="line">            xsrf = match_obj.group(<span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> xsrf:</div><div class="line">            post_data = &#123;</div><div class="line">                <span class="string">"_xsrf"</span>: xsrf,</div><div class="line">                <span class="string">"phone_num"</span>: <span class="string">"18951855817"</span>,</div><div class="line">                <span class="string">"password"</span>: <span class="string">"tracy584563542"</span></div><div class="line">            &#125;</div><div class="line">            t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">            captcha_url = <span class="string">'https://www.zhihu.com/captcha.gif?r='</span> + t + <span class="string">"&amp;type=login"</span></div><div class="line">            <span class="keyword">yield</span> scrapy.Request(captcha_url, headers=self.headers, meta=&#123;<span class="string">"post_data"</span>: post_data&#125;,</div><div class="line">                                 callback=self.login_after_captcha)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login_after_captcha</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">with</span> open(<span class="string">"captcha.jpg"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(response.body)</div><div class="line">        im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">        im.show()</div><div class="line">        im.close()</div><div class="line">        captcha = input(<span class="string">"请输入验证码：\n"</span>)</div><div class="line">        post_data = response.meta.get(<span class="string">"post_data"</span>, &#123;&#125;)</div><div class="line">        post_data[<span class="string">"captcha"</span>] = captcha</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        <span class="keyword">return</span> [scrapy.FormRequest(</div><div class="line">            url=post_url,</div><div class="line">            formdata=post_data,</div><div class="line">            headers=self.headers,</div><div class="line">            callback=self.check_login</div><div class="line">        )]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        验证服务器的返回数据判断登录是否成功</div><div class="line">        :param response: </div><div class="line">        :return: </div><div class="line">        """</div><div class="line">        text_json = json.loads(response.text)</div><div class="line">        <span class="keyword">if</span> <span class="string">'msg'</span> <span class="keyword">in</span> text_json <span class="keyword">and</span> text_json[<span class="string">'msg'</span>] == <span class="string">'登陆成功'</span>:</div><div class="line">            <span class="comment"># 从继承的Spider类中拿的内容，恢复到正确执行</span></div><div class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(url, dont_filter=<span class="keyword">True</span>, headers=self.headers)</div></pre></td></tr></table></figure>
<p>首先对 <code>scrapy.Spider</code> 类中的 <code>start_requests(self)</code> 进行重载，改变首先要处理的页面为登录页面。<br>得到登录页面后，获得xsrf，并下载验证码，通过 <code>scrapy.FormRequest</code>构造登录数据，通过check_login回调函数判断登录是否成功。在代码的最后一行转回正常的登录流程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy登录知乎要解决两个问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;session的传递，保证处理登录是同一个状态。&lt;/li&gt;
&lt;li&gt;首个登录页面的改变，由直接爬取的页面变为登录页面，再去爬取页面。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——理解Session和Cookie机制</title>
    <link href="http://yoursite.com/2017/05/09/scrapy-session-cookie/"/>
    <id>http://yoursite.com/2017/05/09/scrapy-session-cookie/</id>
    <published>2017-05-09T06:18:54.000Z</published>
    <updated>2017-05-09T06:43:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Cookie-机制"><a href="#Cookie-机制" class="headerlink" title="Cookie 机制"></a><strong>Cookie 机制</strong></h2><p>Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。</p>
<a id="more"></a>
<p>具体来说cookie机制采用的是在客户端保持状态的方案。它是在用户端的会话状态的存贮机制，他需要用户打开客户端的cookie支持。cookie的作用就是为了解决HTTP协议无状态的缺陷所作的努力。</p>
<p>正统的cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示浏览器按照指示生成相应的cookie。然而纯粹的客户端脚本如JavaScript也可以生成cookie。而cookie的使用是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的cookie，如果某个cookie所声明的作用范围大于等于将要请求的资源所在的位置，则把该cookie附在请求资源的HTTP请求头上发送给服务器。</p>
<p>cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。若不设置过期时间，则表示这个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式。</p>
<p>而session机制采用的是一种在服务器端保持状态的解决方案。同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的。而session提供了方便管理全局变量的方式 。</p>
<p>session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，这个值也可能设置为由get来返回给服务器。</p>
<p>就安全性来说：当你访问一个使用session 的站点，同时在自己机子上建立一个cookie，建立在服务器端的session机制更安全些，因为它不会任意读取客户存储的信息。 </p>
<h2 id="Session-机制"><a href="#Session-机制" class="headerlink" title="Session 机制"></a><strong>Session 机制</strong></h2><p>session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。</p>
<p>当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个session标识（称为session id），如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来使用（检索不到，会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个session id将被在本次响应中返回给客户端保存。</p>
<p>保存这个session id的方式可以采用cookie，这样在交互过程中浏览器可以自动的按照规则把这个标识发挥给服务器。一般这个cookie的名字都是类似于SEEESIONID。但cookie可以被人为的禁止，则必须有其他机制以便在cookie被禁止时仍然能够把session id传递回服务器。</p>
<p>经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路径的后面。还有一种技术叫做表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器。</p>
<p>Cookie与Session都能够进行会话跟踪，但是完成的原理不太一样。普通状况下二者均能够满足需求，但有时分不能够运用Cookie，有时分不能够运用Session。</p>
<h2 id="两者比较"><a href="#两者比较" class="headerlink" title="两者比较"></a><strong>两者比较</strong></h2><h3 id="存取方式不同"><a href="#存取方式不同" class="headerlink" title="存取方式不同"></a>存取方式不同</h3><p>Cookie中只能保管ASCII字符串，假如需求存取Unicode字符或者二进制数据，需求先进行编码。Cookie中也不能直接存取Java对象。若要存储略微复杂的信息，运用Cookie是比拟艰难的。</p>
<p>而Session中能够存取任何类型的数据，包括而不限于String、Integer、List、Map等。Session中也能够直接保管Java Bean乃至任何Java类，对象等，运用起来十分便当。能够把Session看做是一个Java容器类。</p>
<h3 id="隐私策略不同"><a href="#隐私策略不同" class="headerlink" title="隐私策略不同"></a>隐私策略不同</h3><p>Cookie存储在客户端阅读器中，对客户端是可见的，客户端的一些程序可能会窥探、复制以至修正Cookie中的内容。而Session存储在服务器上，对客户端是透明的，不存在敏感信息泄露的风险。</p>
<p>假如选用Cookie，比较好的方法是，敏感的信息如账号密码等尽量不要写到Cookie中。最好是像Google、Baidu那样将Cookie信息加密，提交到服务器后再进行解密，保证Cookie中的信息只要本人能读得懂。而假如选择Session就省事多了，反正是放在服务器上，Session里任何隐私都能够有效的保护。</p>
<h3 id="服务器压力不同"><a href="#服务器压力不同" class="headerlink" title="服务器压力不同"></a>服务器压力不同</h3><p>Session是保管在服务器端的，每个用户都会产生一个Session。假如并发访问的用户十分多，会产生十分多的Session，耗费大量的内存。因而像Google、Baidu、Sina这样并发访问量极高的网站，是不太可能运用Session来追踪客户会话的。</p>
<p>而Cookie保管在客户端，不占用服务器资源。假如并发阅读的用户十分多，Cookie是很好的选择。关于Google、Baidu、Sina来说，Cookie或许是唯一的选择。</p>
<h3 id="浏览器支持不同"><a href="#浏览器支持不同" class="headerlink" title="浏览器支持不同"></a>浏览器支持不同</h3><p>Cookie是需要客户端浏览器支持的。假如客户端禁用了Cookie，或者不支持Cookie，则会话跟踪会失效。关于WAP上的应用，常规的Cookie就派不上用场了。</p>
<p>假如客户端浏览器不支持Cookie，需要运用Session以及URL地址重写。需要注意的是一切的用到Session程序的URL都要进行URL地址重写，否则Session会话跟踪还会失效。关于WAP应用来说，Session+URL地址重写或许是它唯一的选择。</p>
<p>假如客户端支持Cookie，则Cookie既能够设为本浏览器窗口以及子窗口内有效（把过期时间设为–1），也能够设为一切阅读器窗口内有效（把过期时间设为某个大于0的整数）。但Session只能在本阅读器窗口以及其子窗口内有效。假如两个浏览器窗口互不相干，它们将运用两个不同的Session。（IE8下不同窗口Session相干）</p>
<h3 id="跨域支持不同"><a href="#跨域支持不同" class="headerlink" title="跨域支持不同"></a>跨域支持不同</h3><p>Cookie支持跨域名访问，例如将domain属性设置为“.biaodianfu.com”，则以“.biaodianfu.com”为后缀的一切域名均能够访问该Cookie。跨域名Cookie如今被普遍用在网络中，例如Google、Baidu、Sina等。而Session则不会支持跨域名访问。Session仅在他所在的域名内有效。</p>
<h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><p>仅运用Cookie或者仅运用Session可能完成不了理想的效果。这时应该尝试一下同时运用Cookie与Session。Cookie与Session的搭配运用在实践项目中会完成很多意想不到的效果。</p>
<h2 id="Python-Django-中实现两种机制"><a href="#Python-Django-中实现两种机制" class="headerlink" title="Python Django 中实现两种机制"></a>Python Django 中实现两种机制</h2><h3 id="Cookie-设置"><a href="#Cookie-设置" class="headerlink" title="Cookie 设置"></a>Cookie 设置</h3><p>以下是Cookie设置的详细流程：</p>
<ol>
<li>客户端发起一个请求连接（如HTTP GET）</li>
<li>服务器在http响应头上加上Set-Cookie，里面存放字符串的键值对</li>
<li>客户端随后的http请求头加上Cookie首部，它包含了之前服务器响应中设置cookie的信息。</li>
</ol>
<p>根据这个Cookie首部的信息，服务器便能“记住”当前用户的信息。</p>
<p>下面就来看看Python中如何设置Cookie：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> BaseHTTPServer <span class="keyword">import</span> HTTPServer</div><div class="line"><span class="keyword">from</span> SimpleHTTPServer <span class="keyword">import</span> SimpleHTTPRequestHandler</div><div class="line"><span class="keyword">import</span> Cookie</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRequestHandler</span><span class="params">(SimpleHTTPRequestHandler)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_GET</span><span class="params">(self)</span>:</span></div><div class="line">        content = <span class="string">"&lt;html&gt;&lt;body&gt;Path is: %s&lt;/body&gt;&lt;/html&gt;"</span> % self.path</div><div class="line">        self.send_response(<span class="number">200</span>)</div><div class="line">        self.send_header(<span class="string">'Content-type'</span>, <span class="string">'text/html'</span>)</div><div class="line">        self.send_header(<span class="string">'Content-length'</span>, str(len(content)))</div><div class="line"></div><div class="line">        cookie = Cookie.SimpleCookie()</div><div class="line">        cookie[<span class="string">'id'</span>] = <span class="string">'some_value_42'</span></div><div class="line"></div><div class="line">        self.wfile.write(cookie.output())</div><div class="line">        self.wfile.write(<span class="string">'\r\n'</span>)</div><div class="line"></div><div class="line">        self.end_headers()</div><div class="line">        self.wfile.write(content)</div><div class="line"></div><div class="line">server = HTTPServer((<span class="string">''</span>, <span class="number">59900</span>), MyRequestHandler)</div><div class="line">server.serve_forever()</div></pre></td></tr></table></figure>
<p>查看服务器端的http响应头，会发现以下字段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Set-Cookie: id=some_value_42</div></pre></td></tr></table></figure>
<p>在Django中，可以用如下的方式获取或设置Cookie：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_cookie</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'id'</span> <span class="keyword">in</span> request.COOKIES:</div><div class="line">        cookie_id = request.COOKIES[<span class="string">'id'</span>]</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'Got cookie with id=%s'</span> % cookie_id)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        resp = HttpResponse(<span class="string">'No id cookie! Sending cookie to client'</span>)</div><div class="line">        resp.set_cookie(<span class="string">'id'</span>, <span class="string">'some_value_99'</span>)</div><div class="line">        <span class="keyword">return</span> resp</div></pre></td></tr></table></figure>
<p>Django通过一系列的包装使得封装Cookie的操作变得更加简单，那么它在其中是怎么实现cookie的读取的呢，下面来窥探原理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_cookies</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'_cookies'</span>):</div><div class="line">        self._cookies = http.parse_cookie(self.environ.get(<span class="string">'HTTP_COOKIE'</span>, <span class="string">''</span>))</div><div class="line">    <span class="keyword">return</span> self._cookies</div></pre></td></tr></table></figure>
<p>可以看出，获取cookie的操作用了Lazy initialization（延迟加载）的技术，因为如果客户端不需要用到cookie，这个过程只会浪费不必要的操作。</p>
<p>再来看parse_cookie的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_cookie</span><span class="params">(cookie)</span>:</span></div><div class="line">    <span class="keyword">if</span> cookie == <span class="string">''</span>:</div><div class="line">        <span class="keyword">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(cookie, Cookie.BaseCookie):</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            c = SimpleCookie()</div><div class="line">            c.load(cookie, ignore_parse_errors=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">except</span> Cookie.CookieError:</div><div class="line">            <span class="comment"># 无效cookie</span></div><div class="line">            <span class="keyword">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        c = cookie</div><div class="line">    cookiedict = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> c.keys():</div><div class="line">        cookiedict[key] = c.get(key).value</div><div class="line">    <span class="keyword">return</span> cookiedict</div></pre></td></tr></table></figure>
<p>它负责解析Cookie并把结果集成到一个dict（字典）对象中，并返回字典。而设置cookie的操作则会被WSGIHandler执行。</p>
<p>注：Django的底层实现了WSGI的接口（如WSGIRequest，WSGIServer等）。</p>
<h3 id="Session-应用"><a href="#Session-应用" class="headerlink" title="Session 应用"></a>Session 应用</h3><p>下面看一个简单的session应用例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_count_session</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'count'</span> <span class="keyword">in</span> request.session:</div><div class="line">        request.session[<span class="string">'count'</span>] += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'new count=%s'</span> % request.session[<span class="string">'count'</span>])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        request.session[<span class="string">'count'</span>] = <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'No count in session. Setting to 1'</span>)</div></pre></td></tr></table></figure>
<p>它用session实现了一个计数器，当每一个请求到来时，就为计数器加一，把新的结果更新到session中。</p>
<p>查看http的响应头，会得到类似下面的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Set-Cookie:sessionid=a92d67e44a9b92d7dafca67e507985c0;</div><div class="line">           expires=Thu, <span class="number">07</span>-Jul<span class="number">-2011</span> <span class="number">04</span>:<span class="number">16</span>:<span class="number">28</span> GMT;</div><div class="line">           Max-Age=<span class="number">1209600</span>;</div><div class="line">           Path=/</div></pre></td></tr></table></figure>
<p>里面包含了session_id以及过期时间等信息。</p>
<p>那么服务器端是如何保存session的呢？</p>
<p>在django中，默认会把session保存在setting指定的数据库中，除此之外，也可以通过指定session engine，使session保存在文件(file)，内存(cache)中。</p>
<p>如果保存在数据库中，django会在数据库中创建一个如下的session表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE &quot;django_session&quot; (</div><div class="line">    &quot;session_key&quot; varchar(40) NOT NULL PRIMARY KEY,</div><div class="line">    &quot;session_data&quot; text NOT NULL,</div><div class="line">    &quot;expire_date&quot; datetime NOT NULL</div><div class="line">);</div></pre></td></tr></table></figure>
<p>session_key是放置在cookie中的id，它是唯一的，而session_data则存放序列化后的session数据字符串。</p>
<p>通过session_key可以在数据库中取得这条session的信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.contrib.sessions.models <span class="keyword">import</span> Session</div><div class="line"><span class="comment">#...</span></div><div class="line">sess = Session.objects.get(pk=<span class="string">'a92d67e44a9b92d7dafca67e507985c0'</span>)</div><div class="line">print(sess.session_data)</div><div class="line">print(sess.get_decoded())</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ZmEyNDVhNTBhMTk2ZmRjNzVlYzQ4NTFjZDk2Y2UwODc3YmVjNWVjZjqAAn1xAVUFY291bnRxAksG</div><div class="line">cy4=&#123;<span class="string">'count'</span>: <span class="number">6</span>&#125;</div></pre></td></tr></table></figure>
<p>回看第一个例子，我们是通过request.session来获取session的，为什么请求对象会附带一个session对象呢，这其中做了什么呢？</p>
<p>这就引出了下面要说的django里的中间件技术 <code>Session middleware</code>。</p>
<p>关于中间件，<code>&lt;&lt;the Django Book&gt;&gt;</code>是这样解释的：</p>
<p>Django的中间件框架，是django处理请求和响应的一套钩子函数的集合。</p>
<p>我们看传统的django视图模式一般是这样的：http请求-&gt;view-&gt;http响应，而加入中间件框架后，则变为：http请求-&gt;中间件处理-&gt;app-&gt;中间件处理-&gt;http响应。而在django中这两个处理分别对应process_request和process_response函数，这两个钩子函数将会在特定的时候被触发。</p>
<p>直接看SessionMiddleware可能更清晰一些：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SessionMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        engine = import_module(settings.SESSION_ENGINE)</div><div class="line">        self.SessionStore = engine.SessionStore</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request)</span>:</span></div><div class="line">        session_key = request.COOKIES.get(settings.SESSION_COOKIE_NAME)</div><div class="line">        request.session = self.SessionStore(session_key)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        If request.session was modified, or if the configuration is to save the</div><div class="line">        session every time, save the changes and set a session cookie or delete</div><div class="line">        the session cookie if the session has been emptied.</div><div class="line">        """</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            accessed = request.session.accessed</div><div class="line">            modified = request.session.modified</div><div class="line">            empty = request.session.is_empty()</div><div class="line">        <span class="keyword">except</span> AttributeError:</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># First check if we need to delete this cookie.</span></div><div class="line">            <span class="comment"># The session should be deleted only if the session is entirely empty</span></div><div class="line">            <span class="keyword">if</span> settings.SESSION_COOKIE_NAME <span class="keyword">in</span> request.COOKIES <span class="keyword">and</span> empty:</div><div class="line">                response.delete_cookie(settings.SESSION_COOKIE_NAME,</div><div class="line">                    domain=settings.SESSION_COOKIE_DOMAIN)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">if</span> accessed:</div><div class="line">                    patch_vary_headers(response, (<span class="string">'Cookie'</span>,))</div><div class="line">                <span class="keyword">if</span> (modified <span class="keyword">or</span> settings.SESSION_SAVE_EVERY_REQUEST) <span class="keyword">and</span> <span class="keyword">not</span> empty:</div><div class="line">                    <span class="keyword">if</span> request.session.get_expire_at_browser_close():</div><div class="line">                        max_age = <span class="keyword">None</span></div><div class="line">                        expires = <span class="keyword">None</span></div><div class="line">                    <span class="keyword">else</span>:</div><div class="line">                        max_age = request.session.get_expiry_age()</div><div class="line">                        expires_time = time.time() + max_age</div><div class="line">                        expires = cookie_date(expires_time)</div><div class="line">                    <span class="comment"># Save the session data and refresh the client cookie.</span></div><div class="line">                    <span class="comment"># Skip session save for 500 responses, refs #3881.</span></div><div class="line">                    <span class="keyword">if</span> response.status_code != <span class="number">500</span>:</div><div class="line">                        <span class="keyword">try</span>:</div><div class="line">                            request.session.save()</div><div class="line">                        <span class="keyword">except</span> UpdateError:</div><div class="line">                            <span class="comment"># The user is now logged out; redirecting to same</span></div><div class="line">                            <span class="comment"># page will result in a redirect to the login page</span></div><div class="line">                            <span class="comment"># if required.</span></div><div class="line">                            <span class="keyword">return</span> redirect(request.path)</div><div class="line">                        response.set_cookie(settings.SESSION_COOKIE_NAME,</div><div class="line">                                request.session.session_key, max_age=max_age,</div><div class="line">                                expires=expires, domain=settings.SESSION_COOKIE_DOMAIN,</div><div class="line">                                path=settings.SESSION_COOKIE_PATH,</div><div class="line">                                secure=settings.SESSION_COOKIE_SECURE <span class="keyword">or</span> <span class="keyword">None</span>,</div><div class="line">                                httponly=settings.SESSION_COOKIE_HTTPONLY <span class="keyword">or</span> <span class="keyword">None</span>)</div><div class="line">        <span class="keyword">return</span> response</div></pre></td></tr></table></figure>
<p>在请求到来后，SessionMiddleware的process_request在请求取出session_key，并把一个新的session对象赋给request.session，而在返回响应时，process_response则判断session是否被修改或过期，来更新session的信息。</p>
<h3 id="Django-用户认证中的-Session"><a href="#Django-用户认证中的-Session" class="headerlink" title="Django 用户认证中的 Session"></a>Django 用户认证中的 Session</h3><p>在django中，用下面的方法来验证用户是否登录是常见的事情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_user</span><span class="params">(request)</span>:</span></div><div class="line">    user_str = str(request.user)</div><div class="line">    <span class="keyword">if</span> request.user.is_authenticated():</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'%s is logged in'</span> % user_str)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'%s is not logged in'</span> % user_str)</div></pre></td></tr></table></figure>
<p>其实request.user的实现也借助到了session。</p>
<p>在这个例子中，成功登录后，session表会保存类似下面的信息，里面记录了用户的id，以后进行验证时，便会到这个表中获取用户的信息。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;'_auth_user_id': 1, '_auth_user_backend': 'django.contrib.auth.backends.ModelBackend'&#125;</div></pre></td></tr></table></figure>
<p>跟上面提到的Session中间件相似，用户验证也有一个中间件：AuthenticationMiddleware，在process_request中，通过request.<strong>class</strong>.user = LazyUser()在request设置了一个全局的可缓存的用户对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyUser</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, request, obj_type=None)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(request, <span class="string">'_cached_user'</span>):</div><div class="line">            <span class="keyword">from</span> django.contrib.auth <span class="keyword">import</span> get_user</div><div class="line">            request._cached_user = get_user(request)</div><div class="line">        <span class="keyword">return</span> request._cached_user</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuthenticationMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request)</span>:</span></div><div class="line">        request.__class__.user = LazyUser()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div></pre></td></tr></table></figure>
<p>在get_user里，会在检查session中是否存放了当前用户对应的user_id，如果有，则通过id在model查找相应的用户返回，否则返回一个匿名的用户对象(AnonymousUser)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">from</span> django.contrib.auth.models <span class="keyword">import</span> AnonymousUser</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        user_id = request.session[SESSION_KEY]</div><div class="line">        backend_path = request.session[BACKEND_SESSION_KEY]</div><div class="line">        backend = load_backend(backend_path)</div><div class="line">        user = backend.get_user(user_id) <span class="keyword">or</span> AnonymousUser()</div><div class="line">    <span class="keyword">except</span> KeyError:</div><div class="line">        user = AnonymousUser()</div><div class="line">    <span class="keyword">return</span> user</div></pre></td></tr></table></figure>
<h3 id="Django中的Session实现"><a href="#Django中的Session实现" class="headerlink" title="Django中的Session实现"></a>Django中的Session实现</h3><p>Django使用的Session默认都继承于SessionBase类里，这个类实现了一些session操作方法，以及hash，decode，encode等方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SessionBase</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Base class for all Session classes.</div><div class="line">    """</div><div class="line">    TEST_COOKIE_NAME = <span class="string">'testcookie'</span></div><div class="line">    TEST_COOKIE_VALUE = <span class="string">'worked'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, session_key=None)</span>:</span></div><div class="line">        self._session_key = session_key</div><div class="line">        self.accessed = <span class="keyword">False</span></div><div class="line">        self.modified = <span class="keyword">False</span></div><div class="line">        self.serializer = import_string(settings.SESSION_SERIALIZER)</div></pre></td></tr></table></figure>
<p>说的更直白一些，其实django中的session就是一个模拟dict的对象，并实现了一系列的hash和序列化方法，默认持久化在数据库中（有时候也可能由于为了提高性能，用redis之类的内存数据库来缓存session）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Cookie-机制&quot;&gt;&lt;a href=&quot;#Cookie-机制&quot; class=&quot;headerlink&quot; title=&quot;Cookie 机制&quot;&gt;&lt;/a&gt;&lt;strong&gt;Cookie 机制&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Django，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CDjango%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Loaders机制介绍与实例</title>
    <link href="http://yoursite.com/2017/05/08/scrapy-item-loader/"/>
    <id>http://yoursite.com/2017/05/08/scrapy-item-loader/</id>
    <published>2017-05-08T06:18:54.000Z</published>
    <updated>2017-05-08T07:02:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 </p>
<p>从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。</p>
<p>Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。</p>
<a id="more"></a>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="使用-Item-Loaders-来填充-Items"><a href="#使用-Item-Loaders-来填充-Items" class="headerlink" title="使用 Item Loaders 来填充 Items"></a><strong>使用 Item Loaders 来填充 Items</strong></h3><p>要使用 Item Loader, 你必须先将它实例化。你可以使用类似字典的对象(例如: Item or dict)来进行实例化，或者不使用对象也可以，当不用对象进行实例化的时候，Item 会自动使用 <code>ItemLoader.default\_item_class</code> 属性中指定的 Item 类在 <code>Item Loader constructor</code> 中实例化。 </p>
<p>然后，你开始收集数值到 Item Loader 时，通常使用 Selectors。你可以在同一个 item field 里面添加多个数 值；Item Loader 将知道如何用合适的处理函数来“添加”这些数值。 下面是在 Spider 中典型的 Item Loader 的用法，使用 <code>Items chapter</code> 中声明的 <code>Product item</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader </div><div class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> Product</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span> </div><div class="line">    l = ItemLoader(item=Product(), response=response) </div><div class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_name"]'</span>) </div><div class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_title"]'</span>) </div><div class="line">    l.add_xpath(<span class="string">'price'</span>, <span class="string">'//p[@id="price"]'</span>) </div><div class="line">    l.add_css(<span class="string">'stock'</span>, <span class="string">'p#stock]'</span>) </div><div class="line">    l.add_value(<span class="string">'last_updated'</span>, <span class="string">'today'</span>) <span class="comment"># you can also use literal values </span></div><div class="line">    <span class="keyword">return</span> l.load_item()</div></pre></td></tr></table></figure>
<p>快速查看这些代码之后，我们可以看到发现 name 字段被从页面中两个不同的 XPath 位置提取：</p>
<ol>
<li><code>//div[@class=&quot;product_name&quot;]</code></li>
<li><code>//div[@class=&quot;product_title&quot;]</code></li>
</ol>
<p>换言之,数据通过用 <code>add_xpath()</code> 的方法，把从两个不同的 XPath 位置提取的数据收集起来。这是将在以后分配给 <code>name</code> 字段中的数据?</p>
<p>之后，类似的请求被用于 price 和 stock 字段 （后者使用 <code>CSS selector</code> 和 <code>add_css()</code> 方法）， 最后使用不同的方法 <code>add_value()</code> 对 <code>last_update</code> 填充文本值( today )。 最终，当所有数据被收集起来之后，调用 <code>ItemLoader.load_item()</code> 方法，实际上填充并且返回了之前通过调用 <code>add_xpath()</code>，<code>add_css()</code> ，<code>add_value()</code> 所提取和收集到的数据的 Item。</p>
<h3 id="输入和输出处理器"><a href="#输入和输出处理器" class="headerlink" title="输入和输出处理器"></a><strong>输入和输出处理器</strong></h3><p>Item Loader 在每个（Item）字段中都包含了一个输入处理器和一个输出处理器。输入处理器收到数据时立刻提取数据 （通过  <code>add_xpath()</code>， <code>add_css()</code>  或者  <code>add_value()</code>方法）之后输入处理器的结果被收集起来并且保存在ItemLoader内。收集到所有的数据后，调用 <code>ItemLoader.load_item()</code> 方法来填充，并得到填充后的 Item 对象。这是当输出处理器被和之前收集到的数据（和用输入处理器处理的）被调用。输出处理器的结果是被分配到Item的最终值。</p>
<p>让我们看一个例子来说明如何输入和输出处理器被一个特定的字段调用（同样适用于其他field）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">l = ItemLoader(Product(), some_selector)</div><div class="line">l.add_xpath(<span class="string">'name'</span>, xpath1) <span class="comment"># (1)</span></div><div class="line">l.add_xpath(<span class="string">'name'</span>, xpath2) <span class="comment"># (2)</span></div><div class="line">l.add_css(<span class="string">'name'</span>, css) <span class="comment"># (3)</span></div><div class="line">l.add_value(<span class="string">'name'</span>, <span class="string">'test'</span>) <span class="comment"># (4)</span></div><div class="line"><span class="keyword">return</span> l.load_item() <span class="comment"># (5)</span></div></pre></td></tr></table></figure>
<p>发生了这些事情:</p>
<ol>
<li>从 <code>xpath1</code> 提取出的数据,传递给 <em>输入处理器</em> 的 <code>name</code> 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡</li>
<li>从 <code>xpath2</code> 提取出来的数据,传递给(1)中使用的相同的 <em>输入处理器</em> .输入处理器的结果被附加到在(1)中收集的数据(如果有的话) ｡</li>
<li>和之前相似，只不过这里的数据是通过 <code>css</code> CSS selector抽取，之后传输到在(1)和(2)使用 的<em>input processor</em> 中。最终输入处理器的结果被附加到在(1)和(2)中收集的数据之后 (如果存在数据的话)。</li>
<li>这里的处理方式也和之前相似，但是此处的值是通过add_value直接赋予的， 而不是利用XPath表达式或CSS selector获取。得到的值仍然是被传送到输入处理器。 在这里例程中，因为得到的值并非可迭代，所以在传输到输入处理器之前需要将其 转化为可迭代的单个元素，这才是它所接受的形式。</li>
<li>在之前步骤中所收集到的数据被传送到 <em>output processor</em> 的 <code>name</code> field中。 输出处理器的结果就是赋到item中 <code>name</code> field的值。</li>
</ol>
<p><strong>理解：</strong></p>
<p>就是在使用Item Loader 时候，会有一个输入处理器，一个输出处理器，首先是收集好同一个字段的结果，传入到<strong>输入处理器</strong>当中，然后收集完后，会传递给<strong>输出处理器</strong>进行处理。输出处理器的处理结果，就是填充到item的结果。</p>
<p>需要注意的是：输入处理器的返回值会是内部收集的，然后被传递给输出处理器，来填充fields。</p>
<h4 id="Scrapy-内部的处理器"><a href="#Scrapy-内部的处理器" class="headerlink" title="Scrapy 内部的处理器"></a><strong>Scrapy 内部的处理器</strong></h4><p>Scrapy内部，已经有一些设置好的<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#topics-loaders-available-processors" target="_blank" rel="external">内置处理器</a></p>
<ul>
<li>Identity<br>这是最简单的一个处理器，实际上就是什么都不做，传入多少个字段，就存储多少个字段，以list形式。<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import Identity
\&gt;&gt;&gt; proc = Identity()
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</code> </li>
<li>TakeFirst<br>从接受到的list中返回第一个非null/非空的值，<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import TakeFirst
\&gt;&gt;&gt; proc = TakeFirst()
\&gt;&gt;&gt; proc([&#39;&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
&#39;one&#39;</code> </li>
<li>Join<br>返回用分隔符（separator）作为间隔的连接形成的字符串。若不传入separator，则默认使用’ ‘（空格）。<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import Join
\&gt;&gt;&gt; proc = Join()
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
u&#39;one two three&#39;
\&gt;&gt;&gt; proc = Join(&#39;&lt;br&gt;&#39;)
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
u&#39;one&lt;br&gt;two&lt;br&gt;three&#39;</code> </li>
<li>另外还有Compose以及MapCompose，这里不一一详述。</li>
</ul>
<hr>
<h3 id="声明-Item-Loaders"><a href="#声明-Item-Loaders" class="headerlink" title="声明 Item Loaders"></a><strong>声明 Item Loaders</strong></h3><p>声明ItemLoaders 和声明Item类似，使用Class语法，例子：</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="title">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader</div><div class="line"><span class="title">from</span> scrapy.contrib.loader.processor <span class="keyword">import</span> TakeFirst, MapCompose, Join</div><div class="line"><span class="class"></span></div><div class="line"><span class="keyword">class</span> <span class="type">ProductLoader</span>(<span class="type">ItemLoader</span>):</div><div class="line"></div><div class="line">    default_output_processor = <span class="type">TakeFirst</span>()</div><div class="line"></div><div class="line">    name_in = <span class="type">MapCompose</span>(<span class="title">unicode</span>.<span class="title">title</span>)</div><div class="line">    name_out = <span class="type">Join</span>()</div><div class="line"></div><div class="line">    price_in = <span class="type">MapCompose</span>(<span class="title">unicode</span>.<span class="title">strip</span>)</div><div class="line"></div><div class="line">    # ...</div></pre></td></tr></table></figure>
<p>上述代码中:</p>
<ul>
<li>输出处理器，被声明为 <code>_in</code> 前缀，而输出处理器被声明为 <code>_out</code> 前缀。</li>
<li>设置默认处理器 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_input_processor" target="_blank" rel="external"><code>ItemLoader.default_input_processor</code></a>  and  <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_output_processor" target="_blank" rel="external"><code>ItemLoader.default_output_processor</code></a></li>
</ul>
<h3 id="声明输入、输出处理器"><a href="#声明输入、输出处理器" class="headerlink" title="声明输入、输出处理器"></a><strong>声明输入、输出处理器</strong></h3><p>输入、输出可以被如上方那样被声明，这也是最正常的方式。另外，我们也可以在另外的一个地方去声明输入和输出处理器：在item Field，元数据中。</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="title">from</span> scrapy.contrib.loader.processor <span class="keyword">import</span> Join, MapCompose, TakeFirst</div><div class="line"><span class="title">from</span> w3lib.html <span class="keyword">import</span> remove_tags</div><div class="line"></div><div class="line"><span class="title">def</span> filter_price(value):</div><div class="line">    <span class="keyword">if</span> value.isdigit():</div><div class="line">        return value</div><div class="line"><span class="class"></span></div><div class="line"><span class="keyword">class</span> <span class="type">Product</span>(<span class="title">scrapy</span>.<span class="type">Item</span>):</div><div class="line">    name = scrapy.<span class="type">Field</span>(</div><div class="line">        <span class="title">input_processor</span>=<span class="type">MapCompose(remove_tags)</span>,</div><div class="line">        <span class="title">output_processor</span>=<span class="type">Join</span>(),</div><div class="line">    )</div><div class="line">    price = scrapy.<span class="type">Field</span>(</div><div class="line">        <span class="title">input_processor</span>=<span class="type">MapCompose</span>(<span class="title">remove_tags</span>, <span class="title">filter_price</span>),</div><div class="line">        output_processor=<span class="type">TakeFirst</span>(),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>输出和输出处理器的优先级如下：</p>
<ol>
<li>Item Loader field 指定的<code>field_in</code> 和 <code>field_out</code>（最优先）</li>
<li>Field 元数据中(input_processor 和 output_processor key)</li>
<li>item loader 默认。 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_input_processor" target="_blank" rel="external"><code>ItemLoader.default_input_processor()</code></a> and<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_output_processor" target="_blank" rel="external"><code>ItemLoader.default_output_processor()</code></a> (least precedence)</li>
</ol>
<hr>
<p>最后，给出Item Loader的官方说明API：</p>
<p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#itemloader-objects" target="_blank" rel="external">ItemLoader objects</a></p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a><strong>实例</strong></h2><h3 id="通过-Item-loader-加载-Item"><a href="#通过-Item-loader-加载-Item" class="headerlink" title="通过 Item loader 加载 Item"></a><strong>通过 Item loader 加载 Item</strong></h3><p>首先在 <code>jobbole.py</code> 中引入 <code>from scrapy.loader import ItemLoader</code></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</div><div class="line">item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</div><div class="line">item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</div><div class="line">item_loader.add_css(<span class="string">"create_date"</span>, <span class="string">"p.entry-meta-hide-on-mobile::text"</span>)</div><div class="line">front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></div><div class="line">item_loader.add_value(<span class="string">"front_image_url"</span>, [front_image_url])</div><div class="line">item_loader.add_css(<span class="string">"praise_nums"</span>, <span class="string">".vote-post-up h10::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"comment_nums"</span>, <span class="string">"a[href='#article-comment'] span::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"fav_nums"</span>, <span class="string">".bookmark-btn::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"tags"</span>, <span class="string">"p.entry-meta-hide-on-mobile a::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"content"</span>, <span class="string">"div.entry"</span>)</div><div class="line"></div><div class="line">article_item = item_loader.load_item()</div></pre></td></tr></table></figure>
<p>其中第一行中 <code>JobBoleArticleItem()</code> 为在 <code>items.py</code> 中声明的实例，<code>response</code> 为返回的响应。这属于固定写法。<br><code>add_css()</code>中第一个值为 <code>items.py</code> 中定义的值，第二个值为css选择器规则，类似的方法还有 <code>add_xpath()</code>，根据场景进行选择。</p>
<p>同理，<code>add_value()</code>为添加确定值的方法。这里通过值传递附给 <code>front_image_url</code> 再通过add_value的方法，加入到最终的item中。</p>
<p>最后通过调用 <code>load_item()</code> 方法对结果进行解析，所有的结果都是一个list并保存到 <code>article_item</code> 中。</p>
<p>断点调试结果如图：</p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1ffdwrwwdu2j31kw1ax1a5.jpg" alt=""></p>
<p>发现获取到的所有值都是一个list，这样很不方便，但使得代码可读性更高，可维护性更强。</p>
<h3 id="通过-items-py-处理数据"><a href="#通过-items-py-处理数据" class="headerlink" title="通过 items.py 处理数据"></a><strong>通过 items.py 处理数据</strong></h3><p>在 <code>items.py</code> 中引入 <code>from scrapy.loader.processors import MapCompose</code> ，然后可以在定义 <code>scrapy.Field()</code> 时可以加入处理函数（可以使匿名函数），例如：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdx4319hjj30x009kq4b.jpg" alt=""></p>
<p>在 <code>MapCompose()</code> 中可以加入多个函数，在 <code>jobbole.py</code> 中断点调试结果如图：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdx56xexcj31kw0c37aq.jpg" alt=""></p>
<p>在title的结果后面出现了我们想要的后缀。</p>
<p>另外，可以看到，结果都是 <code>list</code>，我们每次都需要提取第一个值。Scrapy给我们提供了 <code>TakeFirst</code> 方法。</p>
<p>同样引入 <code>from scrapy.loader.processors import MapCompose,TakeFirst</code> ，修改代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">title = scrapy.Field(</div><div class="line">        input_processor = MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"-jobbole"</span>, add_jobbole),</div><div class="line">        output_processor = TakeFirst()</div><div class="line">	)</div></pre></td></tr></table></figure>
<p>即可以得到第一个值。由于每一个结果都是取第一个值，每个值全部调用这个方法重复代码过多，可以通过自定义Item loader重载的方法解决。引入 <code>from scrapy.loader import ItemLoader</code> ，这个类提供了以下方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItemLoader</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    default_item_class = Item</div><div class="line">    default_input_processor = Identity()</div><div class="line">    default_output_processor = Identity()</div><div class="line">    default_selector_class = Selector</div></pre></td></tr></table></figure>
<p>我们自定义的Item loader需要继承这个类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></div><div class="line">    default_output_processor = TakeFirst()</div></pre></td></tr></table></figure>
<p>然后在 <code>jobbole.py</code> 文件中，把<br> <code>item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</code> 中的 <code>ItemLoader</code> 变为 <code>ArticleItemLoader</code>，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response)</div></pre></td></tr></table></figure>
<p>这样得到的结果就是一个str而不是list了。</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffdyhajhdvj31kw0bz445.jpg" alt=""></p>
<p>不过在上图是可以看到，它的tags也取了第一个值，但实际上它的值是三个，不满足我们的需要。<br>引入Join方法 <code>from scrapy.loader.processors import MapCompose, TakeFirst, Join</code>，同时不使用自定义的item loader即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tags = scrapy.Field(</div><div class="line">        output_processor=Join(<span class="string">','</span>),</div><div class="line">	)</div></pre></td></tr></table></figure>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdyj0tk9lj30oe00sdg0.jpg" alt=""></p>
<p>和前面一样，有时候tags会有 <code>评论</code> 的不符合要求的tags，还需要自定义函数把相应的字段去掉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_comment</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'评论'</span> <span class="keyword">in</span> value:</div><div class="line">        <span class="keyword">return</span> <span class="string">''</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> value</div></pre></td></tr></table></figure>
<p>在处理图片时，使用pipelines需要传递的是一个列表，这里经过处理后，变成了str。可以通过一个默认函数不让默认的TakeFirst处理即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_value</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="keyword">return</span> value</div></pre></td></tr></table></figure>
<p>调用方法是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">front_image_url = scrapy.Field(</div><div class="line">        output_processor=MapCompose(return_value),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>最后，我们在之前还用正则表达式来清洗点赞数，收藏数，评论数这些数据，在item loader中我们也可以用函数处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_nums</span><span class="params">(value)</span>:</span></div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, value)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        nums = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> nums</div><div class="line"></div><div class="line">praise_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div><div class="line">    comment_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div><div class="line">    fav_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>调试结果中str就变成int类型了：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffdynb5xtsj31kw0c9afc.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 &lt;/p&gt;
&lt;p&gt;从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。&lt;/p&gt;
&lt;p&gt;Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——保存item到json文件</title>
    <link href="http://yoursite.com/2017/05/07/scrapy-item-json/"/>
    <id>http://yoursite.com/2017/05/07/scrapy-item-json/</id>
    <published>2017-05-07T06:18:54.000Z</published>
    <updated>2017-05-07T05:05:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。</p>
<a id="more"></a>
<h2 id="使用系统-exporter-导出为-JSON-文件"><a href="#使用系统-exporter-导出为-JSON-文件" class="headerlink" title="使用系统 exporter 导出为 JSON 文件"></a><strong>使用系统 exporter 导出为 JSON 文件</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 调用Scrapy提供的json exporter导出json文件</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = open(<span class="string">'article.json'</span>, <span class="string">'wb'</span>)</div><div class="line">        self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</div><div class="line">        self.exporter.start_exporting()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.exporter.finish_exporting()</div><div class="line">        self.file.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        self.exporter.export_item(item)</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h2 id="自定义-Pipeline-导出为-JSON-文件"><a href="#自定义-Pipeline-导出为-JSON-文件" class="headerlink" title="自定义 Pipeline 导出为 JSON 文件"></a><strong>自定义 Pipeline 导出为 JSON 文件</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWithEncodingPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 自定义json文件的导出</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></div><div class="line">        self.file.write(lines)</div><div class="line">        <span class="keyword">return</span> item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.file.close()</div></pre></td></tr></table></figure>
<h2 id="函数说明"><a href="#函数说明" class="headerlink" title="函数说明"></a><strong>函数说明</strong></h2><p><code>codecs</code> ：避免打开文件时出现编码错误。<br><code>json.dumps</code> ：dict转成str<br><code>json.loads</code> ：str转成dict<br><code>ensure_ascii=False</code> ：避免处理英文以外语言时出错<br><code>return item</code> ：交给下一个pipeline处理</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——通过Pipeline保存数据到MySQL</title>
    <link href="http://yoursite.com/2017/05/07/scrapy-item-mysql/"/>
    <id>http://yoursite.com/2017/05/07/scrapy-item-mysql/</id>
    <published>2017-05-07T06:18:54.000Z</published>
    <updated>2017-05-07T11:36:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>将数据保存到MySQL数据库，需要用到 <code>mysqlclient</code> 模块，需要在我们的虚拟环境中用 <code>pip</code> 进行安装。</p>
<a id="more"></a>
<h2 id="设计数据表"><a href="#设计数据表" class="headerlink" title="设计数据表"></a><strong>设计数据表</strong></h2><p>需要根据之前Item来设计我们的数据表 <code>jobbole_article</code> ，数据库取名为 <code>article_spider</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    title = scrapy.Field()</div><div class="line">    create_date = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    front_image_url = scrapy.Field()</div><div class="line">    front_image_path = scrapy.Field()</div><div class="line">    praise_nums = scrapy.Field()</div><div class="line">    comment_nums = scrapy.Field()</div><div class="line">    fav_nums = scrapy.Field()</div><div class="line">    tags = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure>
<p>初步设计的数据表如下，在后面使用时还会进行必要的改动：</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1ffcueufjwhj317e0kwwib.jpg" alt=""></p>
<h2 id="采用同步机制写入MySQL"><a href="#采用同步机制写入MySQL" class="headerlink" title="采用同步机制写入MySQL"></a><strong>采用同步机制写入MySQL</strong></h2><p>首先在 <code>pipelines.py</code> 中引入数据库连接模块 <code>import MySQLdb</code> ，然后完善 <code>MysqlPipeline</code> 类的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 采用同步的机制写入mysql</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.conn = MySQLdb.connect(<span class="string">'127.0.0.1'</span>, <span class="string">'root'</span>, <span class="string">'12'</span>, <span class="string">'article_spider'</span>,</div><div class="line">                                    charset=<span class="string">'utf8'</span>, use_unicode=<span class="keyword">True</span>)</div><div class="line">        self.cursor = self.conn.cursor()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                    insert into jobbole_article(title, url, create_date, fav_nums)</div><div class="line">                    VALUES (%s, %s, %s, %s)</div><div class="line">        """</div><div class="line">        self.cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</div><div class="line">        self.conn.commit()</div></pre></td></tr></table></figure>
<p><code>__init__</code> 方法是对数据进行初始化，定义连接信息如host，数据库用户名、密码、数据库名称、数据库编码<br>在 <code>process_item</code> 方法中进行插入数据操作，格式都是固定的。</p>
<p>最后在 <code>settings.py</code> 中把 <code>MysqlPipeline()</code> 加入到 <code>ITEM_PIPELINES</code> 的配置中。</p>
<h2 id="采用异步机制写入MySQL"><a href="#采用异步机制写入MySQL" class="headerlink" title="采用异步机制写入MySQL"></a><strong>采用异步机制写入MySQL</strong></h2><p>在上面的同步机制写入数据库中，我们把连接信息 <code>MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;12&#39;, &#39;article_spider&#39;, charset=&#39;utf8&#39;, use_unicode=True)</code> 直接定义在函数中，如果不经常改动的话，可以把相关信息放到 <code>settings.py</code> 中进行调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MYSQL_HOST = <span class="string">'127.0.0.1'</span></div><div class="line">MYSQL_DBNAME = <span class="string">'article_spider'</span></div><div class="line">MYSQL_USER = <span class="string">'root'</span></div><div class="line">MYSQL_PASSWORD = <span class="string">'12'</span></div></pre></td></tr></table></figure>
<p>在 <code>pipelines.py</code> 中新建 <code>MysqlTwistedPipeline</code> ，写入如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        host = settings[<span class="string">'MYSQL_HOST'</span>]</div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>在 <code>from_settings</code> 这个类方法中，我们获取到了settings配置中的 <code>MYSQL_HOST</code> ，这个方法在Scrapy初始化的时候就会被调用，会将Scrapy的settings对象传递进来，我们在这里进行断点调试，查看是否获取到了这个对象：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffcv0p0d3xj31kw0ejqcr.jpg" alt=""></p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1ffcv1i1po9j319w04kgmy.jpg" alt=""></p>
<p>发现在settings的attributes这个字典中，确实有我们定义的各种属性。</p>
<p>我们的异步操作需要引入twisted，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</div><div class="line"><span class="keyword">import</span> MySQLdb</div><div class="line"><span class="keyword">import</span> MySQLdb.cursors</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></div><div class="line">        self.dbpool = dbpool</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        dbparams = dict(</div><div class="line">            host=settings[<span class="string">'MYSQL_HOST'</span>],</div><div class="line">            db=settings[<span class="string">'MYSQL_DBNAME'</span>],</div><div class="line">            user=settings[<span class="string">'MYSQL_USER'</span>],</div><div class="line">            passwd=settings[<span class="string">'MYSQL_PASSWORD'</span>],</div><div class="line">            charset=<span class="string">'utf8'</span>,</div><div class="line">            cursorclass=MySQLdb.cursors.DictCursor,</div><div class="line">            use_unicode=<span class="keyword">True</span>,</div><div class="line">        )</div><div class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparams)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> cls(dbpool)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="comment"># 使用twisted将mysql插入变成异步执行</span></div><div class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</div><div class="line">        query.addErrback(self.handle_error)  <span class="comment"># 处理异常</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure)</span>:</span></div><div class="line">        <span class="comment"># 处理异步插入异常</span></div><div class="line">        print(failure)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></div><div class="line">        <span class="comment"># 执行具体的插入</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                            insert into jobbole_article(title, url, create_date, fav_nums)</div><div class="line">                            VALUES (%s, %s, %s, %s)</div><div class="line">                """</div><div class="line">        cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</div></pre></td></tr></table></figure>
<p>我们在使用时，绝大部分代码无须变动，只要修改 <code>do_insert</code> 方法中的插入内容，以及自己的信息即可。</p>
<p>在数据量不大时，用同步插入即可。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将数据保存到MySQL数据库，需要用到 &lt;code&gt;mysqlclient&lt;/code&gt; 模块，需要在我们的虚拟环境中用 &lt;code&gt;pip&lt;/code&gt; 进行安装。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Pipeline</title>
    <link href="http://yoursite.com/2017/05/06/scrapy-item-pipeline/"/>
    <id>http://yoursite.com/2017/05/06/scrapy-item-pipeline/</id>
    <published>2017-05-06T06:18:54.000Z</published>
    <updated>2017-05-06T13:35:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。</p>
<p>每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。</p>
<a id="more"></a>
<p>以下是 item pipeline 的一些典型应用：</p>
<ul>
<li>清理 HTML 数据</li>
<li>验证爬取的数据（检查 item 包含某些字段）</li>
<li>查重（并丢弃）</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<h2 id="编写自定义的-Pipeline"><a href="#编写自定义的-Pipeline" class="headerlink" title="编写自定义的 Pipeline"></a><strong>编写自定义的 Pipeline</strong></h2><p>定义一个Python类，然后实现方法 <code>process_item(self, item, spider)</code> 即可，返回一个字典或Item，或者抛出 <code>DropItem</code> 异常丢弃这个Item。</p>
<p>除此之外，还可以实现以下几个方法：</p>
<ul>
<li><code>open_spider(self, spider)</code> ：当spider被开启时，这个方法被调用</li>
<li><code>close_spider(self, spider)</code> ：当spider被关闭时，这个方法被调用</li>
<li><code>from_crawler(cls, crawler)</code> ： 可访问核心组件比如配置和信号，并注册钩子函数到Scrapy中</li>
</ul>
<h2 id="Item-Pipeline示例"><a href="#Item-Pipeline示例" class="headerlink" title="Item Pipeline示例"></a><strong>Item Pipeline示例</strong></h2><h3 id="价格验证"><a href="#价格验证" class="headerlink" title="价格验证"></a><strong>价格验证</strong></h3><p>让我们来看一下以下这个假设的 pipeline，它为那些不含税（<code>price_excludes_vat</code> 属性）的item调整了price属性，同时丢弃了那些没有价格item：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">	vat_factor = <span class="number">1.15</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span> </div><div class="line">        <span class="keyword">if</span> item[<span class="string">'price'</span>]: </div><div class="line">            <span class="keyword">if</span> item[<span class="string">'price_excludes_vat'</span>]:</div><div class="line">				item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor </div><div class="line">            <span class="keyword">return</span> item </div><div class="line">        <span class="keyword">else</span>: </div><div class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</div></pre></td></tr></table></figure>
<h3 id="将item写入Json文件"><a href="#将item写入Json文件" class="headerlink" title="将item写入Json文件"></a><strong>将item写入Json文件</strong></h3><p>下面的这个Pipeline将所有的item写入到一个单独的json文件，，每行包含一个序列化 为 JSON 格式的 item:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'wb'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></div><div class="line">        self.file.write(line)</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>JsonWriterPipeline 的目的只是为了介绍怎样编写 item pipeline，如果你想要将所有爬取的 item 都保存到同 一个 JSON 文件， 你需要使用 Feed exports 。</p>
<h3 id="将item存储到MongoDB中"><a href="#将item存储到MongoDB中" class="headerlink" title="将item存储到MongoDB中"></a>将item存储到MongoDB中</h3><p>这个例子使用<a href="http://api.mongodb.org/python/current/" target="_blank" rel="external">pymongo</a>来演示怎样讲item保存到MongoDB中。 MongoDB的地址和数据库名在配置 <code>settings.py</code> 中指定，这个例子主要是向你展示怎样使用<code>from_crawler()</code>方法，以及如何清理资源。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    collection_name = <span class="string">'scrapy_items'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></div><div class="line">        self.mongo_uri = mongo_uri</div><div class="line">        self.mongo_db = mongo_db</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></div><div class="line">        <span class="keyword">return</span> cls(</div><div class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</div><div class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>, <span class="string">'items'</span>)</div><div class="line">        )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</div><div class="line">        self.db = self.client[self.mongo_db]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        self.db[self.collection_name].insert(dict(item))</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a><strong>去重</strong></h3><p>一个用于去重的过滤器，丢弃那些已经被处理过的 item。让我们假设我们的 item 有一个唯一的 id，但是我们 sp ider 返回的多个 item 中包含有相同的 id:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span> </div><div class="line">        self.ids_seen = set()</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span> </div><div class="line">        <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">in</span> self.ids_seen:</div><div class="line">			<span class="keyword">raise</span> DropItem(<span class="string">"Duplicate item found: %s"</span> % item) </div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.ids_seen.add(item[<span class="string">'id'</span>]) </div><div class="line">            <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h2 id="启用一个-Item-Pipeline-组件"><a href="#启用一个-Item-Pipeline-组件" class="headerlink" title="启用一个 Item Pipeline 组件"></a>启用一个 Item Pipeline 组件</h2><p>为了启用一个 Item Pipeline 组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123; </div><div class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>, </div><div class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>, </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分配给每个类的整型值，确定了他们运行的顺序，item 按数字从低到高的顺序，通过 pipeline，通常将这些数字 定义在 0-1000 范围内。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。&lt;/p&gt;
&lt;p&gt;每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Feed Exports</title>
    <link href="http://yoursite.com/2017/05/06/scrapy-feed-exports/"/>
    <id>http://yoursite.com/2017/05/06/scrapy-feed-exports/</id>
    <published>2017-05-06T06:18:54.000Z</published>
    <updated>2017-05-06T14:46:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。</p>
<p>Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。</p>
<a id="more"></a>
<h2 id="序列化方式（serialization-format）"><a href="#序列化方式（serialization-format）" class="headerlink" title="序列化方式（serialization format）"></a><strong>序列化方式（serialization format）</strong></h2><p>feed 输出使用到了 Item exporters 。其自带支持的类型有:</p>
<ul>
<li>JSON</li>
<li>JSON lines</li>
<li>CSV</li>
<li>XML</li>
</ul>
<p>也可以通过 FEED_EXPORTERS 设置扩展支持的属性。</p>
<p>在 <code>exporters.py</code> 中可以看到所有的 Item exporters：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffbz5887gzj30x603qdge.jpg" alt=""></p>
<p>下表对主要的 Item exporters进行简要的介绍：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>FEED_FORMAT</th>
<th>使用的 exporter</th>
</tr>
</thead>
<tbody>
<tr>
<td>JSON</td>
<td>json</td>
<td>JsonItemExporter</td>
</tr>
<tr>
<td>JSON lines</td>
<td>jsonlines</td>
<td>JsonLinesItemExporter</td>
</tr>
<tr>
<td>CSV</td>
<td>csv</td>
<td>CsvItemExporter</td>
</tr>
<tr>
<td>XML</td>
<td>xml</td>
<td>XmlItemExporter</td>
</tr>
<tr>
<td>Pickle</td>
<td>pickle</td>
<td>PickleItemExporter</td>
</tr>
<tr>
<td>Marshal</td>
<td>marshal</td>
<td>MarshalItemExporter</td>
</tr>
</tbody>
</table>
<h2 id="存储（Storages）"><a href="#存储（Storages）" class="headerlink" title="存储（Storages）"></a><strong>存储（Storages）</strong></h2><p>使用 feed 输出时您可以通过使用 <a href="http://en.wikipedia.org/wiki/Uniform_Resource_Identifier" target="_blank" rel="external">URI</a>（通过 FEED_URI 设置）来定义存储端。feed 输出支持 URI 方式支持的多种存储后端类型。</p>
<p>自带支持的存储后端有：</p>
<ul>
<li>本地文件系统</li>
<li>FTP</li>
<li>S3（需要 boto）</li>
<li>标准输出</li>
</ul>
<p>有些存储后端会因所需的外部库未安装而不可用。例如，S3 只有在 boto 库安装的情况下才可使用。</p>
<h2 id="存储-URI-参数"><a href="#存储-URI-参数" class="headerlink" title="存储 URI 参数"></a><strong>存储 URI 参数</strong></h2><p>存储 URI 也包含参数。当 feed 被创建时这些参数可以被覆盖：</p>
<ul>
<li><code>%(time)s</code> - 当 feed 被创建时被 timestamp 覆盖</li>
<li><code>%(name)s</code> - 被 spider 的名字覆盖</li>
</ul>
<p>其他命名的参数会被 spider 同名的属性所覆盖。例如， 当 feed 被创建时，<code>%(site_id)s</code> 将会被 <code>spider.site_id</code> 属性所覆盖。</p>
<p>下面用一些例子来说明:</p>
<ul>
<li>存储在 FTP，每个 spider 一个目录: <ul>
<li><code>ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</code></li>
</ul>
</li>
<li>存储在 S3，每一个 spider 一个目录:<ul>
<li><code>s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</code></li>
</ul>
</li>
</ul>
<h2 id="存储后端（Storage-backends）"><a href="#存储后端（Storage-backends）" class="headerlink" title="存储后端（Storage backends）"></a><strong>存储后端（Storage backends）</strong></h2><h3 id="本地文件系统"><a href="#本地文件系统" class="headerlink" title="本地文件系统"></a><strong>本地文件系统</strong></h3><p>将 feed 存储在本地系统。</p>
<ul>
<li>URI scheme: <code>file</code></li>
<li>URI 样例: <code>file:///tmp/export.csv</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<p>注意: (只有)存储在本地文件系统时，您可以指定一个绝对路径 /tmp/export.csv 并忽略协议(scheme)。不过这 仅仅只能在 Unix 系统中工作。</p>
<h3 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a><strong>FTP</strong></h3><p>将 feed 存储在 FTP 服务器。</p>
<ul>
<li>URI scheme: <code>ftp</code></li>
<li>URI 样例: <code>ftp://user:pass@ftp.example.com/path/to/export.csv</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<h3 id="S3"><a href="#S3" class="headerlink" title="S3"></a><strong>S3</strong></h3><p>将 feed 存储在 Amazon S3 。 </p>
<ul>
<li>URI scheme: s3 </li>
<li>URI 样例: <ul>
<li>s3://mybucket/path/to/export.csv </li>
<li>s3://aws_key:aws_secret@mybucket/path/to/export.csv </li>
</ul>
</li>
<li>需要的外部依赖库: <code>boto</code></li>
</ul>
<p>您可以通过在 URI 中传递 user/pass 来完成 AWS 认证，或者也可以通过下列的设置来完成: </p>
<p>AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY</p>
<h3 id="标准输出"><a href="#标准输出" class="headerlink" title="标准输出"></a><strong>标准输出</strong></h3><p>feed 输出到 Scrapy 进程的标准输出。</p>
<ul>
<li>URI scheme: <code>stdout</code></li>
<li>URI 样例: <code>stdout</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<h2 id="设定（Settings）"><a href="#设定（Settings）" class="headerlink" title="设定（Settings）"></a><strong>设定（Settings）</strong></h2><p>这些是配置 feed 输出的设定:</p>
<ul>
<li>FEED_URI (必须)</li>
<li>FEED_FORMAT</li>
<li>FEED_STORAGES</li>
<li>FEED_EXPORTERS</li>
<li>FEED_STORE_EMPTY</li>
</ul>
<h3 id="FEED-URI"><a href="#FEED-URI" class="headerlink" title="FEED_URI"></a><strong>FEED_URI</strong></h3><p>Default: <code>None</code> </p>
<p>输出 feed 的 URI。支持的 URI 协议请参见存储后端。</p>
<p> 为了启用 feed 输出，该设定是必须的。</p>
<h3 id="FEED-FORMAT"><a href="#FEED-FORMAT" class="headerlink" title="FEED_FORMAT"></a><strong>FEED_FORMAT</strong></h3><p>输出 feed 的序列化格式。可用的值请参见序列化方式（Serialization formats）。</p>
<h3 id="FEED-STORE-EMPTY"><a href="#FEED-STORE-EMPTY" class="headerlink" title="FEED_STORE_EMPTY"></a><strong>FEED_STORE_EMPTY</strong></h3><p>Default: <code>False</code> </p>
<p>是否输出空 feed（没有 item 的 feed）。</p>
<h3 id="FEED-STORAGES"><a href="#FEED-STORAGES" class="headerlink" title="FEED_STORAGES"></a><strong>FEED_STORAGES</strong></h3><p>Default: <code>{}</code> </p>
<p>包含项目支持的额外 feed 存储端的字典。 字典的键（key）是 URI 协议（scheme），值是存储类（storage class）的路径。</p>
<h3 id="FEED-STORAGES-BASE"><a href="#FEED-STORAGES-BASE" class="headerlink" title="FEED_STORAGES_BASE"></a><strong>FEED_STORAGES_BASE</strong></h3><p>Default:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line"><span class="string">''</span>: <span class="string">'scrapy.contrib.feedexport.FileFeedStorage'</span>, </div><div class="line"><span class="string">'file'</span>: <span class="string">'scrapy.contrib.feedexport.FileFeedStorage'</span>, </div><div class="line"><span class="string">'stdout'</span>: <span class="string">'scrapy.contrib.feedexport.StdoutFeedStorage'</span>, </div><div class="line"><span class="string">'s3'</span>: <span class="string">'scrapy.contrib.feedexport.S3FeedStorage'</span>, </div><div class="line"><span class="string">'ftp'</span>: <span class="string">'scrapy.contrib.feedexport.FTPFeedStorage'</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>包含 Scrapy 内置支持的 feed 存储端的字典。</p>
<h3 id="FEED-EXPORTERS"><a href="#FEED-EXPORTERS" class="headerlink" title="FEED_EXPORTERS"></a><strong>FEED_EXPORTERS</strong></h3><p>Default: <code>{}</code> </p>
<p>包含项目支持的额外输出器（exporter）的字典。 该字典的键（key）是 URI 协议（scheme），值是 Item 输出器（exp orter）类的路径。</p>
<h3 id="FEED-EXPORTERS-BASE"><a href="#FEED-EXPORTERS-BASE" class="headerlink" title="FEED_EXPORTERS_BASE"></a><strong>FEED_EXPORTERS_BASE</strong></h3><p>Default:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">FEED_EXPORTERS_BASE = &#123; </div><div class="line">    <span class="string">'json'</span>: <span class="string">'scrapy.contrib.exporter.JsonItemExporter'</span>, </div><div class="line">    <span class="string">'jsonlines'</span>: <span class="string">'scrapy.contrib.exporter.JsonLinesItemExporter'</span>, </div><div class="line">    <span class="string">'csv'</span>: <span class="string">'scrapy.contrib.exporter.CsvItemExporter'</span>, </div><div class="line">    <span class="string">'xml'</span>: <span class="string">'scrapy.contrib.exporter.XmlItemExporter'</span>, </div><div class="line">    <span class="string">'marshal'</span>: <span class="string">'scrapy.contrib.exporter.MarshalItemExporter'</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>包含 Scrapy 内置支持的 feed 输出器（exporter）的字典。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。&lt;/p&gt;
&lt;p&gt;Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——items设计</title>
    <link href="http://yoursite.com/2017/05/05/scrapy-items-design/"/>
    <id>http://yoursite.com/2017/05/05/scrapy-items-design/</id>
    <published>2017-05-05T06:18:54.000Z</published>
    <updated>2017-05-06T10:36:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 <code>scrapy.Field()</code> 。</p>
<a id="more"></a>
<h2 id="定义-items"><a href="#定义-items" class="headerlink" title="定义 items"></a><strong>定义 items</strong></h2><p>由于需要添加一个封面图，对上面的爬虫添加一个 <code>front_image_url</code> 字段对 <code>parse</code> 函数进行修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析</div><div class="line">    2. 获取下一页的url并交给scrapy进行下载</div><div class="line">    :param response: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># 解析列表页中的所有文章url并交给解析函数进行具体字段的解析</span></div><div class="line">    post_nodes = response.css(<span class="string">"#archive .floated-thumb .post-thumb a"</span>)</div><div class="line">    <span class="keyword">for</span> post_node <span class="keyword">in</span> post_nodes:</div><div class="line">        image_url = post_node.css(<span class="string">"img::attr(src)"</span>).extract_first(<span class="string">""</span>)</div><div class="line">        post_url = post_node.css(<span class="string">"::attr(href)"</span>).extract_first(<span class="string">""</span>)</div><div class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), meta=&#123;<span class="string">"front_image_url"</span>: image_url&#125;, callback=self.parse_detail)</div></pre></td></tr></table></figure>
<p>其中的 <code>meta</code> 字段是传递值的方法。在调试时返回的 <code>response</code> 中会出现 <code>meta</code> 的内容，它是一个字典，故在传递时可以直接通过 <code>response.meta[&#39;front_image_url&#39;]</code> 进行引用（也可以使用get的方法，附默认值防止出现异常）：</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1ffaslwq4mqj30l1058t9b.jpg" alt=""></p>
<p>在 <code>items.py</code> 文件中，定义一个item并声明其字段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    title = scrapy.Field()</div><div class="line">    create_date = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    front_image_url = scrapy.Field()</div><div class="line">    front_image_path = scrapy.Field()</div><div class="line">    praise_nums = scrapy.Field()</div><div class="line">    comment_nums = scrapy.Field()</div><div class="line">    fav_nums = scrapy.Field()</div><div class="line">    tags = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure>
<p>在 <code>jobbole.py</code> 中添加 <code>from ArticleSpider.items import JobBoleArticleItem</code> 对item进行引用，然后在 <code>parse_detail</code> 中进行初始化 <code>article_item = JobBoleArticleItem()</code> ，之后将获取到的字段内容存入初始化的item中，最终代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></div><div class="line">    article_item = JobBoleArticleItem()</div><div class="line"></div><div class="line">    <span class="comment"># 通过css选择器提取字段</span></div><div class="line">    front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></div><div class="line">    title = response.css(<span class="string">".entry-header h1::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</div><div class="line">    praise_nums = int(response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>])</div><div class="line">    fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        fav_nums = <span class="number">0</span></div><div class="line">    comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        comment_nums = <span class="number">0</span></div><div class="line">    content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</div><div class="line">    tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</div><div class="line">    tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">    tags = <span class="string">","</span>.join(tag_list)</div><div class="line"></div><div class="line">    article_item[<span class="string">"title"</span>] = title</div><div class="line">    article_item[<span class="string">"url"</span>] = response.url</div><div class="line">    article_item[<span class="string">"create_date"</span>] = create_date</div><div class="line">    article_item[<span class="string">"front_image_url"</span>] = front_image_url</div><div class="line">    article_item[<span class="string">"praise_nums"</span>] = praise_nums</div><div class="line">    article_item[<span class="string">"comment_nums"</span>] = comment_nums</div><div class="line">    article_item[<span class="string">"fav_nums"</span>] = fav_nums</div><div class="line">    article_item[<span class="string">"content"</span>] = content</div><div class="line">    article_item[<span class="string">"tags"</span>] = tags</div><div class="line">    </div><div class="line">    <span class="keyword">yield</span> article_item</div></pre></td></tr></table></figure>
<p>其中 <code>yield article_item</code> 会自动提交到 <code>settings</code> 中的 <code>ITEM_PIPELINES</code> 进行处理。<br>此时在 <code>pipelines.py</code> 中设置断点调试，可以看到 <code>article_item</code> 中的值已经传递到这里了。</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffasxu8jauj31ge0my46y.jpg" alt=""></p>
<h2 id="自定义-Pipelines"><a href="#自定义-Pipelines" class="headerlink" title="自定义 Pipelines"></a><strong>自定义 Pipelines</strong></h2><p>在 <code>settings.py</code> 中有一个ITEM_PIPELINES的选项，把它的注释去掉增加下载图片的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Configure item pipelines</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    <span class="string">'ArticleSpider.pipelines.ArticlespiderPipeline'</span>: <span class="number">300</span>,</div><div class="line">    <span class="string">'scrapy.pipelines.images.ImagesPipeline'</span>: <span class="number">1</span>,</div><div class="line">&#125;</div><div class="line">IMAGES_URLS_FIELD = <span class="string">"front_image_url"</span></div><div class="line">project_dir = os.path.abspath(os.path.dirname(__file__))</div><div class="line">IMAGES_STORE = os.path.join(project_dir, <span class="string">'images'</span>)</div></pre></td></tr></table></figure>
<p>上面的代码启用了下载图片piplines，并定义了存储地址及想要存储的图片地址。<br>在settings.py同级目录下建立文件夹 <code>images</code> 用来保存图片。当运行爬虫时，图片就会自动下载图片并保存到本地。<br>如果想要得到存储的图片路径的话，需要自定义pipelines。</p>
<p>首先，在 <code>pipeines.py</code> 中引入 <code>from scrapy.pipelines.images import ImagesPipeline</code> ， 然后自定义一个pipeline对ImagesPipeline进行重载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>进行断点调试，查看results中的信息：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffblwnn93tj31aq0t2wmq.jpg" alt=""></p>
<p>调试结果中，results是一个list，第一个值是一个bool值表示图片是否获取成功，第二个值是一个字典，保存了图片路径，图片地址等信息。</p>
<p>最终自定义的pipeline代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></div><div class="line">        <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</div><div class="line">            image_file_path = value[<span class="string">"path"</span>]</div><div class="line">        item[<span class="string">"front_image_path"</span>] = image_file_path</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>上面代码得到image_path保存到 <code>item[&quot;front_image_path&quot;]</code> 中并返回，这时会根据pipelines的顺序进行下一个pipelines进行处理。通过断点调试可以得到想要的结果。</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffbpccuulfj31kw0w7dqk.jpg" alt=""></p>
<h2 id="完善-items-获取"><a href="#完善-items-获取" class="headerlink" title="完善 items 获取"></a><strong>完善 items 获取</strong></h2><p>对之前定义的items中的 <code>url_object_id</code> 字段，需要对url进行md5处理，因此在 <code>items.py</code> 同级目录下新建一个名为 <code>utils</code> 的 <code>python package</code>，新建 <code>common.py</code> ，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> hashlib</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></div><div class="line">    <span class="keyword">if</span> isinstance(url, str):</div><div class="line">        url = url.encode(<span class="string">"utf-8"</span>)</div><div class="line">    m = hashlib.md5()</div><div class="line">    m.update(url)</div><div class="line">    <span class="keyword">return</span> m.hexdigest()</div></pre></td></tr></table></figure>
<p>然后在 <code>jobbole.py</code> 下引入 <code>from ArticleSpider.utils.common import get_md5</code> ，在item内容填充时加上 <code>article_item[&quot;url_object_id&quot;] = get_md5(response.url)</code> 即可。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 &lt;code&gt;scrapy.Field()&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Selectors</title>
    <link href="http://yoursite.com/2017/05/05/scrapy-selectors/"/>
    <id>http://yoursite.com/2017/05/05/scrapy-selectors/</id>
    <published>2017-05-05T06:18:54.000Z</published>
    <updated>2017-05-05T12:51:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： </p>
<ul>
<li><a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">BeautifulSoup</a> 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 </li>
<li><a href="http://lxml.de/index.html" target="_blank" rel="external">lxml</a> 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。</li>
</ul>
<p>Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。</p>
<a id="more"></a>
<p>XPath 是一门用来在 XML 文件中选择节点的语言，也可以用在 HTML 上。</p>
<p>CSS 是一门将 HTML 文档样式化的语言。选择器由它定义，并与特定的 HTML 元素的样式相关连。 </p>
<p>Scrapy 选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。 </p>
<p>本文解释了选择器如何工作，并描述了相应的 API。不同于 lxml API 的臃肿，该 API 短小而简洁。这是因为 lxml 库除了用来选择标记化文档外，还可以用到许多任务上。</p>
<h2 id="使用选择器"><a href="#使用选择器" class="headerlink" title="使用选择器"></a><strong>使用选择器</strong></h2><h3 id="构造选择器"><a href="#构造选择器" class="headerlink" title="构造选择器"></a><strong>构造选择器</strong></h3><p>Scrapy selectors是 <code>Selector</code> 类的实例，通过传入 text 或 <code>TextResponse</code> 来创建，它自动根据传入的类型选择解析规则（XML or HTML）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from scrapy.selector import Selector </div><div class="line">&gt;&gt;&gt; from scrapy.http import HtmlResponse</div></pre></td></tr></table></figure>
<p>以文字构造（都以 xpath 和 css 两种方法解析字段内容，加深理解）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; body = &apos;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&apos;</div><div class="line">&gt;&gt;&gt; Selector(text=body).xpath(&quot;//span/text()&quot;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; Selector(text=body).css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<p>以 response 构造：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response = HtmlResponse(url=&apos;http://example.com&apos;, body=body, encoding=&apos;utf-8&apos;)</div><div class="line">&gt;&gt;&gt; Selector(response=response).xpath(&apos;//span/text()&apos;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; Selector(response=response).css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<p>response 对象以 <em>.selector</em> 属性提供了一个 selector ， 可以随时使用该快捷方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.selector.xpath(&apos;//span/text()&apos;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; response.selector.css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<h3 id="使用选择器-1"><a href="#使用选择器-1" class="headerlink" title="使用选择器"></a><strong>使用选择器</strong></h3><p>我们将使用 Scrapy shell （提供交互测试）和位于 Scrapy 文档服务器的一个样例页面，来解释如何使用选择器：</p>
<p><code>http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</code></p>
<p>该页面源码如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></div><div class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">base</span> <span class="attr">href</span>=<span class="string">'http://example.com/'</span> /&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>Example website<span class="tag">&lt;/<span class="name">title</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">'images'</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image1.html'</span>&gt;</span>Name: My image 1 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image1_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image2.html'</span>&gt;</span>Name: My image 2 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image2_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image3.html'</span>&gt;</span>Name: My image 3 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image3_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image4.html'</span>&gt;</span>Name: My image 4 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image4_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image5.html'</span>&gt;</span>Name: My image 5 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image5_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<p>首先，打开 scrapy shell，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</div></pre></td></tr></table></figure>
<p>当 shell 载入后，您将获得名为 <code>response</code> 的 shell 变量，其为响应的 response，并且在其 <code>response.selector</code> 属性上绑定了一个 selector。</p>
<p>因为我们处理的是 HTML，选择器将自动使用 HTML 语法分析。 那么，通过查看该页面的源码，我们构建一个 XPath 来选择 title 标签内的文字:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.selector.xpath(&quot;//title/text()&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</div></pre></td></tr></table></figure>
<p>由于在 response 中使用 XPath、CSS 查询十分普遍，因此，Scrapy 提供了两个实用的快捷方式：response.xpath() 及 response.css() ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(&quot;title::text&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;descendant-or-self::title/text()&apos; data=&apos;Example website&apos;&gt;]</div></pre></td></tr></table></figure>
<p>现在我们将得到根 URL（base URL）和一些图片链接:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//base/@href'</span>).extract() </div><div class="line">[<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'base::attr(href)'</span>).extract() </div><div class="line">[<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//a[contains(@href, "image")]/@href'</span>).extract() </div><div class="line">[<span class="string">'image1.html'</span>, <span class="string">'image2.html'</span>, <span class="string">'image3.html'</span>, <span class="string">'image4.html'</span>, <span class="string">'image5.html'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'a[href*=image]::attr(href)'</span>).extract() </div><div class="line">[<span class="string">'image1.html'</span>, <span class="string">'image2.html'</span>, <span class="string">'image3.html'</span>, <span class="string">'image4.html'</span>, <span class="string">'image5.html'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//a[contains(@href, "image")]/img/@src'</span>).extract() </div><div class="line">[<span class="string">'image1_thumb.jpg'</span>, <span class="string">'image2_thumb.jpg'</span>, <span class="string">'image3_thumb.jpg'</span>, <span class="string">'image4_thumb.jpg'</span>, <span class="string">'image5_thumb.jpg'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'a[href*=image] img::attr(src)'</span>).extract() </div><div class="line">[<span class="string">'image1_thumb.jpg'</span>, <span class="string">'image2_thumb.jpg'</span>, <span class="string">'image3_thumb.jpg'</span>, <span class="string">'image4_thumb.jpg'</span>, <span class="string">'image5_thumb.jpg'</span>]</div></pre></td></tr></table></figure>
<h3 id="嵌套选择器"><a href="#嵌套选择器" class="headerlink" title="嵌套选择器"></a><strong>嵌套选择器</strong></h3><p>选择器方法（ .xpath() or .css() ）返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; links = response.xpath(&quot;//a[contains(@href,&apos;image&apos;)]&quot;)</div><div class="line"></div><div class="line">&gt;&gt;&gt; links.extract()</div><div class="line">[&apos;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;]</div><div class="line"></div><div class="line">&gt;&gt;&gt; for index, link in enumerate(links):</div><div class="line">...:     args = (index, link.xpath(&apos;@href&apos;).extract(), link.xpath(&apos;img/@src&apos;).extract())</div><div class="line">...:     print(&apos;Link number %d points to url %s and image %s&apos; % args)</div><div class="line">...:</div><div class="line">Link number 0 points to url [&apos;image1.html&apos;] and image [&apos;image1_thumb.jpg&apos;]</div><div class="line">Link number 1 points to url [&apos;image2.html&apos;] and image [&apos;image2_thumb.jpg&apos;]</div><div class="line">Link number 2 points to url [&apos;image3.html&apos;] and image [&apos;image3_thumb.jpg&apos;]</div><div class="line">Link number 3 points to url [&apos;image4.html&apos;] and image [&apos;image4_thumb.jpg&apos;]</div><div class="line">Link number 4 points to url [&apos;image5.html&apos;] and image [&apos;image5_thumb.jpg&apos;]</div></pre></td></tr></table></figure>
<h3 id="结合正则表达式使用选择器"><a href="#结合正则表达式使用选择器" class="headerlink" title="结合正则表达式使用选择器"></a><strong>结合正则表达式使用选择器</strong></h3><p>Selector 也有一个 .re() 方法，用来通过正则表达式来提取数据。然而，不同于使用 .xpath() 或者 .css() 方法，.re()方法返回 unicode 字符串的列表。所以你无法构造嵌套式的 .re() 调用。 </p>
<p>下面是一个例子，从上面的 html 源码中提取图像名字：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//a[contains(@href, &apos;image&apos;)]/text()&quot;).re(r&apos;Name:\s*(.*)&apos;)</div><div class="line">[&apos;My image 1 &apos;, &apos;My image 2 &apos;, &apos;My image 3 &apos;, &apos;My image 4 &apos;, &apos;My image 5 &apos;]</div></pre></td></tr></table></figure>
<h3 id="使用相对-XPaths"><a href="#使用相对-XPaths" class="headerlink" title="使用相对 XPaths"></a><strong>使用相对 XPaths</strong></h3><p>记住如果你使用嵌套的选择器，并使用起始为 <code>/</code> 的 XPath，那么该 XPath 将对文档使用绝对路径，而且对于你调用的 <code>Selector</code> 不是相对路径。</p>
<p>比如，假设你想提取在 <code>&lt;div&gt;</code> 元素中的所有 <code>&lt;p&gt;</code> 元素。首先，你将先得到所有的 <code>&lt;div&gt;</code> 元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; divs = response.xpath(&quot;//div&quot;)</div></pre></td></tr></table></figure>
<p>开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 <code>&lt;div&gt;</code> 元素内部提取所有的 <code>&lt;p&gt;</code> 元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;//p&apos;): # this is wrong - gets all &lt;p&gt; from the whole document </div><div class="line">...		print p.extract()</div></pre></td></tr></table></figure>
<p>下面是比较合适的处理方法(注意 .//p XPath 的点前缀)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;.//p&apos;): # extracts all &lt;p&gt; inside </div><div class="line">... 	print p.extract()</div></pre></td></tr></table></figure>
<p>另一种常见的情况将是提取所有直系 <code>&lt;p&gt;</code> 的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;p&apos;): </div><div class="line">... 	print p.extract()</div></pre></td></tr></table></figure>
<h3 id="使用-EXSLT-扩展"><a href="#使用-EXSLT-扩展" class="headerlink" title="使用 EXSLT 扩展"></a><strong>使用 EXSLT 扩展</strong></h3><p>因建于 lxml 之上，Scrapy 选择器也支持一些 EXSLT 扩展，可以在 XPath 表达式中使用这些预先制定的命名空间：</p>
<table>
<thead>
<tr>
<th>前缀</th>
<th>命名空间</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>re</td>
<td><a href="http://exslt.org/regular-expressions" target="_blank" rel="external">http://exslt.org/regular-expressions</a></td>
<td>正则表达式</td>
</tr>
<tr>
<td>set</td>
<td><a href="http://exslt.org/sets" target="_blank" rel="external">http://exslt.org/sets</a></td>
<td>集合操作</td>
</tr>
</tbody>
</table>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a><strong>正则表达式</strong></h3><p>例如在XPath的 <code>starts-with()</code> 或 <code>contains()</code> 无法满足需求时， <code>test()</code> 函数可以非常有用。</p>
<p>例如在列表中选择有”class”元素且结尾为一个数字的链接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from scrapy import Selector</div><div class="line">&gt;&gt;&gt; doc = &quot;&quot;&quot;</div><div class="line">... &lt;div&gt;</div><div class="line">...      &lt;ul&gt;</div><div class="line">...          &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...       &lt;/ul&gt;</div><div class="line">... &lt;/div&gt;</div><div class="line">...  &quot;&quot;&quot;</div><div class="line">&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)</div><div class="line">&gt;&gt;&gt; sel.xpath(&quot;//li//@href&quot;).extract()</div><div class="line">[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link3.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]</div><div class="line">&gt;&gt;&gt; sel.xpath(&quot;//li[re:test(@class, &apos;item-\d$&apos;)]//@href&quot;).extract()</div><div class="line">[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]</div></pre></td></tr></table></figure>
<p>注意：C语言库 <code>libxslt</code> 不原生支持EXSLT正则表达式，因此 lxml 在实现时使用了Python  <code>re</code>  模块的钩子。 因此，在 XPath 表达式中使用 regexp 函数可能会牺牲少量的性能。</p>
<h3 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a><strong>集合操作</strong></h3><p>集合操作可以方便地用于在提取文字元素前从文档树中去除一些部分。 </p>
<p>例如使用 itemscopes 组和对应的 itemprops 来提取微数据（来自 <a href="http://schema.org/Product" target="_blank" rel="external">http://schema.org/Product</a> 的样本内容）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; doc = &quot;&quot;&quot;</div><div class="line">... &lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</div><div class="line">...   &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</div><div class="line">...   ![](kenmore-microwave-17in.jpg)</div><div class="line">...   &lt;div itemprop=&quot;aggregateRating&quot;</div><div class="line">...     itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</div><div class="line">...    Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</div><div class="line">...    based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</div><div class="line">...     &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   Product description:</div><div class="line">...   &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</div><div class="line">...   Has six preset cooking categories and convenience features like</div><div class="line">...   Add-A-Minute and Child Lock.&lt;/span&gt;</div><div class="line">...</div><div class="line">...   Customer reviews:</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</div><div class="line">...     by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</div><div class="line">...     &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</div><div class="line">...     &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</div><div class="line">...       &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</div><div class="line">...       &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</div><div class="line">...       &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</div><div class="line">...     &lt;/div&gt;</div><div class="line">...     &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</div><div class="line">...     it. &lt;/span&gt;</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</div><div class="line">...     by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</div><div class="line">...     &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</div><div class="line">...     &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</div><div class="line">...       &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</div><div class="line">...       &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</div><div class="line">...       &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</div><div class="line">...     &lt;/div&gt;</div><div class="line">...     &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</div><div class="line">...     fits in my apartment.&lt;/span&gt;</div><div class="line">...   &lt;/div&gt;</div><div class="line">...   ...</div><div class="line">... &lt;/div&gt;</div><div class="line">... &quot;&quot;&quot;</div><div class="line">&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)</div><div class="line">&gt;&gt;&gt; for scope in sel.xpath(&apos;//div[@itemscope]&apos;):</div><div class="line">...     print &quot;current scope:&quot;, scope.xpath(&apos;@itemtype&apos;).extract()</div><div class="line">...     props = scope.xpath(&apos;&apos;&apos;</div><div class="line">...                 set:difference(./descendant::*/@itemprop,</div><div class="line">...                                .//*[@itemscope]/*/@itemprop)&apos;&apos;&apos;)</div><div class="line">...     print &quot;    properties:&quot;, props.extract()</div><div class="line">...     print</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Product&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;aggregateRating&apos;, u&apos;offers&apos;, u&apos;description&apos;, u&apos;review&apos;, u&apos;review&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/AggregateRating&apos;]</div><div class="line">    properties: [u&apos;ratingValue&apos;, u&apos;reviewCount&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Offer&apos;]</div><div class="line">    properties: [u&apos;price&apos;, u&apos;availability&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Review&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Rating&apos;]</div><div class="line">    properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Review&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Rating&apos;]</div><div class="line">    properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]</div></pre></td></tr></table></figure>
<p>这里我们先迭代 <code>itemscope</code> 元素，对于每一个元素，我们寻找所有 <code>itemprops</code> 元素，并排除那些在另一个元素内部的元素 <code>itemscope</code> 。</p>
<h2 id="内置选择器参考"><a href="#内置选择器参考" class="headerlink" title="内置选择器参考"></a>内置选择器参考</h2><h3 id="Selector-实例"><a href="#Selector-实例" class="headerlink" title="Selector 实例"></a><strong>Selector 实例</strong></h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> scrapy.selector.Selector(response=<span class="keyword">None</span>, text=<span class="keyword">None</span>, <span class="keyword">type</span>=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>一个实例<code>Selector</code>是一个包装器响应来选择其内容的某些部分。</p>
<p>response是一个<code>HtmlResponse</code>或一个<code>XmlResponse</code>将被用于选择和提取的数据对象。</p>
<p><code>text</code>是一个<code>unicode</code>字符串或<code>utf-8</code>编码的文本，当一个 <code>response</code>不可用时。使用<code>text</code>和<code>response</code>一起是未定义的行为。</p>
<p><code>type</code>定义选择器类型，它可以是<code>&quot;html&quot;</code>，<code>&quot;xml&quot;</code>或<code>None（默认）</code>。</p>
<p>如果<code>type</code>是<code>None</code>，选择器将根据<code>response</code>类型（见下文）自动选择最佳类型，或者默认<code>&quot;html&quot;</code>情况下与选项一起使用<code>text</code>。</p>
<p>如果<code>type</code>是<code>None</code>和<code>response</code>传递，选择器类型从响应类型推断如下：</p>
<ul>
<li><code>&quot;html&quot;</code>对于HtmlResponse类型</li>
<li><code>&quot;xml&quot;</code>对于XmlResponse类型</li>
<li><code>&quot;html&quot;</code>为任何其他</li>
</ul>
<p>否则，如果<code>type</code>设置，选择器类型将被强制，并且不会发生检测。</p>
<h4 id="xpath（查询）"><a href="#xpath（查询）" class="headerlink" title="xpath（查询）"></a><strong>xpath（查询）</strong></h4><p>查找与<code>xpath</code>匹配的节点<code>query</code>，并将结果作为 <code>SelectorList</code>实例将所有元素展平。列表元素也实现<code>Selector</code>接口。</p>
<p><code>query</code> 是一个包含要应用的XPATH查询的字符串。</p>
<p><strong>注意</strong></p>
<blockquote>
<p>为了方便起见，这种方法可以称为 response.xpath()</p>
</blockquote>
<h4 id="css（查询）"><a href="#css（查询）" class="headerlink" title="css（查询）"></a><strong>css（查询）</strong></h4><p>应用给定的CSS选择器并返回一个SelectorList实例。</p>
<p>query 是一个包含要应用的CSS选择器的字符串。</p>
<p>在后台，CSS查询使用cssselect库和run .xpath()方法转换为XPath查询 。</p>
<p><strong>注意</strong></p>
<blockquote>
<p>为了方便起见，该方法可以称为 response.css()</p>
</blockquote>
<h4 id="extract（）"><a href="#extract（）" class="headerlink" title="extract（）"></a><strong>extract（）</strong></h4><p>序列化并返回匹配的节点作为unicode字符串列表。编码内容的百分比未引用。</p>
<h4 id="re（regex）"><a href="#re（regex）" class="headerlink" title="re（regex）"></a><strong>re（regex）</strong></h4><p>应用给定的正则表达式并返回一个包含匹配项的unicode字符串的列表。</p>
<p><code>regex</code>可以是编译的正则表达式或将被编译为正则表达式的字符串 <code>re.compile(regex)</code></p>
<p><strong>注意</strong></p>
<blockquote>
<p>注意，re()和re_first()解码HTML实体（除\&lt;和\&amp;）。</p>
</blockquote>
<h4 id="register-namespace（prefix，uri）"><a href="#register-namespace（prefix，uri）" class="headerlink" title="register_namespace（prefix，uri）"></a><strong>register_namespace（prefix，uri）</strong></h4><p>注册在此使用的给定命名空间Selector。如果不注册命名空间，则无法从非标准命名空间中选择或提取数据。参见下面的例子。</p>
<h4 id="remove-namespaces（）"><a href="#remove-namespaces（）" class="headerlink" title="remove_namespaces（）"></a><strong>remove_namespaces（）</strong></h4><p>删除所有命名空间，允许使用无命名空间的xpaths遍历文档。参见下面的例子。</p>
<h4 id="nonzero（）"><a href="#nonzero（）" class="headerlink" title="nonzero（）"></a><strong>nonzero（）</strong></h4><p>返回True如果有选择或任何实际的内容False 除外。换句话说，a的布尔值Selector由它选择的内容给出。</p>
<h3 id="SelectorList对象"><a href="#SelectorList对象" class="headerlink" title="SelectorList对象"></a>SelectorList对象</h3><p><code>class scrapy.selector.SelectorList</code></p>
<p>本SelectorList类是内置的一个子list 类，它提供了几个方法。</p>
<h4 id="xpath（查询）-1"><a href="#xpath（查询）-1" class="headerlink" title="xpath（查询）"></a><strong>xpath（查询）</strong></h4><p>调用.xpath()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。</p>
<p>query 是同一个参数 Selector.xpath()</p>
<h4 id="css（查询）-1"><a href="#css（查询）-1" class="headerlink" title="css（查询）"></a><strong>css（查询）</strong></h4><p>调用.css()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。</p>
<p>query 是同一个参数 Selector.css()</p>
<h4 id="extract（）-1"><a href="#extract（）-1" class="headerlink" title="extract（）"></a><strong>extract（）</strong></h4><p>调用.extract()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。</p>
<h4 id="re（）"><a href="#re（）" class="headerlink" title="re（）"></a><strong>re（）</strong></h4><p>调用.re()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。</p>
<h4 id="nonzero（）-1"><a href="#nonzero（）-1" class="headerlink" title="nonzero（）"></a><strong>nonzero（）</strong></h4><p>如果列表不为空，则返回True，否则返回False。</p>
<h3 id="HTML响应的选择器示例"><a href="#HTML响应的选择器示例" class="headerlink" title="HTML响应的选择器示例"></a><strong>HTML响应的选择器示例</strong></h3><p>这里有几个Selector例子来说明几个概念。在所有情况下，我们假设已经Selector实例化了一个HtmlResponse对象，如下：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sel</span> = Selector(html_response)</div></pre></td></tr></table></figure>
<ol>
<li><code>&lt;h1&gt;</code>从HTML响应主体中选择所有元素，返回Selector对象列表 （即SelectorList对象）：</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1"</span>)</div></pre></td></tr></table></figure>
<ol>
<li><code>&lt;h1&gt;</code>从HTML响应正文中提取所有元素的文本，返回unicode字符串</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1"</span>).extract()         <span class="comment"># this includes the h1 tag</span></div><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1/text()"</span>).extract()  <span class="comment"># this excludes the h1 tag</span></div></pre></td></tr></table></figure>
<ol>
<li>迭代所有<code>&lt;p&gt;</code>标签并打印其类属性：</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">for <span class="keyword">node</span> <span class="title">in</span> sel.<span class="keyword">xpath</span>(<span class="string">"//p"</span>):</div><div class="line">    print <span class="keyword">node</span>.<span class="title">xpath</span>(<span class="string">"@class"</span>).extract()</div></pre></td></tr></table></figure>
<h3 id="XML响应的选择器示例"><a href="#XML响应的选择器示例" class="headerlink" title="XML响应的选择器示例"></a><strong>XML响应的选择器示例</strong></h3><p>这里有几个例子来说明几个概念。在这两种情况下，我们假设已经Selector实例化了一个 XmlResponse对象，像这样：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sel</span> = Selector(xml_response)</div></pre></td></tr></table></figure>
<ol>
<li><product>从XML响应主体中选择所有元素，返回Selector对象列表（即SelectorList对象）：</product></li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//product"</span>)</div></pre></td></tr></table></figure>
<ol>
<li>从需要注册命名空间的<a href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799" target="_blank" rel="external">Google Base XML Feed</a>中提取所有价格：</li>
</ol>
<figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">sel</span><span class="selector-class">.register_namespace</span>(<span class="string">"g"</span>, <span class="string">"http://base.google.com/ns/1.0"</span>)</div><div class="line"><span class="selector-tag">sel</span><span class="selector-class">.xpath</span>(<span class="string">"//g:price"</span>)<span class="selector-class">.extract</span>()</div></pre></td></tr></table></figure>
<h3 id="删除名称空间"><a href="#删除名称空间" class="headerlink" title="删除名称空间"></a><strong>删除名称空间</strong></h3><p>当处理抓取项目时，通常很方便地完全删除命名空间，只需处理元素名称，编写更简单/方便的XPath。你可以使用的 Selector.remove_namespaces()方法。</p>
<p>让我们展示一个例子，用GitHub博客atom feed来说明这一点。</p>
<p>首先，我们打开shell和我们想要抓取的url：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy <span class="keyword">shell</span> http<span class="variable">s:</span>//github.<span class="keyword">com</span>/blog.atom</div></pre></td></tr></table></figure>
<p>一旦在shell中，我们可以尝试选择所有<link>对象，并看到它不工作（因为Atom XML命名空间模糊了这些节点）：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;</span>&gt; response.xpath(<span class="string">"//link"</span>)</div><div class="line">[]</div></pre></td></tr></table></figure>
<p>但是一旦我们调用该Selector.remove_namespaces()方法，所有节点都可以直接通过他们的名字访问：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>response.selector.remove_namespaces()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">"//link"</span>)</div><div class="line">[&lt;Selector xpath=<span class="string">'//link'</span> data=<span class="string">u'&lt;link xmlns="http://www.w3.org/2005/Atom'</span>&gt;,</div><div class="line"> &lt;Selector xpath=<span class="string">'//link'</span> data=<span class="string">u'&lt;link xmlns="http://www.w3.org/2005/Atom'</span>&gt;,</div><div class="line"> ...</div></pre></td></tr></table></figure>
<p>如果你想知道为什么默认情况下不调用命名空间删除过程，而不是手动调用它，这是因为两个原因，按照相关性的顺序：</p>
<ol>
<li>删除命名空间需要迭代和修改文档中的所有节点，这对于Scrapy爬取的所有文档来说是一个相当昂贵的操作</li>
<li>可能有一些情况下，实际上需要使用命名空间，以防某些元素名称在命名空间之间冲突。这些情况非常罕见。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://beautifulsoup.readthedocs.io/zh_CN/latest/&quot;&gt;BeautifulSoup&lt;/a&gt; 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://lxml.de/index.html&quot;&gt;lxml&lt;/a&gt; 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——编写Spider爬取伯乐在线所有文章</title>
    <link href="http://yoursite.com/2017/04/26/scrapy-jobbole-spider/"/>
    <id>http://yoursite.com/2017/04/26/scrapy-jobbole-spider/</id>
    <published>2017-04-26T06:18:54.000Z</published>
    <updated>2017-04-26T07:58:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>仍然是以 <code>http://blog.jobbole.com/all-posts/</code> 页面为例</p>
<a id="more"></a>
<h2 id="提取文章列表页"><a href="#提取文章列表页" class="headerlink" title="提取文章列表页"></a><strong>提取文章列表页</strong></h2><p>首页使用CSS选择器获取页面中的文章url列表：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff04stoom0j30gw0n4te3.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;#archive .floated-thumb .post-thumb a::attr(href)&quot;).extract()</div><div class="line">&gt;&gt;&gt;[&apos;http://blog.jobbole.com/111005/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/108468/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/110975/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/110986/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110957/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110976/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110923/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110962/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110958/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110140/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110939/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110941/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110931/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110934/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110929/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110835/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110906/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110916/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110913/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110903/&apos;]</div></pre></td></tr></table></figure>
<p>先在Spider头部引入<code>from scrapy.http import Request</code>，使用Request进行对文章url列表获取函数的调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析</div><div class="line">    2. 获取下一页的url并交给scrapy进行下载</div><div class="line">    :param response: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># 解析列表页中的所有文章url并交给解析函数进行具体字段的解析</span></div><div class="line">    post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</div><div class="line">    <span class="keyword">for</span> post_url <span class="keyword">in</span> post_urls:</div><div class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</div></pre></td></tr></table></figure>
<p>其中，最后 <code>yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</code> 是对每个url调用parse_detail方法进行字段解析，这里url的参数是带有完整域名的格式，如果不是完整域名，则需要对域名进行拼接成完成域名进行解析。首先要引入 <code>from urllib import parse</code> ，通过parse自带的 <code>parse.urljoin()</code> 进行拼接，代码为： <code>yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</code> 。</p>
<h2 id="循环获取下一个列表页"><a href="#循环获取下一个列表页" class="headerlink" title="循环获取下一个列表页"></a><strong>循环获取下一个列表页</strong></h2><p><img src="http://ww2.sinaimg.cn/large/006tNbRwgy1ff04orjgoej30yc02g74a.jpg" alt=""></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff04p002aoj311800qdg7.jpg" alt=""></p>
<p>每一个列表页都有“下一页”链接，我们通过CSS选择器来获取下一页的链接，然后交给parse函数进行循环解析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 提取下一页并交给scrapy进行下载</span></div><div class="line">next_url = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first()</div><div class="line"><span class="keyword">if</span> next_url:</div><div class="line">	<span class="keyword">yield</span> Request(url=parse.urljoin(response.url, next_url), callback=self.parse)</div></pre></td></tr></table></figure>
<p>其中，extract_first()方法与extract()[0]用法相同，都是提取第一个字符串元素。</p>
<h2 id="解析函数"><a href="#解析函数" class="headerlink" title="解析函数"></a><strong>解析函数</strong></h2><p>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="comment"># 通过css选择器提取字段</span></div><div class="line">    title = response.css(<span class="string">".entry-header h1::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</div><div class="line">    praise_nums = int(response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>])</div><div class="line">    fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">    	fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">    	fav_nums = <span class="number">0</span></div><div class="line">    comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">    	comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">    	comment_nums = <span class="number">0</span></div><div class="line">    content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</div><div class="line">    tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</div><div class="line">    tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">    tags = <span class="string">","</span>.join(tag_list)</div><div class="line">    print(title, create_date, fav_nums, comment_nums, tags)</div></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a><strong>运行结果</strong></h2><p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff04zpbvtzj31g20bkte9.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;仍然是以 &lt;code&gt;http://blog.jobbole.com/all-posts/&lt;/code&gt; 页面为例&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——CSS选择器</title>
    <link href="http://yoursite.com/2017/04/25/css-selector/"/>
    <id>http://yoursite.com/2017/04/25/css-selector/</id>
    <published>2017-04-25T12:18:54.000Z</published>
    <updated>2017-06-09T08:20:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CSS选择器的用法"><a href="#CSS选择器的用法" class="headerlink" title="CSS选择器的用法"></a><strong>CSS选择器的用法</strong></h2><h3 id="CSS选择器简介"><a href="#CSS选择器简介" class="headerlink" title="CSS选择器简介"></a><strong>CSS选择器简介</strong></h3><p>在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。</p>
<a id="more"></a>
<h3 id="常用CSS选择器介绍"><a href="#常用CSS选择器介绍" class="headerlink" title="常用CSS选择器介绍"></a><strong>常用CSS选择器介绍</strong></h3><table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>*</td>
<td>选择所有节点</td>
</tr>
<tr>
<td>#container</td>
<td>选择id为container的节点</td>
</tr>
<tr>
<td>.container</td>
<td>选择所有class包含container的节点</td>
</tr>
<tr>
<td>li a</td>
<td>选取所有li下的所有a节点</td>
</tr>
<tr>
<td>ul + p</td>
<td>选择ul后面的第一个p元素</td>
</tr>
<tr>
<td>div#container &gt; ul</td>
<td>选取id为container的div的第一个ul子元素</td>
</tr>
<tr>
<td>ul ~ p</td>
<td>选取与ul相邻的所有p元素</td>
</tr>
<tr>
<td>a[title]</td>
<td>选取所有有title属性的a元素</td>
</tr>
<tr>
<td>a[href=”<code>http://163.com</code>“]</td>
<td>选取所有href属性为163的a元素</td>
</tr>
<tr>
<td>a[href*=”163”]</td>
<td>选取所有href属性包含163的a元素</td>
</tr>
<tr>
<td>a[href^=”http”]</td>
<td>选取所有href属性以http开头的a元素</td>
</tr>
<tr>
<td>a[href$=”.jpg”]</td>
<td>选取所有href以.jpg结尾的a元素</td>
</tr>
<tr>
<td>input[type=radio]:checked</td>
<td>选择选中的radio的元素</td>
</tr>
<tr>
<td>div:not(#container)</td>
<td>选取所有id非container的div属性</td>
</tr>
<tr>
<td>li:nth-child(3)</td>
<td>选取第三个li元素</td>
</tr>
<tr>
<td>tr:nth-child(2n)</td>
<td>第偶数个tr</td>
</tr>
</tbody>
</table>
<h3 id="Scrapy中CSS选择器用法示例"><a href="#Scrapy中CSS选择器用法示例" class="headerlink" title="Scrapy中CSS选择器用法示例"></a><strong>Scrapy中CSS选择器用法示例</strong></h3><p>仍然是用<a href="http://lawtech0902.com/2017/04/16/xpath-example/" target="_blank" rel="external">Xpath用法示例</a>中的例子来进行测试</p>
<h4 id="获取标题"><a href="#获取标题" class="headerlink" title="获取标题"></a>获取标题</h4><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1fez7spfa23j31aw03ytay.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;.entry-header h1::text&quot;).extract()[0]</div><div class="line">&apos;2016 腾讯软件开发面试题（部分）&apos;</div></pre></td></tr></table></figure>
<p>注意：这里获取文字内容的方法为::text，而不是text()。</p>
<h4 id="获取文章发布时间"><a href="#获取文章发布时间" class="headerlink" title="获取文章发布时间"></a>获取文章发布时间</h4><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1fez7svm5gdj30hs01g74p.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile::text&quot;).extract()[0].strip().replace(&quot;·&quot;, &quot;&quot;).strip()</div><div class="line">&apos;2017/02/18&apos;</div></pre></td></tr></table></figure>
<h4 id="获取点赞数、收藏数、评论数"><a href="#获取点赞数、收藏数、评论数" class="headerlink" title="获取点赞数、收藏数、评论数"></a><strong>获取点赞数</strong>、收藏数、评论数</h4><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1fez7terph3j30gi03amxc.jpg" alt=""></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1fez7tk0m0mj31eg0cq7ag.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 点赞数</div><div class="line">&gt;&gt;&gt; response.css(&quot;.vote-post-up h10::text&quot;).extract()[0]</div><div class="line">&apos;2&apos;</div><div class="line"></div><div class="line"># 收藏数，获取之后需要用正则表达式进行清洗</div><div class="line">&gt;&gt;&gt; response.css(&quot;.bookmark-btn::text&quot;).extract()[0]</div><div class="line">&apos; 23 收藏&apos;</div><div class="line"></div><div class="line"># 评论数，获取之后需要用正则表达式进行清洗</div><div class="line">&gt;&gt;&gt; response.css(&quot;a[href=&apos;#article-comment&apos;] span::text&quot;).extract()[0]</div><div class="line">&apos; 7 评论&apos;</div></pre></td></tr></table></figure>
<p>正则表达式清洗收藏数，评论数的逻辑如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    </div><div class="line">comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	comment_nums = int(match_re.group(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h4 id="获取正文"><a href="#获取正文" class="headerlink" title="获取正文"></a>获取正文</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;div.entry&quot;).extract()[0]</div></pre></td></tr></table></figure>
<h4 id="获取tags"><a href="#获取tags" class="headerlink" title="获取tags"></a>获取tags</h4><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1fez85kutnaj30bi01ydfs.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()</div><div class="line">[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;]</div></pre></td></tr></table></figure>
<p>然后需要对数据进行清洗</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text()"</span>).extract()</div><div class="line">tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">tags = <span class="string">","</span>.join(tag_list)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;CSS选择器的用法&quot;&gt;&lt;a href=&quot;#CSS选择器的用法&quot; class=&quot;headerlink&quot; title=&quot;CSS选择器的用法&quot;&gt;&lt;/a&gt;&lt;strong&gt;CSS选择器的用法&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;CSS选择器简介&quot;&gt;&lt;a href=&quot;#CSS选择器简介&quot; class=&quot;headerlink&quot; title=&quot;CSS选择器简介&quot;&gt;&lt;/a&gt;&lt;strong&gt;CSS选择器简介&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，CSS，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CCSS%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Spiders</title>
    <link href="http://yoursite.com/2017/04/22/scrapy-spiders/"/>
    <id>http://yoursite.com/2017/04/22/scrapy-spiders/</id>
    <published>2017-04-22T06:18:54.000Z</published>
    <updated>2017-04-22T11:34:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p>
<a id="more"></a>
<p>对 spider 来说，爬取的流程如下：</p>
<ol>
<li>先初始化请求URL列表，并指定下载后处理response的回调函数。初次请求URL通过 <code>start_urls</code> 指定，调用 <code>start_requests()</code> 产生 <code>Request</code> 对象，然后注册 <code>parse</code> 方法作为回调</li>
<li>在parse回调中解析response并返回字典, <code>Item</code> 对象, <code>Request</code> 对象或它们的迭代对象。 <code>Request</code> 对象还会包含回调函数，之后Scrapy下载完后会被这里注册的回调函数处理。</li>
<li>在回调函数里面，你通过使用选择器（同样可以使用BeautifulSoup,lxml或其他工具）解析页面内容，并生成解析后的结果Item。</li>
<li>最后返回的这些Item通常会被持久化到数据库中(使用<a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="external">Item Pipeline</a>)或者使用<a href="http://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports" target="_blank" rel="external">Feed exports</a>将其保存到文件中。</li>
</ol>
<p>虽然该循环对任何类型的 spider 都（多少）适用，但 Scrapy 仍然为了不同的需求提供了多种默认 spider。 之后将讨论这些 spider。</p>
<h2 id="Spider-参数"><a href="#Spider-参数" class="headerlink" title="Spider 参数"></a><strong>Spider 参数</strong></h2><p>Spider 可以通过接受参数来修改其功能。 spider 参数一般用来定义初始 URL 或者指定限制爬取网站的部分。 您也可以使用其来配置 spider 的任何功能。 </p>
<p>在运行 <code>crawl</code> 时添加 <code>-a</code> 可以传递 Spider 参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl myspider -a category=electronics</div></pre></td></tr></table></figure>
<p>Spider 在构造器（constructor）中获取参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'myspider'</span></div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, category=None, *args, **kwargs)</span>:</span></div><div class="line">        super(MySpider, self).__init__(*args, **kwargs) </div><div class="line">        self.start_urls = [<span class="string">'http://www.example.com/categories/%s'</span> % category] </div><div class="line">        <span class="comment"># ...</span></div></pre></td></tr></table></figure>
<p>Spider 参数也可以通过 <code>Scrapyd</code> 的 <code>schedule.json API</code> 来传递。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Items</title>
    <link href="http://yoursite.com/2017/04/21/scrapy-items/"/>
    <id>http://yoursite.com/2017/04/21/scrapy-items/</id>
    <published>2017-04-21T06:18:54.000Z</published>
    <updated>2017-04-21T13:14:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。</p>
<p>Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。</p>
<a id="more"></a>
<h2 id="声明-Item"><a href="#声明-Item" class="headerlink" title="声明 Item"></a><strong>声明 Item</strong></h2><p>Item 使用简单的 class 定义语法以及 Field 对象来声明。例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductItem</span><span class="params">(scrapy.Item)</span>:</span> </div><div class="line">    name = scrapy.Field() </div><div class="line">    price = scrapy.Field() </div><div class="line">    stock = scrapy.Field() </div><div class="line">    last_updated = scrapy.Field(serializer=str)</div></pre></td></tr></table></figure>
<p>Scrapy Item 的定义方式与 Django Models 很类似，但是没有 Django 那么多不同的字段类型（Field Type）。</p>
<h2 id="Item-字段（Item-Fields）"><a href="#Item-字段（Item-Fields）" class="headerlink" title="Item 字段（Item Fields）"></a><strong>Item 字段（Item Fields）</strong></h2><p>Field 对象指明了每个字段的元数据（metadata）。例如上面例子中 last_updated 中指明了该字段的序列化函数。</p>
<p>您可以为每个字段指明任何类型的元数据。Field 对象对接受的值没有任何限制。也正是因为这个原因，文档也无法提供所有可用的元数据的键（key）参考列表。Field 对象中保存的每个键可以由多个组件使用，并且只有这些组件知道这个键的存在。您可以根据自己的需求，定义使用其他的 Field 键。 设置 Field 对象的主要目的就是在一个地方定义好所有的元数据。一般来说，那些依赖某个字段的组件肯定使用了特定的键（key）。您必须查看组件相关的文档，查看其用了哪些元数据键（metadata key）。</p>
<p>需要注意的是，用来声明 item 的 Field 对象并没有被赋值为 class 的属性。不过您可以通过 Item.fields 属性进 行访问。</p>
<h2 id="与-Item-配合"><a href="#与-Item-配合" class="headerlink" title="与 Item 配合"></a><strong>与 Item 配合</strong></h2><p>在API这里的操作，和 <code>dict API</code> 非常的相似。</p>
<h3 id="创建-item"><a href="#创建-item" class="headerlink" title="创建 item"></a><strong>创建 item</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> scrapy</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">ProductItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line"><span class="meta">... </span>    name = scrapy.Field()</div><div class="line"><span class="meta">... </span>    price = scrapy.Field()</div><div class="line"><span class="meta">... </span>    stock = scrapy.Field()</div><div class="line"><span class="meta">... </span>    last_updated = scrapy.Field(serializer=str)</div><div class="line">...</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product = ProductItem(name=<span class="string">'Desktop'</span>, price=<span class="number">1000</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(product)</div><div class="line">&#123;<span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="获取字段的值"><a href="#获取字段的值" class="headerlink" title="获取字段的值"></a><strong>获取字段的值</strong></h3><p>两种方式：</p>
<ul>
<li>键值对</li>
<li>get()</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 已设定key和value值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'name'</span>]</div><div class="line"><span class="string">'Desktop'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'name'</span>)</div><div class="line"><span class="string">'Desktop'</span></div><div class="line"></div><div class="line"><span class="comment"># 未设定key和value值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>]</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">59</span>, <span class="keyword">in</span> __getitem__</div><div class="line">    <span class="keyword">return</span> self._values[key]</div><div class="line">KeyError: <span class="string">'last_updated'</span></div><div class="line">    </div><div class="line"><span class="comment"># 默认返回空值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'last_updated'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设定返回值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'last_updated'</span>, <span class="string">'not set value'</span>)</div><div class="line"><span class="string">'not set value'</span></div><div class="line"></div><div class="line"><span class="comment"># 未声明的字段</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'lala'</span>]</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">59</span>, <span class="keyword">in</span> __getitem__</div><div class="line">    <span class="keyword">return</span> self._values[key]</div><div class="line">KeyError: <span class="string">'lala'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'lala'</span>, <span class="string">'not exist'</span>)</div><div class="line"><span class="string">'not exist'</span></div><div class="line"></div><div class="line"><span class="comment"># 字段是否被赋值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'name'</span> <span class="keyword">in</span> product</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'last_updated'</span> <span class="keyword">in</span> product</div><div class="line"><span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 字段是否被声明</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'last_updated'</span> <span class="keyword">in</span> product.fields</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'lala'</span> <span class="keyword">in</span> product.fields</div><div class="line"><span class="keyword">False</span></div></pre></td></tr></table></figure>
<h3 id="设置字段的值"><a href="#设置字段的值" class="headerlink" title="设置字段的值"></a><strong>设置字段的值</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 已经声明的字段</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>] = <span class="string">'today'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>]</div><div class="line"><span class="string">'today'</span></div><div class="line"></div><div class="line"><span class="comment"># 未声明的字段无法赋值</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'lala'</span>] = <span class="string">'test'</span></div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">66</span>, <span class="keyword">in</span> __setitem__</div><div class="line">    (self.__class__.__name__, key))</div><div class="line">KeyError: <span class="string">'ProductItem does not support field: lala'</span></div></pre></td></tr></table></figure>
<h3 id="获取所有能够获取到的值"><a href="#获取所有能够获取到的值" class="headerlink" title="获取所有能够获取到的值"></a><strong>获取所有能够获取到的值</strong></h3><p>可以使用 dict API 来获取所有的值:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.keys()</div><div class="line">dict_keys([<span class="string">'last_updated'</span>, <span class="string">'price'</span>, <span class="string">'name'</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.items()</div><div class="line">ItemsView(&#123;<span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;)</div></pre></td></tr></table></figure>
<h3 id="复制-item"><a href="#复制-item" class="headerlink" title="复制 item"></a><strong>复制 item</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>product2 = ProductItem(product)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(product2)</div><div class="line">&#123;<span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;</div><div class="line"></div><div class="line"><span class="comment"># 推荐使用第二种方法</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product3 = product2.copy()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(product3)</div><div class="line">&#123;<span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>, <span class="string">'price'</span>: <span class="number">1000</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="根据-item-创建字典-dict"><a href="#根据-item-创建字典-dict" class="headerlink" title="根据 item 创建字典(dict)"></a><strong>根据 item 创建字典(dict)</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>dict(product)</div><div class="line">&#123;<span class="string">'price'</span>: <span class="number">1000</span>, <span class="string">'last_updated'</span>: <span class="string">'today'</span>, <span class="string">'name'</span>: <span class="string">'Desktop'</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="根据字典-dict-创建-item"><a href="#根据字典-dict-创建-item" class="headerlink" title="根据字典(dict)创建 item"></a><strong>根据字典(dict)创建 item</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>ProductItem(&#123;<span class="string">'name'</span>:<span class="string">'laptop pc'</span>, <span class="string">'price'</span>:<span class="number">1500</span>&#125;)</div><div class="line">&#123;<span class="string">'name'</span>: <span class="string">'laptop pc'</span>, <span class="string">'price'</span>: <span class="number">1500</span>&#125;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>ProductItem(&#123;<span class="string">'name'</span>:<span class="string">'laptop pc'</span>, <span class="string">'lala'</span>:<span class="number">1500</span>&#125;)</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">56</span>, <span class="keyword">in</span> __init__</div><div class="line">    self[k] = v</div><div class="line">  File <span class="string">"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py"</span>, line <span class="number">66</span>, <span class="keyword">in</span> __setitem__</div><div class="line">    (self.__class__.__name__, key))</div><div class="line">KeyError: <span class="string">'ProductItem does not support field: lala'</span></div></pre></td></tr></table></figure>
<h2 id="扩展-Item"><a href="#扩展-Item" class="headerlink" title="扩展 Item"></a><strong>扩展 Item</strong></h2><p>可以通过继承原始的 Item 来扩展 item(添加更多的字段或者修改某些字段的元数据)。</p>
<ul>
<li>添加新的字段</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiscountedProductItem</span><span class="params">(Product)</span>:</span></div><div class="line">    discount_percent = scrapy.Field(serializer=str)</div><div class="line">    discount_expiration_date = scrapy.Field()</div></pre></td></tr></table></figure>
<ul>
<li>使用原字段的元数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpecificProduct</span><span class="params">(Product)</span>:</span></div><div class="line">    name = scrapy.Field(Product.fields[<span class="string">'name'</span>], serializer=my_serializer)</div><div class="line"><span class="comment">#my_serializer 指序列化的类型</span></div></pre></td></tr></table></figure>
<p>上述代码，在保留了原始的元数据值的情况下，添加（或覆盖）了 <code>name</code> 字段的  <code>serializer</code> 。 存在及覆盖，不存在即添加。</p>
<h2 id="Item-对象"><a href="#Item-对象" class="headerlink" title="Item 对象"></a><strong>Item 对象</strong></h2><p><code>class scrapy.item.Item([arg])</code></p>
<p>返回一个根据给定的参数可选初始化的 <code>item</code> 。</p>
<p><code>Item</code> 复制了标准化的 <code>dict API</code> ，包括初始化函数也是一样。除此之外，唯一添加的额外属性就是 <code>fields</code> 。</p>
<p><code>fields</code> 是一个包含了 item 所有声明的字段的字典，而不仅仅是获取到的字段。该字典的 key 是字段（field）的名字，值是 Item 声明中使用到的 Field 对象。</p>
<h2 id="字段（Field）对象"><a href="#字段（Field）对象" class="headerlink" title="字段（Field）对象"></a>字段（Field）对象</h2><p><code>class scrapy.item.Field([arg])</code></p>
<p><code>Field</code>仅仅是内置的 <code>dict</code> 类的一个别名（继承于 <code>dict</code> ），并没有提供额外的方法或属性。说白了，<code>Field</code>就是完完全全的Python字典，被用来基于类属性的方法支持 <code>Item</code> 声明语法。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。&lt;/p&gt;
&lt;p&gt;Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Redis学习笔记(六)：数据安全与性能保障——处理系统故障</title>
    <link href="http://yoursite.com/2017/04/20/Redis-6/"/>
    <id>http://yoursite.com/2017/04/20/Redis-6/</id>
    <published>2017-04-20T12:18:54.000Z</published>
    <updated>2017-04-20T13:46:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。</p>
<a id="more"></a>
<h2 id="验证快照文件和-AOF-文件"><a href="#验证快照文件和-AOF-文件" class="headerlink" title="验证快照文件和 AOF 文件"></a><strong>验证快照文件和 AOF 文件</strong></h2><p>无论时快照持久化还是AOF持久化，都提供了在遇到系统故障时进行数据回复的工具。Redis提供了两个命令行程序 <code>redis-check-aof</code> 和 <code>redis-check-rdb(redis-check-dump was renamed to redis-check-rdb in redis version 3.2)</code> ，它们可以在系统故障发生之后，检查AOF文件和快照文件的状态，并在有需要的情况下对文件进行修复。</p>
<p>在不给定任何参数的情况下运行这两个程序，就可以看见它们的基本使用方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ redis-check-rdb</div><div class="line">Usage: redis-check-rdb &lt;rdb-file-name&gt;</div><div class="line">$ redis-check-dump</div><div class="line">Usage: redis-check-dump &lt;dump.rdb&gt;</div></pre></td></tr></table></figure>
<p>如果运行 <code>redis-check-aof</code> 程序时给了 <code>--fix</code> 参数，那么会对AOF文件进行修复。修复方法非常简单：扫描给定的 AOF 文件，寻找不正确或不完整的命令，当发现第一个出错命令的时候，程序会删除出错的命令以及位于出错命令之后的所有命令。在大多数情况下，被删除的都是 AOF 文件末尾的不完整的写命令。<br>遗憾的是，目前没有办法修复出错的快照文件。尽管发现快照文件首个出现错误的地方是有可能的，但因为快照文件本身经过了压缩，而出现在快照文件中间的错误有可能会导致快照文件的剩余部分无法读取。因此，最好为重要的快照文件保留多个备份，并在进行数据恢复时，通过计算快照文件的 SHA1 散列值和 SHA256 散列值来对内容进行验证。</p>
<h2 id="更换故障主服务器"><a href="#更换故障主服务器" class="headerlink" title="更换故障主服务器"></a><strong>更换故障主服务器</strong></h2><p>我们来看一下在拥有一个主服务器和一个从服务器的情况下，更换主服务器的具体步骤。假设A、B两台机器都运行着 Redis ，机器A为 master ，机器B为 slave 。机器A因为暂时无法修复的故障而断开了连接，因此决定将同样安装了 Redis 的机器 C 用作新的主服务器。</p>
<p>更换服务器的计划非常简单：首先向机器B发送一个 SAVE 命令，让它创建一个新的快照文件，接着将这个快照文件发送给机器C，并在机器 C 上面启动 Redis 。最后，让B成为机器C的从服务器。由于环境有限，就在同一台机器上用不同的端口进行测试，下面进行演示：</p>
<ol>
<li><p>先进入 Redis 安装位置，再安装两个 Redis 服务并分别修改配置文件 redis.conf 中的 port 为6380和6381</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ cd /usr/local</div><div class="line">$ sudo cp -r redis redis6380</div><div class="line">Password:</div><div class="line">$ sudo chmod -R 777 redis6380</div><div class="line">$ vim redis6380/redis.conf</div><div class="line">$ sudo cp -r redis redis6381</div><div class="line">$ sudo chmod -R 777 redis6381</div><div class="line">$ vim redis6381/redis.conf</div></pre></td></tr></table></figure>
</li>
<li><p>启动机器A端口为6379，机器B端口为6380，并让B成为A的从服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 启动A</div><div class="line">$ cd redis</div><div class="line">$ ./src/redis-server redis.conf</div><div class="line"></div><div class="line"># 启动B</div><div class="line">$ cd redis6380</div><div class="line">$ ./src/redis-server redis.conf</div><div class="line"></div><div class="line"># 让B成为A的从服务器</div><div class="line">$ $ redis-cli -h localhost -p 6380</div><div class="line">localhost:6380&gt; SLAVEOF localhost 6379</div><div class="line">OK</div></pre></td></tr></table></figure>
</li>
<li><p>停止机器A的 Redis 服务，此时只剩 Redis 从服务器B在运行</p>
</li>
<li><p>向机器B发送 SAVE 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">localhost:6380&gt; SAVE</div><div class="line">OK</div></pre></td></tr></table></figure>
</li>
<li><p>将机器B的快照文件复制到机器C的对应目录，并启动 Redis 服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cp -f /usr/local/redis6380/dump.rdb /usr/local/redis6381</div><div class="line">$ cd /usr/local/redis6381</div><div class="line">$ ./src/redis-server redis.conf</div></pre></td></tr></table></figure>
</li>
<li><p>让机器B成为机器C的从服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">localhost:6380&gt; SLAVEOF localhost 6381</div><div class="line">OK</div></pre></td></tr></table></figure>
</li>
<li><p>测试机器B是否能从机器C同步数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ redis-cli -h localhost -p 6381</div><div class="line">localhost:6381&gt; set key new-master</div><div class="line">OK</div><div class="line">$ redis-cli -h localhost -p 6380</div><div class="line">localhost:6380&gt; get key</div><div class="line">&apos;new-master&apos;</div></pre></td></tr></table></figure>
</li>
</ol>
<p>另一种创建新的主服务器的方法，就是将从服务器升级（turn）为主服务器，并为升级后的主服务器创建从服务器。</p>
<p>以上两种方法都可以让 Redis 回到之前的一个主服务器和一个从服务器的状态，而用户接下来需要做的就是更新客户端的配置，让它们去读写正确的服务器。除此之外，如果用户需要重启 Redis 的话，那么可能还需要对服务器的持久化配置进行更新。</p>
<p>Redis Sentinel可以监视指定的Redis主服务器及其下属的从服务器，并在主服务器下线时自动进行故障转移(failover)。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。&lt;/p&gt;
    
    </summary>
    
      <category term="Redis" scheme="http://yoursite.com/categories/Redis/"/>
    
    
      <category term="Redis, Python" scheme="http://yoursite.com/tags/Redis-Python/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy命令行工具</title>
    <link href="http://yoursite.com/2017/04/20/scrapy-command-line-tools/"/>
    <id>http://yoursite.com/2017/04/20/scrapy-command-line-tools/</id>
    <published>2017-04-20T06:18:54.000Z</published>
    <updated>2017-04-20T07:53:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。</p>
<a id="more"></a>
<h2 id="使用-scrapy-工具"><a href="#使用-scrapy-工具" class="headerlink" title="使用 scrapy 工具"></a><strong>使用 scrapy 工具</strong></h2><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a><strong>创建项目</strong></h3><p>一般来说，使用 <code>scrapy</code> 工具的第一件事就是创建 Scrapy 项目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject myproject</div></pre></td></tr></table></figure>
<p>该命令将会在 myproject 目录中创建一个 Scrapy 项目。 接下来，进入到项目目录中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd myproject</div></pre></td></tr></table></figure>
<p>这时候就可以使用 scrapy 命令来管理和控制项目了。</p>
<h3 id="控制项目"><a href="#控制项目" class="headerlink" title="控制项目"></a><strong>控制项目</strong></h3><p>创建一个新的 spider：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy genspider mydomain mydomain.com</div></pre></td></tr></table></figure>
<p>Scrapy 提供了两种类型的命令。一种必须在 Scrapy 项目中运行(针对项目(Project-specific)的命令)，另外一种则不需要(全局命令)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"># 全局命令(不需要在项目中运行)</div><div class="line">startproject</div><div class="line">settings</div><div class="line">runspider</div><div class="line">shell</div><div class="line">fetch</div><div class="line">view</div><div class="line">version</div><div class="line"></div><div class="line"># 项目(Project-only)命令(必须在项目中运行)</div><div class="line">crawl</div><div class="line">check</div><div class="line">list</div><div class="line">edit</div><div class="line">parse</div><div class="line">genspider</div><div class="line">deploy</div><div class="line">bench</div></pre></td></tr></table></figure>
<h2 id="工具命令介绍"><a href="#工具命令介绍" class="headerlink" title="工具命令介绍"></a><strong>工具命令介绍</strong></h2><p>我们可以通过运行命令来获取关于每个命令的详细内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy &lt;command&gt; -h</div></pre></td></tr></table></figure>
<p>也可以查看所有的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy -h</div></pre></td></tr></table></figure>
<p>下面就对这些命令进行介绍。</p>
<h3 id="startproject"><a href="#startproject" class="headerlink" title="startproject"></a>startproject</h3><ul>
<li>语法：<code>scrapy startproject &lt;project_name&gt;</code></li>
<li>全局命令</li>
</ul>
<p>在 project_name 文件夹下创建一个名为 project_name 的 Scrapy 项目。</p>
<h3 id="genspider"><a href="#genspider" class="headerlink" title="genspider"></a>genspider</h3><ul>
<li>语法：<code>scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</code></li>
<li>项目命令</li>
</ul>
<p>在当前项目中创建 spider。这仅仅是创建 spider 的一种快捷方法。该方法可以使用提前定义好的模板来生成 spider。您也可以自己创建 spider 的源码文件。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"># 查看模板</div><div class="line">$ scrapy genspider -l</div><div class="line">Available templates:</div><div class="line">  basic</div><div class="line">  crawl</div><div class="line">  csvfeed</div><div class="line">  xmlfeed</div><div class="line"></div><div class="line"># 编辑模板</div><div class="line">$ scrapy genspider -d basic</div><div class="line"># -*- coding: utf-8 -*-</div><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class $classname(scrapy.Spider):</div><div class="line">    name = &quot;$name&quot;</div><div class="line">    allowed_domains = [&quot;$domain&quot;]</div><div class="line">    start_urls = [&apos;http://$domain/&apos;]</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line">        pass</div><div class="line">    </div><div class="line"># 根据模板来生成spider</div><div class="line">$ scrapy genspider -t basic example example.com </div><div class="line">Created spider &apos;example&apos; using template &apos;basic&apos; in module: tutorial.spiders.example</div></pre></td></tr></table></figure>
<h3 id="crawl"><a href="#crawl" class="headerlink" title="crawl"></a>crawl</h3><ul>
<li>语法：<code>scrapy crawl myspider</code></li>
<li>项目命令</li>
</ul>
<p>使用 spider 进行爬取。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy crawl myspider </div><div class="line">[ ... myspider starts crawling ... ]</div></pre></td></tr></table></figure>
<h3 id="check"><a href="#check" class="headerlink" title="check"></a>check</h3><ul>
<li>语法：<code>scrapy check [-l] &lt;spider&gt;</code></li>
<li>项目命令</li>
</ul>
<p>运行 contract 检查。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ scrapy check -l </div><div class="line">first_spider </div><div class="line">* parse </div><div class="line">* parse_item </div><div class="line">second_spider </div><div class="line">* parse </div><div class="line">* parse_item</div><div class="line"></div><div class="line">$ scrapy check </div><div class="line">[FAILED] first_spider:parse_item </div><div class="line">&gt;&gt;&gt; &apos;RetailPricex&apos; field is missing</div><div class="line"></div><div class="line">[FAILED] first_spider:parse </div><div class="line">&gt;&gt;&gt; Returned 92 requests, expected 0..4</div></pre></td></tr></table></figure>
<h3 id="list"><a href="#list" class="headerlink" title="list"></a>list</h3><ul>
<li>语法：<code>scrapy list</code></li>
<li>项目命令</li>
</ul>
<p>列出当前项目中所有可用的 spider。每行输出一个 spider。 </p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ scrapy list </div><div class="line">spider1 </div><div class="line">spider2</div></pre></td></tr></table></figure>
<h3 id="edit"><a href="#edit" class="headerlink" title="edit"></a>edit</h3><ul>
<li>语法：<code>scrapy edit &lt;spider&gt;</code></li>
<li>项目命令</li>
</ul>
<p>使用 EDITOR 中设定的编辑器编辑给定的 spider </p>
<p>该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者 IDE 来编写调试 spider。 </p>
<h3 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h3><ul>
<li>语法：<code>scrapy fetch &lt;url&gt;</code></li>
<li>全局命令</li>
</ul>
<p>使用 Scrapy 下载器(downloader)下载给定的 URL，并将获取到的内容送到标准输出。 </p>
<p>该命令以 spider 下载页面的方式获取页面。例如，如果 spider 有 USER_AGENT 属性修改了 User Agen t，该命令将会使用该属性。</p>
<p>因此，您可以使用该命令来查看 spider 如何获取某个特定页面。 该命令如果非项目中运行则会使用默认 Scrapy downloader 设定。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ scrapy fetch --nolog http://www.example.com/some/page.html </div><div class="line">[ ... html content here ... ]</div><div class="line"></div><div class="line">$ scrapy fetch --nolog --headers http://www.example.com/ </div><div class="line">&#123;&apos;Accept-Ranges&apos;: [&apos;bytes&apos;],</div><div class="line">&apos;Age&apos;: [&apos;1263 &apos;], </div><div class="line">&apos;Connection&apos;: [&apos;close &apos;], </div><div class="line">&apos;Content-Length&apos;: [&apos;596&apos;], </div><div class="line">&apos;Content-Type&apos;: [&apos;text/html; charset=UTF-8&apos;], </div><div class="line">&apos;Date&apos;: [&apos;Wed, 18 Aug 2010 23:59:46 GMT&apos;], </div><div class="line">&apos;Etag&apos;: [&apos;&quot;573c1-254-48c9c87349680&quot;&apos;], </div><div class="line">&apos;Last-Modified&apos;: [&apos;Fri, 30 Jul 2010 15:30:18 GMT&apos;], </div><div class="line">&apos;Server&apos;: [&apos;Apache/2.2.3 (CentOS)&apos;]&#125;</div></pre></td></tr></table></figure>
<h3 id="view"><a href="#view" class="headerlink" title="view"></a>view</h3><ul>
<li>语法：<code>scrapy view &lt;url&gt;</code></li>
<li>全局命令</li>
</ul>
<p>在浏览器中打开给定的 URL，并以 Scrapy spider 获取到的形式展现。 有些时候 spider 获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查 spider 所获取到的页面，并确认这是您所期望的。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy view http://www.example.com/some/page.html </div><div class="line">[ ... browser starts ... ]</div></pre></td></tr></table></figure>
<h3 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h3><ul>
<li>语法：<code>scrapy shell [url]</code></li>
<li>全局命令</li>
</ul>
<p>以给定的 URL(如果给出)或者空(没有给出 URL)启动 Scrapy shell。查看 Scrapy 终端(Scrapy shell) 获取更多信息。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy shell http://www.example.com/some/page.html </div><div class="line">[ ... scrapy shell starts ... ]</div></pre></td></tr></table></figure>
<h3 id="parse"><a href="#parse" class="headerlink" title="parse"></a>parse</h3><ul>
<li>语法：<code>scrapy parse &lt;url&gt; [options]</code></li>
<li>项目命令</li>
</ul>
<p>获取给定的 URL 并使用相应的 spider 分析处理。如果提供 <code>--callback</code> 选项，则使用 spider 的该方法处理，否则使用 <code>parse</code> 。</p>
<h3 id="settings"><a href="#settings" class="headerlink" title="settings"></a>settings</h3><ul>
<li>语法：<code>scrapy settings [option]</code></li>
<li>全局命令</li>
</ul>
<p>获取 Scrapy 的设定 </p>
<p>在项目中运行时，该命令将会输出项目的设定值，否则输出 Scrapy 默认设定。 </p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ scrapy settings --get BOT_NAME </div><div class="line">scrapybot </div><div class="line">$ scrapy settings --get DOWNLOAD_DELAY </div><div class="line">0</div></pre></td></tr></table></figure>
<h3 id="runspider"><a href="#runspider" class="headerlink" title="runspider"></a>runspider</h3><ul>
<li>语法：<code>scrapy runspider &lt;spider_file.py&gt;</code></li>
<li>全局命令</li>
</ul>
<p>在未创建项目的情况下，运行一个编写在 Python 文件中的 spider。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scrapy runspider myspider.py </div><div class="line">[ ... spider starts crawling ... ]</div></pre></td></tr></table></figure>
<h3 id="version"><a href="#version" class="headerlink" title="version"></a>version</h3><ul>
<li>语法：<code>scrapy version [-v]</code></li>
<li>全局命令</li>
</ul>
<p>输出 Scrapy 版本。配合 -v 运行时，该命令同时输出 Python，Twisted 以及平台的信息，方便 bug 提交。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ scrapy version -v</div><div class="line">Scrapy    : 1.3.3</div><div class="line">lxml      : 3.7.3.0</div><div class="line">libxml2   : 2.9.4</div><div class="line">cssselect : 1.0.1</div><div class="line">parsel    : 1.1.0</div><div class="line">w3lib     : 1.17.0</div><div class="line">Twisted   : 17.1.0</div><div class="line">Python    : 3.5.2 (default, Oct 11 2016, 04:59:56) - [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)]</div><div class="line">pyOpenSSL : 16.2.0 (OpenSSL 1.1.0e  16 Feb 2017)</div><div class="line">Platform  : Darwin-16.6.0-x86_64-i386-64bit</div></pre></td></tr></table></figure>
<h3 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h3><ul>
<li>语法：<code>scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ]</code></li>
<li>项目命令</li>
</ul>
<p>将项目部署到 Scrapyd 服务。</p>
<h3 id="bench"><a href="#bench" class="headerlink" title="bench"></a>bench</h3><ul>
<li>语法：<code>scrapy bench</code></li>
<li>全局命令</li>
</ul>
<p>运行 benchmark 测试。Benchmarking。</p>
<h2 id="自定义项目命令"><a href="#自定义项目命令" class="headerlink" title="自定义项目命令"></a>自定义项目命令</h2><p>您也可以通过 COMMANDS_MODULE 来添加您自己的项目命令。您可以以 scrapy/commands 中 Scrapy commands 为例来了解如何实现您的命令。</p>
<h2 id="COMMANDS-MODULE"><a href="#COMMANDS-MODULE" class="headerlink" title="COMMANDS_MODULE"></a>COMMANDS_MODULE</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Default: &apos;&apos; (empty string)</div></pre></td></tr></table></figure>
<p>用于查找添加自定义 Scrapy 命令的模块。 例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">COMMANDS_MODULE = &apos;mybot.commands&apos;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法示例</title>
    <link href="http://yoursite.com/2017/04/16/xpath-example/"/>
    <id>http://yoursite.com/2017/04/16/xpath-example/</id>
    <published>2017-04-16T12:18:54.000Z</published>
    <updated>2017-06-09T08:20:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scrapy-中-XPath-获取相应内容"><a href="#Scrapy-中-XPath-获取相应内容" class="headerlink" title="Scrapy 中 XPath 获取相应内容"></a><strong>Scrapy 中 XPath 获取相应内容</strong></h2><p>为了方便调试，在终端下输入以下命令进入Scrapy shell：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy shell &apos;http://blog.jobbole.com/110287&apos;</div></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="获取标题"><a href="#获取标题" class="headerlink" title="获取标题"></a><strong>获取标题</strong></h3><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1feomdj05qkj31aw03yq47.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//div[@class=&apos;entry-header&apos;]/h1/text()&quot;).extract()[0]</div><div class="line">&apos;2016 腾讯软件开发面试题（部分）&apos;</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//*[@id=&quot;post-110287&quot;]/div[1]/h1/text()&apos;).extract()[0]</div><div class="line">&apos;2016 腾讯软件开发面试题（部分）&apos;</div></pre></td></tr></table></figure>
<p>以上两种方法都可以得到文章标题，第一种方法通过标题class的属性得到，第二种方法通过确定id，然后通过列表切片得到标题字符串。</p>
<h3 id="获得文章发布时间"><a href="#获得文章发布时间" class="headerlink" title="获得文章发布时间"></a><strong>获得文章发布时间</strong></h3><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1feomdvqxpxj30hs01g3yo.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0]</div><div class="line">&apos;\r\n\r\n            2017/02/18 ·  &apos;</div><div class="line">&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0].strip().replace(&apos;·&apos;, &apos;&apos;).strip()</div><div class="line">&apos;2017/02/18&apos;</div></pre></td></tr></table></figure>
<p>第一条命令只能获取p标签中的内容，还需要对获取的数据用 <code>strip()</code> 和 <code>replace()</code> 方法进行清洗。</p>
<h3 id="获取点赞数、收藏数、评论数"><a href="#获取点赞数、收藏数、评论数" class="headerlink" title="获取点赞数、收藏数、评论数"></a><strong>获取点赞数</strong>、收藏数、评论数</h3><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1feomjmf5h8j30gi03at8w.jpg" alt=""></p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1feomjq25ixj31eg0cq77q.jpg" alt=""></p>
<p>对于含有多个属性的class如：<code>class=&quot; btn-bluet-bigger href-style vote-post-up   register-user-only &quot;</code>，若只使用其中的一个属性得到值，可以使用<code>contains</code>。</p>
<h4 id="获取点赞数"><a href="#获取点赞数" class="headerlink" title="获取点赞数"></a><strong>获取点赞数</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; int(response.xpath(&quot;//span[contains(@class, &apos;vote-post-up&apos;)]/h10/text()&quot;).extract()[0])</div><div class="line">2</div></pre></td></tr></table></figure>
<h4 id="获取收藏数、评论数"><a href="#获取收藏数、评论数" class="headerlink" title="获取收藏数、评论数"></a><strong>获取收藏数、评论数</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//span[contains(@class, &apos;bookmark-btn&apos;)]/text()&quot;).extract()[0]</div><div class="line">&apos; 23 收藏&apos;</div></pre></td></tr></table></figure>
<p>得到的内容为’ 23 收藏’，需要使用正则表达式进行数据清洗。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fav_nums = response.xpath(<span class="string">"//span[contains(@class, 'bookmark-btn')]/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	fav_nums = int(match_re.group(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>同样的，评论数的获取也需要正则表达式的帮忙。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">comment_nums = response.xpath(<span class="string">"//a[@href='#article-comment']/span/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	comment_nums = int(match_re.group(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h3 id="获取正文"><a href="#获取正文" class="headerlink" title="获取正文"></a><strong>获取正文</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">content = response.xpath(<span class="string">"//div[@class='entry']"</span>).extract()[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<h3 id="获取tags"><a href="#获取tags" class="headerlink" title="获取tags"></a><strong>获取tags</strong></h3><p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1feooy0mzgpj30bi01ydfw.jpg" alt=""></p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1feooya6zapj30x2072jt4.jpg" alt=""></p>
<p>所有的tag都在a标签下，类似获得日期的方式，增加一个a标签路径即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/a/text()&quot;).extract()</div><div class="line">[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;]</div></pre></td></tr></table></figure>
<p>现在需要对数据进行清洗，去除评论标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tag_list = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/a/text()"</span>).extract()</div><div class="line">tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">tags = <span class="string">","</span>.join(tag_list)</div></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>最后我们构造的spider文件如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"jobbole"</span></div><div class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</div><div class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287'</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        title = response.xpath(<span class="string">"//div[@class='entry-header']/h1/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">        create_date = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/text()"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,</div><div class="line">                                                                                                                          <span class="string">""</span>).strip()</div><div class="line">        praise_nums = int(response.xpath(<span class="string">"//span[contains(@class, 'vote-post-up')]/h10/text()"</span>).extract()[<span class="number">0</span>])</div><div class="line">        fav_nums = response.xpath(<span class="string">"//span[contains(@class, 'bookmark-btn')]/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">        match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">        <span class="keyword">if</span> match_re:</div><div class="line">            fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">        comment_nums = response.xpath(<span class="string">"//a[@href='#article-comment']/span/text()"</span>).extract()[<span class="number">0</span>]</div><div class="line">        match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">        <span class="keyword">if</span> match_re:</div><div class="line">            comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">        content = response.xpath(<span class="string">"//div[@class='entry']"</span>).extract()[<span class="number">0</span>]</div><div class="line">        tag_list = response.xpath(<span class="string">"//p[@class='entry-meta-hide-on-mobile']/a/text()"</span>).extract()</div><div class="line">        tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">        tags = <span class="string">","</span>.join(tag_list)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Scrapy-中-XPath-获取相应内容&quot;&gt;&lt;a href=&quot;#Scrapy-中-XPath-获取相应内容&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 中 XPath 获取相应内容&quot;&gt;&lt;/a&gt;&lt;strong&gt;Scrapy 中 XPath 获取相应内容&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;为了方便调试，在终端下输入以下命令进入Scrapy shell：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;scrapy shell &amp;apos;http://blog.jobbole.com/110287&amp;apos;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，XPath，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CXPath%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy简单入门</title>
    <link href="http://yoursite.com/2017/04/16/scrapy-simple-intro/"/>
    <id>http://yoursite.com/2017/04/16/scrapy-simple-intro/</id>
    <published>2017-04-16T06:18:54.000Z</published>
    <updated>2017-04-16T07:13:21.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a><strong>创建项目</strong></h2><p>开始爬取前，首先需要创建一个新的Scrapy项目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy startproject tutorial</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>该命令将会创建包含下列内容的 tutorial 目录:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">tutorial/</div><div class="line">  scrapy.cfg </div><div class="line">  tutorial/ </div><div class="line">    __init__.py </div><div class="line">    items.py </div><div class="line">    pipelines.py</div><div class="line">    settings.py </div><div class="line">    spiders/ </div><div class="line">      __init__.py </div><div class="line">      ...</div></pre></td></tr></table></figure>
<p>这些文件分别是:</p>
<ul>
<li>scrapy.cfg：项目的配置文件</li>
<li>tutorial/：该项目的 python 模块，之后我们将在此加入代码。 </li>
<li>tutorial/items.py：项目中的 item 文件。 </li>
<li>tutorial/pipelines.py：项目中的 pipelines 文件。 </li>
<li>tutorial/settings.py：项目的设置文件。 </li>
<li>tutorial/spiders/：放置 spider 代码的目录。</li>
</ul>
<h2 id="定义-Item"><a href="#定义-Item" class="headerlink" title="定义 Item"></a><strong>定义 Item</strong></h2><p>Item 是保存爬取到的数据的容器；其使用方法和 python 字典类似， 并且提供了额外保护机制来避免拼写错误导 致的未定义字段错误。</p>
<p>类似在 ORM 中做的一样，您可以通过创建一个<code>scrapy.Item</code>类， 并且定义类型为<code>scrapy.Field</code>的类属性来定义一个 Item。 (如果不了解 ORM, 不用担心，您会发现这个步骤非常简单)</p>
<p>首先根据需要从 dmoz.org 获取到的数据对 item 进行建模。 我们需要从 dmoz 中获取名字，url，以及网站的描 述。 对此，在 item 中定义相应的字段。编辑<code>tutorial</code>目录中的<code>items.py</code>文件:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozItem</span><span class="params">(scrapy.Item)</span>:</span> </div><div class="line">    title = scrapy.Field() </div><div class="line">    link = scrapy.Field() </div><div class="line">    desc = scrapy.Field()</div></pre></td></tr></table></figure>
<p>可能一开始这有些复杂，但是通过定义 item， 我们可以很方便的使用 Scrapy 的其他方法，而这些方法需要知道我们的 item 的定义。</p>
<h2 id="编写第一个爬虫"><a href="#编写第一个爬虫" class="headerlink" title="编写第一个爬虫"></a><strong>编写第一个爬虫</strong></h2><p>Spider 是用户编写用于从单个网站(或者一些网站)爬取数据的类。 </p>
<p>其包含了一个用于下载的初始 URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方 法。 </p>
<p>为了创建一个 Spider，我们必须继承<code>scrapy.Spider</code>类， 且定义以下三个属性: </p>
<ul>
<li><code>name</code> : 用于区别 Spider。 该名字必须是唯一的，您不可以为不同的 Spider 设定相同的名字。</li>
<li><code>start_urls</code> : 包含了 Spider 在启动时进行爬取的 url 列表。 因此，第一个被获取到的页面将是其中之一。 后续的 URL 则从初始的 URL 获取到的数据中提取。 </li>
<li><code>parse()</code> : spider 的一个方法。 被调用时，每个初始 URL 完成下载后生成的<code>Response</code>对象将会作为 唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成 item)以及生成需 要进一步处理的 URL 的<code>Request</code>对象。 </li>
</ul>
<p>以下为我们的第一个 Spider 代码，保存在<code>tutorial/spiders</code>目录下的<code>dmoz_spider.py</code>文件中:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">      <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">      <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span>  </div><div class="line">    ]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        filename = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>] </div><div class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f: </div><div class="line">            f.write(response.body)</div></pre></td></tr></table></figure>
<p>其中，allow_domains是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页。</p>
<h3 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a><strong>爬取</strong></h3><p>进入项目的根目录，执行以下命令启动spider</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl dmoz</div></pre></td></tr></table></figure>
<p><code>crawl dmoz</code>启动用于爬取<code>dmoz.org</code>的 spider，可以得到如下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">2017-04-15 21:51:39 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)</div><div class="line">2017-04-15 21:51:39 [scrapy.utils.log] INFO: Overridden settings: &#123;&apos;BOT_NAME&apos;: &apos;tutorial&apos;, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;]&#125;</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled extensions:</div><div class="line">[&apos;scrapy.extensions.corestats.CoreStats&apos;,</div><div class="line"> &apos;scrapy.extensions.logstats.LogStats&apos;,</div><div class="line"> &apos;scrapy.extensions.telnet.TelnetConsole&apos;]</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:</div><div class="line">[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;,</div><div class="line"> &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:</div><div class="line">[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;,</div><div class="line"> &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]</div><div class="line">2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled item pipelines:</div><div class="line">[]</div><div class="line">2017-04-15 21:51:39 [scrapy.core.engine] INFO: Spider opened</div><div class="line">2017-04-15 21:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</div><div class="line">2017-04-15 21:51:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)</div><div class="line">2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;: HTTP status code is not handled or not allowed</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)</div><div class="line">2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt;: HTTP status code is not handled or not allowed</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] INFO: Closing spider (finished)</div><div class="line">2017-04-15 21:51:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</div><div class="line">&#123;&apos;downloader/request_bytes&apos;: 734,</div><div class="line"> &apos;downloader/request_count&apos;: 3,</div><div class="line"> &apos;downloader/request_method_count/GET&apos;: 3,</div><div class="line"> &apos;downloader/response_bytes&apos;: 3525,</div><div class="line"> &apos;downloader/response_count&apos;: 3,</div><div class="line"> &apos;downloader/response_status_count/403&apos;: 3,</div><div class="line"> &apos;finish_reason&apos;: &apos;finished&apos;,</div><div class="line"> &apos;finish_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 41, 968931),</div><div class="line"> &apos;log_count/DEBUG&apos;: 4,</div><div class="line"> &apos;log_count/INFO&apos;: 9,</div><div class="line"> &apos;response_received_count&apos;: 3,</div><div class="line"> &apos;scheduler/dequeued&apos;: 2,</div><div class="line"> &apos;scheduler/dequeued/memory&apos;: 2,</div><div class="line"> &apos;scheduler/enqueued&apos;: 2,</div><div class="line"> &apos;scheduler/enqueued/memory&apos;: 2,</div><div class="line"> &apos;start_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 39, 764494)&#125;</div><div class="line">2017-04-15 21:51:41 [scrapy.core.engine] INFO: Spider closed (finished)</div></pre></td></tr></table></figure>
<p>查看包含<code>[dmoz]</code>的输出，可以看到输出的 log 中包含定义在<code>start_urls</code>的初始 URL，并且与 spider 中是一 一对应的。在 log 中可以看到其没有指向其他页面( (<code>referer:None</code>) )。 除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含 url 所对应的内容的文件被创建 了: Book，Resources 。</p>
<h3 id="发生了什么？"><a href="#发生了什么？" class="headerlink" title="发生了什么？"></a><strong>发生了什么？</strong></h3><p>Scrapy 为 Spider 的<code>start_urls</code>属性中的每个 URL 创建了<code>scrapy.Request</code>对象，并将<code>parse</code>方法作为回调函数(callback)赋值给了<code>Request</code>。 <code>Request</code>对象经过调度，执行生成<code>scrapy.http.Response</code>对象并送回给<code>spider parse()</code>方法。</p>
<h3 id="提取-Item"><a href="#提取-Item" class="headerlink" title="提取 Item"></a><strong>提取 Item</strong></h3><h4 id="Selectors-选择器简介"><a href="#Selectors-选择器简介" class="headerlink" title="Selectors 选择器简介"></a><strong>Selectors 选择器简介</strong></h4><p>从网页中提取数据有很多方法。Scrapy 使用了一种基于 XPath 和 CSS 表达式机制: <code>Scrapy Selectors</code>。关于 selector 和其他提取机制的信息请参考<code>Selector</code>文档。 </p>
<p>关于Xpath的简单使用方法，可以查看之前的一篇博客<a href="http://lawtech0902.com/2017/04/11/xpath-usage/" target="_blank" rel="external">Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法</a></p>
<p>为了配合 XPath，Scrapy 除了提供了<code>Selector</code>之外，还提供了方法来避免每次从 response 中提取数据时生成 selector 的麻烦。</p>
<p>Selector 有四个基本的方法：</p>
<ul>
<li><code>xpath()</code>：传入 xpath 表达式，返回该表达式所对应的所有节点的 selector list 列表 。</li>
<li><code>css()</code>：传入 CSS 表达式，返回该表达式所对应的所有节点的 selector list 列表。</li>
<li><code>extract()</code>：序列化该节点为 unicode 字符串并返回 list。</li>
<li><code>re()</code>：根据传入的正则表达式对数据进行提取，返回 unicode 字符串 list 列表。</li>
</ul>
<h4 id="在-Shell-中尝试-Selector-选择器"><a href="#在-Shell-中尝试-Selector-选择器" class="headerlink" title="在 Shell 中尝试 Selector 选择器"></a><strong>在 Shell 中尝试 Selector 选择器</strong></h4><p>为了介绍 Selector 的使用方法，接下来我们将要使用内置的 Scrapy shell。Scrapy Shell 需要我们预装好 IPython(一个扩展的 Python 终端)。 我们需要进入项目的根目录，执行下列命令来启动 shell:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</div></pre></td></tr></table></figure>
<p>Shell 的输出类似于：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2017-04-15 22:04:22 [scrapy.core.engine] INFO: Spider opened</div><div class="line">2017-04-15 22:04:23 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)</div><div class="line">2017-04-15 22:04:24 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)</div><div class="line">[s] Available Scrapy objects:</div><div class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</div><div class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x109728ac8&gt;</div><div class="line">[s]   item       &#123;&#125;</div><div class="line">[s]   request    &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;</div><div class="line">[s]   response   &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;</div><div class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x10a2a0a58&gt;</div><div class="line">[s]   spider     &lt;DefaultSpider &apos;default&apos; at 0x10a4dc3c8&gt;</div><div class="line">[s] Useful shortcuts:</div><div class="line">[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)</div><div class="line">[s]   fetch(req)                  Fetch a scrapy.Request and update local objects</div><div class="line">[s]   shelp()           Shell help (print this help)</div><div class="line">[s]   view(response)    View response in a browser</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>当 shell 载入后，我们将得到一个包含 response 数据的本地 response 变量。输入 response.body 将输出 resp onse 的包体，输出 response.headers 可以看到 response 的包头。 </p>
<p>更为重要的是，当输入 response.selector 时， 我们将获取到一个可以用于查询返回数据的 selector(选择器)， 以及映射到 response.selector.xpath() 、response.selector.css() 的 快捷方法(shortcut): response.xpat h() 和 response.css() 。 </p>
<p>下面就来试试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title&apos;)</div><div class="line">[&lt;Selector xpath=&apos;//title&apos; data=&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;&gt;]</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title&apos;).extract()</div><div class="line">[&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;]</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;)</div><div class="line">[&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;DMOZ&apos;&gt;]</div><div class="line">&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;).extract()</div><div class="line">[&apos;DMOZ&apos;]</div></pre></td></tr></table></figure>
<h4 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a><strong>提取数据</strong></h4><p>现在，我们来尝试从这些页面中提取些有用的数据。 </p>
<p>我们可以在终端中输入 response.body 来观察 HTML 源码并确定合适的 XPath 表达式。不过，这任务非常无聊且不易。您可以考虑使用 Firefox 的 Firebug 扩展来使得工作更为轻松。</p>
<p>在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。 </ul></p>
<p>我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:</li></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li&apos;)</div></pre></td></tr></table></figure>
<p>网站的描述：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li/text()&apos;).extract()</div></pre></td></tr></table></figure>
<p>网站的标题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li/a/text()&apos;).extract()</div></pre></td></tr></table></figure>
<p>以及网站的链接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">response.xpath(&apos;//ul/li/a/@href&apos;).extract()</div></pre></td></tr></table></figure>
<p>之前提到过，每个 <code>.xpath()</code> 调用返回 selector 组成的 list，因此我们可以拼接更多的  <code>.xpath()</code> 来进一步获取某个节点。我们将在下边使用这样的特性:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> response <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</div><div class="line">	title = response.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">    link = response.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">    desc = response.xpath(<span class="string">'text()'</span>).extract()</div><div class="line">    print(title, link, desc)</div></pre></td></tr></table></figure>
<p>在我们的 spider 中加入如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [      	     <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> response <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</div><div class="line">            title = response.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">            link = response.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">            desc = response.xpath(<span class="string">'text()'</span>).extract()</div><div class="line">            print(title, link, desc)</div></pre></td></tr></table></figure>
<p>现在尝试再次爬取 dmoz.org，您将看到爬取到的网站信息被成功输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl dmoz</div></pre></td></tr></table></figure>
<h3 id="使用-Item"><a href="#使用-Item" class="headerlink" title="使用 Item"></a><strong>使用 Item</strong></h3><p>Item 对象是自定义的 python 字典。您可以使用标准的字典语法来获取到其每个字段的值。(字段就是我们之前用 Field 赋值的属性):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; item = DmozItem() </div><div class="line">&gt;&gt;&gt; item[&apos;title&apos;] = &apos;Example title&apos; </div><div class="line">&gt;&gt;&gt; item[&apos;title&apos;] </div><div class="line">&apos;Example title&apos;</div></pre></td></tr></table></figure>
<p>一般来说，Spider 将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> DmozItem</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"dmoz"</span></div><div class="line">    allow_domains = [<span class="string">"dmoz.org"</span>]</div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</div><div class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> response <span class="keyword">in</span> response.xpath(<span class="string">'//ul/li'</span>):</div><div class="line">            item = DmozItem()</div><div class="line">            item[<span class="string">'title'</span>] = response.xpath(<span class="string">'a/text()'</span>).extract()</div><div class="line">            item[<span class="string">'link'</span>] = response.xpath(<span class="string">'a/@href'</span>).extract()</div><div class="line">            item[<span class="string">'desc'</span>] = response.xpath(<span class="string">'text()'</span>).extract()</div><div class="line">            <span class="keyword">yield</span> item</div></pre></td></tr></table></figure>
<p>现在对 dmoz.org 进行爬取将会产生 <code>DmozItem</code> 对象:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author webs &apos;link&apos;: [u&apos;http://gnosis.cx/TPiP/&apos;], &apos;title&apos;: [u&apos;Text Processing in Python&apos;]&#125; [dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applic &apos;link&apos;: [u&apos;http://www.informit.com/store/product.aspx?isbn=0130211192&apos;], &apos;title&apos;: [u&apos;XML Processing with Python&apos;]&#125;</div></pre></td></tr></table></figure>
<h2 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a><strong>保存爬取到的数据</strong></h2><p>最简单存储爬取的数据的方式是使用 <code>Feed exports</code> :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl dmoz -o items.json</div></pre></td></tr></table></figure>
<p>该命令将采用 JSON 格式对爬取的数据进行序列化，生成 <code>items.json</code> 文件。</p>
<p>在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的 item 做更多更为复杂的 操作，您可以编写 <code>Item Pipeline</code> 。 类似于我们在创建项目时对 Item 做的，用于您编写自己的 <code>tutorial/pipelines.py</code> 也被创建。 不过如果您仅仅想要保存 item，您不需要实现任何的 pipeline。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;创建项目&quot;&gt;&lt;a href=&quot;#创建项目&quot; class=&quot;headerlink&quot; title=&quot;创建项目&quot;&gt;&lt;/a&gt;&lt;strong&gt;创建项目&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;开始爬取前，首先需要创建一个新的Scrapy项目&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;scrapy startproject tutorial&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
</feed>
