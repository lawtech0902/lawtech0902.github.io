<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LawTech&#39;s Blog</title>
  <subtitle>不破不立</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-07-03T07:07:17.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LawTech.</name>
    <email>584563542@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——scrapy-redis分布式爬虫</title>
    <link href="http://yoursite.com/2017/07/01/scrapy-redis/"/>
    <id>http://yoursite.com/2017/07/01/scrapy-redis/</id>
    <published>2017-07-01T12:18:54.000Z</published>
    <updated>2017-07-03T07:07:17.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分布式爬虫要点"><a href="#分布式爬虫要点" class="headerlink" title="分布式爬虫要点"></a><strong>分布式爬虫要点</strong></h2><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fh4hbyf2moj30h50byq4t.jpg" alt=""></p>
<h3 id="分布式爬虫的优点"><a href="#分布式爬虫的优点" class="headerlink" title="分布式爬虫的优点"></a><strong>分布式爬虫的优点</strong></h3><ul>
<li>充分利用多机器的宽带加速爬取</li>
<li>充分利用多机的IP加速爬取速度</li>
</ul>
<p>问题：<strong>为什么scrapy不支持分布式？</strong></p>
<p>答：在scrapy中scheduler是运行在队列中的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理，所以scrapy不支持分布式。</p>
<h3 id="分布式爬虫需要解决的问题"><a href="#分布式爬虫需要解决的问题" class="headerlink" title="分布式爬虫需要解决的问题"></a><strong>分布式爬虫需要解决的问题</strong></h3><ul>
<li>requests队列集中管理</li>
<li>去重集中管理</li>
</ul>
<p>综上，我们需要使用Redis来解决这些问题。</p>
<a id="more"></a>
<h2 id="Redis基础知识"><a href="#Redis基础知识" class="headerlink" title="Redis基础知识"></a><strong>Redis基础知识</strong></h2><p>Redis的基础知识在我早前的文章中已经学习过了，在这里就不介绍了，直接看之前的文章就行。</p>
<p>传送门：<a href="http://lawtech0902.com/categories/Redis/" target="_blank" rel="external">Redis学习笔记</a></p>
<h2 id="scrapy-redis编写分布式爬虫代码"><a href="#scrapy-redis编写分布式爬虫代码" class="headerlink" title="scrapy-redis编写分布式爬虫代码"></a><strong>scrapy-redis编写分布式爬虫代码</strong></h2><p>传送门：1.<a href="https://github.com/rmax/scrapy-redis" target="_blank" rel="external">scapy-redis Github</a>  2.<a href="http://scrapy-redis.readthedocs.io/en/stable/" target="_blank" rel="external">scrapy-redis 文档</a></p>
<p>其实大部分的逻辑是一样的，只需要在spider中加入<code>redis_key = &#39;spidername:start_urls&#39;</code>，以及修改一些settings.py中配置即可。</p>
<h2 id="scrapy-redis源码解析"><a href="#scrapy-redis源码解析" class="headerlink" title="scrapy-redis源码解析"></a><strong>scrapy-redis源码解析</strong></h2><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fh5szm7gkpj30920bo0tq.jpg" alt=""></p>
<h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a><strong>项目结构</strong></h3><p><strong>connection.py</strong></p>
<p>负责根据setting中配置实例化redis连接。被dupefilter和scheduler调用，总之涉及到redis存取的都要使用到这个模块。</p>
<p><strong>dupefilter.py</strong></p>
<p>负责执行requst的去重，实现的很有技巧性，使用redis的set数据结构。但是注意scheduler并不使用其中用于在这个模块中实现的dupefilter键做request的调度，而是使用queue.py模块中实现的queue。</p>
<p>当request不重复时，将其存入到queue中，调度时将其弹出。</p>
<p><strong>queue.py</strong></p>
<p>其作用如II所述，但是这里实现了三种方式的queue：</p>
<p>FIFO的SpiderQueue，SpiderPriorityQueue，以及LIFI的SpiderStack。默认使用的是第二中，这也就是出现之前文章中所分析情况的原因（链接：）。</p>
<p><strong>pipelines.py</strong></p>
<p>这是是用来实现分布式处理的作用。它将Item存储在redis中以实现分布式处理。</p>
<p>另外可以发现，同样是编写pipelines，在这里的编码实现不同于文章（链接：）中所分析的情况，由于在这里需要读取配置，所以就用到了from_crawler()函数。</p>
<p><strong>scheduler.py</strong></p>
<p>此扩展是对scrapy中自带的scheduler的替代（在settings的SCHEDULER变量中指出），正是利用此扩展实现crawler的分布式调度。其利用的数据结构来自于queue中实现的数据结构。</p>
<p>scrapy-redis所实现的两种分布式：爬虫分布式以及item处理分布式就是由模块scheduler和模块pipelines实现。上述其它模块作为为二者辅助的功能模块。</p>
<p><strong>spider.py</strong></p>
<p>设计的这个spider从redis中读取要爬的url,然后执行爬取，若爬取过程中返回更多的url，那么继续进行直至所有的request完成。之后继续从redis中读取url，循环这个过程。</p>
<p>分析：在这个spider中通过connect signals.spider_idle信号实现对crawler状态的监视。当idle时，返回新的make_requests_from_url(url)给引擎，进而交给调度器调度。</p>
<h3 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a><strong>架构解析</strong></h3><p>Scrapy架构：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fh5vaf92i0j30jk0ba0un.jpg" alt=""></p>
<p>scrapy-redis架构：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fh5v9jlr53j30j50b240g.jpg" alt=""></p>
<p>如上图所示，scrapy-redis在scrapy的架构上增加了redis，基于redis的特性拓展了如下组件：</p>
<ul>
<li>调度器（Scheduler）：scrapy-redis调度器通过redis的set不重复的特性，巧妙的实现了Duplication Filter去重（DupeFilter set存放爬取过的request）。Spider新生成的request，将request的指纹到redis的DupeFilter set检查是否重复，并将不重复的request push写入redis的request队列。调度器每次从redis的request队列里根据优先级pop出一个request, 将此request发给spider处理。</li>
<li>Item Pipeline：将Spider爬取到的Item给scrapy-redis的Item Pipeline，将爬取到的Item存入redis的items队列。可以很方便的从items队列中提取item，从而实现items processes 集群</li>
</ul>
<h2 id="集成bloomfilter到scrapy-redis中"><a href="#集成bloomfilter到scrapy-redis中" class="headerlink" title="集成bloomfilter到scrapy-redis中"></a><strong>集成bloomfilter到scrapy-redis中</strong></h2><p>传送门：<a href="http://www.it610.com/article/4376832.htm" target="_blank" rel="external">bloomfilter算法详解及实例</a></p>
<p>算法实现：<a href="https://github.com/liyaopinner/BloomFilter_imooc" target="_blank" rel="external">bloomfilter_imooc</a></p>
<p><code>dupefilter.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.dupefilters <span class="keyword">import</span> BaseDupeFilter</div><div class="line"><span class="keyword">from</span> scrapy.utils.request <span class="keyword">import</span> request_fingerprint</div><div class="line"></div><div class="line"><span class="keyword">from</span> . <span class="keyword">import</span> defaults</div><div class="line"><span class="keyword">from</span> .connection <span class="keyword">import</span> get_redis_from_settings</div><div class="line"><span class="keyword">from</span> ScrapyRedisTest.utils.bloomfilter <span class="keyword">import</span> PyBloomFilter, conn</div><div class="line"></div><div class="line">logger = logging.getLogger(__name__)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># <span class="doctag">TODO:</span> Rename class to RedisDupeFilter.</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RFPDupeFilter</span><span class="params">(BaseDupeFilter)</span>:</span></div><div class="line">    <span class="string">"""Redis-based request duplicates filter.</span></div><div class="line"></div><div class="line">    This class can also be used with default Scrapy's scheduler.</div><div class="line"></div><div class="line">    """</div><div class="line"></div><div class="line">    logger = logger</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, server, key, debug=False)</span>:</span></div><div class="line">        <span class="string">"""Initialize the duplicates filter.</span></div><div class="line"></div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        server : redis.StrictRedis</div><div class="line">            The redis server instance.</div><div class="line">        key : str</div><div class="line">            Redis key Where to store fingerprints.</div><div class="line">        debug : bool, optional</div><div class="line">            Whether to log filtered requests.</div><div class="line"></div><div class="line">        """</div><div class="line">        self.server = server</div><div class="line">        self.key = key</div><div class="line">        self.debug = debug</div><div class="line">        self.logdupes = <span class="keyword">True</span></div><div class="line"></div><div class="line">        self.bf = PyBloomFilter(conn=conn, key=key)</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        <span class="string">"""Returns an instance from given settings.</span></div><div class="line"></div><div class="line">        This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the</div><div class="line">        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as</div><div class="line">        it needs to pass the spider name in the key.</div><div class="line"></div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        settings : scrapy.settings.Settings</div><div class="line"></div><div class="line">        Returns</div><div class="line">        -------</div><div class="line">        RFPDupeFilter</div><div class="line">            A RFPDupeFilter instance.</div><div class="line"></div><div class="line"></div><div class="line">        """</div><div class="line">        server = get_redis_from_settings(settings)</div><div class="line">        <span class="comment"># <span class="doctag">XXX:</span> This creates one-time key. needed to support to use this</span></div><div class="line">        <span class="comment"># class as standalone dupefilter with scrapy's default scheduler</span></div><div class="line">        <span class="comment"># if scrapy passes spider on open() method this wouldn't be needed</span></div><div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Use SCRAPY_JOB env as default and fallback to timestamp.</span></div><div class="line">        key = defaults.DUPEFILTER_KEY % &#123;<span class="string">'timestamp'</span>: int(time.time())&#125;</div><div class="line">        debug = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>)</div><div class="line">        <span class="keyword">return</span> cls(server, key=key, debug=debug)</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></div><div class="line">        <span class="string">"""Returns instance from crawler.</span></div><div class="line"></div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        crawler : scrapy.crawler.Crawler</div><div class="line"></div><div class="line">        Returns</div><div class="line">        -------</div><div class="line">        RFPDupeFilter</div><div class="line">            Instance of RFPDupeFilter.</div><div class="line"></div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> cls.from_settings(crawler.settings)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></div><div class="line">        <span class="string">"""Returns True if request was already seen.</span></div><div class="line"></div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        request : scrapy.http.Request</div><div class="line"></div><div class="line">        Returns</div><div class="line">        -------</div><div class="line">        bool</div><div class="line"></div><div class="line">        """</div><div class="line">        fp = self.request_fingerprint(request)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.bf.is_exist(fp):</div><div class="line">            <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.bf.add(fp)</div><div class="line">            <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="comment"># This returns the number of values added, zero if already exists.</span></div><div class="line">        <span class="comment"># added = self.server.sadd(self.key, fp)</span></div><div class="line">        <span class="comment"># return added == 0</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_fingerprint</span><span class="params">(self, request)</span>:</span></div><div class="line">        <span class="string">"""Returns a fingerprint for a given request.</span></div><div class="line"></div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        request : scrapy.http.Request</div><div class="line"></div><div class="line">        Returns</div><div class="line">        -------</div><div class="line">        str</div><div class="line"></div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> request_fingerprint(request)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason=<span class="string">''</span>)</span>:</span></div><div class="line">        <span class="string">"""Delete data on close. Called by Scrapy's scheduler.</span></div><div class="line"></div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        reason : str, optional</div><div class="line"></div><div class="line">        """</div><div class="line">        self.clear()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""Clears fingerprints data."""</span></div><div class="line">        self.server.delete(self.key)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span></div><div class="line">        <span class="string">"""Logs given request.</span></div><div class="line"></div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        request : scrapy.http.Request</div><div class="line">        spider : scrapy.spiders.Spider</div><div class="line"></div><div class="line">        """</div><div class="line">        <span class="keyword">if</span> self.debug:</div><div class="line">            msg = <span class="string">"Filtered duplicate request: %(request)s"</span></div><div class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</div><div class="line">        <span class="keyword">elif</span> self.logdupes:</div><div class="line">            msg = (<span class="string">"Filtered duplicate request %(request)s"</span></div><div class="line">                   <span class="string">" - no more duplicates will be shown"</span></div><div class="line">                   <span class="string">" (see DUPEFILTER_DEBUG to show all duplicates)"</span>)</div><div class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</div><div class="line">            self.logdupes = <span class="keyword">False</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;分布式爬虫要点&quot;&gt;&lt;a href=&quot;#分布式爬虫要点&quot; class=&quot;headerlink&quot; title=&quot;分布式爬虫要点&quot;&gt;&lt;/a&gt;&lt;strong&gt;分布式爬虫要点&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79gy1fh4hbyf2moj30h50byq4t.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;分布式爬虫的优点&quot;&gt;&lt;a href=&quot;#分布式爬虫的优点&quot; class=&quot;headerlink&quot; title=&quot;分布式爬虫的优点&quot;&gt;&lt;/a&gt;&lt;strong&gt;分布式爬虫的优点&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;充分利用多机器的宽带加速爬取&lt;/li&gt;
&lt;li&gt;充分利用多机的IP加速爬取速度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;问题：&lt;strong&gt;为什么scrapy不支持分布式？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;答：在scrapy中scheduler是运行在队列中的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理，所以scrapy不支持分布式。&lt;/p&gt;
&lt;h3 id=&quot;分布式爬虫需要解决的问题&quot;&gt;&lt;a href=&quot;#分布式爬虫需要解决的问题&quot; class=&quot;headerlink&quot; title=&quot;分布式爬虫需要解决的问题&quot;&gt;&lt;/a&gt;&lt;strong&gt;分布式爬虫需要解决的问题&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;requests队列集中管理&lt;/li&gt;
&lt;li&gt;去重集中管理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;综上，我们需要使用Redis来解决这些问题。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python，Redis" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython%EF%BC%8CRedis/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy 进阶开发</title>
    <link href="http://yoursite.com/2017/06/29/scrapy-advanced-dev/"/>
    <id>http://yoursite.com/2017/06/29/scrapy-advanced-dev/</id>
    <published>2017-06-29T06:18:54.000Z</published>
    <updated>2017-07-04T06:14:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要介绍selenium的使用、其余的一些动态网页获取技术以及scrapy的一些进阶知识。</p>
<a id="more"></a>
<h2 id="Selenium的使用"><a href="#Selenium的使用" class="headerlink" title="Selenium的使用"></a><strong>Selenium的使用</strong></h2><h3 id="Selenium介绍"><a href="#Selenium介绍" class="headerlink" title="Selenium介绍"></a><strong>Selenium介绍</strong></h3><p><a href="http://selenium-python-zh.readthedocs.io/en/latest/index.html" target="_blank" rel="external">Selenium</a>是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本。</p>
<h3 id="Selenium安装"><a href="#Selenium安装" class="headerlink" title="Selenium安装"></a><strong>Selenium安装</strong></h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fgu3vu4nj3j31kw054abx.jpg" alt=""></p>
<p>Selenium安装完成之后，还需要下载浏览器对应的webdriver才能开始使用，我们这里选择Chrome的<a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" target="_blank" rel="external">ChromeDriver</a>。</p>
<h3 id="Selenium动态网页请求"><a href="#Selenium动态网页请求" class="headerlink" title="Selenium动态网页请求"></a><strong>Selenium动态网页请求</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</div><div class="line"></div><div class="line">browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line"></div><div class="line">browser.get(<span class="string">"https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5"</span>)</div><div class="line"></div><div class="line">t_selector = Selector(text=browser.page_source)</div><div class="line"></div><div class="line">print(t_selector.css(<span class="string">".tm-promo-price .tm-price::text"</span>).extract())</div><div class="line"></div><div class="line">browser.quit()</div></pre></td></tr></table></figure>
<p>我们用Selenium请求一个天猫商品的动态网页，并用Scrapy Selector来获取对应的商品价格信息。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fgu40xqud2j31d403et98.jpg" alt=""></p>
<h3 id="Selenium模拟登录知乎"><a href="#Selenium模拟登录知乎" class="headerlink" title="Selenium模拟登录知乎"></a><strong>Selenium模拟登录知乎</strong></h3><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fgu4vmkm5hj314o0dyn2r.jpg" alt=""></p>
<p>调试观察之后，采用Selenium自带的选择器方法来模拟输入账号密码并且点击登录。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line">browser.get(<span class="string">"https://www.zhihu.com/#signin"</span>)</div><div class="line"></div><div class="line">browser.find_element_by_css_selector(<span class="string">".view-signin input[name='account']"</span>).send_keys(<span class="string">"your_username"</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">".view-signin input[name='password']"</span>).send_keys(<span class="string">"your_password"</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">".view_signin button.sign-button"</span>).click()</div></pre></td></tr></table></figure>
<h3 id="Selenium模拟登录微博"><a href="#Selenium模拟登录微博" class="headerlink" title="Selenium模拟登录微博"></a><strong>Selenium模拟登录微博</strong></h3><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fgu8zpu487j30xu05wjtd.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fgu8zttkrzj30xy060ac1.jpg" alt=""></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fgu8zzmjavj30y405uq4o.jpg" alt=""></p>
<p>首先调试观察微博登录页面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line">browser.get(<span class="string">"https://www.weibo.com"</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">"#loginname"</span>).send_keys(<span class="string">"your_username"</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">".info_list.password input[node-type='password']"</span>).send_keys(<span class="string">"your_password"</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">".info_list.login_btn a[node-type='submitBtn']"</span>).click()</div></pre></td></tr></table></figure>
<p>发现如下错误</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fgu91pgp2uj31kw0ezgrc.jpg" alt=""></p>
<p>原因：我们在页面还没有请求完成时就进行了下一步操作，导致元素获取不到。</p>
<p>在请求发出之后，休眠一段时间等待页面加载完成即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line">browser.get(<span class="string">"https://www.weibo.com"</span>)</div><div class="line">time.sleep(<span class="number">15</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">"#loginname"</span>).send_keys(<span class="string">"584563542@qq.com"</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">".info_list.password input[node-type='password']"</span>).send_keys(<span class="string">"tracy584563542"</span>)</div><div class="line">browser.find_element_by_css_selector(<span class="string">".info_list.login_btn a[node-type='submitBtn']"</span>).click()</div></pre></td></tr></table></figure>
<h3 id="Selenium模拟鼠标下拉"><a href="#Selenium模拟鼠标下拉" class="headerlink" title="Selenium模拟鼠标下拉"></a><strong>Selenium模拟鼠标下拉</strong></h3><p>这样的操作是通过JS脚本来进行的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line">browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line">browser.get(<span class="string">"https://www.oschina.net/blog"</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">    browser.execute_script(<span class="string">"window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;"</span>)</div><div class="line">    time.sleep(<span class="number">3</span>)</div></pre></td></tr></table></figure>
<h3 id="设置ChromeDriver不加载图片"><a href="#设置ChromeDriver不加载图片" class="headerlink" title="设置ChromeDriver不加载图片"></a><strong>设置ChromeDriver不加载图片</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置ChromeDriver不加载图片</span></div><div class="line">chrome_opt = webdriver.ChromeOptions()</div><div class="line">prefs = &#123;<span class="string">"profile.managed_default_content_settings.images"</span>: <span class="number">2</span>&#125;</div><div class="line">chrome_opt.add_experimental_option(<span class="string">"prefs"</span>, prefs)</div><div class="line">browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>, chrome_options=chrome_opt)</div><div class="line">browser.get(<span class="string">"https://www.taobao.com"</span>)</div></pre></td></tr></table></figure>
<h3 id="PhantomJS获取动态网页"><a href="#PhantomJS获取动态网页" class="headerlink" title="PhantomJS获取动态网页"></a><strong>PhantomJS获取动态网页</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># phantomjs, 无界面的浏览器， 多进程情况下phantomjs性能会下降很严重</span></div><div class="line">browser = webdriver.PhantomJS(executable_path=<span class="string">"/Users/lawtech/TempSpace/phantomjs-2.1.1-macosx/bin/phantomjs"</span>)</div><div class="line">browser.get(<span class="string">"https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5"</span>)</div><div class="line">t_selector = Selector(text=browser.page_source)</div><div class="line">print(t_selector.css(<span class="string">".tm-promo-price .tm-price::text"</span>).extract())</div><div class="line">browser.quit()</div></pre></td></tr></table></figure>
<h3 id="Selenium集成到Scrapy中"><a href="#Selenium集成到Scrapy中" class="headerlink" title="Selenium集成到Scrapy中"></a><strong>Selenium集成到Scrapy中</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JSPageMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    通过Chrome动态请求网页</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line">        super(JSPageMiddleware, self).__init__()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></div><div class="line">        <span class="keyword">if</span> spider.name == <span class="string">"jobbole"</span>:</div><div class="line">            self.browser.get(request.url)</div><div class="line">            <span class="keyword">import</span> time</div><div class="line">            time.sleep(<span class="number">3</span>)</div><div class="line">            print(<span class="string">"访问&#123;0&#125;"</span>.format(request.url))</div><div class="line"></div><div class="line">            <span class="keyword">return</span> HtmlResponse(url=self.browser.current_url, body=self.browser.page_source, encoding=<span class="string">'utf-8'</span>)</div></pre></td></tr></table></figure>
<p>在<code>middlewares.py</code>中添加如上代码之后，别忘了在<code>settings.py</code>中将其配置好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">DOWNLOADER_MIDDLEWARES = &#123;</div><div class="line">    <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">543</span>,</div><div class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>,</div><div class="line">    <span class="string">'ArticleSpider.middlewares.JSPageMiddleware'</span>: <span class="number">1</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其实我们可以把Chrome放到Spider中，此时就引入了信号量的问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.xlib.pydispatch <span class="keyword">import</span> dispatcher</div><div class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</div><div class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"jobbole"</span></div><div class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</div><div class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/all-posts/'</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line">        super(JobboleSpider, self).__init__()</div><div class="line">        dispatcher.connect(self.spider_closed, signals.spider_closed)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        当爬虫退出的时候关闭Chrome</div><div class="line">        :param spider:</div><div class="line">        :return:</div><div class="line">        """</div><div class="line">        print(<span class="string">"spider closed"</span>)</div><div class="line">        self.browser.quit()</div></pre></td></tr></table></figure>
<h2 id="其余动态网页获取技术"><a href="#其余动态网页获取技术" class="headerlink" title="其余动态网页获取技术"></a><strong>其余动态网页获取技术</strong></h2><h3 id="Chrome无界面运行"><a href="#Chrome无界面运行" class="headerlink" title="Chrome无界面运行"></a><strong>Chrome无界面运行</strong></h3><p>首先安装<code>pyvirtualdisplay</code>：<code>pip install pyvirtualdisplay -i https://pypi.douban.com/simple/</code></p>
<p>主要代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyvirtualdisplay <span class="keyword">import</span> Display</div><div class="line">display = Display(visible=<span class="number">0</span>, size=(<span class="number">800</span>, <span class="number">600</span>))</div><div class="line">display.start()</div><div class="line"></div><div class="line">browser = webdriver.Chrome(executable_path=<span class="string">"/Users/lawtech/TempSpace/chromedriver"</span>)</div><div class="line">browser.get(<span class="string">"https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5"</span>)</div><div class="line">browser.quit()</div></pre></td></tr></table></figure>
<h3 id="scrapy-splash"><a href="#scrapy-splash" class="headerlink" title="scrapy-splash"></a><strong>scrapy-splash</strong></h3><p><a href="https://github.com/scrapy-plugins/scrapy-splash" target="_blank" rel="external">https://github.com/scrapy-plugins/scrapy-splash</a></p>
<p>稳定性没有Chrome高</p>
<h3 id="selenium-grid"><a href="#selenium-grid" class="headerlink" title="selenium-grid"></a><strong>selenium-grid</strong></h3><p><a href="https://github.com/SeleniumHQ/selenium/wiki/Grid2" target="_blank" rel="external">https://github.com/SeleniumHQ/selenium/wiki/Grid2</a></p>
<h3 id="splinter"><a href="#splinter" class="headerlink" title="splinter"></a><strong>splinter</strong></h3><p><a href="https://github.com/cobrateam/splinter" target="_blank" rel="external">https://github.com/cobrateam/splinter</a></p>
<h2 id="Scrapy的暂停与重启"><a href="#Scrapy的暂停与重启" class="headerlink" title="Scrapy的暂停与重启"></a><strong>Scrapy的暂停与重启</strong></h2><p><code>scrapy crawl lagou -s JOBDIR=jobinfo/001</code></p>
<p>上面这条命令即可完成lagouspider的暂停与重启，中途可以你可以使用ctrl+c终止采集程序的运行，恢复时，还是运行上面这条命令即可，连按两次ctrl+c就可以完全终止。</p>
<p>其中jobinfo/001 是一个保存采集列表状态的目录，千万不要同时开多个爬虫程序使用同一个目录，会导致混乱。</p>
<p>还有更简单的方法，就是在settings.py文件里加入下面的代码：</p>
<p><code>JOBDIR=&#39;jobinfo/001&#39;</code></p>
<p>使用命令<code>scrapy crawl lagou</code>，就会自动生成一个jobinfo/001的目录，然后将工作列表放到这个文件夹里。</p>
<h2 id="Scrapy-url去重原理"><a href="#Scrapy-url去重原理" class="headerlink" title="Scrapy url去重原理"></a><strong>Scrapy url去重原理</strong></h2><p>对url进行hash运算映射到某个地址，将该url和hash值当做键值对存放到hash表中，当需要检测新的url的时候，只需要对该url进行hash映射，如果得到的地址在hash表中已经存在，则说明已经被爬取过，则放弃爬取，否则，进行爬取并记录键值对。这样只需要维护一个hash表即可，需要考虑的问题是hash碰撞的问题，互联网上数据如瀚海般，如果hash函数设计不当，碰撞还是很容易发生的。scrapy框架下可以在pipeline中写一个Duplicates filter,背后采用的是hash值存储。</p>
<p>相关代码都在<code>dupefilter.py</code>中，其实就是做了一个哈希摘要，放在set中，去查新的url是否在set中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RFPDupeFilter</span><span class="params">(BaseDupeFilter)</span>:</span></div><div class="line">    <span class="string">"""Request Fingerprint duplicates filter"""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, path=None, debug=False)</span>:</span></div><div class="line">        self.file = <span class="keyword">None</span></div><div class="line">        self.fingerprints = set()</div><div class="line">        self.logdupes = <span class="keyword">True</span></div><div class="line">        self.debug = debug</div><div class="line">        self.logger = logging.getLogger(__name__)</div><div class="line">        <span class="keyword">if</span> path:</div><div class="line">            self.file = open(os.path.join(path, <span class="string">'requests.seen'</span>), <span class="string">'a+'</span>)</div><div class="line">            self.file.seek(<span class="number">0</span>)</div><div class="line">            self.fingerprints.update(x.rstrip() <span class="keyword">for</span> x <span class="keyword">in</span> self.file)</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        debug = settings.getbool(<span class="string">'DUPEFILTER_DEBUG'</span>)</div><div class="line">        <span class="keyword">return</span> cls(job_dir(settings), debug)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></div><div class="line">        fp = self.request_fingerprint(request)</div><div class="line">        <span class="keyword">if</span> fp <span class="keyword">in</span> self.fingerprints:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        self.fingerprints.add(fp)</div><div class="line">        <span class="keyword">if</span> self.file:</div><div class="line">            self.file.write(fp + os.linesep)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_fingerprint</span><span class="params">(self, request)</span>:</span></div><div class="line">        <span class="keyword">return</span> request_fingerprint(request)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.file:</div><div class="line">            self.file.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.debug:</div><div class="line">            msg = <span class="string">"Filtered duplicate request: %(request)s"</span></div><div class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</div><div class="line">        <span class="keyword">elif</span> self.logdupes:</div><div class="line">            msg = (<span class="string">"Filtered duplicate request: %(request)s"</span></div><div class="line">                   <span class="string">" - no more duplicates will be shown"</span></div><div class="line">                   <span class="string">" (see DUPEFILTER_DEBUG to show all duplicates)"</span>)</div><div class="line">            self.logger.debug(msg, &#123;<span class="string">'request'</span>: request&#125;, extra=&#123;<span class="string">'spider'</span>: spider&#125;)</div><div class="line">            self.logdupes = <span class="keyword">False</span></div><div class="line"></div><div class="line">        spider.crawler.stats.inc_value(<span class="string">'dupefilter/filtered'</span>, spider=spider)</div></pre></td></tr></table></figure>
<h2 id="Scrapy-telnet服务"><a href="#Scrapy-telnet服务" class="headerlink" title="Scrapy telnet服务"></a><strong>Scrapy telnet服务</strong></h2><p>Scrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。 telnet仅仅是一个运行在Scrapy进程中的普通python终端。</p>
<p>telnet终端监听设置中定义的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/telnetconsole.html#std:setting-TELNETCONSOLE_PORT" target="_blank" rel="external"><code>TELNETCONSOLE_PORT</code></a> ，默认为 <code>6023</code> 。 访问telnet请输入:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">telnet localhost 6023</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>Scrapy官方文档对<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/telnetconsole.html" target="_blank" rel="external">telnet</a>做了更详尽的介绍。</p>
<h2 id="Spider-middleware-详解"><a href="#Spider-middleware-详解" class="headerlink" title="Spider middleware 详解"></a><strong>Spider middleware 详解</strong></h2><p>传送门：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spider-middleware.html" target="_blank" rel="external">Spider Middleware</a></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fh4fmabuzjj30a00763z1.jpg" alt=""></p>
<p>上图为Scrapy源码中spidermiddlewares的结构</p>
<p>depth.py:爬取深度的设置</p>
<p>httperror.py：状态的设置，比如是不是要把404的也抓取下来，等等。</p>
<h2 id="Scrapy的数据收集"><a href="#Scrapy的数据收集" class="headerlink" title="Scrapy的数据收集"></a><strong>Scrapy的数据收集</strong></h2><p>传送门：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/stats.html" target="_blank" rel="external">数据收集</a></p>
<h2 id="Scrapy信号详解"><a href="#Scrapy信号详解" class="headerlink" title="Scrapy信号详解"></a><strong>Scrapy信号详解</strong></h2><p>传送门：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/signals.html" target="_blank" rel="external">信号</a></p>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 收集伯乐在线所有404的url以及404页面数</span></div><div class="line">handle_httpstatus_list = [<span class="number">404</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">    self.fail_urls = []</div><div class="line">    super(JobboleSpider, self).__init__()</div><div class="line">    dispatcher.connect(self.handle_spider_closed, signals.spider_closed)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_spider_closed</span><span class="params">(self, spider, reason)</span>:</span></div><div class="line">    self.crawler.stats.set_value(<span class="string">"failed_urls"</span>, <span class="string">","</span>.join(self.fail_urls))</div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析</div><div class="line">    2. 获取下一页的url并交给scrapy进行下载</div><div class="line">    :param response: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> response.status == <span class="number">404</span>:</div><div class="line">        self.fail_urls.append(response.url)</div><div class="line">        self.crawler.stats.inc_value(<span class="string">"failed_url"</span>)</div></pre></td></tr></table></figure>
<p>调试结果：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fh4h054xnxj30se0gwjv6.jpg" alt=""></p>
<h2 id="Scrapy扩展开发"><a href="#Scrapy扩展开发" class="headerlink" title="Scrapy扩展开发"></a><strong>Scrapy扩展开发</strong></h2><p>传送门：<a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/extensions.html" target="_blank" rel="external">扩展</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇主要介绍selenium的使用、其余的一些动态网页获取技术以及scrapy的一些进阶知识。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——云打码实现验证码识别</title>
    <link href="http://yoursite.com/2017/06/12/scrapy-yundama/"/>
    <id>http://yoursite.com/2017/06/12/scrapy-yundama/</id>
    <published>2017-06-12T06:18:54.000Z</published>
    <updated>2017-06-22T07:19:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>验证码识别大致有如下几种方式：</p>
<ul>
<li>编码实现（Tesseract-OCR）</li>
<li>在线打码</li>
<li>人工打码</li>
</ul>
<a id="more"></a>
<p>这里我们简单介绍一下在线打码，选择的打码平台为：<a href="http://www.yundama.com" target="_blank" rel="external">云打码</a></p>
<p>具体方式查看调用示例即可。</p>
<p>下面给出代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">YDMHttp</span><span class="params">(object)</span>:</span></div><div class="line">    apiurl = <span class="string">'http://api.yundama.com/api.php'</span></div><div class="line">    username = <span class="string">''</span></div><div class="line">    password = <span class="string">''</span></div><div class="line">    appid = <span class="string">''</span></div><div class="line">    appkey = <span class="string">''</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, username, password, appid, appkey)</span>:</span></div><div class="line">        self.username = username</div><div class="line">        self.password = password</div><div class="line">        self.appid = str(appid)</div><div class="line">        self.appkey = appkey</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">balance</span><span class="params">(self)</span>:</span></div><div class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'balance'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid,</div><div class="line">                <span class="string">'appkey'</span>: self.appkey&#125;</div><div class="line">        response_data = requests.post(self.apiurl, data=data)</div><div class="line">        ret_data = json.loads(response_data.text)</div><div class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"获取剩余积分"</span>, ret_data[<span class="string">"balance"</span>])</div><div class="line">            <span class="keyword">return</span> ret_data[<span class="string">"balance"</span>]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></div><div class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'login'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid,</div><div class="line">                <span class="string">'appkey'</span>: self.appkey&#125;</div><div class="line">        response_data = requests.post(self.apiurl, data=data)</div><div class="line">        ret_data = json.loads(response_data.text)</div><div class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"登录成功"</span>, ret_data[<span class="string">"uid"</span>])</div><div class="line">            <span class="keyword">return</span> ret_data[<span class="string">"uid"</span>]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, filename, codetype, timeout)</span>:</span></div><div class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'upload'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid,</div><div class="line">                <span class="string">'appkey'</span>: self.appkey, <span class="string">'codetype'</span>: str(codetype), <span class="string">'timeout'</span>: str(timeout)&#125;</div><div class="line">        files = &#123;<span class="string">'file'</span>: open(filename, <span class="string">'rb'</span>)&#125;</div><div class="line">        response_data = requests.post(self.apiurl, files=files, data=data)</div><div class="line">        ret_data = json.loads(response_data.text)</div><div class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"识别成功"</span>, ret_data[<span class="string">"text"</span>])</div><div class="line">            <span class="keyword">return</span> ret_data[<span class="string">"text"</span>]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ydm</span><span class="params">(file_path)</span>:</span></div><div class="line">    username = <span class="string">'da_ge_da1'</span></div><div class="line">    <span class="comment"># 密码</span></div><div class="line">    password = <span class="string">'da_ge_da'</span></div><div class="line">    <span class="comment"># 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></div><div class="line">    appid = <span class="number">3129</span></div><div class="line">    <span class="comment"># 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></div><div class="line">    appkey = <span class="string">'40d5ad41c047179fc797631e3b9c3025'</span></div><div class="line">    <span class="comment"># 图片文件</span></div><div class="line">    filename = <span class="string">'image/captcha.jpg'</span></div><div class="line">    <span class="comment"># 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html</span></div><div class="line">    codetype = <span class="number">5000</span></div><div class="line">    <span class="comment"># 超时时间，秒</span></div><div class="line">    timeout = <span class="number">60</span></div><div class="line">    <span class="comment"># 检查</span></div><div class="line"></div><div class="line">    yundama = YDMHttp(username, password, appid, appkey)</div><div class="line">    <span class="keyword">if</span> (username == <span class="string">'username'</span>):</div><div class="line">        print(<span class="string">'请设置好相关参数再测试'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果</span></div><div class="line">        <span class="keyword">return</span> yundama.decode(file_path, codetype, timeout);</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    <span class="comment"># 用户名</span></div><div class="line">    username = <span class="string">'da_ge_da1'</span></div><div class="line">    <span class="comment"># 密码</span></div><div class="line">    password = <span class="string">'da_ge_da'</span></div><div class="line">    <span class="comment"># 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></div><div class="line">    appid = <span class="number">3129</span></div><div class="line">    <span class="comment"># 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></div><div class="line">    appkey = <span class="string">'40d5ad41c047179fc797631e3b9c3025'</span></div><div class="line">    <span class="comment"># 图片文件</span></div><div class="line">    filename = <span class="string">'image/captcha.jpg'</span></div><div class="line">    <span class="comment"># 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html</span></div><div class="line">    codetype = <span class="number">5000</span></div><div class="line">    <span class="comment"># 超时时间，秒</span></div><div class="line">    timeout = <span class="number">60</span></div><div class="line">    <span class="comment"># 检查</span></div><div class="line">    <span class="keyword">if</span> (username == <span class="string">'username'</span>):</div><div class="line">        print(<span class="string">'请设置好相关参数再测试'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># 初始化</span></div><div class="line">        yundama = YDMHttp(username, password, appid, appkey)</div><div class="line"></div><div class="line">        <span class="comment"># 登陆云打码</span></div><div class="line">        uid = yundama.login()</div><div class="line">        print(<span class="string">'uid: %s'</span> % uid)</div><div class="line"></div><div class="line">        <span class="comment"># 登陆云打码</span></div><div class="line">        uid = yundama.login()</div><div class="line">        print(<span class="string">'uid: %s'</span> % uid)</div><div class="line"></div><div class="line">        <span class="comment"># 查询余额</span></div><div class="line">        balance = yundama.balance()</div><div class="line">        print(<span class="string">'balance: %s'</span> % balance)</div><div class="line"></div><div class="line">        <span class="comment"># 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果</span></div><div class="line">        text = yundama.decode(filename, codetype, timeout)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;验证码识别大致有如下几种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;编码实现（Tesseract-OCR）&lt;/li&gt;
&lt;li&gt;在线打码&lt;/li&gt;
&lt;li&gt;人工打码&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——cookie禁用、自动限速、自定义Spider的settings</title>
    <link href="http://yoursite.com/2017/06/12/scrapy-cookies-settings/"/>
    <id>http://yoursite.com/2017/06/12/scrapy-cookies-settings/</id>
    <published>2017-06-12T06:18:54.000Z</published>
    <updated>2017-06-22T08:29:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>像cookie禁用、自动限速这样的设置都在<code>settings.py</code>文件中，下面我们就来简单介绍一下。</p>
<a id="more"></a>
<h2 id="cookie禁用"><a href="#cookie禁用" class="headerlink" title="cookie禁用"></a><strong>cookie禁用</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Disable cookies (enabled by default)</span></div><div class="line">COOKIES_ENABLED = <span class="keyword">True</span></div></pre></td></tr></table></figure>
<p>默认设置为False。</p>
<h2 id="自动限速"><a href="#自动限速" class="headerlink" title="自动限速"></a><strong>自动限速</strong></h2><p>自动限速是通过自动限速(AutoThrottle)扩展来实现的，该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。</p>
<h3 id="限速算法"><a href="#限速算法" class="headerlink" title="限速算法"></a><strong>限速算法</strong></h3><p>算法根据以下规则调整下载延迟及并发数:</p>
<ol>
<li>spider永远以1并发请求数及 <code>AUTOTHROTTLE_START_DELAY</code> 中指定的下载延迟启动。</li>
<li>当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。</li>
</ol>
<p><code>AutoThrottle</code> 扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比<code>DOWNLOAD_DELAY</code> 更低的下载延迟或者比 <code>CONCURRENT_REQUESTS_PER_DOMAIN</code> 更高的并发数 (或 <code>CONCURRENT_REQUESTS_PER_IP</code> ，取决于您使用哪一个)。</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>下面是控制 <code>AutoThrottle</code> 扩展的设置:</p>
<ul>
<li><code>AUTOTHROTTLE_ENABLED</code></li>
<li><code>AUTOTHROTTLE_START_DELAY</code></li>
<li><code>AUTOTHROTTLE_MAX_DELAY</code></li>
<li><code>AUTOTHROTTLE_DEBUG</code></li>
<li><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></li>
<li><code>CONCURRENT_REQUESTS_PER_IP</code></li>
<li><code>DOWNLOAD_DELAY</code></li>
</ul>
<h3 id="AUTOTHROTTLE-ENABLED"><a href="#AUTOTHROTTLE-ENABLED" class="headerlink" title="AUTOTHROTTLE_ENABLED"></a>AUTOTHROTTLE_ENABLED</h3><p>默认: <code>False</code></p>
<p>启用AutoThrottle扩展。</p>
<h3 id="AUTOTHROTTLE-START-DELAY"><a href="#AUTOTHROTTLE-START-DELAY" class="headerlink" title="AUTOTHROTTLE_START_DELAY"></a>AUTOTHROTTLE_START_DELAY</h3><p>默认: <code>5.0</code></p>
<p>初始下载延迟(单位:秒)。</p>
<h3 id="AUTOTHROTTLE-MAX-DELAY"><a href="#AUTOTHROTTLE-MAX-DELAY" class="headerlink" title="AUTOTHROTTLE_MAX_DELAY"></a>AUTOTHROTTLE_MAX_DELAY</h3><p>默认: <code>60.0</code></p>
<p>在高延迟情况下最大的下载延迟(单位秒)。</p>
<h3 id="AUTOTHROTTLE-DEBUG"><a href="#AUTOTHROTTLE-DEBUG" class="headerlink" title="AUTOTHROTTLE_DEBUG"></a>AUTOTHROTTLE_DEBUG</h3><p>默认: <code>False</code></p>
<p>起用AutoThrottle调试(debug)模式，展示每个接收到的response。 您可以通过此来查看限速参数是如何实时被调整的。</p>
<h2 id="自定义Spider的settings"><a href="#自定义Spider的settings" class="headerlink" title="自定义Spider的settings"></a><strong>自定义Spider的settings</strong></h2><p>每个Spider可以定义自己的设置，这些设置将优先覆盖项目目录中的设置，可以通过设置 <code>custom_settings</code> 属性来实现。</p>
<p>例如，我们的项目中，<code>zhihu.py</code> 中需要设置开启cookie，那么只需要在该文件中如下设置即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">custom_settings = &#123;</div><div class="line">    <span class="string">"COOKIES_ENABLED"</span>: <span class="keyword">True</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="settings-py详解"><a href="#settings-py详解" class="headerlink" title="settings.py详解"></a><strong>settings.py详解</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line"><span class="comment"># Scrapy settings for ArticleSpider project</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># For simplicity, this file contains only settings considered important or</span></div><div class="line"><span class="comment"># commonly used. You can find more settings consulting the documentation:</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#     http://doc.scrapy.org/en/latest/topics/settings.html</span></div><div class="line"><span class="comment">#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></div><div class="line"><span class="comment">#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></div><div class="line"></div><div class="line">BOT_NAME = <span class="string">'ArticleSpider'</span>  <span class="comment">#Scrapy项目的名字这将用来构造默认User-Agent，同时也用来log，当您使用startproject命令创建项目时其也被自动赋值。</span></div><div class="line"></div><div class="line">SPIDER_MODULES = [<span class="string">'ArticleSpider.spiders'</span>]  <span class="comment">#Scrapy搜索spider的模块列表 默认: [xxx.spiders]</span></div><div class="line">NEWSPIDER_MODULE = <span class="string">'ArticleSpider.spiders'</span>  <span class="comment">#使用genspider命令创建新spider的模块 默认: 'xxx.spiders'</span></div><div class="line"></div><div class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></div><div class="line"><span class="comment"># USER_AGENT = 'ArticleSpider (+http://www.yourdomain.com)'  #爬取的默认User-Agent，除非被覆盖</span></div><div class="line"></div><div class="line"><span class="comment"># Obey robots.txt rules</span></div><div class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span>  <span class="comment">#如果启用,Scrapy将会采用robots.txt策略 </span></div><div class="line"></div><div class="line"><span class="comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span></div><div class="line"><span class="comment">#Scrapy downloader并发请求(concurrent requests)的最大值，默认: 16</span></div><div class="line"><span class="comment"># CONCURRENT_REQUESTS = 32</span></div><div class="line"></div><div class="line"><span class="comment"># Configure a delay for requests for the same website (default: 0)</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span></div><div class="line"><span class="comment"># See also autothrottle settings and docs</span></div><div class="line"><span class="comment">#为同一网站的请求配置延迟（默认值：0）下载器在下载同一个网站下一个页面前需要等待的时间,该选项可以用来限制爬取速度,减轻服务器压力。同时也支持小数:0.25 以秒为单位  </span></div><div class="line">DOWNLOAD_DELAY = <span class="number">10</span></div><div class="line"><span class="comment"># The download delay setting will honor only one of:</span></div><div class="line"><span class="comment">#下载延迟设置只有一个有效</span></div><div class="line"><span class="comment"># CONCURRENT_REQUESTS_PER_DOMAIN = 16  对单个网站进行并发请求的最大值。</span></div><div class="line"><span class="comment"># CONCURRENT_REQUESTS_PER_IP = 16  对单个IP进行并发请求的最大值。如果非0,则忽略 CONCURRENT_REQUESTS_PER_DOMAIN 设定,使用该设定。 也就是说,并发限制将针对IP,而不是网站。该设定也影响 DOWNLOAD_DELAY: 如果 CONCURRENT_REQUESTS_PER_IP 非0,下载延迟应用在IP而不是网站上。  </span></div><div class="line"></div><div class="line"><span class="comment"># Disable cookies (enabled by default)</span></div><div class="line"><span class="comment">#禁用Cookie（默认情况下启用）</span></div><div class="line">COOKIES_ENABLED = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># Disable Telnet Console (enabled by default)</span></div><div class="line"><span class="comment">#禁用Telnet控制台（默认启用）</span></div><div class="line"><span class="comment"># TELNETCONSOLE_ENABLED = False</span></div><div class="line"></div><div class="line"><span class="comment"># Override the default request headers:</span></div><div class="line"><span class="comment">#覆盖默认请求头：</span></div><div class="line"><span class="comment"># DEFAULT_REQUEST_HEADERS = &#123;</span></div><div class="line"><span class="comment">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span></div><div class="line"><span class="comment">#   'Accept-Language': 'en',</span></div><div class="line"><span class="comment"># &#125;</span></div><div class="line"></div><div class="line"><span class="comment"># Enable or disable spider middlewares</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></div><div class="line"><span class="comment">#启用或禁用爬虫中间件</span></div><div class="line"><span class="comment"># SPIDER_MIDDLEWARES = &#123;</span></div><div class="line"><span class="comment">#    'ArticleSpider.middlewares.ArticlespiderSpiderMiddleware': 543,</span></div><div class="line"><span class="comment"># &#125;</span></div><div class="line"></div><div class="line"><span class="comment"># Enable or disable downloader middlewares</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></div><div class="line"><span class="comment">#启用或禁用下载器中间件</span></div><div class="line">DOWNLOADER_MIDDLEWARES = &#123;</div><div class="line">    <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">543</span>,</div><div class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>,</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># Enable or disable extensions</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span></div><div class="line"><span class="comment">#启用或禁用扩展程序</span></div><div class="line"><span class="comment"># EXTENSIONS = &#123;</span></div><div class="line"><span class="comment">#    'scrapy.extensions.telnet.TelnetConsole': None,</span></div><div class="line"><span class="comment"># &#125;</span></div><div class="line"></div><div class="line"><span class="comment"># Configure item pipelines</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    <span class="comment"># 'ArticleSpider.pipelines.JsonExporterPipeline': 2,</span></div><div class="line">    <span class="comment"># # 'scrapy.pipelines.images.ImagesPipeline': 1,</span></div><div class="line">    <span class="comment"># 'ArticleSpider.pipelines.ArticleImagePipeline': 1,</span></div><div class="line">    <span class="string">'ArticleSpider.pipelines.MysqlTwistedPipeline'</span>: <span class="number">1</span>,</div><div class="line">&#125;</div><div class="line">IMAGES_URLS_FIELD = <span class="string">"front_image_url"</span></div><div class="line">project_dir = os.path.abspath(os.path.dirname(__file__))</div><div class="line">IMAGES_STORE = os.path.join(project_dir, <span class="string">'images'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设置搜索路径</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line">BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))</div><div class="line">sys.path.insert(<span class="number">0</span>, os.path.join(BASE_DIR, <span class="string">'ArticleSpider'</span>))</div><div class="line"></div><div class="line">USER_AGENT = <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4"</span></div><div class="line"></div><div class="line">RANDOM_UA_TYPE = <span class="string">'random'</span></div><div class="line"></div><div class="line"><span class="comment"># IMAGES_MIN_HEIGHT = 100</span></div><div class="line"><span class="comment"># IMAGES_MIN_WIDTH = 100</span></div><div class="line"></div><div class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)</span></div><div class="line"><span class="comment"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</span></div><div class="line"><span class="comment">#启用和配置AutoThrottle扩展（默认情况下禁用）</span></div><div class="line">AUTOTHROTTLE_ENABLED = <span class="keyword">True</span></div><div class="line"><span class="comment"># The initial download delay</span></div><div class="line"><span class="comment"># AUTOTHROTTLE_START_DELAY = 5</span></div><div class="line"><span class="comment"># The maximum download delay to be set in case of high latencies</span></div><div class="line"><span class="comment"># AUTOTHROTTLE_MAX_DELAY = 60</span></div><div class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to</span></div><div class="line"><span class="comment"># each remote server</span></div><div class="line"><span class="comment"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span></div><div class="line"><span class="comment"># Enable showing throttling stats for every response received:</span></div><div class="line"><span class="comment"># AUTOTHROTTLE_DEBUG = False</span></div><div class="line"></div><div class="line"><span class="comment"># Enable and configure HTTP caching (disabled by default)</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span></div><div class="line"><span class="comment">#启用和配置HTTP缓存（默认情况下禁用）</span></div><div class="line"><span class="comment"># HTTPCACHE_ENABLED = True</span></div><div class="line"><span class="comment"># HTTPCACHE_EXPIRATION_SECS = 0</span></div><div class="line"><span class="comment"># HTTPCACHE_DIR = 'httpcache'</span></div><div class="line"><span class="comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span></div><div class="line"><span class="comment"># HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></div><div class="line"></div><div class="line">MYSQL_HOST = <span class="string">"127.0.0.1"</span></div><div class="line">MYSQL_DBNAME = <span class="string">"article_spider"</span></div><div class="line">MYSQL_USER = <span class="string">"root"</span></div><div class="line">MYSQL_PASSWORD = <span class="string">"123"</span></div><div class="line"></div><div class="line">SQL_DATETIME_FORMAT = <span class="string">"%Y-%m-%d %H:%M:%S"</span></div><div class="line">SQL_DATE_FORMAT = <span class="string">"%Y-%m-%d"</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;像cookie禁用、自动限速这样的设置都在&lt;code&gt;settings.py&lt;/code&gt;文件中，下面我们就来简单介绍一下。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy随机更换User-Agent和实现IP代理池</title>
    <link href="http://yoursite.com/2017/06/11/scrapy-useragent-proxyip/"/>
    <id>http://yoursite.com/2017/06/11/scrapy-useragent-proxyip/</id>
    <published>2017-06-11T06:18:54.000Z</published>
    <updated>2017-06-21T10:13:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前有一节用于介绍<code>Request</code>和<code>Response</code>，还是<a href="https://doc.scrapy.org/en/latest/index.html" target="_blank" rel="external">官方文档</a>介绍的比较详尽，所以就不做笔记了。</p>
<p>这一节用于介绍随机更换User-Agent和实现IP代理池的方法，首先来看一下现在网站中所做的反爬虫工作。</p>
<a id="more"></a>
<h2 id="常见的反爬虫和应对方法"><a href="#常见的反爬虫和应对方法" class="headerlink" title="常见的反爬虫和应对方法"></a><strong>常见的反爬虫和应对方法</strong></h2><p>一般网站从三个方面反爬虫：用户请求的Headers，用户行为，网站目录和数据加载方式。前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，这样增大了爬取的难度。</p>
<h3 id="通过Headers反爬虫"><a href="#通过Headers反爬虫" class="headerlink" title="通过Headers反爬虫"></a><strong>通过</strong>Headers反爬虫</h3><p>从用户请求的Headers反爬虫是最常见的反爬虫策略。很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）。如果遇到了这类反爬虫机制，可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中；或者将Referer值修改为目标网站域名。对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。</p>
<p>针对这样的反爬虫方法，我们在Scrapy中实现随机更换User-Agent就很有必要了。</p>
<h3 id="基于用户行为反爬虫"><a href="#基于用户行为反爬虫" class="headerlink" title="基于用户行为反爬虫"></a>基于用户行为反爬虫</h3><p>还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。</p>
<p>大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。这样的代理ip爬虫经常会用到，最好自己准备一个。有了大量代理ip后可以每请求几次更换一个ip，这在requests或者urllib2中很容易做到，这样就能很容易的绕过第一种反爬虫。</p>
<p>对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。</p>
<p>对于这样的反爬虫方法，我们可以在Scrapy中实现IP代理池，问题便迎刃而解。</p>
<h3 id="动态页面的反爬虫"><a href="#动态页面的反爬虫" class="headerlink" title="动态页面的反爬虫"></a>动态页面的反爬虫</h3><p>上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。首先用Firebug或者HttpFox对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests或者urllib2模拟ajax请求，对响应的json进行分析得到需要的数据。</p>
<p>能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。我这几天爬的那个网站就是这样，除了加密ajax参数，它还把一些基本的功能都封装了，全部都是在调用自己的接口，而接口参数都是加密的。遇到这样的网站，我们就不能用上面的方法了，我用的是selenium+phantomJS框架，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 </p>
<p>用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利用 selenium+phantomJS能干很多事情，例如识别点触式（12306）或者滑动式的验证码，对页面表单进行暴力破解等等。它在自动化渗透中还 会大展身手，以后还会提到这个。</p>
<h2 id="随机更换User-Agent"><a href="#随机更换User-Agent" class="headerlink" title="随机更换User-Agent"></a><strong>随机更换User-Agent</strong></h2><p>首先将下面的代码添加到<code>settings.py</code>文件，替换默认的user-agent处理模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">DOWNLOADER_MIDDLEWARES = &#123;</div><div class="line">    <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">543</span>,</div><div class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在<code>middlewares.py</code>文件中自定义User-Agent处理模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    随机更换User-Agent</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, crawler)</span>:</span></div><div class="line">        super(RandomUserAgentMiddleware, self).__init__()</div><div class="line">        self.ua = UserAgent()</div><div class="line">        self.ua_type = crawler.settings.get(<span class="string">'RANDOM_UA_TYPE'</span>, <span class="string">'random'</span>)</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></div><div class="line">        <span class="keyword">return</span> cls(crawler)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_ua</span><span class="params">()</span>:</span></div><div class="line">            <span class="keyword">return</span> getattr(self.ua, self.ua_type)</div><div class="line"></div><div class="line">        request.headers.setdefault(<span class="string">'User-Agent'</span>, get_ua())</div></pre></td></tr></table></figure>
<p>其中，<a href="https://github.com/hellysmile/fake-useragent" target="_blank" rel="external">fake_useragent</a>这个第三方库维护了大量的User-Agent，我们就没必要自己去维护了，直接使用就好。</p>
<p>下图为其的安装和使用方法：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fgspyd3gulj31e8150n6o.jpg" alt=""></p>
<h2 id="实现IP代理池"><a href="#实现IP代理池" class="headerlink" title="实现IP代理池"></a><strong>实现IP代理池</strong></h2><p>我们用<a href="http://www.xicidaili.com/nn" target="_blank" rel="external">西刺免费代理IP网站</a>来实现这个IP代理池。我们选择爬取这个网站，然后将获取的数据写入数据库以供我们自己使用。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fgsqbcp5abj31kw180b29.jpg" alt=""></p>
<p>调试之后，用CSS选择器来获取我们所需的内容，并将其保存到数据库中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</div><div class="line"><span class="keyword">import</span> MySQLdb</div><div class="line"></div><div class="line">conn = MySQLdb.connect(host=<span class="string">"127.0.0.1"</span>, user=<span class="string">"root"</span>, passwd=<span class="string">"123"</span>, db=<span class="string">"article_spider"</span>, charset=<span class="string">"utf8"</span>)</div><div class="line">cursor = conn.cursor()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl_ips</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    爬取西刺网的免费代理IP</div><div class="line">    """</div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4"</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2093</span>):</div><div class="line">        re = requests.get(<span class="string">"http://www.xicidaili.com/nn/&#123;0&#125;"</span>.format(i), headers=headers)</div><div class="line"></div><div class="line">        selector = Selector(text=re.text)</div><div class="line">        all_trs = selector.css(<span class="string">"#ip_list tr"</span>)</div><div class="line"></div><div class="line">        ip_list = []</div><div class="line"></div><div class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> all_trs[<span class="number">1</span>:]:</div><div class="line">            speed_str = tr.css(<span class="string">".bar::attr(title)"</span>).extract()[<span class="number">0</span>]</div><div class="line">            <span class="keyword">if</span> speed_str:</div><div class="line">                speed = float(speed_str.split(<span class="string">"秒"</span>)[<span class="number">0</span>])</div><div class="line">            ip = tr.css(<span class="string">"td:nth-child(2)::text"</span>).extract_first()</div><div class="line">            port = tr.css(<span class="string">"td:nth-child(3)::text"</span>).extract_first()</div><div class="line">            proxy_type = tr.css(<span class="string">"td:nth-child(6)::text"</span>).extract_first()</div><div class="line">            ip_list.append((ip, port, proxy_type, speed))</div><div class="line"></div><div class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> ip_list:</div><div class="line">            cursor.execute(</div><div class="line">                <span class="string">"insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', '&#123;2&#125;', '&#123;3&#125;')"</span>.format(</div><div class="line">                    ip_info[<span class="number">0</span>], ip_info[<span class="number">1</span>], ip_info[<span class="number">3</span>], ip_info[<span class="number">2</span>]</div><div class="line">                )</div><div class="line">            )</div><div class="line">            conn.commit()</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetIP</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete_ip</span><span class="params">(self, ip)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        从数据库中删除无效的IP </div><div class="line">        """</div><div class="line">        delete_sql = <span class="string">"""</span></div><div class="line">            DELETE FROM proxy_ip WHERE ip='&#123;0&#125;'</div><div class="line">        """.format(ip)</div><div class="line">        cursor.execute(delete_sql)</div><div class="line">        conn.commit()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">judge_ip</span><span class="params">(self, ip, port)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        判断IP是否可用</div><div class="line">        """</div><div class="line">        http_url = <span class="string">"http://www.baidu.com"</span></div><div class="line">        proxy_url = <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            proxy_dict = &#123;</div><div class="line">                <span class="string">"http"</span>: proxy_url,</div><div class="line">            &#125;</div><div class="line">            response = requests.get(http_url, proxies=proxy_dict)</div><div class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">            print(<span class="string">"invalid ip and port"</span>)</div><div class="line">            self.delete_ip(ip)</div><div class="line">            <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            code = response.status_code</div><div class="line">            <span class="keyword">if</span> code &gt;= <span class="number">200</span> <span class="keyword">and</span> code &lt; <span class="number">300</span>:</div><div class="line">                print(<span class="string">"effective ip"</span>)</div><div class="line">                <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                print(<span class="string">"invalid ip and port"</span>)</div><div class="line">                self.delete_ip(ip)</div><div class="line">                <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_random_ip</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        从数据库中随机获取一个可用的IP</div><div class="line">        """</div><div class="line">        random_sql = <span class="string">"""</span></div><div class="line">            SELECT ip, port FROM proxy_ip</div><div class="line">            ORDER BY RAND()</div><div class="line">            LIMIT 1</div><div class="line">        """</div><div class="line">        result = cursor.execute(random_sql)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> cursor.fetchall():</div><div class="line">            ip = ip_info[<span class="number">0</span>]</div><div class="line">            port = ip_info[<span class="number">1</span>]</div><div class="line"></div><div class="line">            judge_re = self.judge_ip(ip, port)</div><div class="line">            <span class="keyword">if</span> judge_re:</div><div class="line">                <span class="keyword">return</span> <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">return</span> self.get_random_ip()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    get_ip = GetIP()</div><div class="line">    get_ip.get_random_ip()</div></pre></td></tr></table></figure>
<p>最后，在<code>middlewares.py</code>文件中写入我们用于设置IP代理的逻辑即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomProxyMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    动态设置IP代理</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></div><div class="line">        get_ip = GetIP()</div><div class="line">        request.meta[<span class="string">"proxy"</span>] = get_ip.get_random_ip()</div></pre></td></tr></table></figure>
<h2 id="可参考的第三方库"><a href="#可参考的第三方库" class="headerlink" title="可参考的第三方库"></a><strong>可参考的第三方库</strong></h2><ul>
<li><a href="https://github.com/scrapy-plugins/scrapy-crawlera" target="_blank" rel="external"><strong>scrapy-crawlera</strong></a></li>
<li><a href="https://github.com/aivarsk/scrapy-proxies" target="_blank" rel="external"><strong>scrapy-proxies</strong></a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前有一节用于介绍&lt;code&gt;Request&lt;/code&gt;和&lt;code&gt;Response&lt;/code&gt;，还是&lt;a href=&quot;https://doc.scrapy.org/en/latest/index.html&quot;&gt;官方文档&lt;/a&gt;介绍的比较详尽，所以就不做笔记了。&lt;/p&gt;
&lt;p&gt;这一节用于介绍随机更换User-Agent和实现IP代理池的方法，首先来看一下现在网站中所做的反爬虫工作。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy核心架构分析</title>
    <link href="http://yoursite.com/2017/06/11/scrapy-framework/"/>
    <id>http://yoursite.com/2017/06/11/scrapy-framework/</id>
    <published>2017-06-11T06:18:54.000Z</published>
    <updated>2017-06-21T04:04:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a><strong>概览</strong></h2><p>首先看一下Scrapy的架构图：</p>
<a id="more"></a>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fgsotbkiowj31240oagp5.jpg" alt=""></p>
<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a><strong>核心组件</strong></h2><p>Scrapy有以下几大组件：</p>
<ul>
<li><code>Scrapy Engine</code>：核心引擎，负责控制和调度各个组件，保证数据流转；</li>
<li><code>Scheduler</code>：负责管理任务、过滤任务、输出任务的调度器，存储、去重任务都在此控制；</li>
<li><code>Downloader</code>：下载器，负责在网络上下载网页数据，输入待下载URL，输出下载结果；</li>
<li><code>Spiders</code>：用户自己编写的爬虫脚本，可自定义抓取意图；</li>
<li><code>Item Pipeline</code>：负责输出结构化数据，可自定义输出位置；</li>
</ul>
<p>除此之外，还有两大中间件组件：</p>
<ul>
<li><code>Downloader middlewares</code>：介于引擎和下载器之间，可以在网页在下载前、后进行逻辑处理；</li>
<li><code>Spider middlewares</code>：介于引擎和爬虫之间，可以在调用爬虫输入下载结果和输出请求/数据时进行逻辑处理</li>
</ul>
<h2 id="数据流转"><a href="#数据流转" class="headerlink" title="数据流转"></a><strong>数据流转</strong></h2><p>按照架构图的序号，数据流转大概是这样的：</p>
<ol>
<li><strong>引擎</strong>从<strong>自定义爬虫</strong>中获取初始化请求（也叫种子URL）；</li>
<li>引擎把该请求放入<strong>调度器</strong>中，同时<strong>引擎向调度器</strong>获取一个<strong>待下载的请求</strong>（这两部是异步执行的）；</li>
<li>调度器返回给引擎一个<strong>待下载</strong>的请求；</li>
<li>引擎发送请求给<strong>下载器</strong>，中间会经过一系列<strong>下载器中间件</strong>；</li>
<li>这个请求通过下载器下载完成后，生成一个<strong>响应对象</strong>，返回给引擎，这中间会再次经过一系列<strong>下载器中间件</strong>；</li>
<li>引擎接收到下载返回的响应对象后，然后发送给爬虫，执行<strong>自定义爬虫逻辑</strong>，中间会经过一系列<strong>爬虫中间件</strong>；</li>
<li>爬虫执行对应的回调方法，处理这个响应，完成用户逻辑后，会生成<strong>结果对象</strong>或<strong>新的请求对象</strong>给引擎，再次经过一系列<strong>爬虫中间件</strong>；</li>
<li>引擎把爬虫返回的结果对象交由<strong>结果处理器</strong>处理，把<strong>新的请求</strong>对象通过引擎再交给<strong>调度器</strong>；</li>
<li>从1开始重复执行，直到<strong>调度器</strong>中没有新的请求处理；</li>
</ol>
<h2 id="核心组件交互图"><a href="#核心组件交互图" class="headerlink" title="核心组件交互图"></a><strong>核心组件交互图</strong></h2><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fgsowftl9fj31f60zjtco.jpg" alt=""></p>
<p>这里需要说明一下图中的<code>Scrapyer</code>，其实这也是在源码的一个核心类，但官方架构图中没有展示出来，这个类其实是处于<code>Engine</code>、<code>Spiders</code>、<code>Pipeline</code>之间，是连通这3个组件的桥梁。</p>
<h2 id="核心类图"><a href="#核心类图" class="headerlink" title="核心类图"></a><strong>核心类图</strong></h2><p>涉及到的一些核心类如下：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fgsoxkf4ntj30tc14hq6c.jpg" alt=""></p>
<p>其中没有样式的<strong>黑色文字</strong>是类的核心<strong>属性</strong>，<strong>黄色样式</strong>的文字都是核心<strong>方法</strong>。</p>
<p>可以看到，Scrapy的核心类，其实主要包含5大组件、4大中间件管理器、爬虫类和爬虫管理器、请求、响应对象和数据解析类这几大块。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概览&quot;&gt;&lt;/a&gt;&lt;strong&gt;概览&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;首先看一下Scrapy的架构图：&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Requests，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRequests%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取拉勾网</title>
    <link href="http://yoursite.com/2017/06/09/scrapy-crawlspider-lagou/"/>
    <id>http://yoursite.com/2017/06/09/scrapy-crawlspider-lagou/</id>
    <published>2017-06-09T06:18:54.000Z</published>
    <updated>2017-06-09T08:31:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前的Spider都是默认根据basic的templates创建，现在我们要用crawl的方式创建Spider，以爬取拉勾网整站信息。</p>
<a id="more"></a>
<h2 id="CrawlSpider源码解析"><a href="#CrawlSpider源码解析" class="headerlink" title="CrawlSpider源码解析"></a><strong>CrawlSpider源码解析</strong></h2><p>Spider基本上能做很多事情了，但是如果你想爬取知乎或者是简书全站的话，你可能需要一个更强大的武器。<br>CrawlSpider基于Spider，但是可以说是为全站爬取而生。</p>
<h3 id="简要说明"><a href="#简要说明" class="headerlink" title="简要说明"></a><strong>简要说明</strong></h3><p>CrawlSpider是爬取那些具有一定规则网站的常用的爬虫，它基于Spider并有一些独特属性</p>
<ul>
<li>rules: 是<em>Rule</em>对象的集合，用于匹配目标网站并排除干扰</li>
<li>parse_start_url: 用于爬取起始响应，必须要返回<em>Item</em>，<em>Request</em>中的一个。</li>
</ul>
<p>因为rules是Rule对象的集合，所以这里也要介绍一下Rule。它有几个参数：link_extractor、callback=None、cb_kwargs=None、follow=None、process_links=None、process_request=None<br>其中的link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为：</p>
<ul>
<li>allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</li>
<li>deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。</li>
<li>allow_domains：会被提取的链接的domains。</li>
<li>deny_domains：一定不会被提取链接的domains。</li>
<li><strong>restrict_xpaths</strong>：使用<strong>xpath</strong>表达式，和<strong>allow</strong>共同作用过滤链接。还有一个类似的<strong>restrict_css</strong></li>
</ul>
<p>下面是官方提供的例子，我将从源代码的角度开始解读一些常见问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'example.com'</span></div><div class="line">    allowed_domains = [<span class="string">'example.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</div><div class="line"></div><div class="line">    rules = (</div><div class="line">        <span class="comment"># Extract links matching 'category.php' (but not matching 'subsection.php')</span></div><div class="line">        <span class="comment"># and follow links from them (since no callback means follow=True by default).</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</div><div class="line"></div><div class="line">        <span class="comment"># Extract links matching 'item.php' and parse them with the spider's method parse_item</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        self.logger.info(<span class="string">'Hi, this is an item page! %s'</span>, response.url)</div><div class="line">        item = scrapy.Item()</div><div class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</div><div class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h3 id="问题：CrawlSpider如何工作的？"><a href="#问题：CrawlSpider如何工作的？" class="headerlink" title="问题：CrawlSpider如何工作的？"></a><strong>问题：CrawlSpider如何工作的？</strong></h3><p><strong>因为CrawlSpider继承了Spider，所以具有Spider的所有函数。</strong><br>首先由 <code>start_requests</code> 对 <code>start_urls</code> 中的每一个url发起请求（ <code>make_requests_from_url</code> )，这个请求会被parse接收。在Spider里面的parse需要我们定义，但CrawlSpider定义 <code>parse</code> 去解析响应（ <code>self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)</code> ）<br><strong>_parse_response</strong>根据有无 <code>callback</code> ， <code>follow</code> 和 <code>self.follow_links</code> 执行不同的操作</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(<span class="keyword">self</span>, response, callback, cb_kwargs, follow=True)</span></span>:</div><div class="line"><span class="comment">##如果传入了callback，使用这个callback解析页面并获取解析得到的reques或item</span></div><div class="line">    <span class="keyword">if</span> <span class="symbol">callback:</span></div><div class="line">        cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</div><div class="line">        cb_res = <span class="keyword">self</span>.process_results(response, cb_res)</div><div class="line">        <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</div><div class="line">            <span class="keyword">yield</span> requests_or_item</div><div class="line"><span class="comment">## 其次判断有无follow，用_requests_to_follow解析响应是否有符合要求的link。</span></div><div class="line">    <span class="keyword">if</span> follow <span class="keyword">and</span> <span class="keyword">self</span>.<span class="symbol">_follow_links:</span></div><div class="line">        <span class="keyword">for</span> request_or_item <span class="keyword">in</span> <span class="keyword">self</span>._requests_to_follow(response):</div><div class="line">            <span class="keyword">yield</span> request_or_item</div></pre></td></tr></table></figure>
<p>其中<code>_requests_to_follow</code>又会获取<code>link_extractor</code>（这个是我们传入的LinkExtractor）解析页面得到的link<code>（link_extractor.extract_links(response)）</code>,对url进行加工（process_links，需要自定义），对符合的link发起Request。使用<code>.process_request</code>(需要自定义）处理响应。</p>
<h3 id="问题：CrawlSpider如何获取rules？"><a href="#问题：CrawlSpider如何获取rules？" class="headerlink" title="问题：CrawlSpider如何获取rules？"></a><strong>问题：CrawlSpider如何获取rules？</strong></h3><p>CrawlSpider类会在<code>__init__</code>方法中调用<code>_compile_rules</code>方法，然后在其中浅拷贝<code>rules</code>中的各个<code>Rule</code>获取要用于回调(callback)，要进行处理的链接（process_links）和要进行的处理请求（process_request)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></div><div class="line">        <span class="keyword">if</span> callable(method):</div><div class="line">            <span class="keyword">return</span> method</div><div class="line">        <span class="keyword">elif</span> isinstance(method, six.string_types):</div><div class="line">            <span class="keyword">return</span> getattr(self, method, <span class="keyword">None</span>)</div><div class="line"></div><div class="line">    self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</div><div class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</div><div class="line">        rule.callback = get_method(rule.callback)</div><div class="line">        rule.process_links = get_method(rule.process_links)</div><div class="line">        rule.process_request = get_method(rule.process_request)</div></pre></td></tr></table></figure>
<p>那么<code>Rule</code>是怎么样定义的呢？</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class Rule(object):</div><div class="line"></div><div class="line">    def __init__(<span class="built_in">self</span>, link_extractor, callback=<span class="literal">None</span>, cb_kwargs=<span class="literal">None</span>, follow=<span class="literal">None</span>, process_links=<span class="literal">None</span>, process_request=identity):</div><div class="line">        <span class="built_in">self</span>.link_extractor = link_extractor</div><div class="line">        <span class="built_in">self</span>.callback = callback</div><div class="line">        <span class="built_in">self</span>.cb_kwargs = cb_kwargs <span class="literal">or</span> &#123;&#125;</div><div class="line">        <span class="built_in">self</span>.process_links = process_links</div><div class="line">        <span class="built_in">self</span>.process_request = process_request</div><div class="line">        <span class="keyword">if</span> follow is <span class="literal">None</span>:</div><div class="line">            <span class="built_in">self</span>.follow = <span class="literal">False</span> <span class="keyword">if</span> callback <span class="keyword">else</span> <span class="literal">True</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="built_in">self</span>.follow = follow</div></pre></td></tr></table></figure>
<p>因此LinkExtractor会传给link_extractor。</p>
<h3 id="有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？"><a href="#有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？" class="headerlink" title="有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？"></a><strong>有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？</strong></h3><p>由上面的讲解可以发现<code>_parse_response</code>会处理有<code>callback</code>的（响应）respons。<br>cb_res = callback(response, **cb_kwargs) or ()<br>而<code>_requests_to_follow</code>会将<code>self._response_downloaded</code>传给<code>callback</code>用于对页面中匹配的url发起请求（request）。<br>r = Request(url=link.url, callback=self._response_downloaded) </p>
<h3 id="如何在CrawlSpider进行模拟登陆"><a href="#如何在CrawlSpider进行模拟登陆" class="headerlink" title="如何在CrawlSpider进行模拟登陆"></a><strong>如何在CrawlSpider进行模拟登陆</strong></h3><p>因为CrawlSpider和Spider一样，都要使用start_requests发起请求，用从<a href="http://www.jianshu.com/users/4ee453b72aff" target="_blank" rel="external">Andrew_liu</a>大神借鉴的代码说明如何模拟登陆：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">##替换原来的start_requests，callback为</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> [Request(<span class="string">"http://www.zhihu.com/#signin"</span>, meta = &#123;<span class="string">'cookiejar'</span> : <span class="number">1</span>&#125;, callback = self.post_login)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Preparing login'</span></div><div class="line">    <span class="comment">#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单</span></div><div class="line">    xsrf = Selector(response).xpath(<span class="string">'//input[@name="_xsrf"]/@value'</span>).extract()[<span class="number">0</span>]</div><div class="line">    <span class="keyword">print</span> xsrf</div><div class="line">    <span class="comment">#FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单</span></div><div class="line">    <span class="comment">#登陆成功后, 会调用after_login回调函数</span></div><div class="line">    <span class="keyword">return</span> [FormRequest.from_response(response,   <span class="comment">#"http://www.zhihu.com/login",</span></div><div class="line">                        meta = &#123;<span class="string">'cookiejar'</span> : response.meta[<span class="string">'cookiejar'</span>]&#125;,</div><div class="line">                        headers = self.headers,</div><div class="line">                        formdata = &#123;</div><div class="line">                        <span class="string">'_xsrf'</span>: xsrf,</div><div class="line">                        <span class="string">'email'</span>: <span class="string">'1527927373@qq.com'</span>,</div><div class="line">                        <span class="string">'password'</span>: <span class="string">'321324jia'</span></div><div class="line">                        &#125;,</div><div class="line">                        callback = self.after_login,</div><div class="line">                        dont_filter = <span class="keyword">True</span></div><div class="line">                        )]</div><div class="line"></div><div class="line"><span class="comment">#make_requests_from_url会调用parse，就可以与CrawlSpider的parse进行衔接了</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">after_login</span><span class="params">(self, response)</span> :</span></div><div class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls :</div><div class="line">        <span class="keyword">yield</span> self.make_requests_from_url(url)</div></pre></td></tr></table></figure>
<h3 id="源码及注释"><a href="#源码及注释" class="headerlink" title="源码及注释"></a><strong>源码及注释</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrawlSpider</span><span class="params">(Spider)</span>:</span></div><div class="line"></div><div class="line">    rules = ()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *a, **kw)</span>:</span></div><div class="line">        super(CrawlSpider, self).__init__(*a, **kw)</div><div class="line">        self._compile_rules()</div><div class="line">	</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        首先调用parse()来处理start_urls中返回的response对象  </div><div class="line">    	parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url()  </div><div class="line">    	设置了跟进标志位True  </div><div class="line">    	parse将返回item和跟进了的Request对象 </div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_start_url</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理start_url中返回的response，需要重写</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> []</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_results</span><span class="params">(self, response, results)</span>:</span></div><div class="line">        <span class="keyword">return</span> results</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回 </div><div class="line">        """</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</div><div class="line">            <span class="keyword">return</span></div><div class="line">        seen = set()</div><div class="line">        <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</div><div class="line">            <span class="comment"># 抽取之内的所有链接，只要通过任意一个'规则'，即表示合法 </span></div><div class="line">            links = [lnk <span class="keyword">for</span> lnk <span class="keyword">in</span> rule.link_extractor.extract_links(response)</div><div class="line">                     <span class="keyword">if</span> lnk <span class="keyword">not</span> <span class="keyword">in</span> seen] </div><div class="line">            <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</div><div class="line">                <span class="comment"># 使用用户指定的process_links处理每个连接 </span></div><div class="line">                links = rule.process_links(links)</div><div class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> links:</div><div class="line">                <span class="comment"># 将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded()  </span></div><div class="line">                seen.add(link)</div><div class="line">                <span class="comment"># 构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数  </span></div><div class="line">                r = Request(url=link.url, callback=self._response_downloaded)</div><div class="line">                r.meta.update(rule=n, link_text=link.text)</div><div class="line">                <span class="comment"># 对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request.</span></div><div class="line">                <span class="keyword">yield</span> rule.process_request(r)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_response_downloaded</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理通过rule提取出的连接，并返回item以及request</div><div class="line">        """</div><div class="line">        rule = self._rules[response.meta[<span class="string">'rule'</span>]]</div><div class="line">        <span class="keyword">return</span> self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(self, response, callback, cb_kwargs, follow=True)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        解析response对象，会用callback解析处理他，并返回request或Item对象  </div><div class="line">        首先判断是否设置了回调函数。（该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数）  </div><div class="line">        如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象，  </div><div class="line">        然后再交给process_results处理。返回cb_res的一个列表  </div><div class="line">        """</div><div class="line">        <span class="keyword">if</span> callback:</div><div class="line">            <span class="comment"># 如果是parse调用的，则会解析成Request对象  </span></div><div class="line">            <span class="comment"># 如果是rule callback，则会解析成Item </span></div><div class="line">            cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</div><div class="line">            cb_res = self.process_results(response, cb_res)</div><div class="line">            <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</div><div class="line">                <span class="keyword">yield</span> requests_or_item</div><div class="line"></div><div class="line">        <span class="keyword">if</span> follow <span class="keyword">and</span> self._follow_links:</div><div class="line">            <span class="comment">#如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象 </span></div><div class="line">            <span class="keyword">for</span> request_or_item <span class="keyword">in</span> self._requests_to_follow(response):</div><div class="line">                <span class="keyword">yield</span> request_or_item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></div><div class="line">            <span class="keyword">if</span> callable(method):</div><div class="line">                <span class="keyword">return</span> method</div><div class="line">            <span class="keyword">elif</span> isinstance(method, six.string_types):</div><div class="line">                <span class="keyword">return</span> getattr(self, method, <span class="keyword">None</span>)</div><div class="line"></div><div class="line">        self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</div><div class="line">        <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</div><div class="line">            rule.callback = get_method(rule.callback)</div><div class="line">            rule.process_links = get_method(rule.process_links)</div><div class="line">            rule.process_request = get_method(rule.process_request)</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, *args, **kwargs)</span>:</span></div><div class="line">        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)</div><div class="line">        spider._follow_links = crawler.settings.getbool(</div><div class="line">            <span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> spider</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_crawler</span><span class="params">(self, crawler)</span>:</span></div><div class="line">        super(CrawlSpider, self).set_crawler(crawler)</div><div class="line">        self._follow_links = crawler.settings.getbool(<span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<h2 id="创建CrawlSpider"><a href="#创建CrawlSpider" class="headerlink" title="创建CrawlSpider"></a><strong>创建CrawlSpider</strong></h2><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fgf0mz0pcuj31fu0vawon.jpg" alt=""></p>
<p>出现问题：<code>ImportError: No module named &#39;utils&#39;</code></p>
<p>原因：我们之前将项目目录下的 <code>ArticleSpider</code> 文件夹Mark为 <code>Sources Root</code> 导致。</p>
<p>解决办法：自己在 <code>Settings.py</code> 中设置搜索路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置搜索路径</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line">BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))</div><div class="line">sys.path.insert(<span class="number">0</span>, os.path.join(BASE_DIR, <span class="string">'ArticleSpider'</span>))</div></pre></td></tr></table></figure>
<p>设置之后重新创建spider</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy genspider -t crawl lagou www.lagou.com</div></pre></td></tr></table></figure>
<h2 id="数据表结构及items设计"><a href="#数据表结构及items设计" class="headerlink" title="数据表结构及items设计"></a><strong>数据表结构及items设计</strong></h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fgf10fvkqvj31400qe79r.jpg" alt=""></p>
<h2 id="完整代码逻辑"><a href="#完整代码逻辑" class="headerlink" title="完整代码逻辑"></a><strong>完整代码逻辑</strong></h2><h3 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a><strong>items.py</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_splash</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    去除工作城市的斜杠</div><div class="line">    """</div><div class="line">    <span class="keyword">return</span> value.replace(<span class="string">"/"</span>, <span class="string">""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_jobaddr</span><span class="params">(value)</span>:</span></div><div class="line">    addr_list = value.split(<span class="string">"\n"</span>)</div><div class="line">    addr_list = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> addr_list <span class="keyword">if</span> item.strip() != <span class="string">"查看地图"</span>]</div><div class="line">    <span class="keyword">return</span> <span class="string">""</span>.join(addr_list)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItemLoader</span><span class="params">(ItemLoader)</span>:</span></div><div class="line">    default_output_processor = TakeFirst()</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouJobItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    拉勾网职位信息</div><div class="line">    """</div><div class="line">    title = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    salary = scrapy.Field()</div><div class="line">    job_city = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_splash),</div><div class="line">    )</div><div class="line">    work_years = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_splash),</div><div class="line">    )</div><div class="line">    degree_need = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_splash),</div><div class="line">    )</div><div class="line">    job_type = scrapy.Field()</div><div class="line">    publish_time = scrapy.Field()</div><div class="line">    job_advantage = scrapy.Field()</div><div class="line">    job_desc = scrapy.Field()</div><div class="line">    job_addr = scrapy.Field(</div><div class="line">        input_processor=MapCompose(remove_tags, handle_jobaddr),</div><div class="line">    )</div><div class="line">    company_name = scrapy.Field()</div><div class="line">    company_url = scrapy.Field()</div><div class="line">    tags = scrapy.Field(</div><div class="line">        input_processor=Join(<span class="string">","</span>)</div><div class="line">    )</div><div class="line">    crawl_time = scrapy.Field()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">            insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need,</div><div class="line">            job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url,</div><div class="line">            tags, crawl_time)</div><div class="line">            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</div><div class="line">            ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc)</div><div class="line">        """</div><div class="line"></div><div class="line">        params = (</div><div class="line">            self[<span class="string">"title"</span>], self[<span class="string">"url"</span>], self[<span class="string">"url_object_id"</span>], self[<span class="string">"salary"</span>], self[<span class="string">"job_city"</span>],</div><div class="line">            self[<span class="string">"work_years"</span>], self[<span class="string">"degree_need"</span>], self[<span class="string">"job_type"</span>],</div><div class="line">            self[<span class="string">"publish_time"</span>], self[<span class="string">"job_advantage"</span>], self[<span class="string">"job_desc"</span>],</div><div class="line">            self[<span class="string">"job_addr"</span>], self[<span class="string">"company_name"</span>], self[<span class="string">"company_url"</span>],</div><div class="line">            self[<span class="string">"tags"</span>], self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="keyword">return</span> insert_sql, params</div></pre></td></tr></table></figure>
<h3 id="lagou-py"><a href="#lagou-py" class="headerlink" title="lagou.py"></a><strong>lagou.py</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"></div><div class="line"><span class="keyword">from</span> items <span class="keyword">import</span> LagouJobItemLoader, LagouJobItem</div><div class="line"><span class="keyword">from</span> utils.common <span class="keyword">import</span> get_md5</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LagouSpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'lagou'</span></div><div class="line">    allowed_domains = [<span class="string">'www.lagou.com'</span>]</div><div class="line">    start_urls = [<span class="string">'https://www.lagou.com/'</span>]</div><div class="line"></div><div class="line">    rules = (</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'zhaopin/.*'</span>,)), ),</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'gongsi/j\d+.html'</span>,)), ),</div><div class="line">        Rule(LinkExtractor(allow=<span class="string">r'jobs/\d+.html'</span>), callback=<span class="string">'parse_job'</span>, follow=<span class="keyword">True</span>),</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_job</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        解析拉勾网的职位</div><div class="line">        """</div><div class="line">        item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response)</div><div class="line">        item_loader.add_css(<span class="string">"title"</span>, <span class="string">".job-name::attr(title)"</span>)</div><div class="line">        item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">        item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</div><div class="line">        item_loader.add_css(<span class="string">"salary"</span>, <span class="string">".job_request .salary::text"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"job_city"</span>, <span class="string">"//*[@class='job_request']/p/span[2]/text()"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"work_years"</span>, <span class="string">"//*[@class='job_request']/p/span[3]/text()"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"degree_need"</span>, <span class="string">"//*[@class='job_request']/p/span[4]/text()"</span>)</div><div class="line">        item_loader.add_xpath(<span class="string">"job_type"</span>, <span class="string">"//*[@class='job_request']/p/span[5]/text()"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"publish_time"</span>, <span class="string">".publish_time::text"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"job_advantage"</span>, <span class="string">".job-advantage p::text"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"job_desc"</span>, <span class="string">".job_bt div"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"job_addr"</span>, <span class="string">".work_addr"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"company_name"</span>, <span class="string">"#job_company dt a img::attr(alt)"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"company_url"</span>, <span class="string">"#job_company dt a::attr(href)"</span>)</div><div class="line">        item_loader.add_css(<span class="string">"tags"</span>, <span class="string">".position-label li::text"</span>)</div><div class="line">        item_loader.add_value(<span class="string">"crawl_time"</span>, datetime.now())</div><div class="line"></div><div class="line">        job_item = item_loader.load_item()</div><div class="line"></div><div class="line">        <span class="keyword">return</span> job_item</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前的Spider都是默认根据basic的templates创建，现在我们要用crawl的方式创建Spider，以爬取拉勾网整站信息。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Requests，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRequests%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取知乎</title>
    <link href="http://yoursite.com/2017/06/07/scrapy-zhihu/"/>
    <id>http://yoursite.com/2017/06/07/scrapy-zhihu/</id>
    <published>2017-06-07T06:18:54.000Z</published>
    <updated>2017-06-07T10:41:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>在完成了Scrapy模拟登录知乎后，下一步要进行的就是进行对知乎页面，问题以及答案等内容的爬取工作了。</p>
<a id="more"></a>
<h2 id="通过Scrapy-Shell进行调试"><a href="#通过Scrapy-Shell进行调试" class="headerlink" title="通过Scrapy Shell进行调试"></a><strong>通过Scrapy Shell进行调试</strong></h2><p>在使用shell调试时，直接通过 <code>scrapy shell https://www.zhihu.com/question/58765535</code> 会出现error 500错误。这是因为没有加headers的原因。<br>正确添加headers的方法是：<code>scrapy -s USER_AGENT=&quot;任意的User Agent&quot;</code> 。此时，就可以在shell中进行分析了。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fgct2bwtn7j30vo0n8gsb.jpg" alt=""></p>
<h2 id="获得要分析的链接"><a href="#获得要分析的链接" class="headerlink" title="获得要分析的链接"></a>获得要分析的链接</h2><p>在登录完成进入首页之后，通过深度优先算法获得首页需要的链接，然后打开这些链接再次获得里面的链接，不断重复，获得所有内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="comment"># 因为没有具体的入口，采用深度优先的算法</span></div><div class="line">        all_urls = response.css(<span class="string">"a::attr(href)"</span>).extract()</div><div class="line">        all_urls = [parse.urljoin(response.url, url) <span class="keyword">for</span> url <span class="keyword">in</span> all_urls]</div><div class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> all_urls:</div><div class="line">            <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>在分析页面内容之后，设计我们所需的数据表zhihu_question和zhihu_answer</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fgctdm536rj317a0k4jvj.jpg" alt=""></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fgctdu7ub8j317k0kwaek.jpg" alt=""></p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a><strong>完整代码</strong></h2><ul>
<li>zhihu.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</div><div class="line"><span class="keyword">from</span> items <span class="keyword">import</span> ZhihuAnswerItem, ZhihuQuestionItem</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">import</span> urlparse <span class="keyword">as</span> parse</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    <span class="keyword">from</span> urllib <span class="keyword">import</span> parse</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"zhihu"</span></div><div class="line">    allowed_domains = [<span class="string">"www.zhihu.com"</span>]</div><div class="line">    start_urls = [<span class="string">'https://www.zhihu.com/'</span>]</div><div class="line"></div><div class="line">    <span class="comment"># question的第一页answer的请求url</span></div><div class="line">    start_answer_url = <span class="string">"https://www.zhihu.com/api/v4/questions/&#123;0&#125;/answers?sort_by=default&amp;include=data%5B%2A%5D.is_normal%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccollapsed_counts%2Creviewing_comments_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Crelationship.is_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.is_blocking%2Cis_blocked%2Cis_followed%2Cvoteup_count%2Cmessage_thread_token%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=&#123;1&#125;&amp;offset=&#123;2&#125;"</span></div><div class="line"></div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">"Host"</span>: <span class="string">"www.zhihu.com"</span>,</div><div class="line">        <span class="string">"Referer"</span>: <span class="string">"https://www.zhihu.com/"</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4"</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        提取出html页面中的所有url 并跟踪这些url进行一步爬取</div><div class="line">        如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数</div><div class="line">        """</div><div class="line">        all_urls = response.css(<span class="string">"a::attr(href)"</span>).extract()</div><div class="line">        all_urls = [parse.urljoin(response.url, url) <span class="keyword">for</span> url <span class="keyword">in</span> all_urls]</div><div class="line">        all_urls = filter(<span class="keyword">lambda</span> x: <span class="keyword">True</span> <span class="keyword">if</span> x.startswith(<span class="string">"https"</span>) <span class="keyword">else</span> <span class="keyword">False</span>, all_urls)</div><div class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> all_urls:</div><div class="line">            match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, url)</div><div class="line">            <span class="keyword">if</span> match_obj:</div><div class="line">                <span class="comment"># 如果提取到question相关的页面则下载后交由提取函数进行提取</span></div><div class="line">                request_url = match_obj.group(<span class="number">1</span>)</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(request_url, headers=self.headers, callback=self.parse_question)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="comment"># 如果不是question页面则直接进一步跟踪</span></div><div class="line">                <span class="keyword">yield</span> scrapy.Request(url, headers=self.headers, callback=self.parse)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_question</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理question页面， 从页面中提取出具体的question item</div><div class="line">        """</div><div class="line">        <span class="comment"># 处理question页面， 从页面中提取出具体的question item</span></div><div class="line">        <span class="keyword">if</span> <span class="string">"QuestionHeader-title"</span> <span class="keyword">in</span> response.text:</div><div class="line">            <span class="comment"># 处理新版本</span></div><div class="line">            match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, response.url)</div><div class="line">            <span class="keyword">if</span> match_obj:</div><div class="line">                question_id = int(match_obj.group(<span class="number">2</span>))</div><div class="line"></div><div class="line">            item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response)</div><div class="line">            item_loader.add_css(<span class="string">"title"</span>, <span class="string">"h1.QuestionHeader-title::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"content"</span>, <span class="string">".QuestionHeader-detail"</span>)</div><div class="line">            item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">            item_loader.add_value(<span class="string">"zhihu_id"</span>, question_id)</div><div class="line">            item_loader.add_css(<span class="string">"answer_num"</span>, <span class="string">".List-headerText span::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"comments_num"</span>, <span class="string">".QuestionHeader-Comment button::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"watch_user_num"</span>, <span class="string">".NumberBoard-value::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"topics"</span>, <span class="string">".QuestionHeader-topics .Popover div::text"</span>)</div><div class="line"></div><div class="line">            question_item = item_loader.load_item()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># 处理老版本页面的item提取</span></div><div class="line">            match_obj = re.match(<span class="string">"(.*zhihu.com/question/(\d+))(/|$).*"</span>, response.url)</div><div class="line">            <span class="keyword">if</span> match_obj:</div><div class="line">                question_id = int(match_obj.group(<span class="number">2</span>))</div><div class="line"></div><div class="line">            item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response)</div><div class="line">            <span class="comment"># item_loader.add_css("title", ".zh-question-title h2 a::text")</span></div><div class="line">            item_loader.add_xpath(<span class="string">"title"</span>,</div><div class="line">                                  <span class="string">"//*[@id='zh-question-title']/h2/a/text()|//*[@id='zh-question-title']/h2/span/text()"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"content"</span>, <span class="string">"#zh-question-detail"</span>)</div><div class="line">            item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">            item_loader.add_value(<span class="string">"zhihu_id"</span>, question_id)</div><div class="line">            item_loader.add_css(<span class="string">"answer_num"</span>, <span class="string">"#zh-question-answer-num::text"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"comments_num"</span>, <span class="string">"#zh-question-meta-wrap a[name='addcomment']::text"</span>)</div><div class="line">            <span class="comment"># item_loader.add_css("watch_user_num", "#zh-question-side-header-wrap::text")</span></div><div class="line">            item_loader.add_xpath(<span class="string">"watch_user_num"</span>,</div><div class="line">                                  <span class="string">"//*[@id='zh-question-side-header-wrap']/text()|//*[@class='zh-question-followers-sidebar']/div/a/strong/text()"</span>)</div><div class="line">            item_loader.add_css(<span class="string">"topics"</span>, <span class="string">".zm-tag-editor-labels a::text"</span>)</div><div class="line"></div><div class="line">            question_item = item_loader.load_item()</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> scrapy.Request(self.start_answer_url.format(question_id, <span class="number">20</span>, <span class="number">0</span>), headers=self.headers, callback=self.parse_answer)</div><div class="line">        <span class="keyword">yield</span> question_item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_answer</span><span class="params">(self, reponse)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        处理question的answer</div><div class="line">        """</div><div class="line">        ans_json = json.loads(reponse.text)</div><div class="line">        is_end = ans_json[<span class="string">"paging"</span>][<span class="string">"is_end"</span>]</div><div class="line">        next_url = ans_json[<span class="string">"paging"</span>][<span class="string">"next"</span>]</div><div class="line"></div><div class="line">        <span class="comment"># 提取answer的具体字段</span></div><div class="line">        <span class="keyword">for</span> answer <span class="keyword">in</span> ans_json[<span class="string">"data"</span>]:</div><div class="line">            answer_item = ZhihuAnswerItem()</div><div class="line">            answer_item[<span class="string">"zhihu_id"</span>] = answer[<span class="string">"id"</span>]</div><div class="line">            answer_item[<span class="string">"url"</span>] = answer[<span class="string">"url"</span>]</div><div class="line">            answer_item[<span class="string">"question_id"</span>] = answer[<span class="string">"question"</span>][<span class="string">"id"</span>]</div><div class="line">            answer_item[<span class="string">"author_id"</span>] = answer[<span class="string">"author"</span>][<span class="string">"id"</span>] <span class="keyword">if</span> <span class="string">"id"</span> <span class="keyword">in</span> answer[<span class="string">"author"</span>] <span class="keyword">else</span> <span class="keyword">None</span></div><div class="line">            answer_item[<span class="string">"content"</span>] = answer[<span class="string">"content"</span>] <span class="keyword">if</span> <span class="string">"content"</span> <span class="keyword">in</span> answer <span class="keyword">else</span> <span class="keyword">None</span></div><div class="line">            answer_item[<span class="string">"praise_num"</span>] = answer[<span class="string">"voteup_count"</span>]</div><div class="line">            answer_item[<span class="string">"comments_num"</span>] = answer[<span class="string">"comment_count"</span>]</div><div class="line">            answer_item[<span class="string">"create_time"</span>] = answer[<span class="string">"created_time"</span>]</div><div class="line">            answer_item[<span class="string">"update_time"</span>] = answer[<span class="string">"updated_time"</span>]</div><div class="line">            answer_item[<span class="string">"crawl_time"</span>] = datetime.datetime.now()</div><div class="line"></div><div class="line">            <span class="keyword">yield</span> answer_item</div><div class="line"></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_end:</div><div class="line">            <span class="keyword">yield</span> scrapy.Request(next_url, headers=self.headers, callback=self.parse_answer)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> [scrapy.Request(<span class="string">'https://www.zhihu.com/#signin'</span>, headers=self.headers, callback=self.login)]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></div><div class="line">        response_text = response.text</div><div class="line">        match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response_text, re.DOTALL)</div><div class="line">        xsrf = <span class="string">''</span></div><div class="line">        <span class="keyword">if</span> match_obj:</div><div class="line">            xsrf = (match_obj.group(<span class="number">1</span>))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> xsrf:</div><div class="line">            post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">            post_data = &#123;</div><div class="line">                <span class="string">"_xsrf"</span>: xsrf,</div><div class="line">                <span class="string">"phone_num"</span>: <span class="string">"18251556927"</span>,</div><div class="line">                <span class="string">"password"</span>: <span class="string">"lawtech0301520"</span>,</div><div class="line">                <span class="string">"captcha"</span>: <span class="string">""</span></div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">import</span> time</div><div class="line">            t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">            captcha_url = <span class="string">"https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login"</span>.format(t)</div><div class="line">            <span class="keyword">yield</span> scrapy.Request(captcha_url, headers=self.headers, meta=&#123;<span class="string">"post_data"</span>: post_data&#125;,</div><div class="line">                                 callback=self.login_after_captcha)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login_after_captcha</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">with</span> open(<span class="string">"captcha.jpg"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(response.body)</div><div class="line">            f.close()</div><div class="line"></div><div class="line">        <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">            im.show()</div><div class="line">            im.close()</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line">        captcha = input(<span class="string">"输入验证码\n&gt;"</span>)</div><div class="line"></div><div class="line">        post_data = response.meta.get(<span class="string">"post_data"</span>, &#123;&#125;)</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        post_data[<span class="string">"captcha"</span>] = captcha</div><div class="line">        <span class="keyword">return</span> [scrapy.FormRequest(</div><div class="line">            url=post_url,</div><div class="line">            formdata=post_data,</div><div class="line">            headers=self.headers,</div><div class="line">            callback=self.check_login</div><div class="line">        )]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        验证服务器的返回数据判断是否成功</div><div class="line">        """</div><div class="line">        text_json = json.loads(response.text)</div><div class="line">        <span class="keyword">if</span> <span class="string">"msg"</span> <span class="keyword">in</span> text_json <span class="keyword">and</span> text_json[<span class="string">"msg"</span>] == <span class="string">"登录成功"</span>:</div><div class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(url, dont_filter=<span class="keyword">True</span>, headers=self.headers)</div></pre></td></tr></table></figure>
<p>为了用同一个Pipeline处理所有的数据库存储操作，因此将操作都放入items中，再有Pipeline进行统一处理。</p>
<ul>
<li>items.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuQuestionItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎问题Item</div><div class="line">    """</div><div class="line">    zhihu_id = scrapy.Field()</div><div class="line">    topics = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    title = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div><div class="line">    answer_num = scrapy.Field()</div><div class="line">    comments_num = scrapy.Field()</div><div class="line">    watch_user_num = scrapy.Field()</div><div class="line">    click_num = scrapy.Field()</div><div class="line">    crawl_time = scrapy.Field()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 插入知乎question表的sql语句</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                    insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num,</div><div class="line">                      watch_user_num, click_num, crawl_time</div><div class="line">                      )</div><div class="line">                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) </div><div class="line">                    ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num),</div><div class="line">              watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num)</div><div class="line">                """</div><div class="line"></div><div class="line">        zhihu_id = self[<span class="string">"zhihu_id"</span>][<span class="number">0</span>]</div><div class="line">        topics = <span class="string">","</span>.join(self[<span class="string">"topics"</span>])</div><div class="line">        url = self[<span class="string">"url"</span>][<span class="number">0</span>]</div><div class="line">        title = <span class="string">""</span>.join(self[<span class="string">"title"</span>])</div><div class="line">        content = <span class="string">""</span>.join(self[<span class="string">"content"</span>])</div><div class="line">        answer_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"answer_num"</span>]))</div><div class="line">        comments_num = extract_num(<span class="string">""</span>.join(self[<span class="string">"comments_num"</span>]))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> len(self[<span class="string">"watch_user_num"</span>]) == <span class="number">2</span>:</div><div class="line">            watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</div><div class="line">            click_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">1</span>])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            watch_user_num = int(self[<span class="string">"watch_user_num"</span>][<span class="number">0</span>])</div><div class="line">            click_num = <span class="number">0</span></div><div class="line"></div><div class="line">        crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT)</div><div class="line"></div><div class="line">        params = (zhihu_id, topics, url, title, content, answer_num, comments_num,</div><div class="line">                  watch_user_num, click_num, crawl_time)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> insert_sql, params</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuAnswerItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎回答Item</div><div class="line">    """</div><div class="line">    zhihu_id = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    question_id = scrapy.Field()</div><div class="line">    author_id = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div><div class="line">    praise_num = scrapy.Field()</div><div class="line">    comments_num = scrapy.Field()</div><div class="line">    create_time = scrapy.Field()</div><div class="line">    update_time = scrapy.Field()</div><div class="line">    crawl_time = scrapy.Field()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_insert_sql</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 插入知乎question表的sql语句</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                    insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, praise_num, comments_num,</div><div class="line">              create_time, update_time, crawl_time</div><div class="line">              ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</div><div class="line">              ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), praise_num=VALUES(praise_num),</div><div class="line">              update_time=VALUES(update_time)</div><div class="line">                """</div><div class="line"></div><div class="line">        create_time = datetime.datetime.fromtimestamp(self[<span class="string">'create_time'</span>]).strftime(SQL_DATETIME_FORMAT)</div><div class="line">        update_time = datetime.datetime.fromtimestamp(self[<span class="string">'update_time'</span>]).strftime(SQL_DATETIME_FORMAT)</div><div class="line"></div><div class="line">        params = (</div><div class="line">            self[<span class="string">"zhihu_id"</span>], self[<span class="string">"url"</span>], self[<span class="string">"question_id"</span>],</div><div class="line">            self[<span class="string">"author_id"</span>], self[<span class="string">"content"</span>], self[<span class="string">"praise_num"</span>],</div><div class="line">            self[<span class="string">"comments_num"</span>], create_time, update_time,</div><div class="line">            self[<span class="string">"crawl_time"</span>].strftime(SQL_DATETIME_FORMAT),</div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="keyword">return</span> insert_sql, params</div></pre></td></tr></table></figure>
<ul>
<li>Pipelines.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 采用异步的机制写入mysql</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></div><div class="line">        self.dbpool = dbpool</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        dbparms = dict(</div><div class="line">            host=settings[<span class="string">"MYSQL_HOST"</span>],</div><div class="line">            db=settings[<span class="string">"MYSQL_DBNAME"</span>],</div><div class="line">            user=settings[<span class="string">"MYSQL_USER"</span>],</div><div class="line">            passwd=settings[<span class="string">"MYSQL_PASSWORD"</span>],</div><div class="line">            charset=<span class="string">'utf8'</span>,</div><div class="line">            cursorclass=MySQLdb.cursors.DictCursor,</div><div class="line">            use_unicode=<span class="keyword">True</span>,</div><div class="line">        )</div><div class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparms)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> cls(dbpool)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="comment"># 使用twisted将mysql插入变成异步执行</span></div><div class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</div><div class="line">        query.addErrback(self.handle_error, item, spider)  <span class="comment"># 处理异常</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure, item, spider)</span>:</span></div><div class="line">        <span class="comment"># 处理异步插入的异常</span></div><div class="line">        print(failure)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></div><div class="line">        <span class="comment"># 执行具体的插入</span></div><div class="line">        <span class="comment"># 根据不同的item 构建不同的sql语句并插入到mysql中</span></div><div class="line">        insert_sql, params = item.get_insert_sql()</div><div class="line">        cursor.execute(insert_sql, params)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在完成了Scrapy模拟登录知乎后，下一步要进行的就是进行对知乎页面，问题以及答案等内容的爬取工作了。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Requests，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRequests%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Requests模拟登录知乎</title>
    <link href="http://yoursite.com/2017/05/11/scrapy-requests-zhihu-login/"/>
    <id>http://yoursite.com/2017/05/11/scrapy-requests-zhihu-login/</id>
    <published>2017-05-11T06:18:54.000Z</published>
    <updated>2017-05-12T06:55:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="external">Requests</a> 是以 <a href="http://www.python.org/dev/peps/pep-0020" target="_blank" rel="external">PEP 20</a> 的箴言为中心开发的</p>
<ol>
<li>Beautiful is better than ugly.(美丽优于丑陋)</li>
<li>Explicit is better than implicit.(直白优于含蓄)</li>
<li>Simple is better than complex.(简单优于复杂)</li>
<li>Complex is better than complicated.(复杂优于繁琐)</li>
<li>Readability counts.(可读性很重要)</li>
</ol>
<a id="more"></a>
<h2 id="常见状态码"><a href="#常见状态码" class="headerlink" title="常见状态码"></a><strong>常见状态码</strong></h2><table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>200</td>
<td>请求被正确执行</td>
</tr>
<tr>
<td>301/302</td>
<td>永久性重定向/临时性重定向</td>
</tr>
<tr>
<td>403</td>
<td>没有权限访问</td>
</tr>
<tr>
<td>404</td>
<td>没有资源访问</td>
</tr>
<tr>
<td>500</td>
<td>服务器错误</td>
</tr>
<tr>
<td>503</td>
<td>服务器停机或正在维护</td>
</tr>
</tbody>
</table>
<h2 id="登录分析"><a href="#登录分析" class="headerlink" title="登录分析"></a><strong>登录分析</strong></h2><p><img src="http://ww2.sinaimg.cn/large/006tNbRwgy1ffikxe4s70j30lo05sjsg.jpg" alt="">在登录界面，输入手机号和密码，返回的地址为 <code>Request URL:https://www.zhihu.com/login/phone_num</code><br>当输入email地址后返回的地址为 <code>Request URL:https://www.zhihu.com/login/email</code><br>并且在formdata中出现 <code>_xsrf:a71f46d549979fa192c09e11e4a463b5</code> 这样的字符串。</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNbRwgy1ffiky17f7cj30uk05mabd.jpg" alt=""></p>
<h2 id="抓取xsrf的值"><a href="#抓取xsrf的值" class="headerlink" title="抓取xsrf的值"></a><strong>抓取xsrf的值</strong></h2><p>正则匹配抓取xsrf需要使用header头来进行源代码的获取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_xsrf</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    获取xsrf code</div><div class="line">    :return: xsrf code</div><div class="line">    """</div><div class="line">    response = requests.get(<span class="string">"https://www.zhihu.com"</span>, headers=headers)</div><div class="line">    match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response.text)</div><div class="line">    <span class="keyword">if</span> match_obj:</div><div class="line">        print(match_obj.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">""</span></div></pre></td></tr></table></figure>
<h2 id="验证码获取"><a href="#验证码获取" class="headerlink" title="验证码获取"></a><strong>验证码获取</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_captcha</span><span class="params">()</span>:</span></div><div class="line">    t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">    captcha_url = <span class="string">'https://www.zhihu.com/captcha.gif?r='</span> + t + <span class="string">"&amp;type=login"</span></div><div class="line">    r = session.get(captcha_url, headers=headers)</div><div class="line">    <span class="keyword">with</span> open(<span class="string">'captcha.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(r.content)</div><div class="line">        f.close()</div><div class="line">    im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">    im.show()</div><div class="line">    im.close()</div><div class="line">    captcha = input(<span class="string">"请输入验证码：\n"</span>)</div><div class="line">    <span class="keyword">return</span> captcha</div></pre></td></tr></table></figure>
<h2 id="登录逻辑"><a href="#登录逻辑" class="headerlink" title="登录逻辑"></a><strong>登录逻辑</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zhihu_login</span><span class="params">(account, password)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎登录</div><div class="line">    :param account: </div><div class="line">    :param password: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> re.match(<span class="string">"^1\d&#123;10&#125;$"</span>, account):</div><div class="line">        print(<span class="string">"手机号码登录 \n"</span>)</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">"_xsrf"</span>: get_xsrf(),</div><div class="line">            <span class="string">"phone_num"</span>: account,</div><div class="line">            <span class="string">"password"</span>: password</div><div class="line">        &#125;</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">if</span> <span class="string">"@"</span> <span class="keyword">in</span> account:</div><div class="line">            print(<span class="string">"邮箱登录 \n"</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">"你的账号输入有问题，请重新登录"</span>)</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        post_url = <span class="string">'https://www.zhihu.com/login/email'</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">'_xsrf'</span>: get_xsrf(),</div><div class="line">            <span class="string">'password'</span>: password,</div><div class="line">            <span class="string">'email'</span>: account</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="comment"># 不需要验证码直接登录成功</span></div><div class="line">    login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">    login_code = login_page.json()</div><div class="line">    <span class="keyword">if</span> login_code[<span class="string">'r'</span>] == <span class="number">1</span>:</div><div class="line">        <span class="comment"># 不输入验证码登录失败</span></div><div class="line">        <span class="comment"># 使用需要输入验证码的方式登录</span></div><div class="line">        post_data[<span class="string">"captcha"</span>] = get_captcha()</div><div class="line">        login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">        login_code = login_page.json()</div><div class="line">        print(login_code[<span class="string">'msg'</span>])</div><div class="line">    <span class="comment"># 保存 cookies 到文件，</span></div><div class="line">    <span class="comment"># 下次可以使用 cookie 直接登录，不需要输入账号和密码</span></div><div class="line">    session.cookies.save()</div></pre></td></tr></table></figure>
<p>以上代码是通过引入requests库，使用它的session方法，进行连接，构造post_data，把自己的用户名密码等信息发送到网站，并通过正则判断发送的是邮箱或是手机进行登录。<br>引入<code>import http.cookiejar as cookielib</code>，通过session.cookies.save()，对cookie进行保存。</p>
<h2 id="通过Cookie登录"><a href="#通过Cookie登录" class="headerlink" title="通过Cookie登录"></a><strong>通过Cookie登录</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 使用登录cookie信息</span></div><div class="line">session = requests.session()</div><div class="line">session.cookies = cookielib.LWPCookieJar(filename=<span class="string">"cookies.txt"</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    session.cookies.load(ignore_discard=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"Cookie未能加载"</span>)</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_login</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    通过查看用户个人信息来判断是否已经登录</div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    url = <span class="string">"https://www.zhihu.com/settings/profile"</span></div><div class="line">    response = session.get(url, headers=headers, allow_redirects=<span class="keyword">False</span>)</div><div class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure>
<p>登录只能一次，如果再次登录，可以直接通过查看cookie来判断是否为登录状态。</p>
<p>首先把cookie通过session.cookies.load装载进来，执行is_login()函数，如果成功可以访问inbox_url页面，则状态码为200表示成功。这里一定要注意allow_redirects=False，当不允许且登录时会自动跳转到登录页面，则状态码是301或者302。</p>
<h2 id="完整代码示例"><a href="#完整代码示例" class="headerlink" title="完整代码示例"></a><strong>完整代码示例</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"><span class="string">"""</span></div><div class="line">__author__ = 'lawtech'</div><div class="line">__date__ = '2017/5/9 下午3:18'</div><div class="line">"""</div><div class="line"></div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">import</span> cookielib</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    <span class="keyword">import</span> http.cookiejar <span class="keyword">as</span> cookielib</div><div class="line"></div><div class="line"><span class="comment"># 构造requests headers</span></div><div class="line">agent = <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36'</span></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">"Host"</span>: <span class="string">"www.zhihu.com"</span>,</div><div class="line">    <span class="string">"Referer"</span>: <span class="string">"https://www.zhihu.com/"</span>,</div><div class="line">    <span class="string">'User-Agent'</span>: agent</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># 使用登录cookie信息</span></div><div class="line">session = requests.session()</div><div class="line">session.cookies = cookielib.LWPCookieJar(filename=<span class="string">"cookies.txt"</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    session.cookies.load(ignore_discard=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">except</span>:</div><div class="line">    print(<span class="string">"Cookie未能加载"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_xsrf</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    获取xsrf code</div><div class="line">    :return: xsrf code</div><div class="line">    """</div><div class="line">    response = requests.get(<span class="string">"https://www.zhihu.com"</span>, headers=headers)</div><div class="line">    match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response.text)</div><div class="line">    <span class="keyword">if</span> match_obj:</div><div class="line">        print(match_obj.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="string">""</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_captcha</span><span class="params">()</span>:</span></div><div class="line">    t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">    captcha_url = <span class="string">'https://www.zhihu.com/captcha.gif?r='</span> + t + <span class="string">"&amp;type=login"</span></div><div class="line">    r = session.get(captcha_url, headers=headers)</div><div class="line">    <span class="keyword">with</span> open(<span class="string">'captcha.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">        f.write(r.content)</div><div class="line">        f.close()</div><div class="line">    im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">    im.show()</div><div class="line">    im.close()</div><div class="line">    captcha = input(<span class="string">"请输入验证码：\n"</span>)</div><div class="line">    <span class="keyword">return</span> captcha</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zhihu_login</span><span class="params">(account, password)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    知乎登录</div><div class="line">    :param account: </div><div class="line">    :param password: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> re.match(<span class="string">"^1\d&#123;10&#125;$"</span>, account):</div><div class="line">        print(<span class="string">"手机号码登录 \n"</span>)</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">"_xsrf"</span>: get_xsrf(),</div><div class="line">            <span class="string">"phone_num"</span>: account,</div><div class="line">            <span class="string">"password"</span>: password</div><div class="line">        &#125;</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">if</span> <span class="string">"@"</span> <span class="keyword">in</span> account:</div><div class="line">            print(<span class="string">"邮箱登录 \n"</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">"你的账号输入有问题，请重新登录"</span>)</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        post_url = <span class="string">'https://www.zhihu.com/login/email'</span></div><div class="line">        post_data = &#123;</div><div class="line">            <span class="string">'_xsrf'</span>: get_xsrf(),</div><div class="line">            <span class="string">'password'</span>: password,</div><div class="line">            <span class="string">'email'</span>: account</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="comment"># 不需要验证码直接登录成功</span></div><div class="line">    login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">    login_code = login_page.json()</div><div class="line">    <span class="keyword">if</span> login_code[<span class="string">'r'</span>] == <span class="number">1</span>:</div><div class="line">        <span class="comment"># 不输入验证码登录失败</span></div><div class="line">        <span class="comment"># 使用需要输入验证码的方式登录</span></div><div class="line">        post_data[<span class="string">"captcha"</span>] = get_captcha()</div><div class="line">        login_page = session.post(post_url, post_data, headers=headers)</div><div class="line">        login_code = login_page.json()</div><div class="line">        print(login_code[<span class="string">'msg'</span>])</div><div class="line">    <span class="comment"># 保存 cookies 到文件，</span></div><div class="line">    <span class="comment"># 下次可以使用 cookie 直接登录，不需要输入账号和密码</span></div><div class="line">    session.cookies.save()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_login</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    通过查看用户个人信息来判断是否已经登录</div><div class="line">    :return: </div><div class="line">    """</div><div class="line">    url = <span class="string">"https://www.zhihu.com/settings/profile"</span></div><div class="line">    response = session.get(url, headers=headers, allow_redirects=<span class="keyword">False</span>)</div><div class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">if</span> is_login():</div><div class="line">        print(<span class="string">"您已经登录！"</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        account = input(<span class="string">"请输入用户名：\n"</span>)</div><div class="line">        password = input(<span class="string">"请输入密码：\n"</span>)</div><div class="line">        zhihu_login(account, password)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://docs.python-requests.org/zh_CN/latest/index.html&quot;&gt;Requests&lt;/a&gt; 是以 &lt;a href=&quot;http://www.python.org/dev/peps/pep-0020&quot;&gt;PEP 20&lt;/a&gt; 的箴言为中心开发的&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Beautiful is better than ugly.(美丽优于丑陋)&lt;/li&gt;
&lt;li&gt;Explicit is better than implicit.(直白优于含蓄)&lt;/li&gt;
&lt;li&gt;Simple is better than complex.(简单优于复杂)&lt;/li&gt;
&lt;li&gt;Complex is better than complicated.(复杂优于繁琐)&lt;/li&gt;
&lt;li&gt;Readability counts.(可读性很重要)&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Requests，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CRequests%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy模拟登录知乎</title>
    <link href="http://yoursite.com/2017/05/11/scrapy-login-zhihu/"/>
    <id>http://yoursite.com/2017/05/11/scrapy-login-zhihu/</id>
    <published>2017-05-11T06:18:54.000Z</published>
    <updated>2017-05-12T08:12:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy登录知乎要解决两个问题</p>
<ol>
<li>session的传递，保证处理登录是同一个状态。</li>
<li>首个登录页面的改变，由直接爬取的页面变为登录页面，再去爬取页面。</li>
</ol>
<a id="more"></a>
<p>话不多说，直接上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"zhihu"</span></div><div class="line">    allowed_domains = [<span class="string">"www.zhihu.com"</span>]</div><div class="line">    start_urls = [<span class="string">'http://www.zhihu.com/'</span>]</div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">"Host"</span>: <span class="string">"www.zhihu.com"</span>,</div><div class="line">        <span class="string">"Referer"</span>: <span class="string">"https://www.zhihu.com/"</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36'</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> [scrapy.Request(<span class="string">"https://www.zhihu.com/#signin"</span>, headers=self.headers, callback=self.login)]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        登录</div><div class="line">        :param response: </div><div class="line">        :return: </div><div class="line">        """</div><div class="line">        response_text = response.text</div><div class="line">        match_obj = re.match(<span class="string">'.*name="_xsrf" value="(.*?)"'</span>, response_text, re.DOTALL)</div><div class="line">        xsrf = <span class="string">''</span></div><div class="line">        <span class="keyword">if</span> match_obj:</div><div class="line">            xsrf = match_obj.group(<span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> xsrf:</div><div class="line">            post_data = &#123;</div><div class="line">                <span class="string">"_xsrf"</span>: xsrf,</div><div class="line">                <span class="string">"phone_num"</span>: <span class="string">"18951855817"</span>,</div><div class="line">                <span class="string">"password"</span>: <span class="string">"tracy584563542"</span></div><div class="line">            &#125;</div><div class="line">            t = str(int(time.time() * <span class="number">1000</span>))</div><div class="line">            captcha_url = <span class="string">'https://www.zhihu.com/captcha.gif?r='</span> + t + <span class="string">"&amp;type=login"</span></div><div class="line">            <span class="keyword">yield</span> scrapy.Request(captcha_url, headers=self.headers, meta=&#123;<span class="string">"post_data"</span>: post_data&#125;,</div><div class="line">                                 callback=self.login_after_captcha)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login_after_captcha</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">with</span> open(<span class="string">"captcha.jpg"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(response.body)</div><div class="line">        im = Image.open(<span class="string">'captcha.jpg'</span>)</div><div class="line">        im.show()</div><div class="line">        im.close()</div><div class="line">        captcha = input(<span class="string">"请输入验证码：\n"</span>)</div><div class="line">        post_data = response.meta.get(<span class="string">"post_data"</span>, &#123;&#125;)</div><div class="line">        post_data[<span class="string">"captcha"</span>] = captcha</div><div class="line">        post_url = <span class="string">"https://www.zhihu.com/login/phone_num"</span></div><div class="line">        <span class="keyword">return</span> [scrapy.FormRequest(</div><div class="line">            url=post_url,</div><div class="line">            formdata=post_data,</div><div class="line">            headers=self.headers,</div><div class="line">            callback=self.check_login</div><div class="line">        )]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        验证服务器的返回数据判断登录是否成功</div><div class="line">        :param response: </div><div class="line">        :return: </div><div class="line">        """</div><div class="line">        text_json = json.loads(response.text)</div><div class="line">        <span class="keyword">if</span> <span class="string">'msg'</span> <span class="keyword">in</span> text_json <span class="keyword">and</span> text_json[<span class="string">'msg'</span>] == <span class="string">'登陆成功'</span>:</div><div class="line">            <span class="comment"># 从继承的Spider类中拿的内容，恢复到正确执行</span></div><div class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(url, dont_filter=<span class="keyword">True</span>, headers=self.headers)</div></pre></td></tr></table></figure>
<p>首先对 <code>scrapy.Spider</code> 类中的 <code>start_requests(self)</code> 进行重载，改变首先要处理的页面为登录页面。<br>得到登录页面后，获得xsrf，并下载验证码，通过 <code>scrapy.FormRequest</code>构造登录数据，通过check_login回调函数判断登录是否成功。在代码的最后一行转回正常的登录流程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy登录知乎要解决两个问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;session的传递，保证处理登录是同一个状态。&lt;/li&gt;
&lt;li&gt;首个登录页面的改变，由直接爬取的页面变为登录页面，再去爬取页面。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——理解Session和Cookie机制</title>
    <link href="http://yoursite.com/2017/05/09/scrapy-session-cookie/"/>
    <id>http://yoursite.com/2017/05/09/scrapy-session-cookie/</id>
    <published>2017-05-09T06:18:54.000Z</published>
    <updated>2017-05-09T06:43:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Cookie-机制"><a href="#Cookie-机制" class="headerlink" title="Cookie 机制"></a><strong>Cookie 机制</strong></h2><p>Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。</p>
<a id="more"></a>
<p>具体来说cookie机制采用的是在客户端保持状态的方案。它是在用户端的会话状态的存贮机制，他需要用户打开客户端的cookie支持。cookie的作用就是为了解决HTTP协议无状态的缺陷所作的努力。</p>
<p>正统的cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示浏览器按照指示生成相应的cookie。然而纯粹的客户端脚本如JavaScript也可以生成cookie。而cookie的使用是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的cookie，如果某个cookie所声明的作用范围大于等于将要请求的资源所在的位置，则把该cookie附在请求资源的HTTP请求头上发送给服务器。</p>
<p>cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。若不设置过期时间，则表示这个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式。</p>
<p>而session机制采用的是一种在服务器端保持状态的解决方案。同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的。而session提供了方便管理全局变量的方式 。</p>
<p>session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，这个值也可能设置为由get来返回给服务器。</p>
<p>就安全性来说：当你访问一个使用session 的站点，同时在自己机子上建立一个cookie，建立在服务器端的session机制更安全些，因为它不会任意读取客户存储的信息。 </p>
<h2 id="Session-机制"><a href="#Session-机制" class="headerlink" title="Session 机制"></a><strong>Session 机制</strong></h2><p>session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。</p>
<p>当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个session标识（称为session id），如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来使用（检索不到，会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个session id将被在本次响应中返回给客户端保存。</p>
<p>保存这个session id的方式可以采用cookie，这样在交互过程中浏览器可以自动的按照规则把这个标识发挥给服务器。一般这个cookie的名字都是类似于SEEESIONID。但cookie可以被人为的禁止，则必须有其他机制以便在cookie被禁止时仍然能够把session id传递回服务器。</p>
<p>经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路径的后面。还有一种技术叫做表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器。</p>
<p>Cookie与Session都能够进行会话跟踪，但是完成的原理不太一样。普通状况下二者均能够满足需求，但有时分不能够运用Cookie，有时分不能够运用Session。</p>
<h2 id="两者比较"><a href="#两者比较" class="headerlink" title="两者比较"></a><strong>两者比较</strong></h2><h3 id="存取方式不同"><a href="#存取方式不同" class="headerlink" title="存取方式不同"></a>存取方式不同</h3><p>Cookie中只能保管ASCII字符串，假如需求存取Unicode字符或者二进制数据，需求先进行编码。Cookie中也不能直接存取Java对象。若要存储略微复杂的信息，运用Cookie是比拟艰难的。</p>
<p>而Session中能够存取任何类型的数据，包括而不限于String、Integer、List、Map等。Session中也能够直接保管Java Bean乃至任何Java类，对象等，运用起来十分便当。能够把Session看做是一个Java容器类。</p>
<h3 id="隐私策略不同"><a href="#隐私策略不同" class="headerlink" title="隐私策略不同"></a>隐私策略不同</h3><p>Cookie存储在客户端阅读器中，对客户端是可见的，客户端的一些程序可能会窥探、复制以至修正Cookie中的内容。而Session存储在服务器上，对客户端是透明的，不存在敏感信息泄露的风险。</p>
<p>假如选用Cookie，比较好的方法是，敏感的信息如账号密码等尽量不要写到Cookie中。最好是像Google、Baidu那样将Cookie信息加密，提交到服务器后再进行解密，保证Cookie中的信息只要本人能读得懂。而假如选择Session就省事多了，反正是放在服务器上，Session里任何隐私都能够有效的保护。</p>
<h3 id="服务器压力不同"><a href="#服务器压力不同" class="headerlink" title="服务器压力不同"></a>服务器压力不同</h3><p>Session是保管在服务器端的，每个用户都会产生一个Session。假如并发访问的用户十分多，会产生十分多的Session，耗费大量的内存。因而像Google、Baidu、Sina这样并发访问量极高的网站，是不太可能运用Session来追踪客户会话的。</p>
<p>而Cookie保管在客户端，不占用服务器资源。假如并发阅读的用户十分多，Cookie是很好的选择。关于Google、Baidu、Sina来说，Cookie或许是唯一的选择。</p>
<h3 id="浏览器支持不同"><a href="#浏览器支持不同" class="headerlink" title="浏览器支持不同"></a>浏览器支持不同</h3><p>Cookie是需要客户端浏览器支持的。假如客户端禁用了Cookie，或者不支持Cookie，则会话跟踪会失效。关于WAP上的应用，常规的Cookie就派不上用场了。</p>
<p>假如客户端浏览器不支持Cookie，需要运用Session以及URL地址重写。需要注意的是一切的用到Session程序的URL都要进行URL地址重写，否则Session会话跟踪还会失效。关于WAP应用来说，Session+URL地址重写或许是它唯一的选择。</p>
<p>假如客户端支持Cookie，则Cookie既能够设为本浏览器窗口以及子窗口内有效（把过期时间设为–1），也能够设为一切阅读器窗口内有效（把过期时间设为某个大于0的整数）。但Session只能在本阅读器窗口以及其子窗口内有效。假如两个浏览器窗口互不相干，它们将运用两个不同的Session。（IE8下不同窗口Session相干）</p>
<h3 id="跨域支持不同"><a href="#跨域支持不同" class="headerlink" title="跨域支持不同"></a>跨域支持不同</h3><p>Cookie支持跨域名访问，例如将domain属性设置为“.biaodianfu.com”，则以“.biaodianfu.com”为后缀的一切域名均能够访问该Cookie。跨域名Cookie如今被普遍用在网络中，例如Google、Baidu、Sina等。而Session则不会支持跨域名访问。Session仅在他所在的域名内有效。</p>
<h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><p>仅运用Cookie或者仅运用Session可能完成不了理想的效果。这时应该尝试一下同时运用Cookie与Session。Cookie与Session的搭配运用在实践项目中会完成很多意想不到的效果。</p>
<h2 id="Python-Django-中实现两种机制"><a href="#Python-Django-中实现两种机制" class="headerlink" title="Python Django 中实现两种机制"></a>Python Django 中实现两种机制</h2><h3 id="Cookie-设置"><a href="#Cookie-设置" class="headerlink" title="Cookie 设置"></a>Cookie 设置</h3><p>以下是Cookie设置的详细流程：</p>
<ol>
<li>客户端发起一个请求连接（如HTTP GET）</li>
<li>服务器在http响应头上加上Set-Cookie，里面存放字符串的键值对</li>
<li>客户端随后的http请求头加上Cookie首部，它包含了之前服务器响应中设置cookie的信息。</li>
</ol>
<p>根据这个Cookie首部的信息，服务器便能“记住”当前用户的信息。</p>
<p>下面就来看看Python中如何设置Cookie：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> BaseHTTPServer <span class="keyword">import</span> HTTPServer</div><div class="line"><span class="keyword">from</span> SimpleHTTPServer <span class="keyword">import</span> SimpleHTTPRequestHandler</div><div class="line"><span class="keyword">import</span> Cookie</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRequestHandler</span><span class="params">(SimpleHTTPRequestHandler)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_GET</span><span class="params">(self)</span>:</span></div><div class="line">        content = <span class="string">"&lt;html&gt;&lt;body&gt;Path is: %s&lt;/body&gt;&lt;/html&gt;"</span> % self.path</div><div class="line">        self.send_response(<span class="number">200</span>)</div><div class="line">        self.send_header(<span class="string">'Content-type'</span>, <span class="string">'text/html'</span>)</div><div class="line">        self.send_header(<span class="string">'Content-length'</span>, str(len(content)))</div><div class="line"></div><div class="line">        cookie = Cookie.SimpleCookie()</div><div class="line">        cookie[<span class="string">'id'</span>] = <span class="string">'some_value_42'</span></div><div class="line"></div><div class="line">        self.wfile.write(cookie.output())</div><div class="line">        self.wfile.write(<span class="string">'\r\n'</span>)</div><div class="line"></div><div class="line">        self.end_headers()</div><div class="line">        self.wfile.write(content)</div><div class="line"></div><div class="line">server = HTTPServer((<span class="string">''</span>, <span class="number">59900</span>), MyRequestHandler)</div><div class="line">server.serve_forever()</div></pre></td></tr></table></figure>
<p>查看服务器端的http响应头，会发现以下字段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Set-Cookie: id=some_value_42</div></pre></td></tr></table></figure>
<p>在Django中，可以用如下的方式获取或设置Cookie：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_cookie</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'id'</span> <span class="keyword">in</span> request.COOKIES:</div><div class="line">        cookie_id = request.COOKIES[<span class="string">'id'</span>]</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'Got cookie with id=%s'</span> % cookie_id)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        resp = HttpResponse(<span class="string">'No id cookie! Sending cookie to client'</span>)</div><div class="line">        resp.set_cookie(<span class="string">'id'</span>, <span class="string">'some_value_99'</span>)</div><div class="line">        <span class="keyword">return</span> resp</div></pre></td></tr></table></figure>
<p>Django通过一系列的包装使得封装Cookie的操作变得更加简单，那么它在其中是怎么实现cookie的读取的呢，下面来窥探原理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_cookies</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'_cookies'</span>):</div><div class="line">        self._cookies = http.parse_cookie(self.environ.get(<span class="string">'HTTP_COOKIE'</span>, <span class="string">''</span>))</div><div class="line">    <span class="keyword">return</span> self._cookies</div></pre></td></tr></table></figure>
<p>可以看出，获取cookie的操作用了Lazy initialization（延迟加载）的技术，因为如果客户端不需要用到cookie，这个过程只会浪费不必要的操作。</p>
<p>再来看parse_cookie的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_cookie</span><span class="params">(cookie)</span>:</span></div><div class="line">    <span class="keyword">if</span> cookie == <span class="string">''</span>:</div><div class="line">        <span class="keyword">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(cookie, Cookie.BaseCookie):</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            c = SimpleCookie()</div><div class="line">            c.load(cookie, ignore_parse_errors=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">except</span> Cookie.CookieError:</div><div class="line">            <span class="comment"># 无效cookie</span></div><div class="line">            <span class="keyword">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        c = cookie</div><div class="line">    cookiedict = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> c.keys():</div><div class="line">        cookiedict[key] = c.get(key).value</div><div class="line">    <span class="keyword">return</span> cookiedict</div></pre></td></tr></table></figure>
<p>它负责解析Cookie并把结果集成到一个dict（字典）对象中，并返回字典。而设置cookie的操作则会被WSGIHandler执行。</p>
<p>注：Django的底层实现了WSGI的接口（如WSGIRequest，WSGIServer等）。</p>
<h3 id="Session-应用"><a href="#Session-应用" class="headerlink" title="Session 应用"></a>Session 应用</h3><p>下面看一个简单的session应用例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_count_session</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'count'</span> <span class="keyword">in</span> request.session:</div><div class="line">        request.session[<span class="string">'count'</span>] += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'new count=%s'</span> % request.session[<span class="string">'count'</span>])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        request.session[<span class="string">'count'</span>] = <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'No count in session. Setting to 1'</span>)</div></pre></td></tr></table></figure>
<p>它用session实现了一个计数器，当每一个请求到来时，就为计数器加一，把新的结果更新到session中。</p>
<p>查看http的响应头，会得到类似下面的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Set-Cookie:sessionid=a92d67e44a9b92d7dafca67e507985c0;</div><div class="line">           expires=Thu, <span class="number">07</span>-Jul<span class="number">-2011</span> <span class="number">04</span>:<span class="number">16</span>:<span class="number">28</span> GMT;</div><div class="line">           Max-Age=<span class="number">1209600</span>;</div><div class="line">           Path=/</div></pre></td></tr></table></figure>
<p>里面包含了session_id以及过期时间等信息。</p>
<p>那么服务器端是如何保存session的呢？</p>
<p>在django中，默认会把session保存在setting指定的数据库中，除此之外，也可以通过指定session engine，使session保存在文件(file)，内存(cache)中。</p>
<p>如果保存在数据库中，django会在数据库中创建一个如下的session表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE &quot;django_session&quot; (</div><div class="line">    &quot;session_key&quot; varchar(40) NOT NULL PRIMARY KEY,</div><div class="line">    &quot;session_data&quot; text NOT NULL,</div><div class="line">    &quot;expire_date&quot; datetime NOT NULL</div><div class="line">);</div></pre></td></tr></table></figure>
<p>session_key是放置在cookie中的id，它是唯一的，而session_data则存放序列化后的session数据字符串。</p>
<p>通过session_key可以在数据库中取得这条session的信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.contrib.sessions.models <span class="keyword">import</span> Session</div><div class="line"><span class="comment">#...</span></div><div class="line">sess = Session.objects.get(pk=<span class="string">'a92d67e44a9b92d7dafca67e507985c0'</span>)</div><div class="line">print(sess.session_data)</div><div class="line">print(sess.get_decoded())</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ZmEyNDVhNTBhMTk2ZmRjNzVlYzQ4NTFjZDk2Y2UwODc3YmVjNWVjZjqAAn1xAVUFY291bnRxAksG</div><div class="line">cy4=&#123;<span class="string">'count'</span>: <span class="number">6</span>&#125;</div></pre></td></tr></table></figure>
<p>回看第一个例子，我们是通过request.session来获取session的，为什么请求对象会附带一个session对象呢，这其中做了什么呢？</p>
<p>这就引出了下面要说的django里的中间件技术 <code>Session middleware</code>。</p>
<p>关于中间件，<code>&lt;&lt;the Django Book&gt;&gt;</code>是这样解释的：</p>
<p>Django的中间件框架，是django处理请求和响应的一套钩子函数的集合。</p>
<p>我们看传统的django视图模式一般是这样的：http请求-&gt;view-&gt;http响应，而加入中间件框架后，则变为：http请求-&gt;中间件处理-&gt;app-&gt;中间件处理-&gt;http响应。而在django中这两个处理分别对应process_request和process_response函数，这两个钩子函数将会在特定的时候被触发。</p>
<p>直接看SessionMiddleware可能更清晰一些：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SessionMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        engine = import_module(settings.SESSION_ENGINE)</div><div class="line">        self.SessionStore = engine.SessionStore</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request)</span>:</span></div><div class="line">        session_key = request.COOKIES.get(settings.SESSION_COOKIE_NAME)</div><div class="line">        request.session = self.SessionStore(session_key)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        If request.session was modified, or if the configuration is to save the</div><div class="line">        session every time, save the changes and set a session cookie or delete</div><div class="line">        the session cookie if the session has been emptied.</div><div class="line">        """</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            accessed = request.session.accessed</div><div class="line">            modified = request.session.modified</div><div class="line">            empty = request.session.is_empty()</div><div class="line">        <span class="keyword">except</span> AttributeError:</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># First check if we need to delete this cookie.</span></div><div class="line">            <span class="comment"># The session should be deleted only if the session is entirely empty</span></div><div class="line">            <span class="keyword">if</span> settings.SESSION_COOKIE_NAME <span class="keyword">in</span> request.COOKIES <span class="keyword">and</span> empty:</div><div class="line">                response.delete_cookie(settings.SESSION_COOKIE_NAME,</div><div class="line">                    domain=settings.SESSION_COOKIE_DOMAIN)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">if</span> accessed:</div><div class="line">                    patch_vary_headers(response, (<span class="string">'Cookie'</span>,))</div><div class="line">                <span class="keyword">if</span> (modified <span class="keyword">or</span> settings.SESSION_SAVE_EVERY_REQUEST) <span class="keyword">and</span> <span class="keyword">not</span> empty:</div><div class="line">                    <span class="keyword">if</span> request.session.get_expire_at_browser_close():</div><div class="line">                        max_age = <span class="keyword">None</span></div><div class="line">                        expires = <span class="keyword">None</span></div><div class="line">                    <span class="keyword">else</span>:</div><div class="line">                        max_age = request.session.get_expiry_age()</div><div class="line">                        expires_time = time.time() + max_age</div><div class="line">                        expires = cookie_date(expires_time)</div><div class="line">                    <span class="comment"># Save the session data and refresh the client cookie.</span></div><div class="line">                    <span class="comment"># Skip session save for 500 responses, refs #3881.</span></div><div class="line">                    <span class="keyword">if</span> response.status_code != <span class="number">500</span>:</div><div class="line">                        <span class="keyword">try</span>:</div><div class="line">                            request.session.save()</div><div class="line">                        <span class="keyword">except</span> UpdateError:</div><div class="line">                            <span class="comment"># The user is now logged out; redirecting to same</span></div><div class="line">                            <span class="comment"># page will result in a redirect to the login page</span></div><div class="line">                            <span class="comment"># if required.</span></div><div class="line">                            <span class="keyword">return</span> redirect(request.path)</div><div class="line">                        response.set_cookie(settings.SESSION_COOKIE_NAME,</div><div class="line">                                request.session.session_key, max_age=max_age,</div><div class="line">                                expires=expires, domain=settings.SESSION_COOKIE_DOMAIN,</div><div class="line">                                path=settings.SESSION_COOKIE_PATH,</div><div class="line">                                secure=settings.SESSION_COOKIE_SECURE <span class="keyword">or</span> <span class="keyword">None</span>,</div><div class="line">                                httponly=settings.SESSION_COOKIE_HTTPONLY <span class="keyword">or</span> <span class="keyword">None</span>)</div><div class="line">        <span class="keyword">return</span> response</div></pre></td></tr></table></figure>
<p>在请求到来后，SessionMiddleware的process_request在请求取出session_key，并把一个新的session对象赋给request.session，而在返回响应时，process_response则判断session是否被修改或过期，来更新session的信息。</p>
<h3 id="Django-用户认证中的-Session"><a href="#Django-用户认证中的-Session" class="headerlink" title="Django 用户认证中的 Session"></a>Django 用户认证中的 Session</h3><p>在django中，用下面的方法来验证用户是否登录是常见的事情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_user</span><span class="params">(request)</span>:</span></div><div class="line">    user_str = str(request.user)</div><div class="line">    <span class="keyword">if</span> request.user.is_authenticated():</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'%s is logged in'</span> % user_str)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> HttpResponse(<span class="string">'%s is not logged in'</span> % user_str)</div></pre></td></tr></table></figure>
<p>其实request.user的实现也借助到了session。</p>
<p>在这个例子中，成功登录后，session表会保存类似下面的信息，里面记录了用户的id，以后进行验证时，便会到这个表中获取用户的信息。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;'_auth_user_id': 1, '_auth_user_backend': 'django.contrib.auth.backends.ModelBackend'&#125;</div></pre></td></tr></table></figure>
<p>跟上面提到的Session中间件相似，用户验证也有一个中间件：AuthenticationMiddleware，在process_request中，通过request.<strong>class</strong>.user = LazyUser()在request设置了一个全局的可缓存的用户对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyUser</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, request, obj_type=None)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(request, <span class="string">'_cached_user'</span>):</div><div class="line">            <span class="keyword">from</span> django.contrib.auth <span class="keyword">import</span> get_user</div><div class="line">            request._cached_user = get_user(request)</div><div class="line">        <span class="keyword">return</span> request._cached_user</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuthenticationMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request)</span>:</span></div><div class="line">        request.__class__.user = LazyUser()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div></pre></td></tr></table></figure>
<p>在get_user里，会在检查session中是否存放了当前用户对应的user_id，如果有，则通过id在model查找相应的用户返回，否则返回一个匿名的用户对象(AnonymousUser)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">from</span> django.contrib.auth.models <span class="keyword">import</span> AnonymousUser</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        user_id = request.session[SESSION_KEY]</div><div class="line">        backend_path = request.session[BACKEND_SESSION_KEY]</div><div class="line">        backend = load_backend(backend_path)</div><div class="line">        user = backend.get_user(user_id) <span class="keyword">or</span> AnonymousUser()</div><div class="line">    <span class="keyword">except</span> KeyError:</div><div class="line">        user = AnonymousUser()</div><div class="line">    <span class="keyword">return</span> user</div></pre></td></tr></table></figure>
<h3 id="Django中的Session实现"><a href="#Django中的Session实现" class="headerlink" title="Django中的Session实现"></a>Django中的Session实现</h3><p>Django使用的Session默认都继承于SessionBase类里，这个类实现了一些session操作方法，以及hash，decode，encode等方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SessionBase</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Base class for all Session classes.</div><div class="line">    """</div><div class="line">    TEST_COOKIE_NAME = <span class="string">'testcookie'</span></div><div class="line">    TEST_COOKIE_VALUE = <span class="string">'worked'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, session_key=None)</span>:</span></div><div class="line">        self._session_key = session_key</div><div class="line">        self.accessed = <span class="keyword">False</span></div><div class="line">        self.modified = <span class="keyword">False</span></div><div class="line">        self.serializer = import_string(settings.SESSION_SERIALIZER)</div></pre></td></tr></table></figure>
<p>说的更直白一些，其实django中的session就是一个模拟dict的对象，并实现了一系列的hash和序列化方法，默认持久化在数据库中（有时候也可能由于为了提高性能，用redis之类的内存数据库来缓存session）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Cookie-机制&quot;&gt;&lt;a href=&quot;#Cookie-机制&quot; class=&quot;headerlink&quot; title=&quot;Cookie 机制&quot;&gt;&lt;/a&gt;&lt;strong&gt;Cookie 机制&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Django，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CDjango%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Loaders机制介绍与实例</title>
    <link href="http://yoursite.com/2017/05/08/scrapy-item-loader/"/>
    <id>http://yoursite.com/2017/05/08/scrapy-item-loader/</id>
    <published>2017-05-08T06:18:54.000Z</published>
    <updated>2017-05-08T07:02:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 </p>
<p>从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。</p>
<p>Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。</p>
<a id="more"></a>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="使用-Item-Loaders-来填充-Items"><a href="#使用-Item-Loaders-来填充-Items" class="headerlink" title="使用 Item Loaders 来填充 Items"></a><strong>使用 Item Loaders 来填充 Items</strong></h3><p>要使用 Item Loader, 你必须先将它实例化。你可以使用类似字典的对象(例如: Item or dict)来进行实例化，或者不使用对象也可以，当不用对象进行实例化的时候，Item 会自动使用 <code>ItemLoader.default\_item_class</code> 属性中指定的 Item 类在 <code>Item Loader constructor</code> 中实例化。 </p>
<p>然后，你开始收集数值到 Item Loader 时，通常使用 Selectors。你可以在同一个 item field 里面添加多个数 值；Item Loader 将知道如何用合适的处理函数来“添加”这些数值。 下面是在 Spider 中典型的 Item Loader 的用法，使用 <code>Items chapter</code> 中声明的 <code>Product item</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader </div><div class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> Product</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span> </div><div class="line">    l = ItemLoader(item=Product(), response=response) </div><div class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_name"]'</span>) </div><div class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_title"]'</span>) </div><div class="line">    l.add_xpath(<span class="string">'price'</span>, <span class="string">'//p[@id="price"]'</span>) </div><div class="line">    l.add_css(<span class="string">'stock'</span>, <span class="string">'p#stock]'</span>) </div><div class="line">    l.add_value(<span class="string">'last_updated'</span>, <span class="string">'today'</span>) <span class="comment"># you can also use literal values </span></div><div class="line">    <span class="keyword">return</span> l.load_item()</div></pre></td></tr></table></figure>
<p>快速查看这些代码之后，我们可以看到发现 name 字段被从页面中两个不同的 XPath 位置提取：</p>
<ol>
<li><code>//div[@class=&quot;product_name&quot;]</code></li>
<li><code>//div[@class=&quot;product_title&quot;]</code></li>
</ol>
<p>换言之,数据通过用 <code>add_xpath()</code> 的方法，把从两个不同的 XPath 位置提取的数据收集起来。这是将在以后分配给 <code>name</code> 字段中的数据?</p>
<p>之后，类似的请求被用于 price 和 stock 字段 （后者使用 <code>CSS selector</code> 和 <code>add_css()</code> 方法）， 最后使用不同的方法 <code>add_value()</code> 对 <code>last_update</code> 填充文本值( today )。 最终，当所有数据被收集起来之后，调用 <code>ItemLoader.load_item()</code> 方法，实际上填充并且返回了之前通过调用 <code>add_xpath()</code>，<code>add_css()</code> ，<code>add_value()</code> 所提取和收集到的数据的 Item。</p>
<h3 id="输入和输出处理器"><a href="#输入和输出处理器" class="headerlink" title="输入和输出处理器"></a><strong>输入和输出处理器</strong></h3><p>Item Loader 在每个（Item）字段中都包含了一个输入处理器和一个输出处理器。输入处理器收到数据时立刻提取数据 （通过  <code>add_xpath()</code>， <code>add_css()</code>  或者  <code>add_value()</code>方法）之后输入处理器的结果被收集起来并且保存在ItemLoader内。收集到所有的数据后，调用 <code>ItemLoader.load_item()</code> 方法来填充，并得到填充后的 Item 对象。这是当输出处理器被和之前收集到的数据（和用输入处理器处理的）被调用。输出处理器的结果是被分配到Item的最终值。</p>
<p>让我们看一个例子来说明如何输入和输出处理器被一个特定的字段调用（同样适用于其他field）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">l = ItemLoader(Product(), some_selector)</div><div class="line">l.add_xpath(<span class="string">'name'</span>, xpath1) <span class="comment"># (1)</span></div><div class="line">l.add_xpath(<span class="string">'name'</span>, xpath2) <span class="comment"># (2)</span></div><div class="line">l.add_css(<span class="string">'name'</span>, css) <span class="comment"># (3)</span></div><div class="line">l.add_value(<span class="string">'name'</span>, <span class="string">'test'</span>) <span class="comment"># (4)</span></div><div class="line"><span class="keyword">return</span> l.load_item() <span class="comment"># (5)</span></div></pre></td></tr></table></figure>
<p>发生了这些事情:</p>
<ol>
<li>从 <code>xpath1</code> 提取出的数据,传递给 <em>输入处理器</em> 的 <code>name</code> 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡</li>
<li>从 <code>xpath2</code> 提取出来的数据,传递给(1)中使用的相同的 <em>输入处理器</em> .输入处理器的结果被附加到在(1)中收集的数据(如果有的话) ｡</li>
<li>和之前相似，只不过这里的数据是通过 <code>css</code> CSS selector抽取，之后传输到在(1)和(2)使用 的<em>input processor</em> 中。最终输入处理器的结果被附加到在(1)和(2)中收集的数据之后 (如果存在数据的话)。</li>
<li>这里的处理方式也和之前相似，但是此处的值是通过add_value直接赋予的， 而不是利用XPath表达式或CSS selector获取。得到的值仍然是被传送到输入处理器。 在这里例程中，因为得到的值并非可迭代，所以在传输到输入处理器之前需要将其 转化为可迭代的单个元素，这才是它所接受的形式。</li>
<li>在之前步骤中所收集到的数据被传送到 <em>output processor</em> 的 <code>name</code> field中。 输出处理器的结果就是赋到item中 <code>name</code> field的值。</li>
</ol>
<p><strong>理解：</strong></p>
<p>就是在使用Item Loader 时候，会有一个输入处理器，一个输出处理器，首先是收集好同一个字段的结果，传入到<strong>输入处理器</strong>当中，然后收集完后，会传递给<strong>输出处理器</strong>进行处理。输出处理器的处理结果，就是填充到item的结果。</p>
<p>需要注意的是：输入处理器的返回值会是内部收集的，然后被传递给输出处理器，来填充fields。</p>
<h4 id="Scrapy-内部的处理器"><a href="#Scrapy-内部的处理器" class="headerlink" title="Scrapy 内部的处理器"></a><strong>Scrapy 内部的处理器</strong></h4><p>Scrapy内部，已经有一些设置好的<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#topics-loaders-available-processors" target="_blank" rel="external">内置处理器</a></p>
<ul>
<li>Identity<br>这是最简单的一个处理器，实际上就是什么都不做，传入多少个字段，就存储多少个字段，以list形式。<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import Identity
\&gt;&gt;&gt; proc = Identity()
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</code> </li>
<li>TakeFirst<br>从接受到的list中返回第一个非null/非空的值，<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import TakeFirst
\&gt;&gt;&gt; proc = TakeFirst()
\&gt;&gt;&gt; proc([&#39;&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
&#39;one&#39;</code> </li>
<li>Join<br>返回用分隔符（separator）作为间隔的连接形成的字符串。若不传入separator，则默认使用’ ‘（空格）。<code>&gt;&gt;&gt; from scrapy.contrib.loader.processor import Join
\&gt;&gt;&gt; proc = Join()
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
u&#39;one two three&#39;
\&gt;&gt;&gt; proc = Join(&#39;&lt;br&gt;&#39;)
\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])
u&#39;one&lt;br&gt;two&lt;br&gt;three&#39;</code> </li>
<li>另外还有Compose以及MapCompose，这里不一一详述。</li>
</ul>
<hr>
<h3 id="声明-Item-Loaders"><a href="#声明-Item-Loaders" class="headerlink" title="声明 Item Loaders"></a><strong>声明 Item Loaders</strong></h3><p>声明ItemLoaders 和声明Item类似，使用Class语法，例子：</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="title">from</span> scrapy.contrib.loader <span class="keyword">import</span> ItemLoader</div><div class="line"><span class="title">from</span> scrapy.contrib.loader.processor <span class="keyword">import</span> TakeFirst, MapCompose, Join</div><div class="line"><span class="class"></span></div><div class="line"><span class="keyword">class</span> <span class="type">ProductLoader</span>(<span class="type">ItemLoader</span>):</div><div class="line"></div><div class="line">    default_output_processor = <span class="type">TakeFirst</span>()</div><div class="line"></div><div class="line">    name_in = <span class="type">MapCompose</span>(<span class="title">unicode</span>.<span class="title">title</span>)</div><div class="line">    name_out = <span class="type">Join</span>()</div><div class="line"></div><div class="line">    price_in = <span class="type">MapCompose</span>(<span class="title">unicode</span>.<span class="title">strip</span>)</div><div class="line"></div><div class="line">    # ...</div></pre></td></tr></table></figure>
<p>上述代码中:</p>
<ul>
<li>输出处理器，被声明为 <code>_in</code> 前缀，而输出处理器被声明为 <code>_out</code> 前缀。</li>
<li>设置默认处理器 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_input_processor" target="_blank" rel="external"><code>ItemLoader.default_input_processor</code></a>  and  <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_output_processor" target="_blank" rel="external"><code>ItemLoader.default_output_processor</code></a></li>
</ul>
<h3 id="声明输入、输出处理器"><a href="#声明输入、输出处理器" class="headerlink" title="声明输入、输出处理器"></a><strong>声明输入、输出处理器</strong></h3><p>输入、输出可以被如上方那样被声明，这也是最正常的方式。另外，我们也可以在另外的一个地方去声明输入和输出处理器：在item Field，元数据中。</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="title">from</span> scrapy.contrib.loader.processor <span class="keyword">import</span> Join, MapCompose, TakeFirst</div><div class="line"><span class="title">from</span> w3lib.html <span class="keyword">import</span> remove_tags</div><div class="line"></div><div class="line"><span class="title">def</span> filter_price(value):</div><div class="line">    <span class="keyword">if</span> value.isdigit():</div><div class="line">        return value</div><div class="line"><span class="class"></span></div><div class="line"><span class="keyword">class</span> <span class="type">Product</span>(<span class="title">scrapy</span>.<span class="type">Item</span>):</div><div class="line">    name = scrapy.<span class="type">Field</span>(</div><div class="line">        <span class="title">input_processor</span>=<span class="type">MapCompose(remove_tags)</span>,</div><div class="line">        <span class="title">output_processor</span>=<span class="type">Join</span>(),</div><div class="line">    )</div><div class="line">    price = scrapy.<span class="type">Field</span>(</div><div class="line">        <span class="title">input_processor</span>=<span class="type">MapCompose</span>(<span class="title">remove_tags</span>, <span class="title">filter_price</span>),</div><div class="line">        output_processor=<span class="type">TakeFirst</span>(),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>输出和输出处理器的优先级如下：</p>
<ol>
<li>Item Loader field 指定的<code>field_in</code> 和 <code>field_out</code>（最优先）</li>
<li>Field 元数据中(input_processor 和 output_processor key)</li>
<li>item loader 默认。 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_input_processor" target="_blank" rel="external"><code>ItemLoader.default_input_processor()</code></a> and<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#scrapy.contrib.loader.ItemLoader.default_output_processor" target="_blank" rel="external"><code>ItemLoader.default_output_processor()</code></a> (least precedence)</li>
</ol>
<hr>
<p>最后，给出Item Loader的官方说明API：</p>
<p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/loaders.html#itemloader-objects" target="_blank" rel="external">ItemLoader objects</a></p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a><strong>实例</strong></h2><h3 id="通过-Item-loader-加载-Item"><a href="#通过-Item-loader-加载-Item" class="headerlink" title="通过 Item loader 加载 Item"></a><strong>通过 Item loader 加载 Item</strong></h3><p>首先在 <code>jobbole.py</code> 中引入 <code>from scrapy.loader import ItemLoader</code></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</div><div class="line">item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</div><div class="line">item_loader.add_value(<span class="string">"url"</span>, response.url)</div><div class="line">item_loader.add_value(<span class="string">"url_object_id"</span>, get_md5(response.url))</div><div class="line">item_loader.add_css(<span class="string">"create_date"</span>, <span class="string">"p.entry-meta-hide-on-mobile::text"</span>)</div><div class="line">front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></div><div class="line">item_loader.add_value(<span class="string">"front_image_url"</span>, [front_image_url])</div><div class="line">item_loader.add_css(<span class="string">"praise_nums"</span>, <span class="string">".vote-post-up h10::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"comment_nums"</span>, <span class="string">"a[href='#article-comment'] span::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"fav_nums"</span>, <span class="string">".bookmark-btn::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"tags"</span>, <span class="string">"p.entry-meta-hide-on-mobile a::text"</span>)</div><div class="line">item_loader.add_css(<span class="string">"content"</span>, <span class="string">"div.entry"</span>)</div><div class="line"></div><div class="line">article_item = item_loader.load_item()</div></pre></td></tr></table></figure>
<p>其中第一行中 <code>JobBoleArticleItem()</code> 为在 <code>items.py</code> 中声明的实例，<code>response</code> 为返回的响应。这属于固定写法。<br><code>add_css()</code>中第一个值为 <code>items.py</code> 中定义的值，第二个值为css选择器规则，类似的方法还有 <code>add_xpath()</code>，根据场景进行选择。</p>
<p>同理，<code>add_value()</code>为添加确定值的方法。这里通过值传递附给 <code>front_image_url</code> 再通过add_value的方法，加入到最终的item中。</p>
<p>最后通过调用 <code>load_item()</code> 方法对结果进行解析，所有的结果都是一个list并保存到 <code>article_item</code> 中。</p>
<p>断点调试结果如图：</p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1ffdwrwwdu2j31kw1ax1a5.jpg" alt=""></p>
<p>发现获取到的所有值都是一个list，这样很不方便，但使得代码可读性更高，可维护性更强。</p>
<h3 id="通过-items-py-处理数据"><a href="#通过-items-py-处理数据" class="headerlink" title="通过 items.py 处理数据"></a><strong>通过 items.py 处理数据</strong></h3><p>在 <code>items.py</code> 中引入 <code>from scrapy.loader.processors import MapCompose</code> ，然后可以在定义 <code>scrapy.Field()</code> 时可以加入处理函数（可以使匿名函数），例如：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdx4319hjj30x009kq4b.jpg" alt=""></p>
<p>在 <code>MapCompose()</code> 中可以加入多个函数，在 <code>jobbole.py</code> 中断点调试结果如图：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdx56xexcj31kw0c37aq.jpg" alt=""></p>
<p>在title的结果后面出现了我们想要的后缀。</p>
<p>另外，可以看到，结果都是 <code>list</code>，我们每次都需要提取第一个值。Scrapy给我们提供了 <code>TakeFirst</code> 方法。</p>
<p>同样引入 <code>from scrapy.loader.processors import MapCompose,TakeFirst</code> ，修改代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">title = scrapy.Field(</div><div class="line">        input_processor = MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"-jobbole"</span>, add_jobbole),</div><div class="line">        output_processor = TakeFirst()</div><div class="line">	)</div></pre></td></tr></table></figure>
<p>即可以得到第一个值。由于每一个结果都是取第一个值，每个值全部调用这个方法重复代码过多，可以通过自定义Item loader重载的方法解决。引入 <code>from scrapy.loader import ItemLoader</code> ，这个类提供了以下方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItemLoader</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    default_item_class = Item</div><div class="line">    default_input_processor = Identity()</div><div class="line">    default_output_processor = Identity()</div><div class="line">    default_selector_class = Selector</div></pre></td></tr></table></figure>
<p>我们自定义的Item loader需要继承这个类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></div><div class="line">    default_output_processor = TakeFirst()</div></pre></td></tr></table></figure>
<p>然后在 <code>jobbole.py</code> 文件中，把<br> <code>item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)</code> 中的 <code>ItemLoader</code> 变为 <code>ArticleItemLoader</code>，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response)</div></pre></td></tr></table></figure>
<p>这样得到的结果就是一个str而不是list了。</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffdyhajhdvj31kw0bz445.jpg" alt=""></p>
<p>不过在上图是可以看到，它的tags也取了第一个值，但实际上它的值是三个，不满足我们的需要。<br>引入Join方法 <code>from scrapy.loader.processors import MapCompose, TakeFirst, Join</code>，同时不使用自定义的item loader即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tags = scrapy.Field(</div><div class="line">        output_processor=Join(<span class="string">','</span>),</div><div class="line">	)</div></pre></td></tr></table></figure>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffdyj0tk9lj30oe00sdg0.jpg" alt=""></p>
<p>和前面一样，有时候tags会有 <code>评论</code> 的不符合要求的tags，还需要自定义函数把相应的字段去掉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_comment</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="string">'评论'</span> <span class="keyword">in</span> value:</div><div class="line">        <span class="keyword">return</span> <span class="string">''</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> value</div></pre></td></tr></table></figure>
<p>在处理图片时，使用pipelines需要传递的是一个列表，这里经过处理后，变成了str。可以通过一个默认函数不让默认的TakeFirst处理即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_value</span><span class="params">(value)</span>:</span></div><div class="line">    <span class="keyword">return</span> value</div></pre></td></tr></table></figure>
<p>调用方法是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">front_image_url = scrapy.Field(</div><div class="line">        output_processor=MapCompose(return_value),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>最后，我们在之前还用正则表达式来清洗点赞数，收藏数，评论数这些数据，在item loader中我们也可以用函数处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_nums</span><span class="params">(value)</span>:</span></div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, value)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        nums = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> nums</div><div class="line"></div><div class="line">praise_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div><div class="line">    comment_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div><div class="line">    fav_nums = scrapy.Field(</div><div class="line">        input_processor=MapCompose(get_nums),</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>调试结果中str就变成int类型了：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffdynb5xtsj31kw0c9afc.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 &lt;/p&gt;
&lt;p&gt;从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。&lt;/p&gt;
&lt;p&gt;Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——保存item到json文件</title>
    <link href="http://yoursite.com/2017/05/07/scrapy-item-json/"/>
    <id>http://yoursite.com/2017/05/07/scrapy-item-json/</id>
    <published>2017-05-07T06:18:54.000Z</published>
    <updated>2017-05-07T05:05:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。</p>
<a id="more"></a>
<h2 id="使用系统-exporter-导出为-JSON-文件"><a href="#使用系统-exporter-导出为-JSON-文件" class="headerlink" title="使用系统 exporter 导出为 JSON 文件"></a><strong>使用系统 exporter 导出为 JSON 文件</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 调用Scrapy提供的json exporter导出json文件</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = open(<span class="string">'article.json'</span>, <span class="string">'wb'</span>)</div><div class="line">        self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</div><div class="line">        self.exporter.start_exporting()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.exporter.finish_exporting()</div><div class="line">        self.file.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        self.exporter.export_item(item)</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h2 id="自定义-Pipeline-导出为-JSON-文件"><a href="#自定义-Pipeline-导出为-JSON-文件" class="headerlink" title="自定义 Pipeline 导出为 JSON 文件"></a><strong>自定义 Pipeline 导出为 JSON 文件</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWithEncodingPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 自定义json文件的导出</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></div><div class="line">        self.file.write(lines)</div><div class="line">        <span class="keyword">return</span> item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.file.close()</div></pre></td></tr></table></figure>
<h2 id="函数说明"><a href="#函数说明" class="headerlink" title="函数说明"></a><strong>函数说明</strong></h2><p><code>codecs</code> ：避免打开文件时出现编码错误。<br><code>json.dumps</code> ：dict转成str<br><code>json.loads</code> ：str转成dict<br><code>ensure_ascii=False</code> ：避免处理英文以外语言时出错<br><code>return item</code> ：交给下一个pipeline处理</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——通过Pipeline保存数据到MySQL</title>
    <link href="http://yoursite.com/2017/05/07/scrapy-item-mysql/"/>
    <id>http://yoursite.com/2017/05/07/scrapy-item-mysql/</id>
    <published>2017-05-07T06:18:54.000Z</published>
    <updated>2017-05-07T11:36:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>将数据保存到MySQL数据库，需要用到 <code>mysqlclient</code> 模块，需要在我们的虚拟环境中用 <code>pip</code> 进行安装。</p>
<a id="more"></a>
<h2 id="设计数据表"><a href="#设计数据表" class="headerlink" title="设计数据表"></a><strong>设计数据表</strong></h2><p>需要根据之前Item来设计我们的数据表 <code>jobbole_article</code> ，数据库取名为 <code>article_spider</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    title = scrapy.Field()</div><div class="line">    create_date = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    front_image_url = scrapy.Field()</div><div class="line">    front_image_path = scrapy.Field()</div><div class="line">    praise_nums = scrapy.Field()</div><div class="line">    comment_nums = scrapy.Field()</div><div class="line">    fav_nums = scrapy.Field()</div><div class="line">    tags = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure>
<p>初步设计的数据表如下，在后面使用时还会进行必要的改动：</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1ffcueufjwhj317e0kwwib.jpg" alt=""></p>
<h2 id="采用同步机制写入MySQL"><a href="#采用同步机制写入MySQL" class="headerlink" title="采用同步机制写入MySQL"></a><strong>采用同步机制写入MySQL</strong></h2><p>首先在 <code>pipelines.py</code> 中引入数据库连接模块 <code>import MySQLdb</code> ，然后完善 <code>MysqlPipeline</code> 类的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="comment"># 采用同步的机制写入mysql</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.conn = MySQLdb.connect(<span class="string">'127.0.0.1'</span>, <span class="string">'root'</span>, <span class="string">'12'</span>, <span class="string">'article_spider'</span>,</div><div class="line">                                    charset=<span class="string">'utf8'</span>, use_unicode=<span class="keyword">True</span>)</div><div class="line">        self.cursor = self.conn.cursor()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                    insert into jobbole_article(title, url, create_date, fav_nums)</div><div class="line">                    VALUES (%s, %s, %s, %s)</div><div class="line">        """</div><div class="line">        self.cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</div><div class="line">        self.conn.commit()</div></pre></td></tr></table></figure>
<p><code>__init__</code> 方法是对数据进行初始化，定义连接信息如host，数据库用户名、密码、数据库名称、数据库编码<br>在 <code>process_item</code> 方法中进行插入数据操作，格式都是固定的。</p>
<p>最后在 <code>settings.py</code> 中把 <code>MysqlPipeline()</code> 加入到 <code>ITEM_PIPELINES</code> 的配置中。</p>
<h2 id="采用异步机制写入MySQL"><a href="#采用异步机制写入MySQL" class="headerlink" title="采用异步机制写入MySQL"></a><strong>采用异步机制写入MySQL</strong></h2><p>在上面的同步机制写入数据库中，我们把连接信息 <code>MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;12&#39;, &#39;article_spider&#39;, charset=&#39;utf8&#39;, use_unicode=True)</code> 直接定义在函数中，如果不经常改动的话，可以把相关信息放到 <code>settings.py</code> 中进行调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MYSQL_HOST = <span class="string">'127.0.0.1'</span></div><div class="line">MYSQL_DBNAME = <span class="string">'article_spider'</span></div><div class="line">MYSQL_USER = <span class="string">'root'</span></div><div class="line">MYSQL_PASSWORD = <span class="string">'12'</span></div></pre></td></tr></table></figure>
<p>在 <code>pipelines.py</code> 中新建 <code>MysqlTwistedPipeline</code> ，写入如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        host = settings[<span class="string">'MYSQL_HOST'</span>]</div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>在 <code>from_settings</code> 这个类方法中，我们获取到了settings配置中的 <code>MYSQL_HOST</code> ，这个方法在Scrapy初始化的时候就会被调用，会将Scrapy的settings对象传递进来，我们在这里进行断点调试，查看是否获取到了这个对象：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffcv0p0d3xj31kw0ejqcr.jpg" alt=""></p>
<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1ffcv1i1po9j319w04kgmy.jpg" alt=""></p>
<p>发现在settings的attributes这个字典中，确实有我们定义的各种属性。</p>
<p>我们的异步操作需要引入twisted，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</div><div class="line"><span class="keyword">import</span> MySQLdb</div><div class="line"><span class="keyword">import</span> MySQLdb.cursors</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></div><div class="line">        self.dbpool = dbpool</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></div><div class="line">        dbparams = dict(</div><div class="line">            host=settings[<span class="string">'MYSQL_HOST'</span>],</div><div class="line">            db=settings[<span class="string">'MYSQL_DBNAME'</span>],</div><div class="line">            user=settings[<span class="string">'MYSQL_USER'</span>],</div><div class="line">            passwd=settings[<span class="string">'MYSQL_PASSWORD'</span>],</div><div class="line">            charset=<span class="string">'utf8'</span>,</div><div class="line">            cursorclass=MySQLdb.cursors.DictCursor,</div><div class="line">            use_unicode=<span class="keyword">True</span>,</div><div class="line">        )</div><div class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparams)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> cls(dbpool)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="comment"># 使用twisted将mysql插入变成异步执行</span></div><div class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</div><div class="line">        query.addErrback(self.handle_error)  <span class="comment"># 处理异常</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure)</span>:</span></div><div class="line">        <span class="comment"># 处理异步插入异常</span></div><div class="line">        print(failure)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></div><div class="line">        <span class="comment"># 执行具体的插入</span></div><div class="line">        insert_sql = <span class="string">"""</span></div><div class="line">                            insert into jobbole_article(title, url, create_date, fav_nums)</div><div class="line">                            VALUES (%s, %s, %s, %s)</div><div class="line">                """</div><div class="line">        cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>], item[<span class="string">"create_date"</span>], item[<span class="string">"fav_nums"</span>]))</div></pre></td></tr></table></figure>
<p>我们在使用时，绝大部分代码无须变动，只要修改 <code>do_insert</code> 方法中的插入内容，以及自己的信息即可。</p>
<p>在数据量不大时，用同步插入即可。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将数据保存到MySQL数据库，需要用到 &lt;code&gt;mysqlclient&lt;/code&gt; 模块，需要在我们的虚拟环境中用 &lt;code&gt;pip&lt;/code&gt; 进行安装。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Feed Exports</title>
    <link href="http://yoursite.com/2017/05/06/scrapy-feed-exports/"/>
    <id>http://yoursite.com/2017/05/06/scrapy-feed-exports/</id>
    <published>2017-05-06T06:18:54.000Z</published>
    <updated>2017-05-06T14:46:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。</p>
<p>Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。</p>
<a id="more"></a>
<h2 id="序列化方式（serialization-format）"><a href="#序列化方式（serialization-format）" class="headerlink" title="序列化方式（serialization format）"></a><strong>序列化方式（serialization format）</strong></h2><p>feed 输出使用到了 Item exporters 。其自带支持的类型有:</p>
<ul>
<li>JSON</li>
<li>JSON lines</li>
<li>CSV</li>
<li>XML</li>
</ul>
<p>也可以通过 FEED_EXPORTERS 设置扩展支持的属性。</p>
<p>在 <code>exporters.py</code> 中可以看到所有的 Item exporters：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffbz5887gzj30x603qdge.jpg" alt=""></p>
<p>下表对主要的 Item exporters进行简要的介绍：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>FEED_FORMAT</th>
<th>使用的 exporter</th>
</tr>
</thead>
<tbody>
<tr>
<td>JSON</td>
<td>json</td>
<td>JsonItemExporter</td>
</tr>
<tr>
<td>JSON lines</td>
<td>jsonlines</td>
<td>JsonLinesItemExporter</td>
</tr>
<tr>
<td>CSV</td>
<td>csv</td>
<td>CsvItemExporter</td>
</tr>
<tr>
<td>XML</td>
<td>xml</td>
<td>XmlItemExporter</td>
</tr>
<tr>
<td>Pickle</td>
<td>pickle</td>
<td>PickleItemExporter</td>
</tr>
<tr>
<td>Marshal</td>
<td>marshal</td>
<td>MarshalItemExporter</td>
</tr>
</tbody>
</table>
<h2 id="存储（Storages）"><a href="#存储（Storages）" class="headerlink" title="存储（Storages）"></a><strong>存储（Storages）</strong></h2><p>使用 feed 输出时您可以通过使用 <a href="http://en.wikipedia.org/wiki/Uniform_Resource_Identifier" target="_blank" rel="external">URI</a>（通过 FEED_URI 设置）来定义存储端。feed 输出支持 URI 方式支持的多种存储后端类型。</p>
<p>自带支持的存储后端有：</p>
<ul>
<li>本地文件系统</li>
<li>FTP</li>
<li>S3（需要 boto）</li>
<li>标准输出</li>
</ul>
<p>有些存储后端会因所需的外部库未安装而不可用。例如，S3 只有在 boto 库安装的情况下才可使用。</p>
<h2 id="存储-URI-参数"><a href="#存储-URI-参数" class="headerlink" title="存储 URI 参数"></a><strong>存储 URI 参数</strong></h2><p>存储 URI 也包含参数。当 feed 被创建时这些参数可以被覆盖：</p>
<ul>
<li><code>%(time)s</code> - 当 feed 被创建时被 timestamp 覆盖</li>
<li><code>%(name)s</code> - 被 spider 的名字覆盖</li>
</ul>
<p>其他命名的参数会被 spider 同名的属性所覆盖。例如， 当 feed 被创建时，<code>%(site_id)s</code> 将会被 <code>spider.site_id</code> 属性所覆盖。</p>
<p>下面用一些例子来说明:</p>
<ul>
<li>存储在 FTP，每个 spider 一个目录: <ul>
<li><code>ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</code></li>
</ul>
</li>
<li>存储在 S3，每一个 spider 一个目录:<ul>
<li><code>s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</code></li>
</ul>
</li>
</ul>
<h2 id="存储后端（Storage-backends）"><a href="#存储后端（Storage-backends）" class="headerlink" title="存储后端（Storage backends）"></a><strong>存储后端（Storage backends）</strong></h2><h3 id="本地文件系统"><a href="#本地文件系统" class="headerlink" title="本地文件系统"></a><strong>本地文件系统</strong></h3><p>将 feed 存储在本地系统。</p>
<ul>
<li>URI scheme: <code>file</code></li>
<li>URI 样例: <code>file:///tmp/export.csv</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<p>注意: (只有)存储在本地文件系统时，您可以指定一个绝对路径 /tmp/export.csv 并忽略协议(scheme)。不过这 仅仅只能在 Unix 系统中工作。</p>
<h3 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a><strong>FTP</strong></h3><p>将 feed 存储在 FTP 服务器。</p>
<ul>
<li>URI scheme: <code>ftp</code></li>
<li>URI 样例: <code>ftp://user:pass@ftp.example.com/path/to/export.csv</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<h3 id="S3"><a href="#S3" class="headerlink" title="S3"></a><strong>S3</strong></h3><p>将 feed 存储在 Amazon S3 。 </p>
<ul>
<li>URI scheme: s3 </li>
<li>URI 样例: <ul>
<li>s3://mybucket/path/to/export.csv </li>
<li>s3://aws_key:aws_secret@mybucket/path/to/export.csv </li>
</ul>
</li>
<li>需要的外部依赖库: <code>boto</code></li>
</ul>
<p>您可以通过在 URI 中传递 user/pass 来完成 AWS 认证，或者也可以通过下列的设置来完成: </p>
<p>AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY</p>
<h3 id="标准输出"><a href="#标准输出" class="headerlink" title="标准输出"></a><strong>标准输出</strong></h3><p>feed 输出到 Scrapy 进程的标准输出。</p>
<ul>
<li>URI scheme: <code>stdout</code></li>
<li>URI 样例: <code>stdout</code></li>
<li>需要的外部依赖库: <code>none</code></li>
</ul>
<h2 id="设定（Settings）"><a href="#设定（Settings）" class="headerlink" title="设定（Settings）"></a><strong>设定（Settings）</strong></h2><p>这些是配置 feed 输出的设定:</p>
<ul>
<li>FEED_URI (必须)</li>
<li>FEED_FORMAT</li>
<li>FEED_STORAGES</li>
<li>FEED_EXPORTERS</li>
<li>FEED_STORE_EMPTY</li>
</ul>
<h3 id="FEED-URI"><a href="#FEED-URI" class="headerlink" title="FEED_URI"></a><strong>FEED_URI</strong></h3><p>Default: <code>None</code> </p>
<p>输出 feed 的 URI。支持的 URI 协议请参见存储后端。</p>
<p> 为了启用 feed 输出，该设定是必须的。</p>
<h3 id="FEED-FORMAT"><a href="#FEED-FORMAT" class="headerlink" title="FEED_FORMAT"></a><strong>FEED_FORMAT</strong></h3><p>输出 feed 的序列化格式。可用的值请参见序列化方式（Serialization formats）。</p>
<h3 id="FEED-STORE-EMPTY"><a href="#FEED-STORE-EMPTY" class="headerlink" title="FEED_STORE_EMPTY"></a><strong>FEED_STORE_EMPTY</strong></h3><p>Default: <code>False</code> </p>
<p>是否输出空 feed（没有 item 的 feed）。</p>
<h3 id="FEED-STORAGES"><a href="#FEED-STORAGES" class="headerlink" title="FEED_STORAGES"></a><strong>FEED_STORAGES</strong></h3><p>Default: <code>{}</code> </p>
<p>包含项目支持的额外 feed 存储端的字典。 字典的键（key）是 URI 协议（scheme），值是存储类（storage class）的路径。</p>
<h3 id="FEED-STORAGES-BASE"><a href="#FEED-STORAGES-BASE" class="headerlink" title="FEED_STORAGES_BASE"></a><strong>FEED_STORAGES_BASE</strong></h3><p>Default:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line"><span class="string">''</span>: <span class="string">'scrapy.contrib.feedexport.FileFeedStorage'</span>, </div><div class="line"><span class="string">'file'</span>: <span class="string">'scrapy.contrib.feedexport.FileFeedStorage'</span>, </div><div class="line"><span class="string">'stdout'</span>: <span class="string">'scrapy.contrib.feedexport.StdoutFeedStorage'</span>, </div><div class="line"><span class="string">'s3'</span>: <span class="string">'scrapy.contrib.feedexport.S3FeedStorage'</span>, </div><div class="line"><span class="string">'ftp'</span>: <span class="string">'scrapy.contrib.feedexport.FTPFeedStorage'</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>包含 Scrapy 内置支持的 feed 存储端的字典。</p>
<h3 id="FEED-EXPORTERS"><a href="#FEED-EXPORTERS" class="headerlink" title="FEED_EXPORTERS"></a><strong>FEED_EXPORTERS</strong></h3><p>Default: <code>{}</code> </p>
<p>包含项目支持的额外输出器（exporter）的字典。 该字典的键（key）是 URI 协议（scheme），值是 Item 输出器（exp orter）类的路径。</p>
<h3 id="FEED-EXPORTERS-BASE"><a href="#FEED-EXPORTERS-BASE" class="headerlink" title="FEED_EXPORTERS_BASE"></a><strong>FEED_EXPORTERS_BASE</strong></h3><p>Default:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">FEED_EXPORTERS_BASE = &#123; </div><div class="line">    <span class="string">'json'</span>: <span class="string">'scrapy.contrib.exporter.JsonItemExporter'</span>, </div><div class="line">    <span class="string">'jsonlines'</span>: <span class="string">'scrapy.contrib.exporter.JsonLinesItemExporter'</span>, </div><div class="line">    <span class="string">'csv'</span>: <span class="string">'scrapy.contrib.exporter.CsvItemExporter'</span>, </div><div class="line">    <span class="string">'xml'</span>: <span class="string">'scrapy.contrib.exporter.XmlItemExporter'</span>, </div><div class="line">    <span class="string">'marshal'</span>: <span class="string">'scrapy.contrib.exporter.MarshalItemExporter'</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>包含 Scrapy 内置支持的 feed 输出器（exporter）的字典。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。&lt;/p&gt;
&lt;p&gt;Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Pipeline</title>
    <link href="http://yoursite.com/2017/05/06/scrapy-item-pipeline/"/>
    <id>http://yoursite.com/2017/05/06/scrapy-item-pipeline/</id>
    <published>2017-05-06T06:18:54.000Z</published>
    <updated>2017-05-06T13:35:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。</p>
<p>每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。</p>
<a id="more"></a>
<p>以下是 item pipeline 的一些典型应用：</p>
<ul>
<li>清理 HTML 数据</li>
<li>验证爬取的数据（检查 item 包含某些字段）</li>
<li>查重（并丢弃）</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<h2 id="编写自定义的-Pipeline"><a href="#编写自定义的-Pipeline" class="headerlink" title="编写自定义的 Pipeline"></a><strong>编写自定义的 Pipeline</strong></h2><p>定义一个Python类，然后实现方法 <code>process_item(self, item, spider)</code> 即可，返回一个字典或Item，或者抛出 <code>DropItem</code> 异常丢弃这个Item。</p>
<p>除此之外，还可以实现以下几个方法：</p>
<ul>
<li><code>open_spider(self, spider)</code> ：当spider被开启时，这个方法被调用</li>
<li><code>close_spider(self, spider)</code> ：当spider被关闭时，这个方法被调用</li>
<li><code>from_crawler(cls, crawler)</code> ： 可访问核心组件比如配置和信号，并注册钩子函数到Scrapy中</li>
</ul>
<h2 id="Item-Pipeline示例"><a href="#Item-Pipeline示例" class="headerlink" title="Item Pipeline示例"></a><strong>Item Pipeline示例</strong></h2><h3 id="价格验证"><a href="#价格验证" class="headerlink" title="价格验证"></a><strong>价格验证</strong></h3><p>让我们来看一下以下这个假设的 pipeline，它为那些不含税（<code>price_excludes_vat</code> 属性）的item调整了price属性，同时丢弃了那些没有价格item：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">	vat_factor = <span class="number">1.15</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span> </div><div class="line">        <span class="keyword">if</span> item[<span class="string">'price'</span>]: </div><div class="line">            <span class="keyword">if</span> item[<span class="string">'price_excludes_vat'</span>]:</div><div class="line">				item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor </div><div class="line">            <span class="keyword">return</span> item </div><div class="line">        <span class="keyword">else</span>: </div><div class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</div></pre></td></tr></table></figure>
<h3 id="将item写入Json文件"><a href="#将item写入Json文件" class="headerlink" title="将item写入Json文件"></a><strong>将item写入Json文件</strong></h3><p>下面的这个Pipeline将所有的item写入到一个单独的json文件，，每行包含一个序列化 为 JSON 格式的 item:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'wb'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></div><div class="line">        self.file.write(line)</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>JsonWriterPipeline 的目的只是为了介绍怎样编写 item pipeline，如果你想要将所有爬取的 item 都保存到同 一个 JSON 文件， 你需要使用 Feed exports 。</p>
<h3 id="将item存储到MongoDB中"><a href="#将item存储到MongoDB中" class="headerlink" title="将item存储到MongoDB中"></a>将item存储到MongoDB中</h3><p>这个例子使用<a href="http://api.mongodb.org/python/current/" target="_blank" rel="external">pymongo</a>来演示怎样讲item保存到MongoDB中。 MongoDB的地址和数据库名在配置 <code>settings.py</code> 中指定，这个例子主要是向你展示怎样使用<code>from_crawler()</code>方法，以及如何清理资源。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    collection_name = <span class="string">'scrapy_items'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></div><div class="line">        self.mongo_uri = mongo_uri</div><div class="line">        self.mongo_db = mongo_db</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></div><div class="line">        <span class="keyword">return</span> cls(</div><div class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</div><div class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>, <span class="string">'items'</span>)</div><div class="line">        )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</div><div class="line">        self.db = self.client[self.mongo_db]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        self.db[self.collection_name].insert(dict(item))</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a><strong>去重</strong></h3><p>一个用于去重的过滤器，丢弃那些已经被处理过的 item。让我们假设我们的 item 有一个唯一的 id，但是我们 sp ider 返回的多个 item 中包含有相同的 id:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span> </div><div class="line">        self.ids_seen = set()</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span> </div><div class="line">        <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">in</span> self.ids_seen:</div><div class="line">			<span class="keyword">raise</span> DropItem(<span class="string">"Duplicate item found: %s"</span> % item) </div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.ids_seen.add(item[<span class="string">'id'</span>]) </div><div class="line">            <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<h2 id="启用一个-Item-Pipeline-组件"><a href="#启用一个-Item-Pipeline-组件" class="headerlink" title="启用一个 Item Pipeline 组件"></a>启用一个 Item Pipeline 组件</h2><p>为了启用一个 Item Pipeline 组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123; </div><div class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>, </div><div class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>, </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分配给每个类的整型值，确定了他们运行的顺序，item 按数字从低到高的顺序，通过 pipeline，通常将这些数字 定义在 0-1000 范围内。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。&lt;/p&gt;
&lt;p&gt;每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——items设计</title>
    <link href="http://yoursite.com/2017/05/05/scrapy-items-design/"/>
    <id>http://yoursite.com/2017/05/05/scrapy-items-design/</id>
    <published>2017-05-05T06:18:54.000Z</published>
    <updated>2017-05-06T10:36:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 <code>scrapy.Field()</code> 。</p>
<a id="more"></a>
<h2 id="定义-items"><a href="#定义-items" class="headerlink" title="定义 items"></a><strong>定义 items</strong></h2><p>由于需要添加一个封面图，对上面的爬虫添加一个 <code>front_image_url</code> 字段对 <code>parse</code> 函数进行修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析</div><div class="line">    2. 获取下一页的url并交给scrapy进行下载</div><div class="line">    :param response: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># 解析列表页中的所有文章url并交给解析函数进行具体字段的解析</span></div><div class="line">    post_nodes = response.css(<span class="string">"#archive .floated-thumb .post-thumb a"</span>)</div><div class="line">    <span class="keyword">for</span> post_node <span class="keyword">in</span> post_nodes:</div><div class="line">        image_url = post_node.css(<span class="string">"img::attr(src)"</span>).extract_first(<span class="string">""</span>)</div><div class="line">        post_url = post_node.css(<span class="string">"::attr(href)"</span>).extract_first(<span class="string">""</span>)</div><div class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), meta=&#123;<span class="string">"front_image_url"</span>: image_url&#125;, callback=self.parse_detail)</div></pre></td></tr></table></figure>
<p>其中的 <code>meta</code> 字段是传递值的方法。在调试时返回的 <code>response</code> 中会出现 <code>meta</code> 的内容，它是一个字典，故在传递时可以直接通过 <code>response.meta[&#39;front_image_url&#39;]</code> 进行引用（也可以使用get的方法，附默认值防止出现异常）：</p>
<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1ffaslwq4mqj30l1058t9b.jpg" alt=""></p>
<p>在 <code>items.py</code> 文件中，定义一个item并声明其字段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    title = scrapy.Field()</div><div class="line">    create_date = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    url_object_id = scrapy.Field()</div><div class="line">    front_image_url = scrapy.Field()</div><div class="line">    front_image_path = scrapy.Field()</div><div class="line">    praise_nums = scrapy.Field()</div><div class="line">    comment_nums = scrapy.Field()</div><div class="line">    fav_nums = scrapy.Field()</div><div class="line">    tags = scrapy.Field()</div><div class="line">    content = scrapy.Field()</div></pre></td></tr></table></figure>
<p>在 <code>jobbole.py</code> 中添加 <code>from ArticleSpider.items import JobBoleArticleItem</code> 对item进行引用，然后在 <code>parse_detail</code> 中进行初始化 <code>article_item = JobBoleArticleItem()</code> ，之后将获取到的字段内容存入初始化的item中，最终代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></div><div class="line">    article_item = JobBoleArticleItem()</div><div class="line"></div><div class="line">    <span class="comment"># 通过css选择器提取字段</span></div><div class="line">    front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)  <span class="comment"># 文章封面图</span></div><div class="line">    title = response.css(<span class="string">".entry-header h1::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</div><div class="line">    praise_nums = int(response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>])</div><div class="line">    fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        fav_nums = <span class="number">0</span></div><div class="line">    comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">        comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        comment_nums = <span class="number">0</span></div><div class="line">    content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</div><div class="line">    tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</div><div class="line">    tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">    tags = <span class="string">","</span>.join(tag_list)</div><div class="line"></div><div class="line">    article_item[<span class="string">"title"</span>] = title</div><div class="line">    article_item[<span class="string">"url"</span>] = response.url</div><div class="line">    article_item[<span class="string">"create_date"</span>] = create_date</div><div class="line">    article_item[<span class="string">"front_image_url"</span>] = front_image_url</div><div class="line">    article_item[<span class="string">"praise_nums"</span>] = praise_nums</div><div class="line">    article_item[<span class="string">"comment_nums"</span>] = comment_nums</div><div class="line">    article_item[<span class="string">"fav_nums"</span>] = fav_nums</div><div class="line">    article_item[<span class="string">"content"</span>] = content</div><div class="line">    article_item[<span class="string">"tags"</span>] = tags</div><div class="line">    </div><div class="line">    <span class="keyword">yield</span> article_item</div></pre></td></tr></table></figure>
<p>其中 <code>yield article_item</code> 会自动提交到 <code>settings</code> 中的 <code>ITEM_PIPELINES</code> 进行处理。<br>此时在 <code>pipelines.py</code> 中设置断点调试，可以看到 <code>article_item</code> 中的值已经传递到这里了。</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffasxu8jauj31ge0my46y.jpg" alt=""></p>
<h2 id="自定义-Pipelines"><a href="#自定义-Pipelines" class="headerlink" title="自定义 Pipelines"></a><strong>自定义 Pipelines</strong></h2><p>在 <code>settings.py</code> 中有一个ITEM_PIPELINES的选项，把它的注释去掉增加下载图片的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Configure item pipelines</span></div><div class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    <span class="string">'ArticleSpider.pipelines.ArticlespiderPipeline'</span>: <span class="number">300</span>,</div><div class="line">    <span class="string">'scrapy.pipelines.images.ImagesPipeline'</span>: <span class="number">1</span>,</div><div class="line">&#125;</div><div class="line">IMAGES_URLS_FIELD = <span class="string">"front_image_url"</span></div><div class="line">project_dir = os.path.abspath(os.path.dirname(__file__))</div><div class="line">IMAGES_STORE = os.path.join(project_dir, <span class="string">'images'</span>)</div></pre></td></tr></table></figure>
<p>上面的代码启用了下载图片piplines，并定义了存储地址及想要存储的图片地址。<br>在settings.py同级目录下建立文件夹 <code>images</code> 用来保存图片。当运行爬虫时，图片就会自动下载图片并保存到本地。<br>如果想要得到存储的图片路径的话，需要自定义pipelines。</p>
<p>首先，在 <code>pipeines.py</code> 中引入 <code>from scrapy.pipelines.images import ImagesPipeline</code> ， 然后自定义一个pipeline对ImagesPipeline进行重载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>进行断点调试，查看results中的信息：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1ffblwnn93tj31aq0t2wmq.jpg" alt=""></p>
<p>调试结果中，results是一个list，第一个值是一个bool值表示图片是否获取成功，第二个值是一个字典，保存了图片路径，图片地址等信息。</p>
<p>最终自定义的pipeline代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></div><div class="line">        <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</div><div class="line">            image_file_path = value[<span class="string">"path"</span>]</div><div class="line">        item[<span class="string">"front_image_path"</span>] = image_file_path</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<p>上面代码得到image_path保存到 <code>item[&quot;front_image_path&quot;]</code> 中并返回，这时会根据pipelines的顺序进行下一个pipelines进行处理。通过断点调试可以得到想要的结果。</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1ffbpccuulfj31kw0w7dqk.jpg" alt=""></p>
<h2 id="完善-items-获取"><a href="#完善-items-获取" class="headerlink" title="完善 items 获取"></a><strong>完善 items 获取</strong></h2><p>对之前定义的items中的 <code>url_object_id</code> 字段，需要对url进行md5处理，因此在 <code>items.py</code> 同级目录下新建一个名为 <code>utils</code> 的 <code>python package</code>，新建 <code>common.py</code> ，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> hashlib</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></div><div class="line">    <span class="keyword">if</span> isinstance(url, str):</div><div class="line">        url = url.encode(<span class="string">"utf-8"</span>)</div><div class="line">    m = hashlib.md5()</div><div class="line">    m.update(url)</div><div class="line">    <span class="keyword">return</span> m.hexdigest()</div></pre></td></tr></table></figure>
<p>然后在 <code>jobbole.py</code> 下引入 <code>from ArticleSpider.utils.common import get_md5</code> ，在item内容填充时加上 <code>article_item[&quot;url_object_id&quot;] = get_md5(response.url)</code> 即可。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 &lt;code&gt;scrapy.Field()&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Selectors</title>
    <link href="http://yoursite.com/2017/05/05/scrapy-selectors/"/>
    <id>http://yoursite.com/2017/05/05/scrapy-selectors/</id>
    <published>2017-05-05T06:18:54.000Z</published>
    <updated>2017-05-05T12:51:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： </p>
<ul>
<li><a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">BeautifulSoup</a> 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 </li>
<li><a href="http://lxml.de/index.html" target="_blank" rel="external">lxml</a> 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。</li>
</ul>
<p>Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。</p>
<a id="more"></a>
<p>XPath 是一门用来在 XML 文件中选择节点的语言，也可以用在 HTML 上。</p>
<p>CSS 是一门将 HTML 文档样式化的语言。选择器由它定义，并与特定的 HTML 元素的样式相关连。 </p>
<p>Scrapy 选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。 </p>
<p>本文解释了选择器如何工作，并描述了相应的 API。不同于 lxml API 的臃肿，该 API 短小而简洁。这是因为 lxml 库除了用来选择标记化文档外，还可以用到许多任务上。</p>
<h2 id="使用选择器"><a href="#使用选择器" class="headerlink" title="使用选择器"></a><strong>使用选择器</strong></h2><h3 id="构造选择器"><a href="#构造选择器" class="headerlink" title="构造选择器"></a><strong>构造选择器</strong></h3><p>Scrapy selectors是 <code>Selector</code> 类的实例，通过传入 text 或 <code>TextResponse</code> 来创建，它自动根据传入的类型选择解析规则（XML or HTML）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from scrapy.selector import Selector </div><div class="line">&gt;&gt;&gt; from scrapy.http import HtmlResponse</div></pre></td></tr></table></figure>
<p>以文字构造（都以 xpath 和 css 两种方法解析字段内容，加深理解）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; body = &apos;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&apos;</div><div class="line">&gt;&gt;&gt; Selector(text=body).xpath(&quot;//span/text()&quot;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; Selector(text=body).css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<p>以 response 构造：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response = HtmlResponse(url=&apos;http://example.com&apos;, body=body, encoding=&apos;utf-8&apos;)</div><div class="line">&gt;&gt;&gt; Selector(response=response).xpath(&apos;//span/text()&apos;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; Selector(response=response).css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<p>response 对象以 <em>.selector</em> 属性提供了一个 selector ， 可以随时使用该快捷方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.selector.xpath(&apos;//span/text()&apos;).extract()</div><div class="line">[&apos;good&apos;]</div><div class="line">&gt;&gt;&gt; response.selector.css(&quot;html body span::text&quot;).extract()</div><div class="line">[&apos;good&apos;]</div></pre></td></tr></table></figure>
<h3 id="使用选择器-1"><a href="#使用选择器-1" class="headerlink" title="使用选择器"></a><strong>使用选择器</strong></h3><p>我们将使用 Scrapy shell （提供交互测试）和位于 Scrapy 文档服务器的一个样例页面，来解释如何使用选择器：</p>
<p><code>http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</code></p>
<p>该页面源码如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></div><div class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">base</span> <span class="attr">href</span>=<span class="string">'http://example.com/'</span> /&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>Example website<span class="tag">&lt;/<span class="name">title</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">'images'</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image1.html'</span>&gt;</span>Name: My image 1 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image1_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image2.html'</span>&gt;</span>Name: My image 2 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image2_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image3.html'</span>&gt;</span>Name: My image 3 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image3_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image4.html'</span>&gt;</span>Name: My image 4 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image4_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image5.html'</span>&gt;</span>Name: My image 5 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image5_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<p>首先，打开 scrapy shell，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</div></pre></td></tr></table></figure>
<p>当 shell 载入后，您将获得名为 <code>response</code> 的 shell 变量，其为响应的 response，并且在其 <code>response.selector</code> 属性上绑定了一个 selector。</p>
<p>因为我们处理的是 HTML，选择器将自动使用 HTML 语法分析。 那么，通过查看该页面的源码，我们构建一个 XPath 来选择 title 标签内的文字:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.selector.xpath(&quot;//title/text()&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</div></pre></td></tr></table></figure>
<p>由于在 response 中使用 XPath、CSS 查询十分普遍，因此，Scrapy 提供了两个实用的快捷方式：response.xpath() 及 response.css() ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(&quot;title::text&quot;)</div><div class="line">&gt;&gt;&gt; [&lt;Selector xpath=&apos;descendant-or-self::title/text()&apos; data=&apos;Example website&apos;&gt;]</div></pre></td></tr></table></figure>
<p>现在我们将得到根 URL（base URL）和一些图片链接:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//base/@href'</span>).extract() </div><div class="line">[<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'base::attr(href)'</span>).extract() </div><div class="line">[<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//a[contains(@href, "image")]/@href'</span>).extract() </div><div class="line">[<span class="string">'image1.html'</span>, <span class="string">'image2.html'</span>, <span class="string">'image3.html'</span>, <span class="string">'image4.html'</span>, <span class="string">'image5.html'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'a[href*=image]::attr(href)'</span>).extract() </div><div class="line">[<span class="string">'image1.html'</span>, <span class="string">'image2.html'</span>, <span class="string">'image3.html'</span>, <span class="string">'image4.html'</span>, <span class="string">'image5.html'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.xpath(<span class="string">'//a[contains(@href, "image")]/img/@src'</span>).extract() </div><div class="line">[<span class="string">'image1_thumb.jpg'</span>, <span class="string">'image2_thumb.jpg'</span>, <span class="string">'image3_thumb.jpg'</span>, <span class="string">'image4_thumb.jpg'</span>, <span class="string">'image5_thumb.jpg'</span>]</div><div class="line"></div><div class="line">&gt;&gt;&gt; response.css(<span class="string">'a[href*=image] img::attr(src)'</span>).extract() </div><div class="line">[<span class="string">'image1_thumb.jpg'</span>, <span class="string">'image2_thumb.jpg'</span>, <span class="string">'image3_thumb.jpg'</span>, <span class="string">'image4_thumb.jpg'</span>, <span class="string">'image5_thumb.jpg'</span>]</div></pre></td></tr></table></figure>
<h3 id="嵌套选择器"><a href="#嵌套选择器" class="headerlink" title="嵌套选择器"></a><strong>嵌套选择器</strong></h3><p>选择器方法（ .xpath() or .css() ）返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; links = response.xpath(&quot;//a[contains(@href,&apos;image&apos;)]&quot;)</div><div class="line"></div><div class="line">&gt;&gt;&gt; links.extract()</div><div class="line">[&apos;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</div><div class="line"> &apos;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;]</div><div class="line"></div><div class="line">&gt;&gt;&gt; for index, link in enumerate(links):</div><div class="line">...:     args = (index, link.xpath(&apos;@href&apos;).extract(), link.xpath(&apos;img/@src&apos;).extract())</div><div class="line">...:     print(&apos;Link number %d points to url %s and image %s&apos; % args)</div><div class="line">...:</div><div class="line">Link number 0 points to url [&apos;image1.html&apos;] and image [&apos;image1_thumb.jpg&apos;]</div><div class="line">Link number 1 points to url [&apos;image2.html&apos;] and image [&apos;image2_thumb.jpg&apos;]</div><div class="line">Link number 2 points to url [&apos;image3.html&apos;] and image [&apos;image3_thumb.jpg&apos;]</div><div class="line">Link number 3 points to url [&apos;image4.html&apos;] and image [&apos;image4_thumb.jpg&apos;]</div><div class="line">Link number 4 points to url [&apos;image5.html&apos;] and image [&apos;image5_thumb.jpg&apos;]</div></pre></td></tr></table></figure>
<h3 id="结合正则表达式使用选择器"><a href="#结合正则表达式使用选择器" class="headerlink" title="结合正则表达式使用选择器"></a><strong>结合正则表达式使用选择器</strong></h3><p>Selector 也有一个 .re() 方法，用来通过正则表达式来提取数据。然而，不同于使用 .xpath() 或者 .css() 方法，.re()方法返回 unicode 字符串的列表。所以你无法构造嵌套式的 .re() 调用。 </p>
<p>下面是一个例子，从上面的 html 源码中提取图像名字：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.xpath(&quot;//a[contains(@href, &apos;image&apos;)]/text()&quot;).re(r&apos;Name:\s*(.*)&apos;)</div><div class="line">[&apos;My image 1 &apos;, &apos;My image 2 &apos;, &apos;My image 3 &apos;, &apos;My image 4 &apos;, &apos;My image 5 &apos;]</div></pre></td></tr></table></figure>
<h3 id="使用相对-XPaths"><a href="#使用相对-XPaths" class="headerlink" title="使用相对 XPaths"></a><strong>使用相对 XPaths</strong></h3><p>记住如果你使用嵌套的选择器，并使用起始为 <code>/</code> 的 XPath，那么该 XPath 将对文档使用绝对路径，而且对于你调用的 <code>Selector</code> 不是相对路径。</p>
<p>比如，假设你想提取在 <code>&lt;div&gt;</code> 元素中的所有 <code>&lt;p&gt;</code> 元素。首先，你将先得到所有的 <code>&lt;div&gt;</code> 元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; divs = response.xpath(&quot;//div&quot;)</div></pre></td></tr></table></figure>
<p>开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 <code>&lt;div&gt;</code> 元素内部提取所有的 <code>&lt;p&gt;</code> 元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;//p&apos;): # this is wrong - gets all &lt;p&gt; from the whole document </div><div class="line">...		print p.extract()</div></pre></td></tr></table></figure>
<p>下面是比较合适的处理方法(注意 .//p XPath 的点前缀)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;.//p&apos;): # extracts all &lt;p&gt; inside </div><div class="line">... 	print p.extract()</div></pre></td></tr></table></figure>
<p>另一种常见的情况将是提取所有直系 <code>&lt;p&gt;</code> 的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;p&apos;): </div><div class="line">... 	print p.extract()</div></pre></td></tr></table></figure>
<h3 id="使用-EXSLT-扩展"><a href="#使用-EXSLT-扩展" class="headerlink" title="使用 EXSLT 扩展"></a><strong>使用 EXSLT 扩展</strong></h3><p>因建于 lxml 之上，Scrapy 选择器也支持一些 EXSLT 扩展，可以在 XPath 表达式中使用这些预先制定的命名空间：</p>
<table>
<thead>
<tr>
<th>前缀</th>
<th>命名空间</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>re</td>
<td><a href="http://exslt.org/regular-expressions" target="_blank" rel="external">http://exslt.org/regular-expressions</a></td>
<td>正则表达式</td>
</tr>
<tr>
<td>set</td>
<td><a href="http://exslt.org/sets" target="_blank" rel="external">http://exslt.org/sets</a></td>
<td>集合操作</td>
</tr>
</tbody>
</table>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a><strong>正则表达式</strong></h3><p>例如在XPath的 <code>starts-with()</code> 或 <code>contains()</code> 无法满足需求时， <code>test()</code> 函数可以非常有用。</p>
<p>例如在列表中选择有”class”元素且结尾为一个数字的链接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from scrapy import Selector</div><div class="line">&gt;&gt;&gt; doc = &quot;&quot;&quot;</div><div class="line">... &lt;div&gt;</div><div class="line">...      &lt;ul&gt;</div><div class="line">...          &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...          &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</div><div class="line">...       &lt;/ul&gt;</div><div class="line">... &lt;/div&gt;</div><div class="line">...  &quot;&quot;&quot;</div><div class="line">&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)</div><div class="line">&gt;&gt;&gt; sel.xpath(&quot;//li//@href&quot;).extract()</div><div class="line">[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link3.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]</div><div class="line">&gt;&gt;&gt; sel.xpath(&quot;//li[re:test(@class, &apos;item-\d$&apos;)]//@href&quot;).extract()</div><div class="line">[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]</div></pre></td></tr></table></figure>
<p>注意：C语言库 <code>libxslt</code> 不原生支持EXSLT正则表达式，因此 lxml 在实现时使用了Python  <code>re</code>  模块的钩子。 因此，在 XPath 表达式中使用 regexp 函数可能会牺牲少量的性能。</p>
<h3 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a><strong>集合操作</strong></h3><p>集合操作可以方便地用于在提取文字元素前从文档树中去除一些部分。 </p>
<p>例如使用 itemscopes 组和对应的 itemprops 来提取微数据（来自 <a href="http://schema.org/Product" target="_blank" rel="external">http://schema.org/Product</a> 的样本内容）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; doc = &quot;&quot;&quot;</div><div class="line">... &lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</div><div class="line">...   &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</div><div class="line">...   ![](kenmore-microwave-17in.jpg)</div><div class="line">...   &lt;div itemprop=&quot;aggregateRating&quot;</div><div class="line">...     itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</div><div class="line">...    Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</div><div class="line">...    based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</div><div class="line">...     &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   Product description:</div><div class="line">...   &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</div><div class="line">...   Has six preset cooking categories and convenience features like</div><div class="line">...   Add-A-Minute and Child Lock.&lt;/span&gt;</div><div class="line">...</div><div class="line">...   Customer reviews:</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</div><div class="line">...     by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</div><div class="line">...     &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</div><div class="line">...     &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</div><div class="line">...       &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</div><div class="line">...       &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</div><div class="line">...       &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</div><div class="line">...     &lt;/div&gt;</div><div class="line">...     &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</div><div class="line">...     it. &lt;/span&gt;</div><div class="line">...   &lt;/div&gt;</div><div class="line">...</div><div class="line">...   &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</div><div class="line">...     &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</div><div class="line">...     by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</div><div class="line">...     &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</div><div class="line">...     &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</div><div class="line">...       &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</div><div class="line">...       &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</div><div class="line">...       &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</div><div class="line">...     &lt;/div&gt;</div><div class="line">...     &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</div><div class="line">...     fits in my apartment.&lt;/span&gt;</div><div class="line">...   &lt;/div&gt;</div><div class="line">...   ...</div><div class="line">... &lt;/div&gt;</div><div class="line">... &quot;&quot;&quot;</div><div class="line">&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)</div><div class="line">&gt;&gt;&gt; for scope in sel.xpath(&apos;//div[@itemscope]&apos;):</div><div class="line">...     print &quot;current scope:&quot;, scope.xpath(&apos;@itemtype&apos;).extract()</div><div class="line">...     props = scope.xpath(&apos;&apos;&apos;</div><div class="line">...                 set:difference(./descendant::*/@itemprop,</div><div class="line">...                                .//*[@itemscope]/*/@itemprop)&apos;&apos;&apos;)</div><div class="line">...     print &quot;    properties:&quot;, props.extract()</div><div class="line">...     print</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Product&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;aggregateRating&apos;, u&apos;offers&apos;, u&apos;description&apos;, u&apos;review&apos;, u&apos;review&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/AggregateRating&apos;]</div><div class="line">    properties: [u&apos;ratingValue&apos;, u&apos;reviewCount&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Offer&apos;]</div><div class="line">    properties: [u&apos;price&apos;, u&apos;availability&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Review&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Rating&apos;]</div><div class="line">    properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Review&apos;]</div><div class="line">    properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]</div><div class="line"></div><div class="line">current scope: [u&apos;http://schema.org/Rating&apos;]</div><div class="line">    properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]</div></pre></td></tr></table></figure>
<p>这里我们先迭代 <code>itemscope</code> 元素，对于每一个元素，我们寻找所有 <code>itemprops</code> 元素，并排除那些在另一个元素内部的元素 <code>itemscope</code> 。</p>
<h2 id="内置选择器参考"><a href="#内置选择器参考" class="headerlink" title="内置选择器参考"></a>内置选择器参考</h2><h3 id="Selector-实例"><a href="#Selector-实例" class="headerlink" title="Selector 实例"></a><strong>Selector 实例</strong></h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> scrapy.selector.Selector(response=<span class="keyword">None</span>, text=<span class="keyword">None</span>, <span class="keyword">type</span>=<span class="keyword">None</span>)</div></pre></td></tr></table></figure>
<p>一个实例<code>Selector</code>是一个包装器响应来选择其内容的某些部分。</p>
<p>response是一个<code>HtmlResponse</code>或一个<code>XmlResponse</code>将被用于选择和提取的数据对象。</p>
<p><code>text</code>是一个<code>unicode</code>字符串或<code>utf-8</code>编码的文本，当一个 <code>response</code>不可用时。使用<code>text</code>和<code>response</code>一起是未定义的行为。</p>
<p><code>type</code>定义选择器类型，它可以是<code>&quot;html&quot;</code>，<code>&quot;xml&quot;</code>或<code>None（默认）</code>。</p>
<p>如果<code>type</code>是<code>None</code>，选择器将根据<code>response</code>类型（见下文）自动选择最佳类型，或者默认<code>&quot;html&quot;</code>情况下与选项一起使用<code>text</code>。</p>
<p>如果<code>type</code>是<code>None</code>和<code>response</code>传递，选择器类型从响应类型推断如下：</p>
<ul>
<li><code>&quot;html&quot;</code>对于HtmlResponse类型</li>
<li><code>&quot;xml&quot;</code>对于XmlResponse类型</li>
<li><code>&quot;html&quot;</code>为任何其他</li>
</ul>
<p>否则，如果<code>type</code>设置，选择器类型将被强制，并且不会发生检测。</p>
<h4 id="xpath（查询）"><a href="#xpath（查询）" class="headerlink" title="xpath（查询）"></a><strong>xpath（查询）</strong></h4><p>查找与<code>xpath</code>匹配的节点<code>query</code>，并将结果作为 <code>SelectorList</code>实例将所有元素展平。列表元素也实现<code>Selector</code>接口。</p>
<p><code>query</code> 是一个包含要应用的XPATH查询的字符串。</p>
<p><strong>注意</strong></p>
<blockquote>
<p>为了方便起见，这种方法可以称为 response.xpath()</p>
</blockquote>
<h4 id="css（查询）"><a href="#css（查询）" class="headerlink" title="css（查询）"></a><strong>css（查询）</strong></h4><p>应用给定的CSS选择器并返回一个SelectorList实例。</p>
<p>query 是一个包含要应用的CSS选择器的字符串。</p>
<p>在后台，CSS查询使用cssselect库和run .xpath()方法转换为XPath查询 。</p>
<p><strong>注意</strong></p>
<blockquote>
<p>为了方便起见，该方法可以称为 response.css()</p>
</blockquote>
<h4 id="extract（）"><a href="#extract（）" class="headerlink" title="extract（）"></a><strong>extract（）</strong></h4><p>序列化并返回匹配的节点作为unicode字符串列表。编码内容的百分比未引用。</p>
<h4 id="re（regex）"><a href="#re（regex）" class="headerlink" title="re（regex）"></a><strong>re（regex）</strong></h4><p>应用给定的正则表达式并返回一个包含匹配项的unicode字符串的列表。</p>
<p><code>regex</code>可以是编译的正则表达式或将被编译为正则表达式的字符串 <code>re.compile(regex)</code></p>
<p><strong>注意</strong></p>
<blockquote>
<p>注意，re()和re_first()解码HTML实体（除\&lt;和\&amp;）。</p>
</blockquote>
<h4 id="register-namespace（prefix，uri）"><a href="#register-namespace（prefix，uri）" class="headerlink" title="register_namespace（prefix，uri）"></a><strong>register_namespace（prefix，uri）</strong></h4><p>注册在此使用的给定命名空间Selector。如果不注册命名空间，则无法从非标准命名空间中选择或提取数据。参见下面的例子。</p>
<h4 id="remove-namespaces（）"><a href="#remove-namespaces（）" class="headerlink" title="remove_namespaces（）"></a><strong>remove_namespaces（）</strong></h4><p>删除所有命名空间，允许使用无命名空间的xpaths遍历文档。参见下面的例子。</p>
<h4 id="nonzero（）"><a href="#nonzero（）" class="headerlink" title="nonzero（）"></a><strong>nonzero（）</strong></h4><p>返回True如果有选择或任何实际的内容False 除外。换句话说，a的布尔值Selector由它选择的内容给出。</p>
<h3 id="SelectorList对象"><a href="#SelectorList对象" class="headerlink" title="SelectorList对象"></a>SelectorList对象</h3><p><code>class scrapy.selector.SelectorList</code></p>
<p>本SelectorList类是内置的一个子list 类，它提供了几个方法。</p>
<h4 id="xpath（查询）-1"><a href="#xpath（查询）-1" class="headerlink" title="xpath（查询）"></a><strong>xpath（查询）</strong></h4><p>调用.xpath()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。</p>
<p>query 是同一个参数 Selector.xpath()</p>
<h4 id="css（查询）-1"><a href="#css（查询）-1" class="headerlink" title="css（查询）"></a><strong>css（查询）</strong></h4><p>调用.css()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。</p>
<p>query 是同一个参数 Selector.css()</p>
<h4 id="extract（）-1"><a href="#extract（）-1" class="headerlink" title="extract（）"></a><strong>extract（）</strong></h4><p>调用.extract()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。</p>
<h4 id="re（）"><a href="#re（）" class="headerlink" title="re（）"></a><strong>re（）</strong></h4><p>调用.re()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。</p>
<h4 id="nonzero（）-1"><a href="#nonzero（）-1" class="headerlink" title="nonzero（）"></a><strong>nonzero（）</strong></h4><p>如果列表不为空，则返回True，否则返回False。</p>
<h3 id="HTML响应的选择器示例"><a href="#HTML响应的选择器示例" class="headerlink" title="HTML响应的选择器示例"></a><strong>HTML响应的选择器示例</strong></h3><p>这里有几个Selector例子来说明几个概念。在所有情况下，我们假设已经Selector实例化了一个HtmlResponse对象，如下：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sel</span> = Selector(html_response)</div></pre></td></tr></table></figure>
<ol>
<li><code>&lt;h1&gt;</code>从HTML响应主体中选择所有元素，返回Selector对象列表 （即SelectorList对象）：</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1"</span>)</div></pre></td></tr></table></figure>
<ol>
<li><code>&lt;h1&gt;</code>从HTML响应正文中提取所有元素的文本，返回unicode字符串</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1"</span>).extract()         <span class="comment"># this includes the h1 tag</span></div><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//h1/text()"</span>).extract()  <span class="comment"># this excludes the h1 tag</span></div></pre></td></tr></table></figure>
<ol>
<li>迭代所有<code>&lt;p&gt;</code>标签并打印其类属性：</li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">for <span class="keyword">node</span> <span class="title">in</span> sel.<span class="keyword">xpath</span>(<span class="string">"//p"</span>):</div><div class="line">    print <span class="keyword">node</span>.<span class="title">xpath</span>(<span class="string">"@class"</span>).extract()</div></pre></td></tr></table></figure>
<h3 id="XML响应的选择器示例"><a href="#XML响应的选择器示例" class="headerlink" title="XML响应的选择器示例"></a><strong>XML响应的选择器示例</strong></h3><p>这里有几个例子来说明几个概念。在这两种情况下，我们假设已经Selector实例化了一个 XmlResponse对象，像这样：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sel</span> = Selector(xml_response)</div></pre></td></tr></table></figure>
<ol>
<li><product>从XML响应主体中选择所有元素，返回Selector对象列表（即SelectorList对象）：</product></li>
</ol>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sel.<span class="keyword">xpath</span>(<span class="string">"//product"</span>)</div></pre></td></tr></table></figure>
<ol>
<li>从需要注册命名空间的<a href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799" target="_blank" rel="external">Google Base XML Feed</a>中提取所有价格：</li>
</ol>
<figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">sel</span><span class="selector-class">.register_namespace</span>(<span class="string">"g"</span>, <span class="string">"http://base.google.com/ns/1.0"</span>)</div><div class="line"><span class="selector-tag">sel</span><span class="selector-class">.xpath</span>(<span class="string">"//g:price"</span>)<span class="selector-class">.extract</span>()</div></pre></td></tr></table></figure>
<h3 id="删除名称空间"><a href="#删除名称空间" class="headerlink" title="删除名称空间"></a><strong>删除名称空间</strong></h3><p>当处理抓取项目时，通常很方便地完全删除命名空间，只需处理元素名称，编写更简单/方便的XPath。你可以使用的 Selector.remove_namespaces()方法。</p>
<p>让我们展示一个例子，用GitHub博客atom feed来说明这一点。</p>
<p>首先，我们打开shell和我们想要抓取的url：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ scrapy <span class="keyword">shell</span> http<span class="variable">s:</span>//github.<span class="keyword">com</span>/blog.atom</div></pre></td></tr></table></figure>
<p>一旦在shell中，我们可以尝试选择所有<link>对象，并看到它不工作（因为Atom XML命名空间模糊了这些节点）：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;</span>&gt; response.xpath(<span class="string">"//link"</span>)</div><div class="line">[]</div></pre></td></tr></table></figure>
<p>但是一旦我们调用该Selector.remove_namespaces()方法，所有节点都可以直接通过他们的名字访问：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>response.selector.remove_namespaces()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">"//link"</span>)</div><div class="line">[&lt;Selector xpath=<span class="string">'//link'</span> data=<span class="string">u'&lt;link xmlns="http://www.w3.org/2005/Atom'</span>&gt;,</div><div class="line"> &lt;Selector xpath=<span class="string">'//link'</span> data=<span class="string">u'&lt;link xmlns="http://www.w3.org/2005/Atom'</span>&gt;,</div><div class="line"> ...</div></pre></td></tr></table></figure>
<p>如果你想知道为什么默认情况下不调用命名空间删除过程，而不是手动调用它，这是因为两个原因，按照相关性的顺序：</p>
<ol>
<li>删除命名空间需要迭代和修改文档中的所有节点，这对于Scrapy爬取的所有文档来说是一个相当昂贵的操作</li>
<li>可能有一些情况下，实际上需要使用命名空间，以防某些元素名称在命名空间之间冲突。这些情况非常罕见。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://beautifulsoup.readthedocs.io/zh_CN/latest/&quot;&gt;BeautifulSoup&lt;/a&gt; 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://lxml.de/index.html&quot;&gt;lxml&lt;/a&gt; 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——编写Spider爬取伯乐在线所有文章</title>
    <link href="http://yoursite.com/2017/04/26/scrapy-jobbole-spider/"/>
    <id>http://yoursite.com/2017/04/26/scrapy-jobbole-spider/</id>
    <published>2017-04-26T06:18:54.000Z</published>
    <updated>2017-04-26T07:58:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>仍然是以 <code>http://blog.jobbole.com/all-posts/</code> 页面为例</p>
<a id="more"></a>
<h2 id="提取文章列表页"><a href="#提取文章列表页" class="headerlink" title="提取文章列表页"></a><strong>提取文章列表页</strong></h2><p>首页使用CSS选择器获取页面中的文章url列表：</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff04stoom0j30gw0n4te3.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;#archive .floated-thumb .post-thumb a::attr(href)&quot;).extract()</div><div class="line">&gt;&gt;&gt;[&apos;http://blog.jobbole.com/111005/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/108468/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/110975/&apos;,</div><div class="line"> 	&apos;http://blog.jobbole.com/110986/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110957/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110976/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110923/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110962/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110958/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110140/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110939/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110941/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110931/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110934/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110929/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110835/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110906/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110916/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110913/&apos;,</div><div class="line">    &apos;http://blog.jobbole.com/110903/&apos;]</div></pre></td></tr></table></figure>
<p>先在Spider头部引入<code>from scrapy.http import Request</code>，使用Request进行对文章url列表获取函数的调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析</div><div class="line">    2. 获取下一页的url并交给scrapy进行下载</div><div class="line">    :param response: </div><div class="line">    :return: </div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># 解析列表页中的所有文章url并交给解析函数进行具体字段的解析</span></div><div class="line">    post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</div><div class="line">    <span class="keyword">for</span> post_url <span class="keyword">in</span> post_urls:</div><div class="line">        <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</div></pre></td></tr></table></figure>
<p>其中，最后 <code>yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</code> 是对每个url调用parse_detail方法进行字段解析，这里url的参数是带有完整域名的格式，如果不是完整域名，则需要对域名进行拼接成完成域名进行解析。首先要引入 <code>from urllib import parse</code> ，通过parse自带的 <code>parse.urljoin()</code> 进行拼接，代码为： <code>yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</code> 。</p>
<h2 id="循环获取下一个列表页"><a href="#循环获取下一个列表页" class="headerlink" title="循环获取下一个列表页"></a><strong>循环获取下一个列表页</strong></h2><p><img src="http://ww2.sinaimg.cn/large/006tNbRwgy1ff04orjgoej30yc02g74a.jpg" alt=""></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff04p002aoj311800qdg7.jpg" alt=""></p>
<p>每一个列表页都有“下一页”链接，我们通过CSS选择器来获取下一页的链接，然后交给parse函数进行循环解析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 提取下一页并交给scrapy进行下载</span></div><div class="line">next_url = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first()</div><div class="line"><span class="keyword">if</span> next_url:</div><div class="line">	<span class="keyword">yield</span> Request(url=parse.urljoin(response.url, next_url), callback=self.parse)</div></pre></td></tr></table></figure>
<p>其中，extract_first()方法与extract()[0]用法相同，都是提取第一个字符串元素。</p>
<h2 id="解析函数"><a href="#解析函数" class="headerlink" title="解析函数"></a><strong>解析函数</strong></h2><p>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="comment"># 通过css选择器提取字段</span></div><div class="line">    title = response.css(<span class="string">".entry-header h1::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    create_date = response.css(<span class="string">"p.entry-meta-hide-on-mobile::text"</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</div><div class="line">    praise_nums = int(response.css(<span class="string">".vote-post-up h10::text"</span>).extract()[<span class="number">0</span>])</div><div class="line">    fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">    	fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">    	fav_nums = <span class="number">0</span></div><div class="line">    comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">    match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line">    <span class="keyword">if</span> match_re:</div><div class="line">    	comment_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">    	comment_nums = <span class="number">0</span></div><div class="line">    content = response.css(<span class="string">"div.entry"</span>).extract()[<span class="number">0</span>]</div><div class="line">    tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text"</span>).extract()</div><div class="line">    tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">    tags = <span class="string">","</span>.join(tag_list)</div><div class="line">    print(title, create_date, fav_nums, comment_nums, tags)</div></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a><strong>运行结果</strong></h2><p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff04zpbvtzj31g20bkte9.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;仍然是以 &lt;code&gt;http://blog.jobbole.com/all-posts/&lt;/code&gt; 页面为例&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CPython/"/>
    
  </entry>
  
  <entry>
    <title>Python分布式爬虫打造搜索引擎项目学习笔记——CSS选择器</title>
    <link href="http://yoursite.com/2017/04/25/css-selector/"/>
    <id>http://yoursite.com/2017/04/25/css-selector/</id>
    <published>2017-04-25T12:18:54.000Z</published>
    <updated>2017-06-09T08:20:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CSS选择器的用法"><a href="#CSS选择器的用法" class="headerlink" title="CSS选择器的用法"></a><strong>CSS选择器的用法</strong></h2><h3 id="CSS选择器简介"><a href="#CSS选择器简介" class="headerlink" title="CSS选择器简介"></a><strong>CSS选择器简介</strong></h3><p>在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。</p>
<a id="more"></a>
<h3 id="常用CSS选择器介绍"><a href="#常用CSS选择器介绍" class="headerlink" title="常用CSS选择器介绍"></a><strong>常用CSS选择器介绍</strong></h3><table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>*</td>
<td>选择所有节点</td>
</tr>
<tr>
<td>#container</td>
<td>选择id为container的节点</td>
</tr>
<tr>
<td>.container</td>
<td>选择所有class包含container的节点</td>
</tr>
<tr>
<td>li a</td>
<td>选取所有li下的所有a节点</td>
</tr>
<tr>
<td>ul + p</td>
<td>选择ul后面的第一个p元素</td>
</tr>
<tr>
<td>div#container &gt; ul</td>
<td>选取id为container的div的第一个ul子元素</td>
</tr>
<tr>
<td>ul ~ p</td>
<td>选取与ul相邻的所有p元素</td>
</tr>
<tr>
<td>a[title]</td>
<td>选取所有有title属性的a元素</td>
</tr>
<tr>
<td>a[href=”<code>http://163.com</code>“]</td>
<td>选取所有href属性为163的a元素</td>
</tr>
<tr>
<td>a[href*=”163”]</td>
<td>选取所有href属性包含163的a元素</td>
</tr>
<tr>
<td>a[href^=”http”]</td>
<td>选取所有href属性以http开头的a元素</td>
</tr>
<tr>
<td>a[href$=”.jpg”]</td>
<td>选取所有href以.jpg结尾的a元素</td>
</tr>
<tr>
<td>input[type=radio]:checked</td>
<td>选择选中的radio的元素</td>
</tr>
<tr>
<td>div:not(#container)</td>
<td>选取所有id非container的div属性</td>
</tr>
<tr>
<td>li:nth-child(3)</td>
<td>选取第三个li元素</td>
</tr>
<tr>
<td>tr:nth-child(2n)</td>
<td>第偶数个tr</td>
</tr>
</tbody>
</table>
<h3 id="Scrapy中CSS选择器用法示例"><a href="#Scrapy中CSS选择器用法示例" class="headerlink" title="Scrapy中CSS选择器用法示例"></a><strong>Scrapy中CSS选择器用法示例</strong></h3><p>仍然是用<a href="http://lawtech0902.com/2017/04/16/xpath-example/" target="_blank" rel="external">Xpath用法示例</a>中的例子来进行测试</p>
<h4 id="获取标题"><a href="#获取标题" class="headerlink" title="获取标题"></a>获取标题</h4><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1fez7spfa23j31aw03ytay.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;.entry-header h1::text&quot;).extract()[0]</div><div class="line">&apos;2016 腾讯软件开发面试题（部分）&apos;</div></pre></td></tr></table></figure>
<p>注意：这里获取文字内容的方法为::text，而不是text()。</p>
<h4 id="获取文章发布时间"><a href="#获取文章发布时间" class="headerlink" title="获取文章发布时间"></a>获取文章发布时间</h4><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1fez7svm5gdj30hs01g74p.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile::text&quot;).extract()[0].strip().replace(&quot;·&quot;, &quot;&quot;).strip()</div><div class="line">&apos;2017/02/18&apos;</div></pre></td></tr></table></figure>
<h4 id="获取点赞数、收藏数、评论数"><a href="#获取点赞数、收藏数、评论数" class="headerlink" title="获取点赞数、收藏数、评论数"></a><strong>获取点赞数</strong>、收藏数、评论数</h4><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1fez7terph3j30gi03amxc.jpg" alt=""></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1fez7tk0m0mj31eg0cq7ag.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 点赞数</div><div class="line">&gt;&gt;&gt; response.css(&quot;.vote-post-up h10::text&quot;).extract()[0]</div><div class="line">&apos;2&apos;</div><div class="line"></div><div class="line"># 收藏数，获取之后需要用正则表达式进行清洗</div><div class="line">&gt;&gt;&gt; response.css(&quot;.bookmark-btn::text&quot;).extract()[0]</div><div class="line">&apos; 23 收藏&apos;</div><div class="line"></div><div class="line"># 评论数，获取之后需要用正则表达式进行清洗</div><div class="line">&gt;&gt;&gt; response.css(&quot;a[href=&apos;#article-comment&apos;] span::text&quot;).extract()[0]</div><div class="line">&apos; 7 评论&apos;</div></pre></td></tr></table></figure>
<p>正则表达式清洗收藏数，评论数的逻辑如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">fav_nums = response.css(<span class="string">".bookmark-btn::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, fav_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	fav_nums = int(match_re.group(<span class="number">1</span>))</div><div class="line">    </div><div class="line">comment_nums = response.css(<span class="string">"a[href='#article-comment'] span::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">match_re = re.match(<span class="string">".*?(\d+).*?"</span>, comment_nums)</div><div class="line"><span class="keyword">if</span> match_re:</div><div class="line">	comment_nums = int(match_re.group(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h4 id="获取正文"><a href="#获取正文" class="headerlink" title="获取正文"></a>获取正文</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;div.entry&quot;).extract()[0]</div></pre></td></tr></table></figure>
<h4 id="获取tags"><a href="#获取tags" class="headerlink" title="获取tags"></a>获取tags</h4><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1fez85kutnaj30bi01ydfs.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()</div><div class="line">[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;]</div></pre></td></tr></table></figure>
<p>然后需要对数据进行清洗</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tag_list = response.css(<span class="string">"p.entry-meta-hide-on-mobile a::text()"</span>).extract()</div><div class="line">tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</div><div class="line">tags = <span class="string">","</span>.join(tag_list)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;CSS选择器的用法&quot;&gt;&lt;a href=&quot;#CSS选择器的用法&quot; class=&quot;headerlink&quot; title=&quot;CSS选择器的用法&quot;&gt;&lt;/a&gt;&lt;strong&gt;CSS选择器的用法&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;CSS选择器简介&quot;&gt;&lt;a href=&quot;#CSS选择器简介&quot; class=&quot;headerlink&quot; title=&quot;CSS选择器简介&quot;&gt;&lt;/a&gt;&lt;strong&gt;CSS选择器简介&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="http://yoursite.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy，CSS，Python" scheme="http://yoursite.com/tags/Scrapy%EF%BC%8CCSS%EF%BC%8CPython/"/>
    
  </entry>
  
</feed>
