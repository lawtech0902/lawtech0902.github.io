<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Golang反射浅析]]></title>
    <url>%2F2018%2F07%2F03%2Fgolang-reflect%2F</url>
    <content type="text"><![CDATA[类型Go 是一种静态类型的语言，我们在代码中定义的每个变量，都会有其类型，例如var a int = 10, b string = &quot;acb&quot;语句中，我们定义了两个变量a和b，它们的类型分别是int和string。 除了系统预定义的类型之外，我们还可以自定义类型，例如下面的语句中，我们定义了一个自定义类型MyInt，然后我们分别定义了两个变量i1和i2，尽管这两个变量的内容是相同的，但是他们由于类型不同，并不能够直接赋值，必须要经过类型转换以后才能赋值。 12345678910package maintype MyInt intfunc main() &#123; var i1 int = 1 var i2 MyInt = 2 i1 = i2 //cannot use i2 (type MyInt) as type int in assignment&#125; 接口接口类型的定义在 Go 的类型系统中，还有一种重要的类型叫做接口类型，一个接口类型代表了一组固定的方法。一个接口变量可以存储任意合适的值，只要这个值实现了这个接口的所有方法。 在下面的代码中，我们定义了一个接口类型Animal，然后定义了两种结构体类型Cat和Dog，由于Cat和Dog都实现了Animal的两个方法，所以我们定义的Animal接口变量i既能存储Dog类型的值，又能存储Cat类型的值。 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( "fmt")type Animal interface &#123; run() jump()&#125;type Dog struct &#123;&#125;func (d Dog) run() &#123; fmt.Println("A dog is running")&#125;func (d Dog) jump() &#123; fmt.Println("A dog is jumping")&#125;type Cat struct &#123;&#125;func (c Cat) run() &#123; fmt.Println("A cat is running")&#125;func (c Cat) jump() &#123; fmt.Println("A cat is jumping")&#125;func main() &#123; var i Animal i = Dog&#123;&#125; i.jump() i = Cat&#123;&#125; i.run()&#125; 空接口在接口类型中，有一种比较特殊，那就是空接口类型interface{}。空接口类型的方法集合为空集，意味着任意类型都实现了空接口，也就是说空接口类型的变量能够存储任意类型的值。正是由于空接口的这个特性，我们就可以动态地获取空接口类型变量的实际类型，并更改它的值，来实现我们的反射机制。 接口类型的底层实现每一个接口类型的变量，它其实是由两部分组成，被赋的值的拷贝，和被赋的值的类型描述器。例如上面代码中的i变量，我们可以用这样一个二元组来表示: (c, Cat)，代表了变量c和它的类型Cat。 Type 和 Value在上文中，我们了解到一个接口变量是由值和值类型两部分组成的，我们的反射相关函数主要就是获取这两部分。当我们调用reflect.ValueOf方法的时候，就是获取接口中的变量，并使用一个reflect.Value类型的变量来代表它。同理，当我们使用reflect.TypeOf方法的时候，它获取的就是接口中存储的变量类型，并使用一个reflect.Type类型的变量来代表它。关于reflect包中这些常用方法的描述如下所示: TypeOf 方法reflect.TypeOf方法的声明如下: func TypeOf(i interface{}) Type。 它接收一个空接口类型变量i作为参数，返回一个Type变量，代表了传入参数的类型。由于这个函数的参数是空接口类型，所以即使我们传入了一个其他类型的变量，参数也首先会被转换成空接口类型。 ValueOf 方法reflect.ValueOf方法的声明如下：func ValueOf(i interface{}) Value 它返回一个Value变量代表传入参数i运行时的数据。 Value.Interface()方法是ValueOf方法的逆向方法，Value可以通过Interface()转换成接口。 Zero 方法reflect.Zero方法的声明如下: func Zero(typ Type) Value 它接收一个Type变量，并且返回一个Value变量代表typ所对应的0值。 Kind 类型reflect.Value和reflect.Type类型有一个Kind()方法，它返回一个reflect.Kind类型的变量，代表了反射类型reflect.Type的具体分类。需要注意的是，Kind方法返回的是底层类型，而不是静态声明的类型，如果我们声明了自定义类型type MyInt int，通过Value.Kind()获取的类型仍然为int。具体例子请参考下面这段代码: 1234567891011121314151617181920212223242526package mainimport ( "fmt" "reflect")type User struct &#123; Name string Age int&#125;type MyInt intfunc main() &#123; u := User&#123;"xff", 19&#125; var i MyInt = 42 t := reflect.TypeOf(u) // reflect.Struct是一个reflect.Kind类型的常量，代表了结构体类型 fmt.Println(t.Kind() == reflect.Struct) // 输出 true t = reflect.TypeOf(i) // 这里变量i的类型是我们自定义的类型MyInt，但是Type.Kind()方法返回的类型是reflect.Int fmt.Println(t.Kind() == reflect.Int) // 输出true&#125; Value 的 settablesettable就是表示一个通过空接口反射出来的Value变量是否是可以修改原始的值，通过调用Value.CanSet()方法，我们可以查看某个Value的settable属性。那么什么情况下Value变量可以修改原始的值呢？请参考下面这段代码: 1234567891011121314151617181920package mainimport ( "fmt" "reflect")func main() &#123; var x float64 = 4.1 xPointerValue := reflect.ValueOf(&amp;x) fmt.Println(xPointerValue.CanSet()) // 输出 false // 只有 Interface 或 Ptr 类型的 Value 才能调用 Elem 方法 xPointerValueElem := xPointerValue.Elem() fmt.Println(xPointerValueElem.CanSet()) // 输出 true xValue := reflect.ValueOf(x) fmt.Println(xValue.CanSet()) // 输出 false _ = xValue.Elem() // 这里程序会报 panic&#125; 我们可以看到，变量x反射出来的变量xValue是不能修改原始值的，这是因为reflect.ValueOf方法其实是将x赋值到了一个空接口变量o上，o中保存的是x的拷贝和x的类型描述器，而ValueOf()方法返回的xValue变量指向的就是空接口变量o中保存的x的拷贝。如果xValue变量可以修改原始值，那么修改的也仅仅只是x的拷贝，并不能够修改x本身，所以xValue是不可修改的。 指针&amp;x反射出来的xPointerValue也是不能修改原始值的，原理和上文中讲述的类似，xPointerValue指向的是空接口变量o中保存的x的指针的拷贝，这个指针中保存的地址是不可修改的。 但是xPointer调用Elem()方法获取到的xPointerValueElem变量就是可以修改原始值的了，这是因为xPointerValue.Elem()方法返回的是这个指针指向的值，也就是x，也就是说xPointerValueElem指向的就是x，所以xPointerValueElem就是可以修改原始值的了。 这里的内容比较绕，理解起来可能不太容易，大家可以参考下面的图进行理解： 反射的示例OK，讲了一大串的理论，大家忍不住想要通过代码来验证一下自己的想法了吧，我们接下来就会通过几个例子，来演示一下 go 中反射的具体用法。 简单数据的类型和内容解析12345678910111213141516171819package mainimport ( "fmt" "reflect")func main() &#123; var x float64 = 4.1 v = reflect.ValueOf(x) fmt.Println("Type:", v.Type()) fmt.Println("Kind is float64:", v.Kind() == reflect.Float64) fmt.Println("value:", v.Float()) // 输出内容 // Type: float64 // Kind is float64: true // value: 4.1&#125; 在上述代码中，我们设置了一个float64类型的变量x，并通过reflect.ValueOf方法来获取这个变量所对应的reflect.Value，并输出了这个reflect.Value的类型和值。 结构体变量的类型和内容解析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport ( "fmt" "reflect")type User struct &#123; Id int Name string Age int&#125;func (u User) Hello() &#123; // 结构体方法的类型名参数其实就是函数的第一个参数，通过反射打印方法类型可以看出 fmt.Println("Hello, World.")&#125;func main() &#123; u := User&#123;1, "wahahah", 23&#125; Info(u)&#125;func Info(o interface&#123;&#125;) &#123; // TypeOf获取的某个对象的所有字段的名称和类型 t := reflect.TypeOf(o) fmt.Println("Type:", t) // 由于`Type.Field()`方法只支持结构体类型，所以这里如果传入的变量不是结构体类型会直接返回。 if k := t.Kind(); k != reflect.Struct &#123; fmt.Println("Error type") return &#125; // ValueOf获取的是某个对象的所有字段的值 fmt.Println("Fields:") v := reflect.ValueOf(o) for i := 0; i &lt; v.NumField(); i++ &#123; // t.Filed(i) 返回的是一个`reflect.StructField`类型的变量，描述了结构体类型中的某个字段的类型信息 field_type := t.Field(i) // v.Filed(i) 返回了一个`reflect.Value`类型的变量，代表了结构体类型中某个字段的值 val := v.Field(i).Interface() fmt.Printf("%6s: %v = %v\n", field_type.Name, field_type.Type, val) &#125; // Method获取的某个对象的所有方法的名字和类型(即方法签名) for i := 0; i &lt; t.NumMethod(); i++ &#123; m := t.Method(i) fmt.Printf("%6s: %v\n", m.Name, m.Type) &#125;&#125; 上述代码中，我们遍历了一个结构体类型，输出了它的所有字段的类型和值，还输出了它所有方法的签名。它的输出结果如下所示: 123456Type: main.UserFields: Id: int = 1 Name: string = wahahah Age: int = 23 Hello: func(main.User) 嵌套结构体变量的类型和内容解析12345678910111213141516171819202122232425262728293031323334package mainimport ( "fmt" "reflect")type User struct &#123; Id int Name string Age int&#125;type Manager struct &#123; User // 这是一个匿名字段，这个字段的名称也是User title string&#125;func main() &#123; m := Manager&#123; User: User&#123;1, "xff", 19&#125;, title: "manager", &#125; t := reflect.TypeOf(m) fmt.Printf("Type: %#v\n", t.Field(0)) // 这里把User类型当做一个字段 fmt.Printf("Type: %#v\n", t.Field(1)) // 这里打印的是title字段 // t.FidleByIndex传入的是一个int的slice，第一个数字表示在大的结构体中的索引，第二个数字表示在User结构体中的索引 // 打印 User 结构体的 ID 字段 fmt.Printf("Type: %#v\n", t.FieldByIndex([]int&#123;0,0&#125;)) // 这里获取的是User结构体的 Name 字段 fmt.Printf("Type: %#v\n", t.FieldByIndex([]int&#123;0,1&#125;))&#125; 简单数据的内容修改1234567891011121314151617181920package mainimport ( "fmt" "reflect")func main() &#123; var x float64 = 4.1 xPointerValue := reflect.ValueOf(&amp;x) val := xPointerValue.Elem() // 检查要更改原始值的 Value 的 settable 属性 if val.CanSet() &#123; val.SetFloat(3.0) &#125; fmt.Println(x) // 输出3.0&#125; 在上面的代码中，我们演示了如何通过反射来更改一个float64类型变量的值。 结构体类型变量的内容修改下面的代码演示了如何从一个结构体变量中获取字段，并改变这个字段的值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package mainimport ( "fmt" "reflect")// 本处代码演示的是如何通过反射动态地修改结构体类型的值type User struct &#123; Id int Name string Age int&#125;func main() &#123; u := User&#123;1, "bwangel", 23&#125; fmt.Println(u) Set(&amp;u) fmt.Println(u)&#125;func Set(o interface&#123;&#125;) &#123; pointerValue := reflect.ValueOf(o) // 判断类型是指针 if pointerValue.Kind() != reflect.Ptr &#123; fmt.Println(pointerValue.Type(), "Cannot be set") return &#125; // pointerValue.Elem() 了一个Value类型的值， 代表的是这个指针所指向的值 value := pointerValue.Elem() if !value.CanSet() &#123; fmt.Println(value, "is not settable") &#125; // 获取字段的名称，并且判断类型是否为字符串 // Value.IsValid 方法判断一个 Value 类型是否有效地代表了一个值 field_name := "Name" if f := value.FieldByName(field_name); f.IsValid() &#123; if f.Kind() == reflect.String &#123; fmt.Println(f, reflect.TypeOf(f)) // Value.SetString 设置了这个类型所代表的对象的值 f.SetString("xff") &#125; &#125; else &#123; fmt.Println("Bad Field", field_name) &#125;&#125; 结构体变量的方法调用下面的代码演示了如何通过反射来实现结构体变量方法的调用 12345678910111213141516171819202122232425262728293031323334package mainimport ( "fmt" "reflect")type User struct &#123; Id int Name string Age int&#125;func (u User) Hello(name string, number int) int&#123; fmt.Println("hello,", name, "my name is", u.Name, number) return 123&#125;func main() &#123; u := User&#123;1, "bwangel", 23&#125; u.Hello("xff", 1) v := reflect.ValueOf(u) mv := v.MethodByName("Hello") // 动态地调用方法传递的参数必须是 reflect.Value 组成的一个切片 // 可以通过 reflect.ValueOf 将任意值转换成 Value 类型 args := []reflect.Value&#123;reflect.ValueOf("xff2"), reflect.ValueOf(2)&#125; // mv.Call函数执行了实际的方法调用 // 它返回的是一个由 Value 类型变量组成的数组([]reflect.Value)，代表了所有的返回值 return_vals := mv.Call(args) fmt.Println(len(return_vals), return_vals, return_vals[0]) // 输出 `1 [&lt;int Value&gt;] 123`&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Imooc Go基础]]></title>
    <url>%2F2018%2F06%2F23%2Fgolang-imooc%2F</url>
    <content type="text"><![CDATA[一些基础！ ###1.byte(4字节8位),rune(char类型,32位,4字节的int32) ###2.强制类型转换,go没有隐式类型转换 123456789101112//勾股定理a, b := 3, 4 //intvar c int//Sqrt(float64)c = math.Sqrt(a*a+b*b)//&gt;&gt; 类型错误//必须显示的进行类型转换c = int(math.Sqrt(float64(a*a+b+b)))//如果是const且没有定义类型的话不会报错const a, b = 3,4c := math.Sqrt(a*a +b*b)//&gt;&gt; 5 ###3.iota 1234567const( b=1 &lt;&lt;(10*iota) kb mb tb pb) ###4.指针 golang只有值传递一种方式下面这种方式看着像引用传递,实际是复制了a的指针地址然后传递达到了引用传递的效果 ###5.Slice slice可以向后扩展,但无法向前扩展 s[i]不可以超越len(s),向后扩展不能超过cap(s) 添加元素时如果超越cap,系统会重新分配更大的底层数组 1234567891011121314151617181920s := []int&#123;0,1,2,3,4,5,6,7&#125;s1 := s[2:6]// [ 2,3,4,5 ]s2 := s1[4]// Error 下标越界了s3 := s1[3:5]// [5,6] 向后扩展了一个6s4 := append(s3,10)// s4: [5,6,10]// s: [0,1,2,3,4,5,6,10] 原来的7被append为10了s5 := append(s4,11)// s5: [5,6,10,11]// s: [0,1,2,3,4,5,6,10]当cap不够用的时候,系统已经重新扩展了一个新的slice就不再是操作原来的slice了.原来的slice如果有人用才会存在,否则垃圾回收机制会把回收了.//copys6 := make([]int, 8, 16)copy(s6, s5)// [5,6,10,11,0,0,0,0] len:8 cap:16//delete因为go没有内置的delete方法,所以用append来实现s7 := append(s5[:3], s5[4:]...)// [5,6,10,0,0,0,0] len:8 cap:16 ###6.Map 创建 make(map[string]string) 获取 m[key] key不存在时,获取的是零值 value, ok :=m[key]判断值是否存在 delete(m, “key”)删除 ###7.Rune(相当于go的char类型) 使用range遍历pos,rune对 使用utf8.RuneCountString()获取字符数量,而不是直接使用len() 使用len()只是获取到字节长度 使用[]byte获得字节 ###8.值接收者VS指针接收者 要改变内容必须使用指针接收者 结构体过大也考虑使用指针接收者 一致性:如有指针接收者,最好都是指针接收者 值接收者是go特有的 值/指针接收者均可以接收值/指针 ###9.接口 Type Assertion.(type类型)获取interface肚子里的类型 1r.(*real.Retriver) ###10.函数式编程 裴波纳契 123456789101112131415func fibonacci() func() int &#123; a, b := 0, 1 return func() int &#123; a, b = b, a+b return a &#125;&#125;func main()&#123; f := fabonacci() f() //1 f() //1 f() //2 f() //3 f() //5&#125; ###11.资源管理 ###12.测试 123go test #测试当前目录go test --coverprofile=c.out #生成c.out文件go tool cover -html=c.out #将c.out文件生成html格式并用浏览器打开 123go test -bench . #benchmark测试go test -bench . -cpuprofile=cpu.outgo tool pprof cpu.out 1godoc -http :6060 #开启webservice文档 ###13.Goroutine 1runtime.Gosched()//手动交出控制权 1go run -race main.go #race condition 数据访问冲突,进行错误检测 ###14.Channel ###15.http 第一种 123import _ &quot;net/http/pprof&quot;//server func ...//然后就可以通过url/debug/pprof进行访问 第二种 12345#会获得30秒的CPU使用率go tool pprof http://localhost:8888/debug/pprof/profile#会获得30秒的内存使用率go tool pprof http://localhost:8888/debug/pprof/heap#期间访问想要测试的http request,30秒后进入prof模式,可以使用web进行查看 ###16.广度优先算法 1.探索顺序:上左下右2.结点的三种状态: 已经发现还未探索(最关键,这些未探索的点需要排队,不能急于探索必须轮到才能探索) 已经发现并且探索 连发现都没发现 一层一层往外递进,确保每到一个点都是用最短的路径到达最终倒过来走就是最短路径3.结束条件: 走到终端 队列为空(死路)]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git工作流程]]></title>
    <url>%2F2018%2F04%2F24%2Fgit-workflow%2F</url>
    <content type="text"><![CDATA[忘了的时候看一看！ Git 作为一个源码管理系统，不可避免涉及到多人协作。 协作必须有一个规范的工作流程，让大家有效地合作，使得项目井井有条地发展下去。”工作流程”在英语里，叫做”workflow”或者”flow”，原意是水流，比喻项目像水流那样，顺畅、自然地向前流动，不会发生冲击、对撞、甚至漩涡。 本文介绍三种广泛使用的工作流程： Git flow Github flow Gitlab flow 如果你对Git还不是很熟悉，可以先阅读下面的文章。 《Git 使用规范流程》 《常用 Git 命令清单》 《Git 远程操作详解》 一、功能驱动本文的三种工作流程，有一个共同点：都采用“功能驱动式开发”（Feature-driven development，简称FDD）。 它指的是，需求是开发的起点，先有需求再有功能分支（feature branch）或者补丁分支（hotfix branch）。完成开发后，该分支就合并到主分支，然后被删除。 二、Git flow最早诞生、并得到广泛采用的一种工作流程，就是Git flow 。 2.1 特点它最主要的特点有两个。 首先，项目存在两个长期分支。 主分支master 开发分支develop 前者用于存放对外发布的版本，任何时候在这个分支拿到的，都是稳定的分布版；后者用于日常开发，存放最新的开发版。 其次，项目存在三种短期分支。 功能分支（feature branch） 补丁分支（hotfix branch） 预发分支（release branch） 一旦完成开发，它们就会被合并进develop或master，然后被删除。 Git flow 的详细介绍，请阅读我翻译的中文版《Git 分支管理策略》。 2.2 评价Git flow的优点是清晰可控，缺点是相对复杂，需要同时维护两个长期分支。大多数工具都将master当作默认分支，可是开发是在develop分支进行的，这导致经常要切换分支，非常烦人。 更大问题在于，这个模式是基于”版本发布”的，目标是一段时间以后产出一个新版本。但是，很多网站项目是”持续发布”，代码一有变动，就部署一次。这时，master分支和develop分支的差别不大，没必要维护两个长期分支。 三、Github flowGithub flow 是Git flow的简化版，专门配合”持续发布”。它是 Github.com 使用的工作流程。 3.1 流程它只有一个长期分支，就是master，因此用起来非常简单。 官方推荐的流程如下。 第一步：根据需求，从master拉出新分支，不区分功能分支或补丁分支。 第二步：新分支开发完成后，或者需要讨论的时候，就向master发起一个pull request（简称PR）。 第三步：Pull Request既是一个通知，让别人注意到你的请求，又是一种对话机制，大家一起评审和讨论你的代码。对话过程中，你还可以不断提交代码。 第四步：你的Pull Request被接受，合并进master，重新部署后，原来你拉出来的那个分支就被删除。（先部署再合并也可。） 3.2 评价Github flow 的最大优点就是简单，对于”持续发布”的产品，可以说是最合适的流程。 问题在于它的假设：master分支的更新与产品的发布是一致的。也就是说，master分支的最新代码，默认就是当前的线上代码。 可是，有些时候并非如此，代码合并进入master分支，并不代表它就能立刻发布。比如，苹果商店的APP提交审核以后，等一段时间才能上架。这时，如果还有新的代码提交，master分支就会与刚发布的版本不一致。另一个例子是，有些公司有发布窗口，只有指定时间才能发布，这也会导致线上版本落后于master分支。 上面这种情况，只有master一个主分支就不够用了。通常，你不得不在master分支以外，另外新建一个production分支跟踪线上版本。 四、Gitlab flowGitlab flow 是 Git flow 与 Github flow 的综合。它吸取了两者的优点，既有适应不同开发环境的弹性，又有单一主分支的简单和便利。它是 Gitlab.com 推荐的做法。 4.1 上游优先Gitlab flow 的最大原则叫做”上游优先”（upsteam first），即只存在一个主分支master，它是所有其他分支的”上游”。只有上游分支采纳的代码变化，才能应用到其他分支。 Chromium项目就是一个例子，它明确规定，上游分支依次为： Linus Torvalds的分支 子系统（比如netdev）的分支 设备厂商（比如三星）的分支 4.2 持续发布Gitlab flow 分成两种情况，适应不同的开发流程。 对于”持续发布”的项目，它建议在master分支以外，再建立不同的环境分支。比如，”开发环境”的分支是master，”预发环境”的分支是pre-production，”生产环境”的分支是production。 开发分支是预发分支的”上游”，预发分支又是生产分支的”上游”。代码的变化，必须由”上游”向”下游”发展。比如，生产环境出现了bug，这时就要新建一个功能分支，先把它合并到master，确认没有问题，再cherry-pick到pre-production，这一步也没有问题，才进入production。 只有紧急情况，才允许跳过上游，直接合并到下游分支。 4.3 版本发布 对于”版本发布”的项目，建议的做法是每一个稳定版本，都要从master分支拉出一个分支，比如2-3-stable、2-4-stable等等。 以后，只有修补bug，才允许将代码合并到这些分支，并且此时要更新小版本号。 五、一些小技巧5.1 Pull Request 功能分支合并进master分支，必须通过Pull Request（Gitlab里面叫做 Merge Request）。 前面说过，Pull Request本质是一种对话机制，你可以在提交的时候，@相关人员或团队，引起他们的注意。 5.2 Protected branchmaster分支应该受到保护，不是每个人都可以修改这个分支，以及拥有审批 Pull Request 的权力。 Github 和 Gitlab 都提供”保护分支”（Protected branch）这个功能。 5.3 IssueIssue 用于 Bug追踪和需求管理。建议先新建 Issue，再新建对应的功能分支。功能分支总是为了解决一个或多个 Issue。 功能分支的名称，可以与issue的名字保持一致，并且以issue的编号起首，比如”15-require-a-password-to-change-it”。 开发完成后，在提交说明里面，可以写上”fixes #14”或者”closes #67”。Github规定，只要commit message里面有下面这些动词 + 编号，就会关闭对应的issue。 close closes closed fix fixes fixed resolve resolves resolved 这种方式还可以一次关闭多个issue，或者关闭其他代码库的issue，格式是username/repository#issue_number。 Pull Request被接受以后，issue关闭，原始分支就应该删除。如果以后该issue重新打开，新分支可以复用原来的名字。 5.4 Merge节点Git有两种合并：一种是”直进式合并”（fast forward），不生成单独的合并节点；另一种是”非直进式合并”（none fast-forword），会生成单独节点。 前者不利于保持commit信息的清晰，也不利于以后的回滚，建议总是采用后者（即使用--no-ff参数）。只要发生合并，就要有一个单独的合并节点。 5.5 Squash 多个commit为了便于他人阅读你的提交，也便于cherry-pick或撤销代码变化，在发起Pull Request之前，应该把多个commit合并成一个。（前提是，该分支只有你一个人开发，且没有跟master合并过。） 这可以采用rebase命令附带的squash操作，具体方法请参考阮一峰老师写的《Git 使用规范流程》。 原文地址：http://www.ruanyifeng.com/blog/2015/12/git-workflow.html]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git基础速记]]></title>
    <url>%2F2018%2F04%2F23%2Fgit-base%2F</url>
    <content type="text"><![CDATA[忘了的时候看一看！ git简介目前世界上最先进的分布式版本控制系统 集中式vs分布式 创建版本库初始化一个Git仓库，使用git init命令。 添加文件到Git仓库，分两步： 第一步，使用命令git add &lt;file&gt;，注意，可反复多次使用，添加多个文件； 第二步，使用命令git commit，完成。git commit --amend可以重新修改提交的message 时光机穿梭 要随时掌握工作区的状态，使用git status命令。 如果git status告诉你有文件被修改过，用git diff可以查看修改内容。 版本回退现在总结一下： HEAD指向的版本就是当前版本，回退到上一个版本可以用git reset --hard HEAD^ 因此，Git允许我们在版本的历史之间穿梭，使用命令git reset --hard commit_id。 穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本(--pretty=oneline)。 要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。 工作区和暂存区 工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 我们把文件往Git版本库里添加的时候，是分两步执行的： 第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区； 第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。 因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以，现在，git commit就是往master分支上提交更改。 管理修改每次修改，如果不add到暂存区，那就不会加入到commit中。 撤销修改场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。 删除文件命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。 远程仓库由于你的本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以，需要一点设置： 第1步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开Shell（Windows下打开Git Bash），创建SSH Key： 1$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可，由于这个Key也不是用于军事目的，所以也无需设置密码。 如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 第2步：登陆GitHub，打开“Account settings”，“SSH Keys”页面： 然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容： 点“Add Key”，你就应该看到已经添加的Key： 为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。 当然，GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。 最后友情提示，在GitHub上免费托管的Git仓库，任何人都可以看到喔（但只有你自己才能改）。所以，不要把敏感信息放进去。 如果你不想让别人看到Git库，有两个办法，一个是交点保护费，让GitHub把公开的仓库变成私有的，这样别人就看不见了（不可读更不可写）。另一个办法是自己动手，搭一个Git服务器，因为是你自己的Git服务器，所以别人也是看不见的。这个方法我们后面会讲到的，相当简单，公司内部开发必备。 添加远程库要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git； 关联后，使用命令git push -u origin master第一次推送master分支的所有内容； 此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改； 分布式版本系统的最大好处之一是在本地工作完全不需要考虑远程库的存在，也就是有没有联网都可以正常工作，而SVN在没有联网的时候是拒绝干活的！当有网络的时候，再把本地提交推送一下就完成了同步，真是太方便了！ 从远程库克隆要克隆一个仓库，首先必须知道仓库的地址，然后使用git clone命令克隆。 Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。 分支管理现在有了分支，就不用怕了。你创建了一个属于你自己的分支，别人看不到，还继续在原来的分支上正常工作，而你在自己的分支上干活，想提交就提交，直到开发完毕后，再一次性合并到原来的分支上，这样，既安全，又不影响别人工作。 其他版本控制系统如SVN等都有分支管理，但是用过之后你会发现，这些版本控制系统创建和切换分支比蜗牛还慢，简直让人无法忍受，结果分支功能成了摆设，大家都不去用。 但Git的分支是与众不同的，无论创建、切换和删除分支，Git在1秒钟之内就能完成！无论你的版本库是1个文件还是1万个文件。 创建和合并分支Git鼓励大量使用分支： 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt; 创建+切换分支：git checkout -b &lt;name&gt; 合并某分支到当前分支：git merge &lt;name&gt; 删除分支：git branch -d &lt;name&gt; 解决冲突当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 用git log --graph( --pretty=oneline --abbrev-commit)命令可以看到分支合并图。 分支管理策略Git分支十分强大，在团队开发中应该充分应用。 合并分支时，加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 Bug分支修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除； 当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场。 Feature分支开发一个新feature，最好新建一个分支； 如果要丢弃一个没有被合并过的分支，可以通过git branch -D &lt;name&gt;强行删除。 多人协作多人协作的工作模式通常是这样： 首先，可以试图用git push origin branch-name推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin branch-name推送就能成功！ 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream branch-name origin/branch-name。 这就是多人协作的工作模式，一旦熟悉了，就非常简单。 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 标签管理发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。 Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。 Git有commit，为什么还要引入tag？ “请把上周一的那个版本打包发布，commit号是6a5819e…” “一串乱七八糟的数字不好找！” 如果换一个办法： “请把上周一的那个版本打包发布，版本号是v1.2” “好的，按照tag v1.2查找commit就行！” 所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 创建标签 命令git tag &lt;name&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id； git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息； git tag -s &lt;tagname&gt; -m &quot;blablabla...&quot;可以用PGP签名标签； 命令git tag可以查看所有标签。 操作标签 命令git push origin &lt;tagname&gt;可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d &lt;tagname&gt;可以删除一个本地标签； 命令git push origin :refs/tags/&lt;tagname&gt;可以删除一个远程标签。 使用Github 在GitHub上，可以任意Fork开源仓库； 自己拥有Fork后的仓库的读写权限； 可以推送pull request给官方仓库来贡献代码。 自定义Git忽略特殊文件 忽略某些文件时，需要编写.gitignore； .gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理！ 例如： 12345678910111213141516171819# 忽略windows上的辣鸡文件# Windows:Thumbs.dbehthumbs.dbDesktop.ini# 忽略python编译产生的.pyc、.pyo、dist等文件或目录# Python:*.py[cod]*.so*.egg*.egg-infodistbuild# 自己定义的文件# My configurations:db.inideploy_key_rsa 记得把.gitignore提交到Git！ 配置别名就是给命令配别名，偷懒！ 搭建Git服务器在远程仓库一节中，我们讲了远程仓库实际上和本地仓库没啥不同，纯粹为了7x24小时开机并交换大家的修改。 GitHub就是一个免费托管开源代码的远程仓库。但是对于某些视源代码如生命的商业公司来说，既不想公开源代码，又舍不得给GitHub交保护费，那就只能自己搭建一台Git服务器作为私有仓库使用。 搭建Git服务器需要准备一台运行Linux的机器，强烈推荐用Ubuntu或Debian，这样，通过几条简单的apt命令就可以完成安装。 假设你已经有sudo权限的用户账号，下面，正式开始安装。 第一步，安装git： 1$ sudo apt-get install git 第二步，创建一个git用户，用来运行git服务： 1$ sudo adduser git 第三步，创建证书登录： 收集所有需要登录的用户的公钥，就是他们自己的id_rsa.pub文件，把所有公钥导入到/home/git/.ssh/authorized_keys文件里，一行一个。 第四步，初始化Git仓库： 先选定一个目录作为Git仓库，假定是/srv/sample.git，在/srv目录下输入命令： 1$ sudo git init --bare sample.git Git就会创建一个裸仓库，裸仓库没有工作区，因为服务器上的Git仓库纯粹是为了共享，所以不让用户直接登录到服务器上去改工作区，并且服务器上的Git仓库通常都以.git结尾。然后，把owner改为git： 1$ sudo chown -R git:git sample.git 第五步，禁用shell登录： 出于安全考虑，第二步创建的git用户不允许登录shell，这可以通过编辑/etc/passwd文件完成。找到类似下面的一行： 1git:x:1001:1001:,,,:/home/git:/bin/bash 改为： 1git:x:1001:1001:,,,:/home/git:/usr/bin/git-shell 这样，git用户可以正常通过ssh使用git，但无法登录shell，因为我们为git用户指定的git-shell每次一登录就自动退出。 第六步，克隆远程仓库： 现在，可以通过git clone命令克隆远程仓库了，在各自的电脑上运行： 123$ git clone git@server:/srv/sample.gitCloning into 'sample'...warning: You appear to have cloned an empty repository. 剩下的推送就简单了。 管理公钥如果团队很小，把每个人的公钥收集起来放到服务器的/home/git/.ssh/authorized_keys文件里就是可行的。如果团队有几百号人，就没法这么玩了，这时，可以用Gitosis来管理公钥。 这里我们不介绍怎么玩Gitosis了，几百号人的团队基本都在500强了，相信找个高水平的Linux管理员问题不大。 管理权限有很多不但视源代码如生命，而且视员工为窃贼的公司，会在版本控制系统里设置一套完善的权限控制，每个人是否有读写权限会精确到每个分支甚至每个目录下。因为Git是为Linux源代码托管而开发的，所以Git也继承了开源社区的精神，不支持权限控制。不过，因为Git支持钩子（hook），所以，可以在服务器端编写一系列脚本来控制提交等操作，达到权限控制的目的。Gitolite就是这个工具。 这里我们也不介绍Gitolite了，不要把有限的生命浪费到权限斗争中。 小结 搭建Git服务器非常简单，通常10分钟即可完成； 要方便管理公钥，用Gitosis； 要像SVN那样变态地控制权限，用Gitolite]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx面试准备]]></title>
    <url>%2F2018%2F03%2F06%2Fnginx-interview%2F</url>
    <content type="text"><![CDATA[什么是Nginx？Nginx是一个web服务器和反向代理服务器，用于HTTP、HTTPS、SMTP、POP3和IMAP协议。多进程异步非阻塞事件处理机制：运用了epoll模型 为什么要用Nginx？①跨平台、配置简单 ②非阻塞、高并发连接：官方检测支持5万并发 ③内存消耗低：Nginx采取了分阶段资源分配技术 ④内置的健康检查功能：如果有一个服务器宕机，会做一个健康检查，再发送的请求就不会发送到宕机的服务器了。重新将请求提交到其他的节点上。 ⑤节省带宽：支持GZIP压缩，可以添加浏览器本地缓存 ⑥稳定性高：宕机的概率低 ⑦master/worker结构：一个master进程，生成一个或多个worker进程 ⑧接收用户异步请求：浏览器将请求发送到nginx服务器，它先将用户请求全部接收下来，再一次性发送给后端web服务器，极大减轻了web服务器的压力，一边接收web服务器的返回数据，一边发送给浏览器客户端 ⑨网络依赖性比较低，只要ping通就可以负载均衡 ⑩可以有多台nginx服务器 Nginx和Apache比较 Nginx相对Apache的优点： ①轻量，配置简单，内存及资源消耗低 ②高并发，Nginx处理请求是异步非阻塞的，而Apache是阻塞的 ③高度模块化的设计，编写模块相对简单 ④社区活跃，高性能模块出品迅速 Apache相对Nginx的优点： ①rewrite强大 ②模块超多 ③少bug，稳定 Nginx的优势是处理静态请求，cpu内存使用率低，Apache适合处理动态请求，所以现在一般前端用Nginx作为反向代理抗住压力，Apache作为后端处理动态请求。 Nginx高性能原因得益于Nginx的事件处理机制：异步非阻塞事件处理机制，运用了epoll模型，提供了一个队列，排队解决 Nginx如何处理HTTP请求Nginx使用反应器模式。主事件循环等待操作系统发出准备事件的信号，这样数据就可以从套接字读取，在该实例中读取到缓冲区并进行处理。单个线程可以提供数万个并发连接。 使用反向代理服务器的优点是什么反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和web服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用web托管服务时。 Nginx服务器的最佳用途Nginx服务器的最佳用法是在网络上部署动态HTTP内容，使用SCGI、WSGI应用程序服务器、用于脚本的FastCGI处理程序。它还可以作为负载均衡器。 Nginx为什么不使用多线程Apache: 创建多个进程或线程，而每个进程或线程都会为其分配cpu和内存（线程要比进程小的多，所以worker支持比perfork高的并发），并发过大会榨干服务器资源。 Nginx: 采用单线程来异步非阻塞处理请求（管理员可以配置Nginx主进程的工作进程的数量）(epoll)，不会为每个请求分配cpu和内存资源，节省了大量资源，同时也减少了大量的CPU的上下文切换。所以才使得Nginx支持更高的并发。 Nginx上的Master和Worker进程分别是什么？Master进程：读取、评估配置和维持 Worker进程：处理请求 如何通过不同于80的端口开启Nginx必须进入/etc/Nginx/sites-enabled/，如果这是默认文件，那么你必须打开名为“default”的文件。编辑文件，并放置在你想要的端口： 1Like server &#123; listen 81; &#125; 是否有可能将Nginx的错误替换为502、503？502：错误网关 503：服务器超载 有可能，可以确保fastcgi_intercept_errors被设置为ON，并使用错误页面指令。 123456Location / &#123;fastcgi_pass 127.0.01:9001;fastcgi_intercept_errors on;error_page 502 =503/error_page.html;#…&#125; Nginx中，如何在URL中保留双斜杠要在URL中保留双斜线，就必须使用merge_slashes_off; 语法:merge_slashes [on/off] 默认值: merge_slashes on 环境: http，server ngx_http_upstream_module的作用是什么ngx_http_upstream_module用于定义可通过fastcgi传递、proxy传递、uwsgi传递、memcached传递和scgi传递指令来引用的服务器组。 什么是C10K问题C10K问题是指无法同时处理大量客户端(10,000)的网络套接字。 stub_status和sub_filter指令的作用是什么Stub_status指令：该指令用于了解Nginx当前状态的当前状态，如当前的活动连接，接受和处理当前读/写/等待连接的总数 Sub_filter指令：它用于搜索和替换响应中的内容，并快速修复陈旧的数据 Nginx是否支持将请求压缩到上游可以使用Nginx模块gunzip将请求压缩到上游。gunzip模块是一个过滤器，它可以对不支持“gzip”编码方法的客户机或服务器使用“内容编码:gzip”来解压缩响应。 如何在Nginx中获取当前时间要获得Nginx的当前时间，必须使用SSI模块、$date_gmt和$date_local的变量。 Proxy_set_header THE-TIME $date_gmt; 如何在Nginx上添加模块在变异过程中，必须选择Nginx模块，因为Nginx不支持模块的运行时间选择]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django celery添加异步任务]]></title>
    <url>%2F2018%2F01%2F06%2Fdjango-celery%2F</url>
    <content type="text"><![CDATA[异步任务的重要性大家在做web项目的时候经常会遇到一些耗时的操作， 比如： 发送邮件、发送短信、生成pdf。这些操作在某些情况下需要立即返回结果给用户，但是可以在后台异步执行。 比如用户邮箱注册的时候， 在发送邮件的时候可以先把”已经发送激活邮件到邮箱”返回给用户， 同时把邮件发送任务提交到异步处理线程中。 现在介绍一款python写的专门用于处理异步任务的框架–celery。当然celery能完成的功能远不止异步任务， 还有一个很常用的功能–定时任务 celery的功能还包括：定义工作流、监控、任务流控制、资源泄露保护以及自定义用户组件等。 celery介绍123说明： 最新版本的celery支持的python版本必须大于2.7.6， 如果是python2.7.6及以下版本的时候import celery是会报错滴。 celery是通过将代码序列然后传输到中间通信组件，这些组件可以采用任何方式实现， 这里最常用的两种是rabbitmq和redis， 然后celery的后台线程不停的从rabbitmq或者redis中读取这些任务并执行然后返回结果到这些组件，这样就实现了一个异步的功能。 Celery 用redis或者rabbitmq做消息通信，这里redis或者rabbitmq被称为中间人（Broker）Celery 系统可包含多个线程和中间人，以此获得高可用性和横向扩展能力。 Celery 虽然是用 Python 编写的，但协议可以用任何语言实现。迄今，已有 Ruby 实现的 RCelery 、node.js 实现的 node-celery 以及一个 PHP 客户端 ，语言互通也可以通过 using webhooks 实现。 django 介绍django作为python最主流也是资格最老的的web开发系统，是一个全栈的开发框架，几乎web开发系统中会用到的所有功能django都有，即使没有也可以在网站找到对应的开源解决方案，在stackoverflow上的问答也是最多的。基本上学习懂了django以后学习其他如flask、tornado都会觉得手到擒来。 本文中我们就介绍一下如何将celery集成到django中来完成django耗时任务的异步执行和定时任务计划。 我们将采用redis来做为中间人 ###celery 安装和使用 celery安装 1pip install -U celery[redis] 该命令会安装celery以及redis开发相关所有的依赖包。安装完成我们可以看到：这里我们可以看到安装了billiard、pytz、vine、amqp、redis、celery等 redis-server安装 既然是用redis做中间人，当然需要安装redis了、我们直接运行： 1sudo apt-getin install redis-server 运行成功以后可以，redis-server直接就作为服务启动了， 我们可以通过： 1ps aux|grep redis 命令来查看redis是否启动如下图： 这里我们可以看到redis已经在6379端口监听了 启动celery的worker 前面介绍了celery的处理流程， 既然我们已经启动了redis， 当然我们需要启动一个随时监听异步处理函数的worker了。 这里我们直接启动celery的worker就行了 首先我们来新建一个tasks.py 文件， 内容如下： 1234567from celery import Celeryapp = Celery('hello', broker='redis://localhost:6379/0')@app.taskdef hello(): return 'hello world' 然后我们允许下面的命令启动celery的worker 1celery -A tasks worker --loglevel=info 注： 12这里tasks表示的是上面创建的文件的名字， 比如如果我们的py文件为tasks.py, 则直接 celery -A tasks worker --loglevel=info。 如果我们的py文件为celery-tasks这命令应该修改为：celery -A celery-tasks worker --loglevel=info 启动后我们就可以看到celery已经启动了线程时刻监听redis中的异步函数，如下： 接下来我们分析一下上面的tasks.py文件： 123 1. 首先直接初始化Celery对象， 并指明使用的redis的连接地址 2. 直接用celery对象的task装饰任何我们需要异步的函数简单两步就完成了celery的异步函数 直接执行异步函数 这一步里面我们直接新建test.py文件， 内容如下： 123from tasks import addadd.delay(1,2) 注意这里对add函数的调用采用的是delay函数而不是直接采用add(1,2),因为这样调用就和普通函数调用没有区别了。所以这里一定要注意。运行test.py文件后我们可以看到celery的输出： 在最后面我们可以清楚的看到调用了add函数， add函数的执行结果会返回到redis中这里delay函数是将函数执行异步放入到redis中交给celery执行， 这样delay之后就会有个问题就是如果我们需要理解得到结果怎么办呢？ 我们可以直接调用： 1add.delay(1,2).get() 这样就变成同步的了，等到返回结果才会去执行下一步 celery添加异步任务 celery的使用非常简单 这里我们可以看到需要将一个函数变为异步函数非常简单， 只需要添加@app.task装饰器就够了。 是不是非常简单啊。 配置celery连接redis app.conf.result_backend = ‘redis://localhost:6379/0’ 配置任务执行结果保存地址 app.conf.result_backend = ‘redis://localhost:6379/0’ 前面我们讲到过celery是从中间人取出函数并执行，但是保存结果也需要保存到中间人， 这里实际上取任务的地方和保存结果的中间人实际上可以不一样， 所有这里就提供了中间结果执行的保存地址 集成celery到django中这里以我的一门django搭建在线教育平台的课程为例来讲解，大家如果有兴趣可以去关注一下，课程强力django+杀手级xadmin 首先我们来看一下完整的系统结构图： 1.修改django项目的MxOnline/settings.py文件， 加上： 123###配置BrokerBROKER_URL = 'redis://127.0.0.1:6379/0'BROKER_TRANSPORT = 'redis' 2.在MxOline下面新建celery.py文件 123456789101112131415from __future__ import absolute_importimport osimport djangofrom celery import Celeryfrom django.conf import settingsos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MxOnline.settings')django.setup()app = Celery('MxOnline')app.config_from_object('django.conf:settings')app.autodiscover_tasks(lambda: settings.INSTALLED_APPS) 结构如图所示： 3.在对应的app下面新建tasks.py文件， 这里我们在users这个app下面新建， 如图所示： 文件源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from MxOnline.celery import app@app.taskdef send_register_email(email, send_type="register"): email_record = EmailVerifyRecord() if send_type == "update_email": code = random_str(4) else: code = random_str(16) email_record.code = code email_record.email = email email_record.send_type = send_type email_record.save() email_title = "" email_body = "" if send_type == "register": email_title = "慕学在线网注册激活链接" email_body = "请点击下面的链接激活你的账号: http://www.imooc.com/active/&#123;0&#125;".format(code) send_status = send_mail(email_title, email_body, EMAIL_FROM, [email]) if send_status: pass elif send_type == "forget": email_title = "慕学在线网注册密码重置链接" email_body = "请点击下面的链接重置密码: http://www.imooc.com/reset/&#123;0&#125;".format(code) send_status = send_mail(email_title, email_body, EMAIL_FROM, [email]) if send_status: pass elif send_type == "update_email": email_title = "慕学在线邮箱修改验证码" email_body = "你的邮箱验证码为: &#123;0&#125;".format(code) send_status = send_mail(email_title, email_body, EMAIL_FROM, [email]) if send_status: pass4. 编辑views.py文件完成邮件发送异步调用： #coding:utf-8 from django.shortcuts import render from django.http import HttpResponse from .tasks import send_register_email def index(request): send_register_email.delay() return HttpResponse(u"邮件发送成功， 请查收")5. 进入MxOnline目录运行： celery -A demo worker -l debug 以此来启动celery的worker服务 至此，大功告成了！我们可以在我们定义的任何apps中添加tasks来定义需要的异步任务。 注：原文地址：http://www.projectsedu.com/2017/02/08/django通过celery添加异步任务/]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django，Celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7下通过nginx+uwsgi部署django应用]]></title>
    <url>%2F2018%2F01%2F06%2Fdjango-nginx-uwsgi%2F</url>
    <content type="text"><![CDATA[1. 安装python3.6 1234567891011121314151. 获取wget https://www.python.org/ftp/python/3.6.2/Python-3.6.2.tgztar -xzvf Python-3.6.2.tgz -C /tmpcd /tmp/Python-3.6.2/2. 把Python3.6安装到 /usr/local 目录./configure --prefix=/usr/localmakemake altinstall3. 更改/usr/bin/python链接ln -s /usr/local/bin/python3.6 /usr/bin/python3 2. maridb1234567891011121314151617181920212223242526272829303132333435361. 安装 sudo yum install mariadb-server2. 启动， 重启 sudo systemctl start mariadb sudo systemctl restart mariadb3. 设置bind-ip vim /etc/my.cnf 在 [mysqld]: 下面加一行 bind-address = 0.0.0.04. 设置外部ip可以访问 先进入mysql才能运行下面命令: mysql 直接进入就行 GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; FLUSH PRIVILEGES5. 设置阿里云的对外端口6. 安装mysqlclient出问题 centos 7： yum install python-devel mariadb-devel -y ubuntu： sudo apt-get install libmysqlclient-dev 然后： pip install mysqlclient 3. 安装nginx1https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-centos-7 4. 安装virtualenvwrapper12yum install python-setuptools python-develpip install virtualenvwrapper 环境配置 12345678910111213141516171819编辑.bashrc文件export WORKON_HOME=$HOME/.virtualenvssource /usr/local/bin/virtualenvwrapper.sh重新加载.bashrc文件source ~/.bashrc新建虚拟环境mkvirtualenv mxonline进入虚拟环境 workon mxonline安装pip包我们可以通过 pip freeze &gt; requirements.txt 将本地的虚拟环境安装包相信信息导出来然后将requirements.txt文件上传到服务器之后运行：pip install -r requirements.txt安装依赖包 5. 安装uwsgi1pip install uwsgi 6. 测试uwsgi1uwsgi --http :8000 --module MxOnline.wsgi 7. 配置nginx1234567891011121314151617181920212223242526272829303132333435新建uc_nginx.conf# the upstream component nginx needs to connect toupstream django &#123;# server unix:///path/to/your/mysite/mysite.sock; # for a file socketserver 127.0.0.1:8000; # for a web port socket (we&apos;ll use this first)&#125;# configuration of the serverserver &#123;# the port your site will be served onlisten 80;# the domain name it will serve forserver_name 你的ip地址 ; # substitute your machine&apos;s IP address or FQDNcharset utf-8;# max upload sizeclient_max_body_size 75M; # adjust to taste# Django medialocation /media &#123; alias 你的目录/Mxonline/media; # 指向django的media目录&#125;location /static &#123; alias 你的目录/Mxonline/static; # 指向django的static目录&#125;# Finally, send all non-media requests to the Django server.location / &#123; uwsgi_pass django; include uwsgi_params; # the uwsgi_params file you installed&#125;&#125; 8. 将该配置文件加入到nginx的启动配置文件中1sudo ln -s 你的目录/Mxonline/conf/nginx/uc_nginx.conf /etc/nginx/conf.d/ 9. 拉取所有需要的static file 到同一个目录12345在django的setting文件中，添加下面一行内容： STATIC_ROOT = os.path.join(BASE_DIR, "static/")运行命令 python manage.py collectstatic 10. 运行nginx1sudo /usr/sbin/nginx 这里需要注意 一定是直接用nginx命令启动， 不要用systemctl启动nginx不然会有权限问题 11. 通过配置文件启动uwsgi123456789101112131415161718192021222324252627282930313233343536新建uwsgi.ini 配置文件， 内容如下： # mysite_uwsgi.ini file [uwsgi] # Django-related settings # the base directory (full path) chdir = /home/bobby/Projects/MxOnline # Django's wsgi file module = MxOnline.wsgi # the virtualenv (full path) # process-related settings # master master = true # maximum number of worker processes processes = 10 # the socket (use the full path to be safe socket = 127.0.0.1:8000 # ... with appropriate permissions - may be needed # chmod-socket = 664 # clear environment on exit vacuum = true virtualenv = /home/bobby/.virtualenvs/mxonline logto = /tmp/mylog.log注： chdir： 表示需要操作的目录，也就是项目的目录 module： wsgi文件的路径 processes： 进程数 virtualenv：虚拟环境的目录workon mxonlineuwsgi -i 你的目录/Mxonline/conf/uwsgi.ini &amp; 12.访问1http://你的ip地址/]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django，Celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7下部署sentry日志管理服务]]></title>
    <url>%2F2018%2F01%2F04%2Fcentos-sentry%2F</url>
    <content type="text"><![CDATA[Docker安装1.卸载旧版本 1234sudo yum remove docker \ docker-common \ docker-selinux \ docker-engine 2.安装依赖包 1sudo yum install -y yum-utils device-mapper-persistent-data lvm2 3.添加稳定的源 123sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo 安装docker ce 1.更新yum包 1sudo yum makecache fast 2.安装docker ce 1sudo yum install docker-ce 3.启动docker 1sudo systemctl start docker 4.测试docker 1234567891011121314151617181920212223sudo docker run hello-world输出：Hello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://cloud.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 安装docker-compose 1231. sudo yum install epel-release2. sudo yum install -y python-pip3. sudo pip install docker-compose 2.sentry安装sentry 依赖的组件比较多 包括 redis、 postgresql、 outbound email 在安装sentry前请确保 docker 版本大于1.10 1.安装git 1sudo yum install git 2.下载docker镜像并构建容器 cd进入到要安装的目录，创建一个程序目录 1mkdir -p data/&#123;sentry,postgres&#125; 3.下载onpremise-master项目，放在与刚才创建的data目录同级 两者选一即可 下载地址：https://github.com/getsentry/onpremise 也可以不用下载进行克隆 1sudo yum install git 12$ git clone https://github.com/getsentry/onpremise.git$ cd onpremise 4.cd onpremise,进入到onpremise-master项目，执行命令生成key 注意：以下所有的目录，都是要在onpremise下执行的 1docker-compose run --rm web config generate-secret-key 复制生成的key写入到docker-compose.yml文件 1vim docker-compose.yml 5.生成数据表 1docker-compose run --rm web upgrade 6.启动项目，在9000端口，如果是阿里云服务器记得开放端口 1docker-compose up -d 访问服务器ip加9000端口 可以看到是英文的 首先要改成中文 1 2 以后启动项目 首先启动docker 1sudo systemctl start docker 然后cd进入到onpremise下执行 1docker-compose up -d 错误日志监控配置 python脚本监控 选择监控类型 python监控 123456789101112#!/usr/bin/env python# -*- coding:utf8 -*-from raven import Client# 设置dns的keyclient = Client(&apos;http://f77284e1694144319ff6e27cf1cf9ae3:dd866ea1b3c34604ad9717deca56c320@47.52.39.160:9000/14&apos;)try: 1 / 0except ZeroDivisionError: # 获取错误推送到错误监控 client.captureException()[![复制代码](http://common.cnblogs.com/images/copycode.gif)](javascript:void(0);) 监控 Docker监控 12345678910111213141516INSTALLED_APPS = [ &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;, &apos;app1&apos;, &apos;social_django&apos;, &apos;raven.contrib.django.raven_compat&apos;, # 配置监控APP]# 配置监控配置RAVEN_CONFIG = &#123; &apos;dsn&apos;: &apos;http://5def24308c64410fab2f8f4dda116195:619079b681ee4fb599ab62db4da8524f@47.52.39.160:9000/15&apos;,&#125;]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django, Centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go基础速查手册]]></title>
    <url>%2F2018%2F01%2F03%2Fgo-base-tips%2F</url>
    <content type="text"><![CDATA[要用的时候看一看！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621/* Golang速学速查速用代码手册*/package mainimport ( "errors" "fmt" "github.com/stretchr/testify/assert" "io" "io/ioutil" "log" "math" "os" "path/filepath" "regexp" "strings" "sync" "testing" "time")// 0. 注释/*规范： 1. 命名：骆驼命名法（不要用下划线）命令: go get github.com/lawtech0902/xxx go build calc go run xxx.go go install calc*/// 1. Hello Worldfunc helloWorld() &#123; fmt.Println("Hello, 世界")&#125;// 2.变量类型func typeDemo() &#123; // 变量声明 var v1 int var ( v2 int v3 string ) //var p *int // 指针类型 // 变量初始化 var v4 int = 10 // 等价于: var v5 = 10 // 一般这样就好 v6 := 10 // 赋值，多重赋值 v1 = 10 v2, v3 = 20, "test" // 匿名变量 _ _, v4 = v5, v6 fmt.Println(v1, v2, v3, v4) // 常量 const Pi float64 = 3.1415926 const MaxPlayer = 10 // 枚举 const ( Sunday = iota // iota从0递增 Mondy Tuesday // ... ) // 类型 // 1. 布尔 var b1 bool b1 = true b1 = (1 == 2) fmt.Println(b1) // 2. 整形 // int8 uint8 int16 uint16 int32 uint32 int64 uint64 int uint uintptr var i32 int32 // 强制转换 i32 = int32(64) // 运算：+, -, *, /, %（求余） // 比较：&gt;, &lt;, ==, &gt;=, &lt;=, != // 位运算：x &lt;&lt; y, x &gt;&gt; y, x ^ y, x &amp; y, x | y, ^x （取反） fmt.Println(i32) // 3. 浮点 // float32, float64 var f1 float64 = 1.0001 var f2 float64 = 1.0002 // 浮点比较 isEqual := math.Dim(f1, f2) &lt; 0.0001 fmt.Println(isEqual) // 4. 字符串 var s1 string s1 = "abc" // 字符串连接 s1 = s1 + "ddd" // 取长度 n := len(s1) // 取字符 c1 := s1[0] // 反引号，不转义，常用于正则表达式 s1 = `\w+` fmt.Println(c1) fmt.Println(strings.HasPrefix("prefix", "pre")) // true fmt.Println(strings.HasSuffix("suffix", "fix")) // true // 字节遍历 for i := 0; i &lt; n; i++ &#123; ch := s1[i] fmt.Println(ch) &#125; // Unicode字符遍历 for i, ch := range s1 &#123; fmt.Println(i, ch) &#125; // 5. 数组 var arr1 [32]int //var arr2 [3][8]int // 二维数组 // 初始化 arr1 = [32]int&#123;0&#125; array := [5]int&#123;1, 2, 3, 4, 5&#125; // 临时结构体数组 structArray := []struct &#123; name string age int &#125;&#123;&#123;"Tim", 18&#125;, &#123;"Jim", 20&#125;&#125; // 数组遍历 for i := 0; i &lt; len(array); i++ &#123; fmt.Println(array[i]) &#125; for i, v := range structArray &#123; fmt.Println(i, v) &#125; // 数组是值类型，每次参数传递都是一份拷贝 // 数组切片Slice var mySlice []int = arr1[:2] mySlice1 := make([]int, 5) mySlice2 := make([]int, 5, 10) fmt.Println("len(mySlice2:", len(mySlice2)) // 5 fmt.Println("cap(mySlice2:", cap(mySlice2)) // 10 mySlice3 := append(mySlice, 2, 3, 4) mySlice4 := append(mySlice, mySlice1...) copy(mySlice3, mySlice4) // 6. Map var m map[int]string m[1] = "ddd" m1 := make(map[int]string) m2 := map[int]string&#123; 1: "a", 2: "b", &#125; delete(m2, 1) value, ok := m1[1] if ok &#123; fmt.Println(value) &#125; for k, v := range m2 &#123; fmt.Println(k, v) &#125;&#125;// 3. 流程控制func flowDemo() &#123; // if else a := 10 if a &lt; 10 &#123; // .. &#125; else &#123; // .. &#125; // switch switch a &#123; case 0: fmt.Println("0") case 10: fmt.Println("10") default: fmt.Println("default") &#125; switch &#123; case a &lt; 10: fmt.Println("&lt;10") case a &lt; 20: fmt.Println("&lt;20") &#125; // 循环 for i := 0; i &lt; 10; i++ &#123; &#125; // 无限循环 sum := 0 for &#123; sum++ if sum &gt; 10 &#123; break // 指定break // break JLoop &#125; &#125; goto JLoopJLoop: // break to here&#125;// 4. 函数// func 函数名(参数列表)(返回值列表) &#123;// &#125;func sum1(value1 int, value2 int) (result int, err error) &#123; // err = errors.New("xxxx") return value1 + value2, nil&#125;func sum2(value1, value2 int) int &#123; return value1 + value2&#125;// 不定参数// myFunc(1, 2, 3, 4, 5)func myFunc(args ...int) &#123; for _, arg := range args &#123; fmt.Println(arg) &#125; // 传递 // myFunc2(args...) // myFunc2(args[1:]...)&#125;// 任意类型的不定参数func myPrintf(args ...interface&#123;&#125;) &#123; for _, arg := range args &#123; switch arg.(type) &#123; case int: fmt.Println(arg, "is int") case string: fmt.Println(arg, "is string") default: fmt.Println(arg, "is unknown") &#125; &#125;&#125;// 匿名函数func anonymousFunc() &#123; f := func(a, b int) int &#123; return a + b &#125; f(1, 2)&#125;// deferfunc deferDemo(path string) &#123; f, err := os.Open(path) if err != nil &#123; return &#125; defer f.Close() // or defer func() &#123; if r := recover(); r != nil &#123; fmt.Printf("Runtime error caught: %v", r) &#125; &#125;()&#125;// 5. 结构体type Rect struct &#123; // 小写为private x, y float64 // 大写为public Width, Height float64&#125;// 大写方法为public，小写为privatefunc (r *Rect) Area() float64 &#123; return r.Width * r.Height&#125;func netRect(x, y, width, height float64) *Rect &#123; // 实例化结构体 // rect1 := new(Rect) // rect2 := &amp;Rect&#123;&#125; // rect3 := &amp;Rect&#123;Width:100, Height:200&#125; return &amp;Rect&#123;x, y, width, height&#125;&#125;// 匿名组合type Base struct &#123; Name string&#125;func (base *Base) Foo() &#123;&#125;func (base *Base) Bar() &#123;&#125;type Foo struct &#123; Base *log.Logger&#125;func (foo *Foo) Bar() &#123; foo.Base.Bar() // ...&#125;// 非侵入式接口type IFile interface &#123; Read(buf []byte) (n int, err error) Write(buf []byte) (n int, err error)&#125;type File struct &#123;&#125;func (file *File) Read(buf []byte) (n int, err error) &#123; return 0, nil&#125;func (file *File) Write(buf []byte) (n int, err error) &#123; return 0, nil&#125;func interfaceDemo() &#123; // 只要实现了Read, Write方法即可 var file IFile = new(File) // 接口查询 // 是否实现了IFile接口 if file2, ok := file.(IFile); ok &#123; file2.Read([]byte&#123;&#125;) &#125; // 实例类型是否是File if file3, ok := file.(*File); ok &#123; file3.Read([]byte&#123;&#125;) &#125; // 类型查询 switch v := file.(type) &#123; &#125;&#125;// 6. 并发编程func counting(ch chan int) &#123; ch &lt;- 1 fmt.Println("counting")&#125;func channelDemo() &#123; chs := make([]chan int, 10) for i := 0; i &lt; len(chs); i++ &#123; chs[i] = make(chan int) // 带缓冲区大小 // c: = make(chan int, 1024) // for i:= range c &#123; // &#125; go counting(chs[i]) &#125; for _, ch := range chs &#123; &lt;-ch // channel select /* select &#123; case &lt;-ch: // ... case ch &lt;- 1: &#125; */ &#125; // 单向Channel var ch1 chan&lt;- int // 只能写入int var ch2 &lt;-chan int // 只能读出int // 关闭Channel close(ch1) _, ok := &lt;-ch2 if !ok &#123; // already closed &#125;&#125;// 锁var m sync.Mutexfunc lockDemo() &#123; m.Lock() // do something defer m.Unlock()&#125;// 全局唯一操作var once sync.Once// once.Do(someFunction)// 7. 网络编程// import "net"// net.Dial("tcp", "127.0.0.1:8080")// 8. json处理// import "encoding/json"// json.Marshal(obj) 序列化// json.Unmarshal() 反序列化// 9. Web开发// import "net/http"// 模板// import "html/template"// 10. 常用库// import "os"// import "io"// import "flag"// import "strconv"// import "crypto/sha1"// import "crypto/md5"// 11. 单元测试// _test结尾的go文件： xxx_test.go// 函数名以Test开头func TestDemo(t *testing.T) &#123; r := sum2(2, 3) if r != 5 &#123; t.Errorf("sum2(2, 3) failed. Got %d, expect 5.", r) &#125; assert.Equal(t, 1, 1)&#125;// 12. 性能测试func benchmarkAdd(b *testing.B) &#123; b.StopTimer() // dosometing b.StartTimer()&#125;/* 其他常用的代码片段*/// 1. 遍历文件 filepath.Walk// import "path/filepath"func doHashWalk(dirPath string) error &#123; fullPath, err := filepath.Abs(dirPath) if err != nil &#123; return err &#125; callback := func(path string, fi os.FileInfo, err error) error &#123; return hashFile(fullPath, path, fi, err) &#125; return filepath.Walk(fullPath, callback)&#125;func hashFile(root string, path string, fi os.FileInfo, err error) error &#123; if fi.IsDir() &#123; return nil &#125; rel, err := filepath.Rel(root, path) if err != nil &#123; return err &#125; log.Println("hash rel:", rel, "abs:", path) return nil&#125;// 2. 读取文件// import "io/ioutil"func readFileDemo(filename string) &#123; content, err := ioutil.ReadFile(filename) if err != nil &#123; //Do something &#125; lines := strings.Split(string(content), "\n") fmt.Println("line count:", len(lines))&#125;// 判断目录或文件是否存在func existsPathCheck(path string) (bool, error) &#123; // 判断不存在 if _, err := os.Stat(path); os.IsNotExist(err) &#123; // 不存在 &#125; // 判断是否存在 _, err := os.Stat(path) if err == nil &#123; return true, nil &#125; if os.IsNotExist(err) &#123; return false, nil &#125; return true, err&#125;// 文件目录操作func fileDirDemo() &#123; // 级联创建目录 os.MkdirAll("/path/to/create", 0777)&#125;// 拷贝文件func copyFile(source string, dest string) (err error) &#123; sf, err := os.Open(source) if err != nil &#123; return err &#125; defer sf.Close() df, err := os.Create(dest) if err != nil &#123; return err &#125; defer df.Close() _, err = io.Copy(df, sf) if err == nil &#123; si, err := os.Stat(source) if err != nil &#123; err = os.Chmod(dest, si.Mode()) &#125; &#125; return&#125;// 拷贝目录func copyDir(source string, dest string) (err error) &#123; fi, err := os.Stat(source) if err != nil &#123; return err &#125; if !fi.IsDir() &#123; return errors.New(source + " is not a directory") &#125; err = os.MkdirAll(dest, fi.Mode()) if err != nil &#123; return err &#125; entries, err := ioutil.ReadDir(source) for _, entry := range entries &#123; sfp := filepath.Join(source, entry.Name()) dfp := filepath.Join(dest, entry.Name()) if entry.IsDir() &#123; err = copyDir(sfp, dfp) if err != nil &#123; fmt.Println(err) &#125; &#125; else &#123; err = copyFile(sfp, dfp) if err != nil &#123; fmt.Println(err) &#125; &#125; &#125; return nil&#125;// 3. 时间处理// import "time"func TestTimeDemo(t *testing.T) &#123; // Parse postDate, err := time.Parse("2006-01-02 15:04:05", "2015-09-30 19:19:00") fmt.Println(postDate, err) // Format assert.Equal(t, "2015/Sep/30 07:19:00", postDate.Format("2006/Jan/02 03:04:05")) assert.Equal(t, "2015-09-30T19:19:00Z", postDate.Format(time.RFC3339))&#125;// 4. 正则表达式// import "regexp"func TestRegexp(t *testing.T) &#123; // 查找匹配 re := regexp.MustCompile(`(\d+)-(\d+)`) r := re.FindAllStringSubmatch("123-666", -1) assert.Equal(t, 1, len(r)) assert.Equal(t, "123", r[0][1]) assert.Equal(t, "666", r[0][2])&#125;func main() &#123; helloWorld()&#125;]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04安装配置GPU版TensorFlow]]></title>
    <url>%2F2017%2F12%2F25%2Fubuntu-tensorflow-gpu%2F</url>
    <content type="text"><![CDATA[基本工作 更新系统（不更新好像也行） 123sudo apt-get updatesudo apt-get upgrade -ysudo apt-get dist-upgrade -y 安装依赖 12# 先执行uname -r，查看结果，比如4.4.0-45-generic，然后再执行sudo apt-get install build-essential pkg-config xserver-xorg-dev linux-headers-4.4.0-45-generic(此处记得替换) 禁用nouveau内核 123456789101112sudo vim /etc/modprobe.d/blacklist.conf # 添加如下内容：blacklist nouveaublacklist lbm-nouveauoptions nouveau modeset=0alias nouveau offalias lbm-nouveau off# 然后运行以下命令即可：echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.confsudo update-initramfs -u 重启之后即可禁用nouveau，可以运行$ lsmod | grep nouveau检查是否禁用成功，如果运行后没有任何输出，则代表禁用成功。 安装显卡驱动首先lspci查看自己的GPU显卡版本，然后到Nvidia官方网站下载对应版本的显卡驱动，之后执行命令sudo service lightdm stop，通过Ctrl+Alt+F1进入init3(文本模式)，输入账号密码进行安装，执行以下命令： 12sudo chmod +x NVIDIA-Linux-x86_64-375.51.runsudo ./NVIDIA-Linux-x86_64-375.51.run -no-x-check -no-nouveau-check -no-opengl-files 除了dkms和open-gl选no，其他一路yes和回车就搞定了。 然后sudo service lightdm start，可以通过sudo nvidia-xconfig -query-gpu-info和nvidia-smi查看GPU信息。 安装CUDA到CUDA官网下载，这里下载的是8.0版本CUDA，然后执行命令安装： 12sudo chmod +x cuda_8.0.61_375.26_linux.runsudo ./cuda_8.0.61_375.26_linux.run 除了OpenGL选no，还有Nvidia选no（因为已经装过了，不然会冲突），其他一路yes和回车就搞定了。如果报错比如X server is running或者unable to locate the kernel的问题，确保之前提到的准备工作全部做好，绝对不会有问题。 安装好CUDA之后，添加环境变量： 123456789101112131415161718192021sudo vim /etc/profile # 添加内容如下：PATH=/usr/local/cuda-8.0/bin:$PATH export PATH export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/lib # 保存退出，使之立即生效source /etc/profile# 添加lib库路径sudo vim /etc/ld.so.conf.d/cuda.conf # 添加内容如下：/usr/local/cuda-8.0/lib64 # 保存退出，使之立即生效sudo ldconfig# 验证安装结果，有服务器信息和cuda版本号即可nvcc -V 安装cuDNN在cuDNN官网上下载，需要注册，必须下载cuDNN v5.1 Library for Linux版本，执行以下命令： 12345678sudo tar -zxvf cudnn-8.0-linux-x64-v5.1.tgzcd cudasudo cp lib64/lib* /usr/local/cuda/lib64/ sudo cp include/cudnn.h /usr/local/cuda/include/ cd /usr/local/cuda/lib64/ sudo rm -rf libcudnn.so libcudnn.so.5 sudo ln -s libcudnn.so.5.1.10 libcudnn.so.5 sudo ln -s libcudnn.so.5 libcudnn.so 然后可以测试CUDA等是否成功配置，编译运行某个sample： 123456cd /usr/local/cuda/samplessudo make all -j4# 全部编译完成之后cd /usr/local/cuda/samples/bin/x86_64/linux/release./deviceQuery # 如果显示有可用的GPU即为成功 安装anaconda下载anaconda，执行命令bash Anaconda3-4.3.1-Linux-x86_64.sh ，然后一路yes即可。 安装TensorFlow**打开一个新的terminal，选择用conda创建独立环境： 1234567891011121314151617conda create -n tensor_test python==3.5source activate tensor_test# 直接用pip安装pip install tensorflow-gpu==1.2# 进入python环境测试import tensorflow as tf# 如下可以查看tensorflow的版本号和安装路径tf.__version__tf.__path__# 如果报ImportError:libcudnn.so.5 cannot open shared object file: No such file or directory, 直接暴力解决sudo cp /usr/local/cuda-8.0/lib64/libcudnn.so /usr/local/lib/libcudnn.so &amp;&amp; sudo ldconfigsudo cp /usr/local/cuda-8.0/lib64/libcudnn.so.5 /usr/local/lib/libcudnn.so.5 &amp;&amp; sudo ldconfigsudo cp /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5 /usr/local/lib/libcudnn.so.5.1.5 &amp;&amp; sudo ldconfig 一定注意版本，tensorflow-gpu最新版本为1.4，但是会报libcudnn.so.6:cannot open sharedobject file: No such file or directory错误，说明1.4已经开始去找cudnn6了，但我们这是5.1，理论上可以换用cudnn6，算了，还是做一个没有梦想的咸鱼吧，能用就行，换用tensorflow-gpu 1.2版本完美解决。 重重困难，还得靠自己解决。（Fuck the Christmas!!!）]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow, GPU, Ubuntu, Cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7部署django应用]]></title>
    <url>%2F2017%2F12%2F22%2Fcentos-django%2F</url>
    <content type="text"><![CDATA[1. 安装python3.61234567891011121314151. 获取wget https://www.python.org/ftp/python/3.6.3/Python-3.6.3.tgztar -xzvf Python-3.6.3.tgz -C /tmpcd /tmp/Python-3.6.3/2. 把Python3.6安装到 /usr/local 目录./configure --prefix=/usr/localmakemake altinstall3. 更改/usr/bin/python链接ln -s /usr/local/bin/python3.6 /usr/bin/python3 2. mariadb12345678910111213141516171819202122232425262728293031323334353637381. 安装 sudo yum install mariadb-server2. 启动， 重启 sudo systemctl start mariadb sudo systemctl restart mariadb3. 设置bind-ip vim /etc/my.cnf 在 [mysqld]: 下面加一行 bind-address = 0.0.0.04. 设置外部ip可以访问 先进入mysql才能运行下面命令: mysql 直接进入就行 GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION; FLUSH PRIVILEGES5. 设置阿里云的对外端口 视频中有讲解这部分6. 安装mysqlclient出问题 centos 7： yum install python-devel mariadb-devel -y ubuntu： sudo apt-get install libmysqlclient-dev 然后： pip install mysqlclient 3. 安装nginx1https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-centos-7 4. 安装virtualenvwrapper12yum install python-setuptools python-develpip install virtualenvwrapper 123&gt; 编辑.bashrc文件&gt;&gt; 12export WORKON_HOME=$HOME/.virtualenvssource /usr/local/bin/virtualenvwrapper.sh 123456789101112&gt; 重新加载.bashrc文件&gt; source ~/.bashrc&gt;&gt; 新建虚拟环境&gt; mkvirtualenv mxonline&gt;&gt; 进入虚拟环境 &gt; workon mxonline&gt;&gt; 安装pip包&gt;&gt; 123456我们可以通过 pip freeze &gt; requirements.txt 将本地的虚拟环境安装包相信信息导出来然后将requirements.txt文件上传到服务器之后运行：pip install -r requirements.txt安装依赖包 5. 安装uwsgi1pip install uwsgi 6. 测试uwsgi1uwsgi --http :8000 --module MxOnline.wsgi 7. 配置nginx1234567891011121314151617181920212223242526272829303132333435新建uc_nginx.conf# the upstream component nginx needs to connect toupstream django &#123;# server unix:///path/to/your/mysite/mysite.sock; # for a file socketserver 127.0.0.1:8000; # for a web port socket (we&apos;ll use this first)&#125;# configuration of the serverserver &#123;# the port your site will be served onlisten 80;# the domain name it will serve forserver_name 你的ip地址 ; # substitute your machine&apos;s IP address or FQDNcharset utf-8;# max upload sizeclient_max_body_size 75M; # adjust to taste# Django medialocation /media &#123; alias 你的目录/Mxonline/media; # 指向django的media目录&#125;location /static &#123; alias 你的目录/Mxonline/static; # 指向django的static目录&#125;# Finally, send all non-media requests to the Django server.location / &#123; uwsgi_pass django; include uwsgi_params; # the uwsgi_params file you installed&#125;&#125; 8. 将该配置文件加入到nginx的启动配置文件中1sudo ln -s 你的目录/Mxonline/conf/nginx/uc_nginx.conf /etc/nginx/conf.d/ 8. 拉取所有需要的static file 到同一个目录12345在django的setting文件中，添加下面一行内容： STATIC_ROOT = os.path.join(BASE_DIR, &quot;static/&quot;)运行命令 python manage.py collectstatic 9. 运行nginx1sudo /usr/sbin/nginx 这里需要注意 一定是直接用nginx命令启动， 不要用systemctl启动nginx不然会有权限问题 10. 通过配置文件启动uwsgi123456789101112131415161718192021222324252627282930313233343536新建uwsgi.ini 配置文件， 内容如下： # mysite_uwsgi.ini file [uwsgi] # Django-related settings # the base directory (full path) chdir = /home/bobby/Projects/MxOnline # Django&apos;s wsgi file module = MxOnline.wsgi # the virtualenv (full path) # process-related settings # master master = true # maximum number of worker processes processes = 10 # the socket (use the full path to be safe socket = 127.0.0.1:8000 # ... with appropriate permissions - may be needed # chmod-socket = 664 # clear environment on exit vacuum = true virtualenv = /home/bobby/.virtualenvs/mxonline logto = /tmp/mylog.log注： chdir： 表示需要操作的目录，也就是项目的目录 module： wsgi文件的路径 processes： 进程数 virtualenv：虚拟环境的目录workon mxonlineuwsgi -i 你的目录/Mxonline/conf/uwsgi.ini &amp; 访问1http://你的ip地址/]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django, Centos, nginx, uwsgi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API设计指南]]></title>
    <url>%2F2017%2F12%2F07%2FRESTful-API%2F</url>
    <content type="text"><![CDATA[网络应用程序，分为前端和后端两个部分。当前的发展趋势，就是前端设备层出不穷（手机、平板、桌面电脑、其他专用设备……）。 因此，必须有一种统一的机制，方便不同的前端设备与后端进行通信。这导致API构架的流行，甚至出现“API First”的设计思想。RESTful API是目前比较成熟的一套互联网应用程序的API设计理论。 一、协议API与用户的通信协议，总是使用HTTPs协议。 二、域名应该尽量将API部署在专用域名之下。 123&gt; https://api.example.com&gt;&gt; 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。 123&gt; https://example.org/api/&gt;&gt; 三、版本（Versioning）应该将API的版本号放入URL。 123&gt; https://api.example.com/v1/&gt;&gt; 另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观。Github采用这种做法。 四、路径（Endpoint）路径又称”终点”（endpoint），表示API的具体网址。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。 https://api.example.com/v1/zoos https://api.example.com/v1/animals https://api.example.com/v1/employees 五、HTTP动词对于资源的具体操作类型，由HTTP动词表示。 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 六、过滤信息（Filtering）如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。 下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。 七、状态码（Status Codes）服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表参见这里。 八、错误处理（Error handling）如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。 12345&gt; &#123;&gt; error: &quot;Invalid API key&quot;&gt; &#125;&gt;&gt; 九、返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范。 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 十、Hypermedia APIRESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。 12345678&gt; &#123;&quot;link&quot;: &#123;&gt; &quot;rel&quot;: &quot;collection https://www.example.com/zoos&quot;,&gt; &quot;href&quot;: &quot;https://api.example.com/zoos&quot;,&gt; &quot;title&quot;: &quot;List of zoos&quot;,&gt; &quot;type&quot;: &quot;application/vnd.yourformat+json&quot;&gt; &#125;&#125;&gt;&gt; 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。 Hypermedia API的设计被称为HATEOAS。Github的API就是这种设计，访问api.github.com会得到一个所有可用API的网址列表。 1234567&gt; &#123;&gt; &quot;current_user_url&quot;: &quot;https://api.github.com/user&quot;,&gt; &quot;authorizations_url&quot;: &quot;https://api.github.com/authorizations&quot;,&gt; // ...&gt; &#125;&gt;&gt; 从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。 123456&gt; &#123;&gt; &quot;message&quot;: &quot;Requires authentication&quot;,&gt; &quot;documentation_url&quot;: &quot;https://developer.github.com/v3&quot;&gt; &#125;&gt;&gt; 上面代码表示，服务器给出了提示信息，以及文档的网址。 十一、其他（1）API的身份认证应该使用OAuth 2.0框架。 （2）服务器返回的数据格式，应该尽量使用JSON，避免使用XML。 （完） 注：原文地址：http://www.ruanyifeng.com/blog/2014/05/restful_api.html]]></content>
      <categories>
        <category>RESTful</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解RESTful架构]]></title>
    <url>%2F2017%2F12%2F07%2FRESTful%2F</url>
    <content type="text"><![CDATA[越来越多的人开始意识到，网站即软件，而且是一种新型的软件。 这种”互联网软件”采用客户端/服务器模式，建立在分布式体系上，通过互联网通信，具有高延时（high latency）、高并发等特点。 网站开发，完全可以采用软件开发的模式。但是传统上，软件和网络是两个不同的领域，很少有交集；软件开发主要针对单机环境，网络则主要研究系统之间的通信。互联网的兴起，使得这两个领域开始融合，现在我们必须考虑，如何开发在互联网环境中使用的软件。 RESTful架构，就是目前最流行的一种互联网软件架构。它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用。 但是，到底什么是RESTful架构，并不是一个容易说清楚的问题。下面，我就谈谈我理解的RESTful架构。 一、起源 REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的。 Fielding是一个非常重要的人，他是HTTP协议（1.0版和1.1版）的主要设计者、Apache服务器软件的作者之一、Apache基金会的第一任主席。所以，他的这篇论文一经发表，就引起了关注，并且立即对互联网开发产生了深远的影响。 他这样介绍论文的写作目的： “本文研究计算机科学两大前沿—-软件和网络—-的交叉点。长期以来，软件研究主要关注软件设计的分类、设计方法的演化，很少客观地评估不同的设计选择对系统行为的影响。而相反地，网络研究主要关注系统之间通信行为的细节、如何改进特定通信机制的表现，常常忽视了一个事实，那就是改变应用程序的互动风格比改变互动协议，对整体表现有更大的影响。我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。“ (This dissertation explores a junction on the frontiers of two research disciplines in computer science: software and networking. Software research has long been concerned with the categorization of software designs and the development of design methodologies, but has rarely been able to objectively evaluate the impact of various design choices on system behavior. Networking research, in contrast, is focused on the details of generic communication behavior between systems and improving the performance of particular communication techniques, often ignoring the fact that changing the interaction style of an application can have more impact on performance than the communication protocols used for that interaction. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. ) 二、名称 Fielding将他对互联网软件的架构原则，定名为REST，即Representational State Transfer的缩写。我对这个词组的翻译是”表现层状态转化”。 如果一个架构符合REST原则，就称它为RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。 三、资源（Resources） REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓”上网”，就是与互联网上一系列的”资源”互动，调用它的URI。 四、表现层（Representation） “资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。 比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 五、状态转化（State Transfer） 访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 六、综述 综合上面的解释，我们总结一下什么是RESTful架构： （1）每一个URI代表一种资源； （2）客户端和服务器之间，传递这种资源的某种表现层； （3）客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 七、误区 RESTful架构有一些典型的设计误区。 最常见的一种设计错误，就是URI包含动词。因为”资源”表示一种实体，所以应该是名词，URI不应该有动词，动词应该放在HTTP协议中。 举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。 如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是： POST /accounts/1/transfer/500/to/2 正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务： POST /transaction HTTP/1.1 Host: 127.0.0.1 from=1&amp;to=2&amp;amount=500.00 另一个设计误区，就是在URI中加入版本号： http://www.example.com/app/1.0/foo http://www.example.com/app/1.1/foo http://www.example.com/app/2.0/foo 因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见Versioning REST Services）： Accept: vnd.example-com.foo+json; version=1.0 Accept: vnd.example-com.foo+json; version=1.1 Accept: vnd.example-com.foo+json; version=2.0 （完） 注：原文地址：http://www.ruanyifeng.com/blog/2011/09/restful.html]]></content>
      <categories>
        <category>RESTful</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前后端分离之JWT用户认证]]></title>
    <url>%2F2017%2F12%2F06%2Fjwt%2F</url>
    <content type="text"><![CDATA[在前后端分离开发时为什么需要用户认证呢？原因是由于HTTP协定是不储存状态的(stateless)，这意味着当我们透过帐号密码验证一个使用者时，当下一个request请求时它就把刚刚的资料忘了。于是我们的程序就不知道谁是谁，就要再验证一次。所以为了保证系统安全，我们就需要验证用户否处于登录状态。 传统方式前后端分离通过Restful API进行数据交互时，如何验证用户的登录信息及权限。在原来的项目中，使用的是最传统也是最简单的方式，前端登录，后端根据用户信息生成一个token，并保存这个 token 和对应的用户id到数据库或Session中，接着把 token 传给用户，存入浏览器 cookie，之后浏览器请求带上这个cookie，后端根据这个cookie值来查询用户，验证是否过期。 但这样做问题就很多，如果我们的页面出现了 XSS 漏洞，由于 cookie 可以被 JavaScript 读取，XSS 漏洞会导致用户 token 泄露，而作为后端识别用户的标识，cookie 的泄露意味着用户信息不再安全。尽管我们通过转义输出内容，使用 CDN 等可以尽量避免 XSS 注入，但谁也不能保证在大型的项目中不会出现这个问题。 在设置 cookie 的时候，其实你还可以设置 httpOnly 以及 secure 项。设置 httpOnly 后 cookie 将不能被 JS 读取，浏览器会自动的把它加在请求的 header 当中，设置 secure 的话，cookie 就只允许通过 HTTPS 传输。secure 选项可以过滤掉一些使用 HTTP 协议的 XSS 注入，但并不能完全阻止。 httpOnly 选项使得 JS 不能读取到 cookie，那么 XSS 注入的问题也基本不用担心了。但设置 httpOnly 就带来了另一个问题，就是很容易的被 XSRF，即跨站请求伪造。当你浏览器开着这个页面的时候，另一个页面可以很容易的跨站请求这个页面的内容。因为 cookie 默认被发了出去。 另外，如果将验证信息保存在数据库中，后端每次都需要根据token查出用户id，这就增加了数据库的查询和存储开销。若把验证信息保存在session中，有加大了服务器端的存储压力。那我们可不可以不要服务器去查询呢？如果我们生成token遵循一定的规律，比如我们使用对称加密算法来加密用户id形成token，那么服务端以后其实只要解密该token就可以知道用户的id是什么了。不过呢，我只是举个例子而已，要是真这么做，只要你的对称加密算法泄露了，其他人可以通过这种加密方式进行伪造token，那么所有用户信息都不再安全了。恩，那用非对称加密算法来做呢，其实现在有个规范就是这样做的，就是我们接下来要介绍的 JWT。 Json Web Token（JWT）JWT 是一个开放标准(RFC 7519)，它定义了一种用于简洁，自包含的用于通信双方之间以 JSON 对象的形式安全传递信息的方法。JWT 可以使用 HMAC 算法或者是 RSA 的公钥密钥对进行签名。它具备两个特点： 简洁(Compact) 可以通过URL, POST 参数或者在 HTTP header 发送，因为数据量小，传输速度快 自包含(Self-contained) 负载中包含了所有用户所需要的信息，避免了多次查询数据库 JWT 组成 Header 头部 头部包含了两部分，token 类型和采用的加密算法 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; 它会使用 Base64 编码组成 JWT 结构的第一部分,如果你使用Node.js，可以用Node.js的包base64url来得到这个字符串。 Base64是一种编码，也就是说，它是可以被翻译回原来的样子来的。它并不是一种加密过程。 Payload 负载 这部分就是我们存放信息的地方了，你可以把用户 ID 等信息放在这里，JWT 规范里面对这部分有进行了比较详细的介绍，常用的由 iss（签发者），exp（过期时间），sub（面向的用户），aud（接收方），iat（签发时间）。 1234567&#123; &quot;iss&quot;: &quot;lion1ou JWT&quot;, &quot;iat&quot;: 1441593502, &quot;exp&quot;: 1441594722, &quot;aud&quot;: &quot;www.example.com&quot;, &quot;sub&quot;: &quot;lion1ou@163.com&quot;&#125; 同样的，它会使用 Base64 编码组成 JWT 结构的第二部分 Signature 签名 前面两部分都是使用 Base64 进行编码的，即前端可以解开知道里面的信息。Signature 需要使用编码后的 header 和 payload 以及我们提供的一个密钥，然后使用 header 中指定的签名算法（HS256）进行签名。签名的作用是保证 JWT 没有被篡改过。 三个部分通过.连接在一起就是我们的 JWT 了，它可能长这个样子，长度貌似和你的加密算法和私钥有关系。 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjU3ZmVmMTY0ZTU0YWY2NGZmYzUzZGJkNSIsInhzcmYiOiI0ZWE1YzUwOGE2NTY2ZTc2MjQwNTQzZjhmZWIwNmZkNDU3Nzc3YmUzOTU0OWM0MDE2NDM2YWZkYTY1ZDIzMzBlIiwiaWF0IjoxNDc2NDI3OTMzfQ.PA3QjeyZSUh7H0GfE0vJaKW4LjKJuC3dVLQiY4hii8s 其实到这一步可能就有人会想了，HTTP 请求总会带上 token，这样这个 token 传来传去占用不必要的带宽啊。如果你这么想了，那你可以去了解下 HTTP2，HTTP2 对头部进行了压缩，相信也解决了这个问题。 签名的目的 最后一步签名的过程，实际上是对头部以及负载内容进行签名，防止内容被窜改。如果有人对头部以及负载的内容解码之后进行修改，再进行编码，最后加上之前的签名组合形成新的JWT的话，那么服务器端会判断出新的头部和负载形成的签名和JWT附带上的签名是不一样的。如果要对新的头部和负载进行签名，在不知道服务器加密时用的密钥的话，得出来的签名也是不一样的。 信息暴露 在这里大家一定会问一个问题：Base64是一种编码，是可逆的，那么我的信息不就被暴露了吗？ 是的。所以，在JWT中，不应该在负载里面加入任何敏感的数据。在上面的例子中，我们传输的是用户的User ID。这个值实际上不是什么敏感内容，一般情况下被知道也是安全的。但是像密码这样的内容就不能被放在JWT中了。如果将用户的密码放在了JWT中，那么怀有恶意的第三方通过Base64解码就能很快地知道你的密码了。 因此JWT适合用于向Web应用传递一些非敏感信息。JWT还经常用于设计用户认证和授权系统，甚至实现Web应用的单点登录。 JWT 使用 首先，前端通过Web表单将自己的用户名和密码发送到后端的接口。这一过程一般是一个HTTP POST请求。建议的方式是通过SSL加密的传输（https协议），从而避免敏感信息被嗅探。 后端核对用户名和密码成功后，将用户的id等其他信息作为JWT Payload（负载），将其与头部分别进行Base64编码拼接后签名，形成一个JWT。形成的JWT就是一个形同lll.zzz.xxx的字符串。 后端将JWT字符串作为登录成功的返回结果返回给前端。前端可以将返回的结果保存在localStorage或sessionStorage上，退出登录时前端删除保存的JWT即可。 前端在每次请求时将JWT放入HTTP Header中的Authorization位。(解决XSS和XSRF问题) 后端检查是否存在，如存在验证JWT的有效性。例如，检查签名是否正确；检查Token是否过期；检查Token的接收方是否是自己（可选）。 验证通过后后端使用JWT中包含的用户信息进行其他逻辑操作，返回相应结果。 和Session方式存储id的差异Session方式存储用户id的最大弊病在于Session是存储在服务器端的，所以需要占用大量服务器内存，对于较大型应用而言可能还要保存许多的状态。一般而言，大型应用还需要借助一些KV数据库和一系列缓存机制来实现Session的存储。 而JWT方式将用户状态分散到了客户端中，可以明显减轻服务端的内存压力。除了用户id之外，还可以存储其他的和用户相关的信息，例如该用户是否是管理员、用户所在的分组等。虽说JWT方式让服务器有一些计算压力（例如加密、编码和解码），但是这些压力相比磁盘存储而言可能就不算什么了。具体是否采用，需要在不同场景下用数据说话。 单点登录 Session方式来存储用户id，一开始用户的Session只会存储在一台服务器上。对于有多个子域名的站点，每个子域名至少会对应一台不同的服务器，例如：www.taobao.com，nv.taobao.com，nz.taobao.com，login.taobao.com。所以如果要实现在login.taobao.com登录后，在其他的子域名下依然可以取到Session，这要求我们在多台服务器上同步Session。使用JWT的方式则没有这个问题的存在，因为用户的状态已经被传送到了客户端。 总结JWT的主要作用在于（一）可附带用户信息，后端直接通过JWT获取相关信息。（二）使用本地保存，通过HTTP Header中的Authorization位提交验证。但其实关于JWT存放到哪里一直有很多讨论，有人说存放到本地存储，有人说存 cookie。个人偏向于放在本地存储，如果你有什么意见和看法欢迎提出。 注：原文地址：http://lion1ou.win/2017/01/18/]]></content>
      <categories>
        <category>JWT</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django从请求到返回都经历了什么]]></title>
    <url>%2F2017%2F12%2F06%2Fdjango-request-process%2F</url>
    <content type="text"><![CDATA[从runserver说起1ruserver是使用django自己的web server，主要用于开发和调试中， 部署到线上环境一般使用nginx+uwsgi模式 manage.py 探秘 12345看一下manager.py的源码，你会发现上面的命令其实是通过Django的execute_from_command_line方法执行了内部实现的runserver命令，那么现在看一下runserver具体做了什么。。通过源码分析可知， ruserserver主要完成两件事：1). 解析参数，并通过django.core.servers.basehttp.get_internal_wsgi_application方法获取wsgi handler;2). 根据ip_address和port生成一个WSGIServer对象，接受用户请求 get_internal_wsgi_application的源码如下： 12通过上面的代码我们可以知道，Django会先根据settings中的WSGI_APPLICATION来获取handler；在创建project的时候，Django会默认创建一个wsgi.py文件，而settings中的WSGI_APPLICATION配置也会默认指向这个文件。看一下这个wsgi.py文件，其实它也和上面的逻辑一样，最终调用get_wsgi_application实现。 django http请求处理流程1Django和其他Web框架一样，HTTP的处理流程基本类似：接受request，返回response内容。Django的具体处理流程大致如下： 1. 加载settings.py12345在通过django-admin.py创建project的时候，Django会自动生成默认的settings文件和manager.py等文件，在创建WSGIServer之前会执行下面的引用：from django.conf import settings上面引用在执行时，会读取os.environ中的DJANGO_SETTINGS_MODULE配置，加载项目配置文件，生成settings对象。所以，在manager.py文件中你可以看到，在获取WSGIServer之前，会先将project的settings路径加到os路径中。 2. 创建WSGIServer123456789101112131415161718192021222324252627不管是使用runserver还是uWSGI运行Django项目，在启动时都会调用django.core.servers.basehttp中的run()方法创建一个django.core.servers.basehttp.WSGIServer类的实例，之后调用其serve_forever()方法启动HTTP服务。run方法的源码如下：def run(addr, port, wsgi_handler, ipv6=False, threading=False): server_address = (addr, port) if threading: httpd_cls = type(str(&apos;WSGIServer&apos;), (socketserver.ThreadingMixIn, WSGIServer), &#123;&#125;) else: httpd_cls = WSGIServer httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6) if threading: # ThreadingMixIn.daemon_threads indicates how threads will behave on an # abrupt shutdown; like quitting the server by the user or restarting # by the auto-reloader. True means the server will not wait for thread # termination before it quits. This will make auto-reloader faster # and will prevent the need to kill the server manually if a thread # isn&apos;t terminating correctly. httpd.daemon_threads = True httpd.set_app(wsgi_handler) httpd.serve_forever()如上，我们可以看到：在创建WSGIServer实例的时候会指定HTTP请求的Handler，上述代码使用WSGIRequestHandler。当用户的HTTP请求到达服务器时，WSGIServer会创建WSGIRequestHandler实例，使用其handler方法来处理HTTP请求(其实最终是调用wsgiref.handlers.BaseHandler中的run方法处理)。WSGIServer通过set_app方法设置一个可调用(callable)的对象作为application，上面提到的handler方法最终会调用设置的application处理request，并返回response。其中，WSGIServer继承自wsgiref.simple_server.WSGIServer，而WSGIRequestHandler继承自wsgiref.simple_server.WSGIRequestHandler，wsgiref是Python标准库给出的WSGI的参考实现。其源码可自行到wsgiref参看，这里不再细说。 3. 处理Request1第二步中说到的application，在Django中一般是django.core.handlers.wsgi.WSGIHandler对象，WSGIHandler继承自django.core.handlers.base.BaseHandler，这个是Django处理request的核心逻辑，它会创建一个WSGIRequest实例，而WSGIRequest是从http.HttpRequest继承而来 4. 返回Response123上面提到的BaseHandler中有个get_response方法，该方法会先加载Django项目的ROOT_URLCONF，然后根据url规则找到对应的view方法(类)，view逻辑会根据request实例生成并返回具体的response。在Django返回结果之后，第二步中提到wsgiref.handlers.BaseHandler.run方法会调用finish_response结束请求，并将内容返回给用户 Django处理Request的详细流程1首先给大家分享两个网上看到的Django流程图： ​ 流程图一 流程图二 上面的两张流程图可以大致描述Django处理request的流程，按照流程图2的标注，可以分为以下几个步骤： 12345678910111213141516171. 用户通过浏览器请求一个页面2.请求到达Request Middlewares，中间件对request做一些预处理或者直接response请求3.URLConf通过urls.py文件和请求的URL找到相应的View4.View Middlewares被访问，它同样可以对request做一些处理或者直接返回response5.调用View中的函数6.View中的方法可以选择性的通过Models访问底层的数据7.所有的Model-to-DB的交互都是通过manager完成的8.如果需要，Views可以使用一个特殊的Context9.Context被传给Template用来生成页面 a.Template使用Filters和Tags去渲染输出 b.输出被返回到View c.HTTPResponse被发送到Response Middlewares d.任何Response Middlewares都可以丰富response或者返回一个完全不同的response e.Response返回到浏览器，呈现给用户上述流程中最主要的几个部分分别是：Middleware(中间件，包括request, view, exception, response)，URLConf(url映射关系)，Template(模板系统)，下面一一介绍一下。 1. Middleware(中间件)1234567891011121314151617181920212223242526Middleware并不是Django所独有的东西，在其他的Web框架中也有这种概念。在Django中，Middleware可以渗入处理流程的四个阶段：request，view，response和exception，相应的，在每个Middleware类中都有rocess_request，process_view， process_response 和 process_exception这四个方法。你可以定义其中任意一个活多个方法，这取决于你希望该Middleware作用于哪个处理阶段。每个方法都可以直接返回response对象。Middleware是在Django BaseHandler的load_middleware方法执行时加载的，加载之后会建立四个列表作为处理器的实例变量：_request_middleware：process_request方法的列表_view_middleware：process_view方法的列表_response_middleware：process_response方法的列表_exception_middleware：process_exception方法的列表Django的中间件是在其配置文件(settings.py)的MIDDLEWARE_CLASSES元组中定义的。在MIDDLEWARE_CLASSES中，中间件组件用字符串表示：指向中间件类名的完整Python路径。例如`MIDDLEWARE_CLASSES = [ &apos;django.middleware.security.SecurityMiddleware&apos;, &apos;django.contrib.sessions.middleware.SessionMiddleware&apos;, &apos;django.middleware.common.CommonMiddleware&apos;, &apos;django.middleware.csrf.CsrfViewMiddleware&apos;, &apos;django.contrib.auth.middleware.AuthenticationMiddleware&apos;, &apos;django.contrib.auth.middleware.SessionAuthenticationMiddleware&apos;, &apos;django.contrib.messages.middleware.MessageMiddleware&apos;, &apos;django.middleware.clickjacking.XFrameOptionsMiddleware&apos;,]`Django项目的安装并不强制要求任何中间件，如果你愿意，MIDDLEWARE_CLASSES可以为空。中间件出现的顺序非常重要：在request和view的处理阶段，Django按照MIDDLEWARE_CLASSES中出现的顺序来应用中间件，而在response和exception异常处理阶段，Django则按逆序来调用它们。也就是说，Django将MIDDLEWARE_CLASSES视为view函数外层的顺序包装子：在request阶段按顺序从上到下穿过，而在response则反过来。以下这张图可以更好地帮助你理解： URLConf(URL映射) 123如果处理request的中间件都没有直接返回response，那么Django会去解析用户请求的URL。URLconf就是Django所支撑网站的目录。它的本质是URL模式以及要为该URL模式调用的视图函数之间的映射表。通过这种方式可以告诉Django，对于这个URL调用这段代码，对于那个URL调用那段代码。具体的，在Django项目的配置文件中有ROOT_URLCONF常量，这个常量加上根目录&quot;/&quot;，作为参数来创建django.core.urlresolvers.RegexURLResolver的实例，然后通过它的resolve方法解析用户请求的URL，找到第一个匹配的view。有关urlconf的内容，大家可以参考 [理解curlConf]() Template(模板) 1大部分web框架都有自己的Template(模板)系统，Django也是。但是，Django模板不同于Mako模板和jinja2模板，在Django模板不能直接写Python代码，只能通过额外的定义filter和template tag实现。由于本文主要介绍Django流程，模板内容就不过多介绍。 注：原文地址：http://projectsedu.com/2016/10/17/django%E4%BB%8E%E8%AF%B7%E6%B1%82%E5%88%B0%E8%BF%94%E5%9B%9E%E9%83%BD%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BB%80%E4%B9%88/]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随笔12.05]]></title>
    <url>%2F2017%2F12%2F05%2Fessay-12-05%2F</url>
    <content type="text"><![CDATA[最近不想写这些无聊的blog了~心情也不好，准备搞一搞Java，emmmmm，又回到最初的起点。]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>12.05</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue+Django REST Framework生鲜电商项目学习笔记——商品类别数据和Vue展示]]></title>
    <url>%2F2017%2F11%2F27%2Fvue-django-6-1%2F</url>
    <content type="text"><![CDATA[分为以下几个环节： 商品类别数据接口 Vue展示商品分类数据 Vue展示商品列表页数据 Vue的商品搜索功能 商品类别数据接口在views.py中进行修改： 1234567class CategoryViewset(mixins.ListModelMixin, viewsets.GenericViewSet): """ list: 商品分类列表数据 """ queryset = GoodsCategory.objects.all() serializer_class = CategorySerializer 在urls.py中进行修改： 12# 配置category的urlrouter.register(r'categorys', CategoryViewset, base_name="categorys") 效果： 此时所有的category按照id进行展示，并没有按照category级别有一个分级的层次结构，下面进行处理（利用serializer嵌套）： 在views.py中进行修改： 1234567class CategoryViewset(mixins.ListModelMixin, mixins.RetrieveModelMixin, viewsets.GenericViewSet): """ list: 商品分类列表数据 """ queryset = GoodsCategory.objects.filter(category_type=1) serializer_class = CategorySerializer 在serializers.py中进行修改： 1234567891011121314151617181920class CategorySerializer3(serializers.ModelSerializer): class Meta: model = GoodsCategory fields = '__all__'class CategorySerializer2(serializers.ModelSerializer): sub_cat = CategorySerializer3(many=True) class Meta: model = GoodsCategory fields = '__all__'class CategorySerializer(serializers.ModelSerializer): sub_cat = CategorySerializer2(many=True) class Meta: model = GoodsCategory fields = '__all__' 效果： 同时，在CategoryViewset中继承mixins.RetrieveModelMixin，可以让我们不用通过复杂的url配置即可获取指定的商品类别数据，例如获取id为24的商品类别数据只需要访问http://localhost:8000/categorys/24/即可： Vue展示商品分类数据在Vue项目源码online-store/src/api/api.js文件中修改： 123456789101112let localhost = 'http://127.0.0.1:8000'...//获取商品类别信息export const getCategory = params =&gt; &#123; if ('id' in params) &#123; return axios.get(`$&#123;local_host&#125;/categorys/` + params.id + '/'); &#125; else &#123; return axios.get(`$&#123;local_host&#125;/categorys/`, params); &#125;&#125;;... 运行项目，进行商品类别信息接口调试，访问127.0.0.1:3000，发现无法展示商品分类中的数据信息： 出现了跨域的错误，所以需要设置一下解决drf的跨域问题。大体上，有两种解决跨域的方法： 前端代理 服务器设置 这里我们选择服务器设置的办法，利用django-cors-headers来完成，按照文档进行安装配置，关键在于在settings.py文件中加上CORS_ORIGIN_ALLOW_ALL = True，重新运行之后，就发现数据出来了。 可以看到Tab中并没有类目，我们在后台管理系统里稍作修改即可，比如修改生鲜食品，是否导航勾选为是，修改类别描述为生鲜食品： 刷新页面： 看到设置生效了，再多设置几个： Vue展示商品列表页数据首先需要展示出商品类别所对应的商品列表数据（懒得写了，艹）： 1234567891011121314151617181920212223# goods/filters.pyimport django_filtersfrom django.db.models import Qfrom .models import Goodsclass GoodsFilter(django_filters.rest_framework.FilterSet): """ 商品的过滤类 """ pricemin = django_filters.NumberFilter(name="shop_price", lookup_expr="gte") pricemax = django_filters.NumberFilter(name="shop_price", lookup_expr="lte") top_category = django_filters.NumberFilter(method='top_category_filter') def top_category_filter(self, queryset, name, value): return queryset.filter(Q(category_id=value) | Q(category__parent_category_id=value) | Q( category__parent_category__parent_category_id=value)) class Meta: model = Goods fields = ['pricemin', 'pricemax', ] Vue的商品搜索功能只需要配置search_fields就行了： search_fields = (&#39;name&#39;, &#39;goods_brief&#39;, &#39;goods_desc&#39;)]]></content>
      <categories>
        <category>Django，Vue</category>
      </categories>
      <tags>
        <tag>Django，Vue，Rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue+Django REST Framework生鲜电商项目学习笔记——商品列表页]]></title>
    <url>%2F2017%2F11%2F16%2Fvue-django-5-1%2F</url>
    <content type="text"><![CDATA[分为以下几个环节： django的view实现商品列表页 django的serializer序列化model apiview方式实现商品列表页 drf的modelserializer实现商品列表页功能 GenericView方式实现商品列表页和分页功能详解 viewsets和router完成商品列表页 drf的Apiview、GenericView、Viewset和router的原理分析 drf的request和response drf的过滤 drf的搜索和排序 django的view实现商品列表页在goods文件夹下新建views_base.py： 12345678910111213141516171819202122232425from django.views.generic.base import Viewfrom goods.models import Goodsclass GoodsListView(View): def get(self, request): """ 通过django的view实现商品列表页 :param request: :return: """ json_list = list() goods = Goods.objects.all()[:10] for good in goods: json_dict = dict() json_dict["name"] = good.name json_dict["category"] = good.category.name json_dict["market_price"] = good.market_price # json_dict["add_time"] = good.add_time json_list.append(json_dict) from django.http import HttpResponse import json return HttpResponse(json.dumps(json_list), content_type="application/json") 这样就通过django view的方式返回了商品列表数据。 但其中有很多问题，例如在json_dict中加入add_time这个字段，就会出问题： django的serializer序列化model上述采用django view实现商品列表页的方式太过繁琐，需要手动获取每一条字段的信息，如下代码可以解决： 1234from django.forms.models import model_to_dict for good in goods: json_dict = model_to_dict(good) json_list.append(json_dict) 但还是会遇到某些字段如Image_field不可序列化的问题，其实，django提供了一种serializer可以解决： 1234567import jsonfrom django.core import serializersjson_data = serializers.serialize("json", goods)json_data = json.loads(json_data)from django.http import JsonResponsereturn JsonResponse(json_data, safe=False) apiview方式实现商品列表页首先根据drf官方文档将必要的包补全：pip install coreapi django-guardian ，然后将&#39;rest_framework&#39;添加到INSTALLED_APPS设置中“ 1234INSTALLED_APPS = ( ... 'rest_framework',) 将drf文档功能和drf登录功能引入根urls.py中： 1234567from rest_framework.documentation import include_docs_urlsurlpatterns = [ url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')), ... url(r'docs/', include_docs_urls(title='慕学生鲜')),] 首先利用drf的serializers在goods下新建serializers.py: 123456from rest_framework import serializersclass GoodsSerializer(serializers.Serializer): name = serializers.CharField(required=True, max_length=100) click_num = serializers.IntegerField(default=0) 这里只用两个字段简单看一下效果 views.py中，参考官方文档中的示例： 1234567891011121314from .serializers import GoodsSerializerfrom rest_framework.views import APIViewfrom rest_framework.response import Responsefrom .models import Goods# Create your views here.class GoodsListView(APIView): def get(self, request, format=None): goods = Goods.objects.all()[:10] goods_serializer = GoodsSerializer(goods, many=True) return Response(goods_serializer.data) 最后，在urls.py中添加from goods.views import GoodsListView，运行访问localhost:8000/goods/： 可以看到完成了商品列表页的功能。 drf的modelserializer实现商品列表页功能可以利用serializers.ModelSerializer精简我们的代码： 12345678910from rest_framework import serializersfrom goods.models import Goodsclass GoodsSerializer(serializers.ModelSerializer): class Meta: model = Goods fields = ('name', 'click_num', 'market_price', 'add_time') 可以将fields改为fields = &#39;__all__&#39;即可序列化所有字段： 为了详细展示出category的信息，可以作如下修改： 1234567891011121314151617from rest_framework import serializersfrom goods.models import Goods, GoodsCategoryclass CategorySerializer(serializers.ModelSerializer): class Meta: model = GoodsCategory fields = '__all__'class GoodsSerializer(serializers.ModelSerializer): category = CategorySerializer() class Meta: model = Goods fields = '__all__' 结果： GenericView方式实现商品列表页和分页功能为了精简GoodsListView中的代码，我们引入drf的mixins和generics： 12345678910111213141516171819from .serializers import GoodsSerializerfrom rest_framework import mixinsfrom rest_framework import genericsfrom .models import Goods# Create your views here.class GoodsListView(mixins.ListModelMixin, generics.GenericAPIView): """ 商品列表页 """ queryset = Goods.objects.all()[:10] serializer_class = GoodsSerializer def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) 其实也可以利用generics.ListAPIView来完成，更加简单： 1234567class ListAPIView(mixins.ListModelMixin, GenericAPIView): """ Concrete view for listing a queryset. """ def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) 可以看到它继承了我们之前使用的mixins.ListModelMixin和GenericAPIView。 所以之前的代码就可以精简为： 1234567class GoodsListView(generics.ListAPIView): """ 商品列表页 """ queryset = Goods.objects.all()[:10] serializer_class = GoodsSerializer 一般，列表页是需要分页的，利用drf进行分页只要在settings.py中简单设置一下即可： 1234REST_FRAMEWORK = &#123; 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination', 'PAGE_SIZE': 10,&#125; 实际上可以通过在Views中通过pagination class来自定义分页效果。 1234567891011121314151617181920212223from .serializers import GoodsSerializerfrom rest_framework import genericsfrom rest_framework.pagination import PageNumberPaginationfrom .models import Goods# Create your views here.class GoodsPagination(PageNumberPagination): page_size = 10 page_size_query_param = "page_size" page_query_param = "p" max_page_size = 100class GoodsListView(generics.ListAPIView): """ 商品列表页 """ queryset = Goods.objects.all() serializer_class = GoodsSerializer pagination_class = GoodsPagination 运行之后，自定义效果生效，并且可以动态添加参数： viewsets和router完成商品列表页如何使用？：viewsets和router 使用viewsets完成商品列表页：views.py中修改： 12345678class GoodsListViewSet(mixins.ListModelMixin, viewsets.GenericViewSet): """ 商品列表页 """ queryset = Goods.objects.all() serializer_class = GoodsSerializer pagination_class = GoodsPagination urls.py中修改： 1234567891011121314from goods.views import GoodsListViewSetgoods_list = GoodsListViewSet.as_view(&#123; 'get': 'list',&#125;)urlpatterns = [ ... # 商品列表页 url(r'goods/$', goods_list, name="goods-list"), ...] 使用router完成商品列表页因为我们使用ViewSet类而不是View类，所以实际上我们不需要自己设计URL。 将资源连接到视图和URL的约定可以使用Router类自动处理。 我们所要做的就是用router注册适当的视图集，然后剩下的就依靠router自动完成。 urls.py中修改： 1234567891011121314151617181920212223from django.conf.urls import url, includeimport xadminfrom MxShop.settings import MEDIA_ROOTfrom django.views.static import servefrom rest_framework.documentation import include_docs_urlsfrom rest_framework.routers import DefaultRouterfrom goods.views import GoodsListViewSetrouter = DefaultRouter()# 配置goods的urlrouter.register(r'goods', GoodsListViewSet)urlpatterns = [ url(r'^xadmin/', xadmin.site.urls), url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')), url(r'^media/(?P&lt;path&gt;.*)$', serve, &#123;"document_root": MEDIA_ROOT&#125;), url(r'^', include(router.urls)), url(r'docs/', include_docs_urls(title='慕学生鲜'))] drf的APIView、GenericView、Viewsets和router的原理分析还是老老实实看源码和文档吧！ 贴一篇博客：http://yindongliang.com/2017/04/20/talk-about-django-rest-framework drf的request和responsetutorial：http://www.django-rest-framework.org/tutorial/2-requests-and-responses/ api：http://www.django-rest-framework.org/api-guide/requests/ http://www.django-rest-framework.org/api-guide/responses/ drf的过滤api：http://www.django-rest-framework.org/api-guide/filtering/ views.py中代码修改： 1234567891011121314151617181920212223242526from .serializers import GoodsSerializerfrom django_filters.rest_framework import DjangoFilterBackendfrom rest_framework.pagination import PageNumberPaginationfrom rest_framework import viewsets, mixinsfrom .models import Goods# Create your views here.class GoodsPagination(PageNumberPagination): page_size = 10 page_size_query_param = "page_size" page_query_param = "p" max_page_size = 100class GoodsListViewSet(mixins.ListModelMixin, viewsets.GenericViewSet): """ 商品列表页 """ queryset = Goods.objects.all() serializer_class = GoodsSerializer pagination_class = GoodsPagination filter_backends = (DjangoFilterBackend,) filter_fields = ('name', 'shop_price') 可以看到页面中多了一个过滤器。也可以参考django-filter官方文档自定义filter： 新建filters.py： 123456789101112131415from django_filters import rest_framework as filtersfrom .models import Goodsclass GoodsFilter(filters.FilterSet): """ 商品的过滤类 """ price_min = filters.NumberFilter(name="shop_price", lookup_expr="gte") price_max = filters.NumberFilter(name="shop_price", lookup_expr="lte") class Meta: model = Goods fields = ['price_min', 'price_max'] views.py中修改： 1234567891011from .filters import GoodsFilter...class GoodsListViewSet(mixins.ListModelMixin, viewsets.GenericViewSet): """ 商品列表页 """ ... filter_class = GoodsFilter 题外话，实现模糊查询效果：name = filters.CharFilter(name=&quot;name&quot;, lookup_expr=&quot;icontains&quot;) drf的搜索和排序搜索依赖于SearchFilter，排序依赖于OrderingFilter 搜索12345678910111213from rest_framework import filters...class GoodsListViewSet(mixins.ListModelMixin, viewsets.GenericViewSet): """ 商品列表页 """ ... filter_backends = (DjangoFilterBackend, filters.SearchFilter) filter_class = GoodsFilter search_fields = ('name', 'goods_brief', 'goods_desc') 结果： 默认情况下，搜索将使用不区分大小写的部分匹配。搜索参数可能包含多个搜索词，它们应该是空格和/或逗号分隔的。如果使用多个搜索条件，则只有在所有提供的条件匹配的情况下，对象才会返回到列表中。 搜索行为可以通过将各种字符预先添加到search_fields来限制。 1234'^'开始 - 搜索。'='完全匹配。'@'全文搜索。 （目前只支持Django的MySQL后端。）'$'正则表达式搜索。 例如：search_fields =（&#39;= username&#39;，&#39;= email&#39;） 默认情况下，搜索参数被命名为“搜索”，但是这可能会被SEARCH_PARAM设置覆盖。 排序简单地配置即可，在filter_backends中加入filters.OrderingFilter，加上ordering_fields = (&#39;sold_num&#39;, &#39;add_time&#39;) 效果： 很多功能配置要多看文档。]]></content>
      <categories>
        <category>Django，Vue</category>
      </categories>
      <tags>
        <tag>Django，Vue，Rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue+Django REST Framework生鲜电商项目学习笔记——Vue的结构和restful api介绍]]></title>
    <url>%2F2017%2F11%2F15%2Fvue-django-4-1%2F</url>
    <content type="text"><![CDATA[分为以下几个环节： restful api介绍 vue的基本概念介绍 vue的源码结构介绍 restful api介绍前后端分离优缺点为什么要前后端分离 pc, app, pad多端适应 SPA开发模式开始流行 前后端开发职责不清 开发效率问题，前后端互相等待 前端一直配合着后端，能力受限 后台开发语言和模板高度耦合，导致开发语言依赖严重 前后端分离缺点 前后端学习门槛增加 数据依赖导致文档重要性增加 前端工作量加大 SEO的难度加大 后端开发模式迁移增加成本 restful apirestful api目前是前后端分离最佳实践： 轻量，直接通过http，不需要额外的协议，post/get/put/delete操作 面向资源，一目了然，具有自解释性 数据描述简单，一般通过json或者xml做数据通信 restful api 重要概念 概念 restful实践 Vue的基本概念介绍几个概念 前端工程化 数据双向绑定 组件化开发 vue开发的几个概念 webpack vue, vuex, vue-router, axios ES6, babel Vue项目源码结构介绍老老实实把Vue学一学吧！]]></content>
      <categories>
        <category>Django，Vue</category>
      </categories>
      <tags>
        <tag>Django，Vue，Rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue+Django REST Framework生鲜电商项目学习笔记——model设计和资源导入]]></title>
    <url>%2F2017%2F11%2F05%2Fvue-django-3-1%2F</url>
    <content type="text"><![CDATA[分为以下几个环节： user用户的model设计 goods商品的model设计 trade交易的model设计 user_operation用户操作的model设计 migrations原理及表生成 xadmin后台管理系统的配置 导入商品和商品类别数据 user用户的model设计代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142from datetime import datetimefrom django.db import modelsfrom django.contrib.auth.models import AbstractUser# Create your models here.class UserProfile(AbstractUser): """ 用户 """ name = models.CharField(max_length=30, null=True, blank=True, verbose_name="姓名") birthday = models.DateField(null=True, blank=True, verbose_name="出生年月") mobile = models.CharField(max_length=11, verbose_name="电话") gender = models.CharField(max_length=6, choices=(("male", "男"), ("female", "女")), default="female", verbose_name="性别") email = models.CharField(max_length=100, null=True, blank=True, verbose_name="邮箱") class Meta: verbose_name = "用户" verbose_name_plural = verbose_name def __str__(self): return self.nameclass VerifyCode(models.Model): """ 短信验证码 """ code = models.CharField(max_length=10, verbose_name="验证码") mobile = models.CharField(max_length=11, verbose_name="电话") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "短信验证码" verbose_name_plural = verbose_name def __str__(self): return self.code 然后在settings文件中添加下面这行代码以替换django本身的用户。 1AUTH_USER_MODEL = 'users.UserProfile' goods商品的model设计代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113from datetime import datetimefrom django.db import modelsfrom DjangoUeditor.models import UEditorField# Create your models here.class GoodsCategory(models.Model): """ 商品类别 """ CATEGORY_TYPE = ( (1, "一级类目"), (2, "二级类目"), (3, "三级类目"), ) name = models.CharField(default="", max_length=30, verbose_name="类别名", help_text="类别名") code = models.CharField(default="", max_length=30, verbose_name="类别code", help_text="类别code") desc = models.TextField(default="", verbose_name="类别描述", help_text="类别描述") category_type = models.IntegerField(choices=CATEGORY_TYPE, verbose_name="类目级别", help_text="类目级别") parent_category = models.ForeignKey("self", null=True, blank=True, verbose_name="父类目级别", help_text="父目录", related_name="sub_cat") is_tab = models.BooleanField(default=False, verbose_name="是否导航", help_text="是否导航") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "商品类别" verbose_name_plural = verbose_name def __str__(self): return self.nameclass GoodsCategoryBrand(models.Model): """ 品牌 """ category = models.ForeignKey(GoodsCategory, related_name='brands', null=True, blank=True, verbose_name="商品类目") name = models.CharField(default="", max_length=30, verbose_name="品牌名", help_text="品牌名") desc = models.TextField(default="", max_length=200, verbose_name="品牌描述", help_text="品牌描述") image = models.ImageField(max_length=200, upload_to="brand/") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "品牌" verbose_name_plural = verbose_name def __str__(self): return self.nameclass Goods(models.Model): """ 商品 """ category = models.ForeignKey(GoodsCategory, verbose_name="商品类目") goods_sn = models.CharField(max_length=50, default="", verbose_name="商品唯一货号") name = models.CharField(max_length=100, verbose_name="商品名") click_num = models.IntegerField(default=0, verbose_name="点击数") sold_num = models.IntegerField(default=0, verbose_name="商品销售量") fav_num = models.IntegerField(default=0, verbose_name="收藏数") goods_num = models.IntegerField(default=0, verbose_name="库存数") market_price = models.FloatField(default=0, verbose_name="市场价格") shop_price = models.FloatField(default=0, verbose_name="本店价格") goods_brief = models.TextField(max_length=500, verbose_name="商品简短描述") goods_desc = UEditorField(verbose_name="内容", imagePath="goods/images/", width=1000, height=300, filePath="goods/files/", default='') ship_free = models.BooleanField(default=True, verbose_name="是否承担运费") goods_front_image = models.ImageField(upload_to="goods/images/", null=True, blank=True, verbose_name="封面图") is_new = models.BooleanField(default=False, verbose_name="是否新品") is_hot = models.BooleanField(default=False, verbose_name="是否热销") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = '商品' verbose_name_plural = verbose_name def __str__(self): return self.nameclass GoodsImage(models.Model): """ 商品轮播图 """ goods = models.ForeignKey(Goods, verbose_name="商品", related_name="images") image = models.ImageField(upload_to="", verbose_name="图片", null=True, blank=True) add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "商品轮播图" verbose_name_plural = verbose_name def __str__(self): return self.goods.nameclass Banner(models.Model): """ 轮播的商品 """ goods = models.ForeignKey(Goods, verbose_name="商品") image = models.ImageField(upload_to='banner', verbose_name="轮播图片") index = models.IntegerField(default=0, verbose_name="轮播顺序") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "轮播商品" verbose_name_plural = verbose_name def __str__(self): return self.goods.name 其中需要使用DjangoUeditor，直接解压放到extra_apps目录下即可，记得在settings.py文件中注册我们的app： 12345678910111213INSTALLED_APPS = [ 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'apps.users.apps.UsersConfig', 'DjangoUeditor', 'users', 'goods', 'trade', 'user_operation'] trade交易的model设计代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374from datetime import datetimefrom django.db import modelsfrom django.contrib.auth import get_user_model()from goods.models import GoodsUser = get_user_model()# Create your models here.class ShoppingCart(models.Model): """ 购物车 """ user = models.ForeignKey(User, verbose_name="用户") goods = models.ForeignKey(Goods, verbose_name="商品") goods_num = models.IntegerField(default=0, verbose_name="购买数量") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "购物车" verbose_name_plural = verbose_name def __str__(self): return "%s(%d)".format(self.goods.name, self.goods_num)class OrderInfo(models.Model): """ 订单 """ ORDER_STATUS = ( ("success", "成功"), ("cancel", "取消"), ("cancel", "待支付"), ) user = models.ForeignKey(User, verbose_name="用户") order_sn = models.CharField(max_length=30, null=True, blank=True, unique=True, verbose_name="订单号") trade_no = models.CharField(max_length=100, unique=True, null=True, blank=True, verbose_name=u"交易号") pay_status = models.CharField(choices=ORDER_STATUS, default="paying", max_length=30, verbose_name="订单状态") post_script = models.CharField(max_length=200, verbose_name="订单留言") order_mount = models.FloatField(default=0.0, verbose_name="订单金额") pay_time = models.DateTimeField(null=True, blank=True, verbose_name="支付时间") address = models.CharField(max_length=100, default="", verbose_name="收货地址") signer_name = models.CharField(max_length=20, default="", verbose_name="签收人") singer_mobile = models.CharField(max_length=11, verbose_name="联系电话") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "订单" verbose_name_plural = verbose_name def __str__(self): return str(self.order_sn)class OrderGoods(models.Model): """ 订单商品详情 """ order = models.ForeignKey(OrderInfo, verbose_name="订单信息", related_name="goods") goods = models.ForeignKey(Goods, verbose_name="商品") goods_num = models.IntegerField(default=0, verbose_name="商品数量") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "订单商品" verbose_name_plural = verbose_name def __str__(self): return str(self.order.order_sn) 注：Django 1.5之后y用户可以自定义User Model，可以通过以下两种方式来获取User Model。 使用django.contrib.auth.get_user_model() 123456789# 使用默认User model时&gt;&gt;&gt; from django.contrib.auth import get_user_model&gt;&gt;&gt; get_user_model()&lt;class 'django.contrib.auth.models.User'&gt;# 使用自定义User model时&gt;&gt;&gt; from django.contrib.auth import get_user_model&gt;&gt;&gt; get_user_model()&lt;class 'xxx.models.UserProfile'&gt; 使用settings.AUTH_USER_MODEL 应该使用django.contrib.auth.get_user_model()来引用用户模型，而不要直接引用User。 这个方法将返回当前正在使用的用户模型 —— 指定的自定义用户模型或者User。当定义一个外键或者到用户模型的多对多关系时，你应该使用 settings.AUTH_USER_MODEL设置来指定自定义的模型。一般来说，在导入时候执行的代码中，应该使用settings.AUTH_USER_MODEL，get_user_model() 只在Django 已经导入所有的模型后才工作。 user_operation用户操作的model设计代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374from datetime import datetimefrom django.db import modelsfrom django.contrib.auth import get_user_modelfrom goods.models import Goods# Create your models here.User = get_user_model()class UserFav(models.Model): """ 用户收藏 """ user = models.ForeignKey(User, verbose_name="用户") goods = models.ForeignKey(Goods, verbose_name="商品", help_text="商品id") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "用户收藏" verbose_name_plural = verbose_name def __str__(self): return self.user.nameclass UserLeavingMessage(models.Model): """ 用户留言 """ MESSAGE_CHOICES = ( (1, "留言"), (2, "投诉"), (3, "询问"), (4, "售后"), (5, "求购") ) user = models.ForeignKey(User, verbose_name="用户") message_type = models.IntegerField(default=1, choices=MESSAGE_CHOICES, verbose_name="留言类型", help_text=u"留言类型: 1(留言),2(投诉),3(询问),4(售后),5(求购)") subject = models.CharField(max_length=100, default="", verbose_name="主题") message = models.TextField(default="", verbose_name="留言内容", help_text="留言内容") file = models.FileField(upload_to="message/images/", verbose_name="上传的文件", help_text="上传的文件") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "用户留言" verbose_name_plural = verbose_name def __str__(self): return self.subjectclass UserAddress(models.Model): """ 用户收货地址 """ user = models.ForeignKey(User, verbose_name="用户") province = models.CharField(max_length=100, default="", verbose_name="省份") city = models.CharField(max_length=100, default="", verbose_name="城市") district = models.CharField(max_length=100, default="", verbose_name="区域") address = models.CharField(max_length=100, default="", verbose_name="详细地址") signer_name = models.CharField(max_length=100, default="", verbose_name="签收人") signer_mobile = models.CharField(max_length=11, default="", verbose_name="电话") add_time = models.DateTimeField(default=datetime.now, verbose_name="添加时间") class Meta: verbose_name = "收货地址" verbose_name_plural = verbose_name def __str__(self): return self.address migrations原理及表生成直接Run manage.py Task然后makemigrations 原理略过，官方文档讲的很详细。 xadmin后台管理系统的配置将xadmin中的xadmin文件夹复制到extra_apps目录下，将其中每个app目录下的adminx.py文件复制到项目对应app目录下。 安装xadmin依赖包： 1pip install django-crispy-forms django-reversion django-formtools future httplib2 six xlwt xlswriter -i https://pypi.douban.com/simple/ 在settings.py文件中添加配置： 12345678910111213141516171819202122INSTALLED_APPS = [ 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'django_filters', 'DjangoUeditor', 'apps.users.apps.UsersConfig', 'goods.apps.GoodsConfig', 'trade.apps.TradeConfig', 'user_operation.apps.UserOperationConfig', 'crispy_forms', 'xadmin',]# 设置时区LANGUAGE_CODE = 'zh-hans' # 中文支持，django1.8以后支持；1.8以前是zh-cnTIME_ZONE = 'Asia/Shanghai'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = False # 默认是Ture，时间是utc时间，由于我们要用本地时间，所用手动修改为false！！！！ 再执行makemigrations和migrate 修改Mxshop/urls.py文件: 123456from django.conf.urls import urlimport xadminurlpatterns = [ url(r'^xadmin/', xadmin.site.urls),] 创建超级用户 Debug运行，打开http://127.0.0.1:8000/xadmin/ 发现app名称仍是英文，下面进入对应app下的apps.py文件进行修改，例如goods: 123456from django.apps import AppConfigclass GoodsConfig(AppConfig): name = 'goods' verbose_name = "商品" OK，大功告成！ 导入商品和商品类别数据将data中的goods和brands文件夹复制到项目的media文件夹中，并在db_tools文件夹下新建data文件夹，拖入category_data.py和product_data.py文件。 在db_tools文件夹下新建import_category_data.py： 12345678910111213141516171819202122232425262728293031323334353637383940# 独立使用django的modelimport sysimport os# 获取当前文件的目录pwd = os.path.dirname(os.path.realpath(__file__))# 获取项目名的目录(因为我的当前文件是在项目名下的文件夹下的文件.所以是../)sys.path.append(pwd + "../")os.environ.setdefault("DJANGO_SETTINGS_MODULE", "MxShop.settings")import djangodjango.setup()from goods.models import GoodsCategoryfrom db_tools.data.category_data import row_datafor lev1_cat in row_data: lev1_instance = GoodsCategory() lev1_instance.code = lev1_cat["code"] lev1_instance.name = lev1_cat["name"] lev1_instance.category_type = 1 lev1_instance.save() for lev2_cat in lev1_cat["sub_categorys"]: lev2_instance = GoodsCategory() lev2_instance.code = lev2_cat["code"] lev2_instance.name = lev2_cat["name"] lev2_instance.category_type = 2 lev2_instance.parent_category = lev1_instance lev2_instance.save() for lev3_cat in lev2_cat["sub_categorys"]: lev3_instance = GoodsCategory() lev3_instance.code = lev3_cat["code"] lev3_instance.name = lev3_cat["name"] lev3_instance.category_type = 3 lev3_instance.parent_category = lev2_instance lev3_instance.save() 运行之后即可导入商品类别数据，可以再navicat或者后台管理系统中查看： 接下来导入商品数据，观察商品数据结构之后，在db_tools文件夹下新建import_goods_data.py： 1234567891011121314151617181920212223242526272829303132333435363738import sysimport os# 获取当前文件的目录pwd = os.path.dirname(os.path.realpath(__file__))# 获取项目名的目录(因为我的当前文件是在项目名下的文件夹下的文件.所以是../)sys.path.append(pwd + "../")os.environ.setdefault("DJANGO_SETTINGS_MODULE", "MxShop.settings")import djangodjango.setup()from goods.models import Goods, GoodsCategory, GoodsImagefrom db_tools.data.product_data import row_datafor goods_detail in row_data: goods = Goods() goods.name = goods_detail["name"] goods.market_price = float(int(goods_detail["market_price"].replace("￥", "").replace("元", ""))) goods.shop_price = float(int(goods_detail["sale_price"].replace("￥", "").replace("元", ""))) goods.goods_brief = goods_detail["desc"] if goods_detail["desc"] is not None else "" goods.goods_desc = goods_detail["goods_desc"] if goods_detail["goods_desc"] is not None else "" goods.goods_front_image = goods_detail["images"][0] if goods_detail["images"] else "" category_name = goods_detail["categorys"][-1] category = GoodsCategory.objects.filter(name=category_name) if category: goods.category = category[0] goods.save() for goods_image in goods_detail["images"]: goods_image_instance = GoodsImage() goods_image_instance.image = goods_image goods_image_instance.goods = goods goods_image_instance.save() 运行即可。 但是发现其中的图片无法显示，需要我们进行配置： 1234567891011121314# settings.py文件中添加MEDIA_URL = "/media/"MEDIA_ROOT = os.path.join(BASE_DIR, "media")# MxShop/urls.py文件中修改from django.conf.urls import urlimport xadminfrom MxShop.settings import MEDIA_ROOTfrom django.views.static import serveurlpatterns = [ url(r'^xadmin/', xadmin.site.urls), url(r'^media/(?P&lt;path&gt;.*)$', serve, &#123;"document_root": MEDIA_ROOT&#125;),] 附Django settings.py的media路径设置]]></content>
      <categories>
        <category>Django，Vue</category>
      </categories>
      <tags>
        <tag>Django，Vue，Rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue+Django REST Framework生鲜电商项目学习笔记——项目初始化]]></title>
    <url>%2F2017%2F11%2F04%2Fvue-django-2-1%2F</url>
    <content type="text"><![CDATA[虚拟环境配置 配置virtualenv和virtualenvwrapper(略) 新建虚拟环境VueShop: 1234pip install djangorestframeworkpip install markdown django-filterpip install djangopip install mysqlclient 最好在pip后面加上-i https://pypi.doban.com/simple豆瓣源加快安装速度，否则太慢了，或者直接在配置文件中进行换源，不用每次都加。 新建项目 创建 修改目录和配置 新建apps和extra_apps两个Python Package，media和db_tools两个Directory，将users拖入apps中，并将apps和extra_apps右键mark为sources root，初步的项目结构如下： 修改settings.py中的配置，添加或修改为如下代码： 123456789101112131415161718import sys# Build paths inside the project like this: os.path.join(BASE_DIR, ...)BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))sys.path.insert(0, BASE_DIR)sys.path.insert(0, os.path.join(BASE_DIR, 'apps'))sys.path.insert(0, os.path.join(BASE_DIR, 'extra_apps'))DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': "mxshop", 'USER': "root", 'PASSWORD': "12", 'HOST': "127.0.0.1", 'OPTIONS': &#123;'init_command': 'SET storage_default_engine=INNODB;'&#125; &#125;&#125; 在更换数据库配置之后，需要在Navicat中创建相应的数据库。]]></content>
      <categories>
        <category>Django，Vue</category>
      </categories>
      <tags>
        <tag>Django，Vue，Rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue+Django REST Framework生鲜电商项目学习笔记——概览]]></title>
    <url>%2F2017%2F11%2F03%2Fvue-django-1-1%2F</url>
    <content type="text"><![CDATA[技术要点 Vue + Django REST Framework前后端分离技术 restful api开发流程 Django REST Framework的功能实现和核心源码分析 Sentry完成线上系统的错误日志的监控和告警 第三方登录和支付宝支付的集成 Django REST Framework技能通过view实现rest api接口 ApiView方式实现api接口 GenericView方式实现api接口 Viewset和router方式实现api接口和url配置 django_filter、SearchFilter、OrderFilter、分页 通用mixins 权限和认证 Authentication用户认证设置 动态设置permission、Authentication Validators实现字段验证 序列化和表单验证 Serializer ModelSerializer 动态设置Serializer 支付、登录和注册 json web token实现登录 手机注册 支付宝支付 第三方登录 进阶开发 django rest framework部分核心源码解读 文档自动化管理 django rest framework的缓存 Throttling对用户和ip进行限速 开发中常见的问题 本地系统不能重现的bug api接口出错不能及时的发现或难找到错误栈 api文档管理问题 大量的url配置造成url配置越来越多难以维护 接口不及时更新文档，对方不知道如何去测试接口，但写文档会话费大量的时间去维护 为了防止爬虫，我们需要针对api的访问频率进行限制，比如一分钟、一小时或者一天用户的访问频率限制问题 某些页面将数据放入缓存，加速某些api的访问速度 开发中常见的问题解决方案 通过介绍pycharm的远程服务器代码调试技巧让大家不仅可以调试支付、第三方登录还可以调试远程服务器的代码来重现服务器上的bug 通过docker搭建sentry来体验错误日志监控系统，让我们不仅可以得到线上的错误栈还能及时在发生系统错误时受到邮件通知。 django rest framework的文档自动化管理以及url的注册管理功能会让我们省去写文档的时间 django rest framework的文档管理功能不仅可以让我们省去写文档的时间还能直接在文档里面测试接口、自动生成的js接口代码、shell测试代码和python测试代码 django rest framework提供的throttle来对api进行访问频率限制 引入第三方框架来设置某些api的缓存 Django进阶知识点 Django migrations原理 Django信号量 Django从请求到响应的完整过程 独立使用Django的Model Vue知识点 Vue技术选型分析 API后端接口数据填充到Vue组件模板 Vue代码结构分析]]></content>
      <categories>
        <category>Django，Vue</category>
      </categories>
      <tags>
        <tag>Django，Vue，Rest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac配置Privoxy设置go get代理]]></title>
    <url>%2F2017%2F10%2F17%2Fmac-privoxy%2F</url>
    <content type="text"><![CDATA[安装Privoxy地址：https://www.privoxy.org/sf-download-mirror/ 配置Privoxy12345cd /Applications/Privoxy# 监听8118端口echo 'listen-address 0.0.0.0:8118' &gt;&gt; /usr/local/etc/privoxy/config# 设置转发socks5服务器echo 'forward-socks5 / localhost:1080 .' &gt;&gt; /usr/local/etc/privoxy/config 发现提示错误： 1-bash: /usr/local/etc/privoxy/config: Permission denied 解决方法： 12# 修改/usr/local目录的所有者与组sudo chown -R "$USER":admin /usr/local 配置HTTP代理1234567891011# 如果你使用bashvim ~/.bashrc# 如果你使用zshvim ~/.zshrc# 加入export http_proxy=http://127.0.0.1:8118/# 保存退出# source 使其即刻生效source ~/.bashrc# orsource ~/.zshrc 测试Privoxy1234567lawtech@lawdeMacBook-Pro:/Applications/Privoxy$ curl www.google.com&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv="content-type" content="text/html;charset=utf-8"&gt;&lt;TITLE&gt;302 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;&lt;H1&gt;302 Moved&lt;/H1&gt;The document has moved&lt;A HREF="http://www.google.co.jp/?gfe_rd=cr&amp;amp;dcr=0&amp;amp;ei=G5flWZmXKa_o8Ae9j7PADw"&gt;here&lt;/A&gt;.&lt;/BODY&gt;&lt;/HTML&gt; 终于可以开心地go get了！]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go，Mac，Privoxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——scrapyd部署scrapy爬虫]]></title>
    <url>%2F2017%2F07%2F27%2Fscrapyd%2F</url>
    <content type="text"><![CDATA[在完成scrapy项目之后，就要进入项目实际部署环节。 传送门：scrapyd 首先在我们之前爬虫项目的虚拟环境article_spider中安装scrapyd： 1(article_spider) lawtech@lawdeMacBook-Pro-2:~$ pip install scrapyd lawtech@lawdeMacBook-Pro-2:~$ pip install scrapyd-client scrapyd-client就没必要在虚拟环境中安装了。 在scrapy项目中，有一个文件scrapy.cfg： 1234567891011# Automatically created by: scrapy startproject## For more information about the [deploy] section see:# https://scrapyd.readthedocs.org/en/latest/deploy.html[settings]default = ArticleSpider.settings[deploy:lawtech]url = http://localhost:6800/project = ArticleSpider 其中的deploy就是为scrapyd服务的。 要部署项目，首先要启动scrapyd： 12345678910lawtech@lawdeMacBook-Pro-2:~$ workon article_spider(article_spider) lawtech@lawdeMacBook-Pro-2:~$ scrapyd2017-07-27T23:14:51+0800 [-] Loading /Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapyd/txapp.py...2017-07-27T23:14:52+0800 [-] Scrapyd web console available at http://127.0.0.1:6800/2017-07-27T23:14:52+0800 [-] Loaded.2017-07-27T23:14:52+0800 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 17.1.0 (/Users/lawtech/myvirtualenvs/article_spider/bin/python3.5 3.5.2) starting up.2017-07-27T23:14:52+0800 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.selectreactor.SelectReactor.2017-07-27T23:14:52+0800 [-] Site starting on 68002017-07-27T23:14:52+0800 [twisted.web.server.Site#info] Starting factory &lt;twisted.web.server.Site object at 0x107a97198&gt;2017-07-27T23:14:52+0800 [Launcher] Scrapyd 1.2.0 started: max_proc=16, runner='scrapyd.runner' 部署项目： 12345(article_spider) lawtech@lawdeMacBook-Pro-2:~/PycharmProjects/ArticleSpider$ scrapyd-deploy lawtech -p ArticleSpiderPacking version 1501168592Deploying to project "ArticleSpider" in http://localhost:6800/addversion.jsonServer response (200):&#123;"status": "ok", "version": "1501168592", "spiders": 3, "project": "ArticleSpider", "node_name": "lawdeMacBook-Pro-2.local"&#125; 现在只是将项目部署到目标地址，但是没有调度爬虫，调度爬虫需要用到curl命令，在http://localhost:6800有提示如下： 1curl http://localhost:6800/schedule.json -d project=default -d spider=somespider 只需要改动一下即可 12lawtech@lawdeMacBook-Pro-2:~/PycharmProjects/ArticleSpider$ curl http://localhost:6800/schedule.json -d project=ArticleSpider -d spider=jobbole&#123;&quot;status&quot;: &quot;ok&quot;, &quot;jobid&quot;: &quot;2466c76872df11e7846da45e60ba3bb7&quot;, &quot;node_name&quot;: &quot;lawdeMacBook-Pro-2.local&quot;&#125; 然后即可在http://127.0.0.1:6800/jobs查看调度结果了。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python，scrapyd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——总结]]></title>
    <url>%2F2017%2F07%2F27%2Fscrapy-summary%2F</url>
    <content type="text"><![CDATA[开发环境搭建 技术选型 伯乐在线爬取 知乎爬虫 拉勾网站整站爬虫 爬虫与反爬虫 scrapy进阶开发 scrapy-redis分布式开发 elasticsearch的基础知识 django搭建搜索网站 scrapyd部署scrapy项目]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python，ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Django搭建搜索网站]]></title>
    <url>%2F2017%2F07%2F20%2Fdjango-elasticsearch%2F</url>
    <content type="text"><![CDATA[在完成Scrapy和Elasticsearch的基本学习之后，下面就要利用Django开始搭建我们的搜索网站。 es完成搜索建议-搜索建议字段保存传送门：Completion Suggester 为了完成搜索时自动补全的功能，我们在es_types.py中加入一个字段suggest = Completion(analyzer=ik_analyzer)，由于源码冲突问题，我们需要自定义一个analyzer： 123456789101112from elasticsearch_dsl.analysis import CustomAnalyzer as _CustomAnalyzerclass CustomAnalyzer(_CustomAnalyzer): """ 自定义Analyzer """ def get_analysis_definition(self): return &#123;&#125;ik_analyzer = CustomAnalyzer("ik_max_word", filter=["lowercase"]) 然后重新生成索引： 我们需要为article生成搜索建议词，所以在save_to_es()函数中加入article.suggest = [{&quot;input&quot;:[], &quot;weight&quot;:2}]，然后我们需要自己写一个方法通过之前的analyze接口生成搜索建议： 123456789101112131415161718192021222324from elasticsearch_dsl.connections import connections# 生成es的实例es = connections.create_connection(ArticleType._doc_type.using)def gen_suggests(index, info_tuple): """ 根据字符串生成搜索建议数组 """ used_words = set() suggests = [] for text, weight in info_tuple: if text: # 调用es的analyze接口分析字符串 words = es.indices.analyze(index=index, analyzer="ik_max_word", params=&#123;"filter": ["lowercase"]&#125;, body=text) analyzed_words = set(r["token"] for r in words["tokens"] if len(r["token"]) &gt; 1) new_words = analyzed_words - used_words else: new_words = set() if new_words: suggests.append(&#123;"input": list(new_words), "weight": weight&#125;) return suggests 所以之前的代码就可以修改为 123456789101112131415161718def save_to_es(self): article = ArticleType() article.title = self['title'] article.create_date = self["create_date"] article.content = remove_tags(self["content"]) article.front_image_url = self["front_image_url"] if "front_image_path" in self: article.front_image_path = self["front_image_path"] article.praise_nums = self["praise_nums"] article.fav_nums = self["fav_nums"] article.comment_nums = self["comment_nums"] article.url = self["url"] article.tags = self["tags"] article.meta.id = self["url_object_id"] article.suggest = gen_suggests(ArticleType._doc_type.index, ((article.title,10),(article.tags, 7))) article.save() return 这样我们就完成了suggest字段的准备。 Django实现Elasticsearch的搜索建议不想写了，没耐心了，巴拉巴拉~~ 直接传送门走起：LcvSearch]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python，Django，Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Elasticsearch搜索引擎的使用]]></title>
    <url>%2F2017%2F07%2F09%2Felasticsearch-usage%2F</url>
    <content type="text"><![CDATA[Elasticsearch介绍ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 我们建立一个网站或应用程序，并要添加搜索功能，但是想要完成搜索工作的创建是非常困难的。我们希望搜索解决方案要运行速度快，我们希望能有一个零配置和一个完全免费的搜索模式，我们希望能够简单地使用JSON通过HTTP来索引数据，我们希望我们的搜索服务器始终可用，我们希望能够从一台开始并扩展到数百台，我们要实时搜索，我们要简单的多租户，我们希望建立一个云的解决方案。因此我们利用Elasticsearch来解决所有这些问题以及可能出现的更多其它问题。 Elasticsearch安装 Elasticsearch-RTF安装 什么是Elasticsearch-RTF？ RTF是Ready To Fly的缩写，在航模里面，表示无需自己组装零件即可直接上手即飞的航空模型，Elasticsearch-RTF是针对中文的一个发行版，即使用最新稳定的elasticsearch版本，并且帮你下载测试好对应的插件，如中文分词插件等，目的是让你可以下载下来就可以直接的使用（虽然es已经很简单了，但是很多新手还是需要去花时间去找配置，中间的过程其实很痛苦），当然等你对这些都熟悉了之后，你完全可以自己去diy了，跟linux的众多发行版是一个意思。 当前版本 Elasticsearch 5.1.1 传送门：Elasticsearch-RTF head插件和kibana的安装 传送门：elasticsearch-head，kibana 安装方式在github地址中都有详细说明。 注意：在elsticsearch.yml中添加如下设置，使得elasticsearch-head能够连接到elasticsearch。 12345# security policy configuration, allowing third-party plugins connection.http.cors.enabled: truehttp.cors.allow-origin: "*"http.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETEhttp.cors.allow-headers: "X-Requested-With, Content-Type, Content-Length, X-User" Elasticsearch的基本概念传送门：基本概念 倒排索引倒排索引（英语：Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。 有两种不同的反向索引形式： 一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。 一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。 后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。 传送门：倒排索引 这个例子很好，所以搬来： 以英文为例，下面是要被索引的文本： T0=&quot;it is what it is&quot; T1=&quot;what is it&quot; T2=”it is a banana” 我们就能得到下面的反向文件索引： 12345&quot;a&quot;: &#123;2&#125;&quot;banana&quot;: &#123;2&#125;&quot;is&quot;: &#123;0, 1, 2&#125;&quot;it&quot;: &#123;0, 1, 2&#125;&quot;what&quot;: &#123;0, 1&#125; 检索的条件”what”, “is” 和 “it” 将对应这个集合：{0,1}∩{0,1,2}∩{0,1,2}={0,1}。 对相同的文字，我们得到后面这些完全反向索引，有文档数量和当前查询的单词结果组成的的成对数据。 同样，文档数量和当前查询的单词结果都从零开始。所以，”banana”: {(2, 3)} 就是说 “banana”在第三个文档里 (T2)，而且在第三个文档的位置是第四个单词(地址为 3)。 12345&quot;a&quot;: &#123;(2, 2)&#125;&quot;banana&quot;: &#123;(2, 3)&#125;&quot;is&quot;: &#123;(0, 1), (0, 4), (1, 1), (2, 1)&#125;&quot;it&quot;: &#123;(0, 0), (0, 3), (1, 2), (2, 0)&#125; &quot;what&quot;: &#123;(0, 2), (1, 0)&#125; 如果我们执行短语搜索”what is it” 我们得到这个短语的全部单词各自的结果所在文档为文档0和文档1。但是这个短语检索的连续的条件仅仅在文档1得到。 Elasticsearch基本的索引和文档CRUD操作我们在Kibana Dev Tools的Console中进行这些操作。 创建索引 添加索引完成后，elasticsearch-head中显示如下： 实际上，在head中也可以添加索引： 这两种方式是等效的，只是在Kibana中我们使用的是Rest API。 获取settings 12345# 获取settingsGET lagou/_settingsGET _all/_settingsGET .kibana,lagou/_settingsGET _settings 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# GET lagou/_settings&#123; "lagou": &#123; "settings": &#123; "index": &#123; "creation_date": "1499158041414", "number_of_shards": "5", "number_of_replicas": "1", "uuid": "BhByVdsdTx65H4xnL4TlWQ", "version": &#123; "created": "5010199" &#125;, "provided_name": "lagou" &#125; &#125; &#125;&#125;# GET _all/_settings&#123; "lagou": &#123; "settings": &#123; "index": &#123; "creation_date": "1499158041414", "number_of_shards": "5", "number_of_replicas": "1", "uuid": "BhByVdsdTx65H4xnL4TlWQ", "version": &#123; "created": "5010199" &#125;, "provided_name": "lagou" &#125; &#125; &#125;, ".kibana": &#123; "settings": &#123; "index": &#123; "creation_date": "1499073856161", "number_of_shards": "1", "number_of_replicas": "1", "uuid": "g_cP7qZERXiVeKEwjbNE1g", "version": &#123; "created": "5010199" &#125;, "provided_name": ".kibana" &#125; &#125; &#125;&#125;# GET .kibana,lagou/_settings&#123; "lagou": &#123; "settings": &#123; "index": &#123; "creation_date": "1499158041414", "number_of_shards": "5", "number_of_replicas": "1", "uuid": "BhByVdsdTx65H4xnL4TlWQ", "version": &#123; "created": "5010199" &#125;, "provided_name": "lagou" &#125; &#125; &#125;, ".kibana": &#123; "settings": &#123; "index": &#123; "creation_date": "1499073856161", "number_of_shards": "1", "number_of_replicas": "1", "uuid": "g_cP7qZERXiVeKEwjbNE1g", "version": &#123; "created": "5010199" &#125;, "provided_name": ".kibana" &#125; &#125; &#125;&#125;# GET _settings&#123; "lagou": &#123; "settings": &#123; "index": &#123; "creation_date": "1499158041414", "number_of_shards": "5", "number_of_replicas": "1", "uuid": "BhByVdsdTx65H4xnL4TlWQ", "version": &#123; "created": "5010199" &#125;, "provided_name": "lagou" &#125; &#125; &#125;, ".kibana": &#123; "settings": &#123; "index": &#123; "creation_date": "1499073856161", "number_of_shards": "1", "number_of_replicas": "1", "uuid": "g_cP7qZERXiVeKEwjbNE1g", "version": &#123; "created": "5010199" &#125;, "provided_name": ".kibana" &#125; &#125; &#125;&#125; 修改settings 12345678910# 修改settingsPUT lagou/_settings&#123; "number_of_replicas": 2&#125;PUT lagou/_settings&#123; "number_of_shards": 2&#125; 运行结果： 12345678910111213141516171819# PUT lagou/_settings&#123; "acknowledged": true&#125;# PUT lagou/_settings&#123; "error": &#123; "root_cause": [ &#123; "type": "illegal_argument_exception", "reason": "can't change the number of shards for an index" &#125; ], "type": "illegal_argument_exception", "reason": "can't change the number of shards for an index" &#125;, "status": 400&#125; 由于shards一旦设置就不能更改，所以第二个操作失败。 获取索引信息 123# 获取索引信息GET _allGET lagou 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# GET _all&#123; "lagou": &#123; "aliases": &#123;&#125;, "mappings": &#123;&#125;, "settings": &#123; "index": &#123; "creation_date": "1499158041414", "number_of_shards": "5", "number_of_replicas": "2", "uuid": "BhByVdsdTx65H4xnL4TlWQ", "version": &#123; "created": "5010199" &#125;, "provided_name": "lagou" &#125; &#125; &#125;, ".kibana": &#123; "aliases": &#123;&#125;, "mappings": &#123; "index-pattern": &#123; "properties": &#123; "fieldFormatMap": &#123; "type": "text" &#125;, "fields": &#123; "type": "text" &#125;, "intervalName": &#123; "type": "text" &#125;, "notExpandable": &#123; "type": "boolean" &#125;, "sourceFilters": &#123; "type": "text" &#125;, "timeFieldName": &#123; "type": "text" &#125;, "title": &#123; "type": "text" &#125; &#125; &#125;, "config": &#123; "properties": &#123; "buildNum": &#123; "type": "keyword" &#125; &#125; &#125;, "timelion-sheet": &#123; "properties": &#123; "description": &#123; "type": "text" &#125;, "hits": &#123; "type": "integer" &#125;, "kibanaSavedObjectMeta": &#123; "properties": &#123; "searchSourceJSON": &#123; "type": "text" &#125; &#125; &#125;, "timelion_chart_height": &#123; "type": "integer" &#125;, "timelion_columns": &#123; "type": "integer" &#125;, "timelion_interval": &#123; "type": "text" &#125;, "timelion_other_interval": &#123; "type": "text" &#125;, "timelion_rows": &#123; "type": "integer" &#125;, "timelion_sheet": &#123; "type": "text" &#125;, "title": &#123; "type": "text" &#125;, "version": &#123; "type": "integer" &#125; &#125; &#125;, "server": &#123; "properties": &#123; "uuid": &#123; "type": "keyword" &#125; &#125; &#125; &#125;, "settings": &#123; "index": &#123; "creation_date": "1499073856161", "number_of_shards": "1", "number_of_replicas": "1", "uuid": "g_cP7qZERXiVeKEwjbNE1g", "version": &#123; "created": "5010199" &#125;, "provided_name": ".kibana" &#125; &#125; &#125;&#125;# GET lagou&#123; "lagou": &#123; "aliases": &#123;&#125;, "mappings": &#123;&#125;, "settings": &#123; "index": &#123; "creation_date": "1499158041414", "number_of_shards": "5", "number_of_replicas": "2", "uuid": "BhByVdsdTx65H4xnL4TlWQ", "version": &#123; "created": "5010199" &#125;, "provided_name": "lagou" &#125; &#125; &#125;&#125; 保存文档 1234567891011121314151617181920212223242526# 保存文档PUT lagou/job/1&#123; "title":"python分布式爬虫开发", "salary_min":15000, "city":"北京", "company":&#123; "name":"百度", "company_addr":"北京市软件园" &#125;, "publish_date":"2017-4-16", "comments":15&#125;POST lagou/job/&#123; "title":"python django 开发工程师", "salary_min":30000, "city":"上海", "company":&#123; "name":"美团科技", "company_addr":"北京市软件园A区" &#125;, "publish_date":"2017-4-16", "comments":20&#125; 运行结果： 1234567891011121314151617181920212223242526272829# PUT lagou/job/1&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 1, "result": "created", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;, "created": true&#125;# POST lagou/job/&#123; "_index": "lagou", "_type": "job", "_id": "AV0MzKkUnOriGeBA_nYi", "_version": 1, "result": "created", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;, "created": true&#125; 在elasticsearch-head中显示如下： 分别采用PUT和POST方法保存两个文档，采用POST方法时未指定id仍然保存成功，id为系统分配的uuid。 获取文档 12345# 获取文档GET lagou/job/1GET lagou/job/1?_source=titleGET lagou/job/1?_source=title,cityGET lagou/job/1?_source 运行结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# GET lagou/job/1&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 1, "found": true, "_source": &#123; "title": "python分布式爬虫开发", "salary_min": 15000, "city": "北京", "company": &#123; "name": "百度", "company_addr": "北京市软件园" &#125;, "publish_date": "2017-4-16", "comments": 15 &#125;&#125;# GET lagou/job/1?_source=title&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 1, "found": true, "_source": &#123; "title": "python分布式爬虫开发" &#125;&#125;# GET lagou/job/1?_source=title,city&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 1, "found": true, "_source": &#123; "city": "北京", "title": "python分布式爬虫开发" &#125;&#125;# GET lagou/job/1?_source&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 1, "found": true, "_source": &#123; "title": "python分布式爬虫开发", "salary_min": 15000, "city": "北京", "company": &#123; "name": "百度", "company_addr": "北京市软件园" &#125;, "publish_date": "2017-4-16", "comments": 15 &#125;&#125; 修改文档 12345678910111213141516171819# 修改文档PUT lagou/job/1&#123; "title":"python分布式爬虫开发", "salary_min":15000, "company":&#123; "name":"百度", "company_addr":"北京市软件园" &#125;, "publish_date":"2017-4-16", "comments":15&#125;POST lagou/job/1/_update&#123; "doc":&#123; "comments":20 &#125;&#125; 运行结果： 12345678910111213141516171819202122232425262728# PUT lagou/job/1&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 2, "result": "updated", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;, "created": false&#125;# POST lagou/job/1/_update&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 3, "result": "updated", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;&#125; 修改文档有两种方式，一种是PUT覆盖更新方式，一种是POST增量更新方式。 删除 1234567# 删除# 删除文档DELETE lagou/job/1# 删除索引DELETE lagou/ 运行结果： 12345678910111213141516171819# DELETE lagou/job/1&#123; "found": true, "_index": "lagou", "_type": "job", "_id": "1", "_version": 4, "result": "deleted", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;&#125;# DELETE lagou/&#123; "acknowledged": true&#125; Elasticsearch的mget和bulk批量操作mget像Elasticsearch一样，检索多个文档依旧非常快。合并多个请求可以避免每个请求单独的网络开销。如果你需要从Elasticsearch中检索多个文档，相对于一个一个的检索，更快的方式是在一个请求中使用multi-get或者mget API。 mget API参数是一个docs数组，数组的每个节点定义一个文档的_index、_type、_id元数据。如果你只想检索一个或几个确定的字段，也可以定义一个_source参数。 为了演示mget操作，我们新建了一个索引testdb，其下文档如下： 第一种： 12345678910111213141516# 查询job1 id为1的数据，job2 id为2的数据GET _mget&#123; "docs":[ &#123; "_index":"testdb", "_type":"job1", "_id":1 &#125;, &#123; "_index":"testdb", "_type":"job2", "_id":2 &#125; ]&#125; 运行结果： 123456789101112131415161718192021222324&#123; "docs": [ &#123; "_index": "testdb", "_type": "job1", "_id": "1", "_version": 1, "found": true, "_source": &#123; "title": "job1_1" &#125; &#125;, &#123; "_index": "testdb", "_type": "job2", "_id": "2", "_version": 1, "found": true, "_source": &#123; "title": "job2_2" &#125; &#125; ]&#125; 第二种方法，在url里传递索引名称： 1234567891011121314# 第二种方法，在url中传递索引名称GET testdb/_mget&#123; "docs":[ &#123; "_type":"job1", "_id":1 &#125;, &#123; "_type":"job2", "_id":2 &#125; ]&#125; 运行结果都是一样的。 如果查询的数据index和type都相同，则可以将type也传入到url中： 123456789101112# 查询的数据索引名称相同，type也相同GET testdb/job1/_mget&#123; "docs":[ &#123; "_id":1 &#125;, &#123; "_id":2 &#125; ]&#125; 另外还有一种简写方法： 12345# 只需要传递id即可GET testdb/job1/_mget&#123; "ids":[1,2]&#125; 两者结果相同。 bulk就像mget允许我们一次性检索多个文档一样，bulk API允许我们使用单一请求来实现多个文档的create、index、update或delete。这对索引类似于日志活动这样的数据流非常有用，它们可以以成百上千的数据为一个批次按序进行索引。 传送门：更新时的批量操作 测试： 123456# bulk操作POST _bulk&#123;"index":&#123;"_index":"lagou", "_type":"job", "_id":1&#125;&#125;&#123;"title":"python分布式爬虫开发","salary_min":15000,"city":"北京","company":&#123;"name":"百度","company_addr":"北京市软件园"&#125;,"publish_date":"2017-4-16","comments":15&#125;&#123;"index":&#123;"_index":"lagou", "_type":"job2", "_id":2&#125;&#125;&#123;"title":"python django开发","salary_min":30000,"city":"成都","company":&#123;"name":"阿里巴巴","company_addr":"北京市软件园B区"&#125;,"publish_date":"2017-4-18","comments":50&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738&#123; "took": 462, "errors": false, "items": [ &#123; "index": &#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true, "status": 201 &#125; &#125;, &#123; "index": &#123; "_index": "lagou", "_type": "job2", "_id": "2", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true, "status": 201 &#125; &#125; ]&#125; Elasticsearch的mapping映射管理为了能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理成全文本（Full-text）或精确的字符串值，Elasticsearch需要知道每个字段里面都包含了什么类型。这些类型和字段的信息存储（包含）在映射（mapping）中。 索引中每个文档都有一个类型(type)。 每个类型拥有自己的映射(mapping)或者模式定义(schema definition)。一个映射定义了字段类型，每个字段的数据类型，以及字段被Elasticsearch处理的方式。映射还用于设置关联到类型上的元数据。 传送门：映射 测试： 创建映射： 123456789101112131415161718192021222324252627282930313233343536373839# 创建索引PUT lagou&#123; "mappings": &#123; "job": &#123; "properties": &#123; "title": &#123; "type": "text" &#125;, "salary_min": &#123; "type": "integer" &#125;, "city": &#123; "type": "keyword" &#125;, "company": &#123; "properties": &#123; "name": &#123; "type": "text" &#125;, "company_addr": &#123; "type": "text" &#125;, "employee_count": &#123; "type": "integer" &#125; &#125; &#125;, "publish_date": &#123; "type": "date", "format": "yyyy-MM-dd" &#125;, "comments": &#123; "type": "integer" &#125; &#125; &#125; &#125;&#125; 放入数据： 1234567891011121314# 放入数据PUT lagou/job/1&#123; "title": "python分布式爬虫开发", "salary_min": 15000, "city": "北京", "company": &#123; "name": "百度", "company_addr": "北京市软件园", "employee_count": 50 &#125;, "publish_date": "2017-4-18", "comments": 15&#125; 运行结果： 12345678910111213&#123; "_index": "lagou", "_type": "job", "_id": "1", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125; 可以在elasticsearch-head中查看结果： 如果我们对放入的数据稍做修改，将&quot;salary_min&quot;:15000改为&quot;salary_min&quot;:&quot;15000&quot;，操作还是可以完成的，这是因为mappings会对其进行类型转换得到我们所要的integer类型。 如果将&quot;salary_min&quot;:15000改为&quot;salary_min&quot;:&quot;abc&quot;，那就会出错： 1234567891011121314151617&#123; "error": &#123; "root_cause": [ &#123; "type": "mapper_parsing_exception", "reason": "failed to parse [salary_min]" &#125; ], "type": "mapper_parsing_exception", "reason": "failed to parse [salary_min]", "caused_by": &#123; "type": "number_format_exception", "reason": "For input string: \"abc\"" &#125; &#125;, "status": 400&#125; 获取mapping： 1234GET lagou/_mappingGET lagou/mapping/jobGET _all/_mappingGET _all/_mapping/job Elasticsearch的简单查询查询分类： 基本查询：使用Elasticsearch内置查询条件进行查询 组合查询：把多个查询组合在一起进行复合查询 过滤：查询同时，通过filter条件下在不影响打分的情况下筛选数据 测试，首先添加映射： 1234567891011121314151617181920212223242526272829# 添加映射PUT lagou&#123; "mappings": &#123; "job": &#123; "properties": &#123; "title": &#123; "store": true, "type": "text", "analyzer": "ik_max_word" &#125;, "company_name": &#123; "store": true, "type": "keyword" &#125;, "desc": &#123; "type": "text" &#125;, "comments": &#123; "type": "integer" &#125;, "add_time": &#123; "type": "date", "format": "yyyy-MM-dd" &#125; &#125; &#125; &#125;&#125; 注：”ik_max_word”是分析器类型的一种，会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合。 传送门：elasticsearch-analysis-ik 然后添加我们准备好的数据： 123456789101112131415161718192021222324252627282930313233343536# 添加数据POST lagou/job&#123; "title": "python django 开发工程师", "company_name": "美团科技有限公司", "desc": "对django的概念熟悉，熟悉python基础知识", "comments": 20, "add_time": "2017-4-1"&#125;POST lagou/job&#123; "title": "python scrapy redis分布式爬虫基本", "company_name": "百度科技有限公司", "desc": "对scrapy的概念熟悉，熟悉redis的基本操作", "comments": 5, "add_time": "2017-4-15"&#125;POST lagou/job&#123; "title": "elasticsearch打造搜索引擎", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉数据结构算法，熟悉python的基本开发", "comments": 15, "add_time": "2017-6-20"&#125;POST lagou/job&#123; "title": "python打造推荐引擎系统", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉推荐引擎的原理以及算法，掌握C语言", "comments": 60, "add_time": "2017-10-20"&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# POST lagou/job&#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWQ-nOriGeBA_nYp", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125;# POST lagou/job&#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWS6nOriGeBA_nYq", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125;# POST lagou/job&#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWUJnOriGeBA_nYr", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125;# POST lagou/job&#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWVXnOriGeBA_nYs", "_version": 1, "result": "created", "_shards": &#123; "total": 2, "successful": 1, "failed": 0 &#125;, "created": true&#125; 接下来就可以测试查询操作了。 match查询， 123456789# match查询GET lagou/job/_search&#123; "query": &#123; "match": &#123; "title": "Python" &#125; &#125;&#125; 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&#123; "took": 200, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 0.25811607, "hits": [ &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWVXnOriGeBA_nYs", "_score": 0.25811607, "_source": &#123; "title": "python打造推荐引擎系统", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉推荐引擎的原理以及算法，掌握C语言", "comments": 60, "add_time": "2017-10-20" &#125; &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWQ-nOriGeBA_nYp", "_score": 0.19944568, "_source": &#123; "title": "python django 开发工程师", "company_name": "美团科技有限公司", "desc": "对django的概念熟悉，熟悉python基础知识", "comments": 20, "add_time": "2017-4-1" &#125; &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWS6nOriGeBA_nYq", "_score": 0.1383129, "_source": &#123; "title": "python scrapy redis分布式爬虫基本", "company_name": "百度科技有限公司", "desc": "对scrapy的概念熟悉，熟悉redis的基本操作", "comments": 5, "add_time": "2017-4-15" &#125; &#125; ] &#125;&#125; 如果把&quot;title&quot;:&quot;python&quot;改为&quot;title&quot;:&quot;Python&quot;，依然能得到和上面一样的搜索结果，因为ik的分词器为自动地进行大小写转换。 term查询： 123456789# term查询GET lagou/job/_search&#123; "query": &#123; "term": &#123; "title": "python" &#125; &#125;&#125; 发现查询的结果和match查询是一样的，但两者是有区别的，match查询经过ik分词，而term查询是把整个词拿去匹配的，就好像type是keyword一样，不对查询词做任何处理。 terms查询 123456789# terms查询GET lagou/_search&#123; "query": &#123; "terms": &#123; "title": ["工程师", "django", "系统"] &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; "took": 11, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 1.5164987, "hits": [ &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWQ-nOriGeBA_nYp", "_score": 1.5164987, "_source": &#123; "title": "python django 开发工程师", "company_name": "美团科技有限公司", "desc": "对django的概念熟悉，熟悉python基础知识", "comments": 20, "add_time": "2017-4-1" &#125; &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWVXnOriGeBA_nYs", "_score": 0.25811607, "_source": &#123; "title": "python打造推荐引擎系统", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉推荐引擎的原理以及算法，掌握C语言", "comments": 60, "add_time": "2017-10-20" &#125; &#125; ] &#125;&#125; terms查询的特点是可以在查询时传入一个列表。 控制查询的返回数量 1234567891011# 控制查询的返回数量GET lagou/_search&#123; "query": &#123; "match": &#123; "title": "python" &#125; &#125;, "from": 1, "size": 2&#125; 通过from和size来控制结果的返回数量。 match_all查询 1234567# match_all查询GET lagou/_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; match_all查询会返回所有结果。 match_phrase查询 123456789101112# match_phrase短语查询GET lagou/_search&#123; "query": &#123; "match_phrase": &#123; "title": &#123; "query": "python系统", "slop": 6 &#125; &#125; &#125;&#125; 运行结果： 12345678910111213141516171819202122232425262728&#123; "took": 82, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 0.1133824, "hits": [ &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWVXnOriGeBA_nYs", "_score": 0.1133824, "_source": &#123; "title": "python打造推荐引擎系统", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉推荐引擎的原理以及算法，掌握C语言", "comments": 60, "add_time": "2017-10-20" &#125; &#125; ] &#125;&#125; slop值表示两个词之间的最小距离。 multi_match查询 123456789101112# multi_match查询# 比如可以指定多个字段# 比如查询title和desc这两个字段里面包含python的关键词文档GET lagou/_search&#123; "query": &#123; "multi_match": &#123; "query": "python", "fields": ["title^3","desc"] &#125; &#125;&#125; &quot;title^3&quot;的意思是为title设置较高的权重，对最后结果的排序有影响。 指定返回字段 123456789GET lagou/_search&#123; "stored_fields": ["title","company_name"], "query": &#123; "match": &#123; "title": "python" &#125; &#125;&#125; 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; "took": 13, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 0.25811607, "hits": [ &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWVXnOriGeBA_nYs", "_score": 0.25811607, "fields": &#123; "title": [ "python打造推荐引擎系统" ], "company_name": [ "阿里巴巴科技有限公司" ] &#125; &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWQ-nOriGeBA_nYp", "_score": 0.19944568, "fields": &#123; "title": [ "python django 开发工程师" ], "company_name": [ "美团科技有限公司" ] &#125; &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWS6nOriGeBA_nYq", "_score": 0.1383129, "fields": &#123; "title": [ "python scrapy redis分布式爬虫基本" ], "company_name": [ "百度科技有限公司" ] &#125; &#125; ] &#125;&#125; 通过sort把结果排序 1234567891011121314# 通过sort把结果排序GET lagou/_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ &#123; "comments": &#123; "order": "desc" &#125; &#125; ]&#125; 运行结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&#123; "took": 21, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 4, "max_score": null, "hits": [ &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWVXnOriGeBA_nYs", "_score": null, "_source": &#123; "title": "python打造推荐引擎系统", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉推荐引擎的原理以及算法，掌握C语言", "comments": 60, "add_time": "2017-10-20" &#125;, "sort": [ 60 ] &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWQ-nOriGeBA_nYp", "_score": null, "_source": &#123; "title": "python django 开发工程师", "company_name": "美团科技有限公司", "desc": "对django的概念熟悉，熟悉python基础知识", "comments": 20, "add_time": "2017-4-1" &#125;, "sort": [ 20 ] &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWUJnOriGeBA_nYr", "_score": null, "_source": &#123; "title": "elasticsearch打造搜索引擎", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉数据结构算法，熟悉python的基本开发", "comments": 15, "add_time": "2017-6-20" &#125;, "sort": [ 15 ] &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWS6nOriGeBA_nYq", "_score": null, "_source": &#123; "title": "python scrapy redis分布式爬虫基本", "company_name": "百度科技有限公司", "desc": "对scrapy的概念熟悉，熟悉redis的基本操作", "comments": 5, "add_time": "2017-4-15" &#125;, "sort": [ 5 ] &#125; ] &#125;&#125; range查询 1234567891011121314# 查询范围# range查询GET lagou/_search&#123; "query": &#123; "range": &#123; "comments": &#123; "gte": 10, "lte": 20, "boost": 2.0 &#125; &#125; &#125;&#125; 查询10&lt;=comments&lt;=20的结果。 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; "took": 37, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 2, "hits": [ &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWQ-nOriGeBA_nYp", "_score": 2, "_source": &#123; "title": "python django 开发工程师", "company_name": "美团科技有限公司", "desc": "对django的概念熟悉，熟悉python基础知识", "comments": 20, "add_time": "2017-4-1" &#125; &#125;, &#123; "_index": "lagou", "_type": "job", "_id": "AV0RoWUJnOriGeBA_nYr", "_score": 2, "_source": &#123; "title": "elasticsearch打造搜索引擎", "company_name": "阿里巴巴科技有限公司", "desc": "熟悉数据结构算法，熟悉python的基本开发", "comments": 15, "add_time": "2017-6-20" &#125; &#125; ] &#125;&#125; 也可以对时间进行range查询： 1234567891011GET lagou/_search&#123; "query": &#123; "range": &#123; "add_time": &#123; "gte": "2017-04-01", "lte": "now" &#125; &#125; &#125;&#125; wildcard查询 123456789101112# wildcard查询GET lagou/_search&#123; "query": &#123; "wildcard": &#123; "title": &#123; "value": "pyth*n", "boost": 2.0 &#125; &#125; &#125;&#125; wildcard查询支持通配符。 Elasticsearch的bool组合查询首先使用bulk操作建立测试数据： 1234567891011121314151617181920# bool查询# 老版本的filtered已经被bool替换# 用bool包括must should must_not filter来完成，格式如下：# bool:&#123;# "filter":[],# "must":[],# "should":[],# "must_not":&#123;&#125;,# &#125;# 建立测试数据POST lagou/testjob/_bulk&#123;"index":&#123;"_id":1&#125;&#125;&#123;"salary":10, "title":"Python"&#125;&#123;"index":&#123;"_id":2&#125;&#125;&#123;"salary":20, "title":"Scrapy"&#125;&#123;"index":&#123;"_id":3&#125;&#125;&#123;"salary":30, "title":"Django"&#125;&#123;"index":&#123;"_id":4&#125;&#125;&#123;"salary":30, "title":"Elasticsearch"&#125; 接下来介绍简单的过滤查询： 最简单的filter查询 1234567891011121314151617# 薪资为20k的工作# select * from testjob where salary=20GET lagou/testjob/_search&#123; "query": &#123; "bool": &#123; "must":&#123; "match_all":&#123;&#125; &#125;, "filter": &#123; "term": &#123; "salary": "20" &#125; &#125; &#125; &#125;&#125; 运行结果： 12345678910111213141516171819202122232425&#123; "took": 43, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "lagou", "_type": "testjob", "_id": "2", "_score": 1, "_source": &#123; "salary": 20, "title": "Scrapy" &#125; &#125; ] &#125;&#125; 也可以指定多个值 12345678910111213141516# 也可以指定多个值GET lagou/testjob/_search&#123; "query": &#123; "bool": &#123; "must":&#123; "match_all":&#123;&#125; &#125;, "filter": &#123; "terms": &#123; "salary":[10,20] &#125; &#125; &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435&#123; "took": 39, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 1, "hits": [ &#123; "_index": "lagou", "_type": "testjob", "_id": "2", "_score": 1, "_source": &#123; "salary": 20, "title": "Scrapy" &#125; &#125;, &#123; "_index": "lagou", "_type": "testjob", "_id": "1", "_score": 1, "_source": &#123; "salary": 10, "title": "Python" &#125; &#125; ] &#125;&#125; 职位查询 12345678910111213141516# select * from testjob where title="Python"GET lagou/testjob/_search&#123; "query": &#123; "bool": &#123; "must":&#123; "match_all":&#123;&#125; &#125;, "filter": &#123; "match": &#123; "title": "Python" &#125; &#125; &#125; &#125;&#125; 或者把&quot;Python&quot;变为小写&quot;python&quot;那么也可以用term。 查看分析器解析的结果 12345GET _analyze&#123; "analyzer": "ik_max_word", "text": "Python网络开发工程师"&#125; 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; "tokens": [ &#123; "token": "python", "start_offset": 0, "end_offset": 6, "type": "ENGLISH", "position": 0 &#125;, &#123; "token": "网络", "start_offset": 6, "end_offset": 8, "type": "CN_WORD", "position": 1 &#125;, &#123; "token": "络", "start_offset": 7, "end_offset": 8, "type": "CN_WORD", "position": 2 &#125;, &#123; "token": "开发", "start_offset": 8, "end_offset": 10, "type": "CN_WORD", "position": 3 &#125;, &#123; "token": "发", "start_offset": 9, "end_offset": 10, "type": "CN_WORD", "position": 4 &#125;, &#123; "token": "工程师", "start_offset": 10, "end_offset": 13, "type": "CN_WORD", "position": 5 &#125;, &#123; "token": "工程", "start_offset": 10, "end_offset": 12, "type": "CN_WORD", "position": 6 &#125;, &#123; "token": "师", "start_offset": 12, "end_offset": 13, "type": "CN_CHAR", "position": 7 &#125; ]&#125; 如果用ik_smart会怎样呢？ 1234567891011121314151617181920212223242526272829303132&#123; "tokens": [ &#123; "token": "python", "start_offset": 0, "end_offset": 6, "type": "ENGLISH", "position": 0 &#125;, &#123; "token": "网络", "start_offset": 6, "end_offset": 8, "type": "CN_WORD", "position": 1 &#125;, &#123; "token": "开发", "start_offset": 8, "end_offset": 10, "type": "CN_WORD", "position": 2 &#125;, &#123; "token": "工程师", "start_offset": 10, "end_offset": 13, "type": "CN_WORD", "position": 3 &#125; ]&#125; bool过滤查询，可以做组合过滤查询 12345678910111213141516# select * from testjob where (salary=20 or title=Python) AND (salary != 30)# 查询薪资等于20k或者工作为python的工作，排除价格为30k的GET lagou/testjob/_search&#123; "query": &#123; "bool": &#123; "should": [ &#123;"term":&#123;"salary":20&#125;&#125;, &#123;"term":&#123;"title":"python"&#125;&#125; ], "must_not": &#123; "term":&#123;"price":30&#125; &#125; &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435&#123; "took": 5, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 1, "hits": [ &#123; "_index": "lagou", "_type": "testjob", "_id": "2", "_score": 1, "_source": &#123; "salary": 20, "title": "Scrapy" &#125; &#125;, &#123; "_index": "lagou", "_type": "testjob", "_id": "1", "_score": 0.2876821, "_source": &#123; "salary": 10, "title": "Python" &#125; &#125; ] &#125;&#125; 嵌套查询 1234567891011121314151617# select * from testjob where title="python" or (title="elasticsearch" AND salary=30)GET lagou/testjob/_search&#123; "query": &#123; "bool": &#123; "should": [ &#123;"term":&#123;"title":"python"&#125;&#125;, &#123;"bool":&#123; "must": [ &#123;"term":&#123;"title":"elasticsearch"&#125;&#125;, &#123;"term":&#123;"salary":30&#125;&#125; ] &#125;&#125; ] &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435&#123; "took": 16, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 1.6931472, "hits": [ &#123; "_index": "lagou", "_type": "testjob", "_id": "4", "_score": 1.6931472, "_source": &#123; "salary": 30, "title": "Elasticsearch" &#125; &#125;, &#123; "_index": "lagou", "_type": "testjob", "_id": "1", "_score": 0.2876821, "_source": &#123; "salary": 10, "title": "Python" &#125; &#125; ] &#125;&#125; 下面需要介绍过滤空和非空的方法 123456789101112131415161718192021222324252627282930313233343536373839404142# 过滤空和非空# 建立测试数据POST lagou/testjob2/_bulk&#123;"index":&#123;"_id":"1"&#125;&#125;&#123;"tags":["search"]&#125;&#123;"index":&#123;"_id":"2"&#125;&#125;&#123;"tags":["search","python"]&#125;&#123;"index":&#123;"_id":"3"&#125;&#125;&#123;"other_field":["some data"]&#125;&#123;"index":&#123;"_id":"4"&#125;&#125;&#123;"tags":null&#125;&#123;"index":&#123;"_id":"5"&#125;&#125;&#123;"tags":["search", null]&#125;# 处理null空值的方法# select tags from testjob2 where tags is not NULLGET lagou/testjob2/_search&#123; "query": &#123; "bool": &#123; "filter": &#123; "exists": &#123; "field": "tags" &#125; &#125; &#125; &#125;&#125;# 过滤非空值GET lagou/testjob2/_search&#123; "query": &#123; "bool": &#123; "must_not": &#123; "exists": &#123; "field": "tags" &#125; &#125; &#125; &#125;&#125; scrapy写入数据到Elasticsearch中我们在scrapy中依靠pipeline来将数据写入到Elasticsearch中，这里需要介绍一个es的python接口elasticsearch-dsl-py。 传送门：elasticsearch-dsl-py 安装：pip install elasticsearch-dsl 在根目录下新建一个models包，在包内建立es_types.py文件： 1234567891011121314151617181920212223242526272829303132from datetime import datetimefrom elasticsearch_dsl import DocType, Date, Nested, Boolean, \ analyzer, InnerObjectWrapper, Completion, Keyword, Text, Integerfrom elasticsearch_dsl.connections import connections# 连接本机esconnections.create_connection(hosts=["localhost"])class ArticleType(DocType): """ 伯乐在线文章类型 """ title = Text(analyzer="ik_max_word") create_date = Date() url = Keyword() url_object_id = Keyword() front_image_url = Keyword() front_image_path = Keyword() praise_nums = Integer() comment_nums = Integer() fav_nums = Integer() tags = Text(analyzer="ik_max_word") content = Text(analyzer="ik_max_word") class Meta: index = "jobbole" doc_type = "article"if __name__ == '__main__': ArticleType.init() 这样就可以很方便地在es中建立我们所需要的mappings，和Django中的models十分相似。 运行后我们可以再es-head中查看： pipelines.py中代码如下： 123456789101112131415161718192021222324252627from models.es_types import ArticleTypefrom w3lib.html import remove_tagsclass ElasticsearchPipeline(object): """ 将数据写入es中 """ def process_item(self, item, spider): # 将item转换为es的数据 article = ArticleType() article.title = item["title"] article.create_date = item["create_date"] article.content = remove_tags(item["content"]) article.front_image_url = item["front_image_url"] if "front_image_path" in item: article.front_image_path = item["front_image_path"] article.praise_nums = item["praise_nums"] article.fav_nums = item["fav_nums"] article.comment_nums = item["comment_nums"] article.url = item["url"] article.tags = item["tags"] article.meta.id = item["url_object_id"] article.save() return item 调试之后发现数据顺利地进入到了es中： 为了进行对所有数据进行统一处理，我们可以把数据转换的逻辑拿到items.py中： 1234567891011121314151617def save_to_es(self): article = ArticleType() article.title = self["title"] article.create_date = self["create_date"] article.content = remove_tags(self["content"]) article.front_image_url = self["front_image_url"] if "front_image_path" in self: article.front_image_path = self["front_image_path"] article.praise_nums = self["praise_nums"] article.fav_nums = self["fav_nums"] article.comment_nums = self["comment_nums"] article.url = self["url"] article.tags = self["tags"] article.meta.id = self["url_object_id"] article.save() return pipelines.py： 12345678910class ElasticsearchPipeline(object): """ 将数据写入es中 """ def process_item(self, item, spider): # 将item转换为es的数据 item.save_to_es() return item 其他的知乎拉勾的爬虫数据转换也基本类似。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python, Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——scrapy-redis分布式爬虫]]></title>
    <url>%2F2017%2F07%2F01%2Fscrapy-redis%2F</url>
    <content type="text"><![CDATA[分布式爬虫要点 分布式爬虫的优点 充分利用多机器的宽带加速爬取 充分利用多机的IP加速爬取速度 问题：为什么scrapy不支持分布式？ 答：在scrapy中scheduler是运行在队列中的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理，所以scrapy不支持分布式。 分布式爬虫需要解决的问题 requests队列集中管理 去重集中管理 综上，我们需要使用Redis来解决这些问题。 Redis基础知识Redis的基础知识在我早前的文章中已经学习过了，在这里就不介绍了，直接看之前的文章就行。 传送门：Redis学习笔记 scrapy-redis编写分布式爬虫代码传送门：1.scapy-redis Github 2.scrapy-redis 文档 其实大部分的逻辑是一样的，只需要在spider中加入redis_key = &#39;spidername:start_urls&#39;，以及修改一些settings.py中配置即可。 scrapy-redis源码解析 项目结构connection.py 负责根据setting中配置实例化redis连接。被dupefilter和scheduler调用，总之涉及到redis存取的都要使用到这个模块。 dupefilter.py 负责执行requst的去重，实现的很有技巧性，使用redis的set数据结构。但是注意scheduler并不使用其中用于在这个模块中实现的dupefilter键做request的调度，而是使用queue.py模块中实现的queue。 当request不重复时，将其存入到queue中，调度时将其弹出。 queue.py 其作用如II所述，但是这里实现了三种方式的queue： FIFO的SpiderQueue，SpiderPriorityQueue，以及LIFI的SpiderStack。默认使用的是第二中，这也就是出现之前文章中所分析情况的原因（链接：）。 pipelines.py 这是是用来实现分布式处理的作用。它将Item存储在redis中以实现分布式处理。 另外可以发现，同样是编写pipelines，在这里的编码实现不同于文章（链接：）中所分析的情况，由于在这里需要读取配置，所以就用到了from_crawler()函数。 scheduler.py 此扩展是对scrapy中自带的scheduler的替代（在settings的SCHEDULER变量中指出），正是利用此扩展实现crawler的分布式调度。其利用的数据结构来自于queue中实现的数据结构。 scrapy-redis所实现的两种分布式：爬虫分布式以及item处理分布式就是由模块scheduler和模块pipelines实现。上述其它模块作为为二者辅助的功能模块。 spider.py 设计的这个spider从redis中读取要爬的url,然后执行爬取，若爬取过程中返回更多的url，那么继续进行直至所有的request完成。之后继续从redis中读取url，循环这个过程。 分析：在这个spider中通过connect signals.spider_idle信号实现对crawler状态的监视。当idle时，返回新的make_requests_from_url(url)给引擎，进而交给调度器调度。 架构解析Scrapy架构： scrapy-redis架构： 如上图所示，scrapy-redis在scrapy的架构上增加了redis，基于redis的特性拓展了如下组件： 调度器（Scheduler）：scrapy-redis调度器通过redis的set不重复的特性，巧妙的实现了Duplication Filter去重（DupeFilter set存放爬取过的request）。Spider新生成的request，将request的指纹到redis的DupeFilter set检查是否重复，并将不重复的request push写入redis的request队列。调度器每次从redis的request队列里根据优先级pop出一个request, 将此request发给spider处理。 Item Pipeline：将Spider爬取到的Item给scrapy-redis的Item Pipeline，将爬取到的Item存入redis的items队列。可以很方便的从items队列中提取item，从而实现items processes 集群 集成bloomfilter到scrapy-redis中传送门：bloomfilter算法详解及实例 算法实现：bloomfilter_imooc dupefilter.py： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import loggingimport timefrom scrapy.dupefilters import BaseDupeFilterfrom scrapy.utils.request import request_fingerprintfrom . import defaultsfrom .connection import get_redis_from_settingsfrom ScrapyRedisTest.utils.bloomfilter import PyBloomFilter, connlogger = logging.getLogger(__name__)# TODO: Rename class to RedisDupeFilter.class RFPDupeFilter(BaseDupeFilter): """Redis-based request duplicates filter. This class can also be used with default Scrapy's scheduler. """ logger = logger def __init__(self, server, key, debug=False): """Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. """ self.server = server self.key = key self.debug = debug self.logdupes = True self.bf = PyBloomFilter(conn=conn, key=key) @classmethod def from_settings(cls, settings): """Returns an instance from given settings. This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as it needs to pass the spider name in the key. Parameters ---------- settings : scrapy.settings.Settings Returns ------- RFPDupeFilter A RFPDupeFilter instance. """ server = get_redis_from_settings(settings) # XXX: This creates one-time key. needed to support to use this # class as standalone dupefilter with scrapy's default scheduler # if scrapy passes spider on open() method this wouldn't be needed # TODO: Use SCRAPY_JOB env as default and fallback to timestamp. key = defaults.DUPEFILTER_KEY % &#123;'timestamp': int(time.time())&#125; debug = settings.getbool('DUPEFILTER_DEBUG') return cls(server, key=key, debug=debug) @classmethod def from_crawler(cls, crawler): """Returns instance from crawler. Parameters ---------- crawler : scrapy.crawler.Crawler Returns ------- RFPDupeFilter Instance of RFPDupeFilter. """ return cls.from_settings(crawler.settings) def request_seen(self, request): """Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool """ fp = self.request_fingerprint(request) if self.bf.is_exist(fp): return True else: self.bf.add(fp) return False # This returns the number of values added, zero if already exists. # added = self.server.sadd(self.key, fp) # return added == 0 def request_fingerprint(self, request): """Returns a fingerprint for a given request. Parameters ---------- request : scrapy.http.Request Returns ------- str """ return request_fingerprint(request) def close(self, reason=''): """Delete data on close. Called by Scrapy's scheduler. Parameters ---------- reason : str, optional """ self.clear() def clear(self): """Clears fingerprints data.""" self.server.delete(self.key) def log(self, request, spider): """Logs given request. Parameters ---------- request : scrapy.http.Request spider : scrapy.spiders.Spider """ if self.debug: msg = "Filtered duplicate request: %(request)s" self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) elif self.logdupes: msg = ("Filtered duplicate request %(request)s" " - no more duplicates will be shown" " (see DUPEFILTER_DEBUG to show all duplicates)") self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) self.logdupes = False]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python，Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy 进阶开发]]></title>
    <url>%2F2017%2F06%2F29%2Fscrapy-advanced-dev%2F</url>
    <content type="text"><![CDATA[本篇主要介绍selenium的使用、其余的一些动态网页获取技术以及scrapy的一些进阶知识。 Selenium的使用Selenium介绍Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本。 Selenium安装 Selenium安装完成之后，还需要下载浏览器对应的webdriver才能开始使用，我们这里选择Chrome的ChromeDriver。 Selenium动态网页请求123456789101112from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver")browser.get("https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5")t_selector = Selector(text=browser.page_source)print(t_selector.css(".tm-promo-price .tm-price::text").extract())browser.quit() 我们用Selenium请求一个天猫商品的动态网页，并用Scrapy Selector来获取对应的商品价格信息。 Selenium模拟登录知乎 调试观察之后，采用Selenium自带的选择器方法来模拟输入账号密码并且点击登录。 12345678from selenium import webdriverbrowser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver")browser.get("https://www.zhihu.com/#signin")browser.find_element_by_css_selector(".view-signin input[name='account']").send_keys("your_username")browser.find_element_by_css_selector(".view-signin input[name='password']").send_keys("your_password")browser.find_element_by_css_selector(".view_signin button.sign-button").click() Selenium模拟登录微博 首先调试观察微博登录页面 1234567from selenium import webdriverbrowser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver")browser.get("https://www.weibo.com")browser.find_element_by_css_selector("#loginname").send_keys("your_username")browser.find_element_by_css_selector(".info_list.password input[node-type='password']").send_keys("your_password")browser.find_element_by_css_selector(".info_list.login_btn a[node-type='submitBtn']").click() 发现如下错误 原因：我们在页面还没有请求完成时就进行了下一步操作，导致元素获取不到。 在请求发出之后，休眠一段时间等待页面加载完成即可。 12345678910import timefrom selenium import webdriverbrowser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver")browser.get("https://www.weibo.com")time.sleep(15)browser.find_element_by_css_selector("#loginname").send_keys("584563542@qq.com")browser.find_element_by_css_selector(".info_list.password input[node-type='password']").send_keys("tracy584563542")browser.find_element_by_css_selector(".info_list.login_btn a[node-type='submitBtn']").click() Selenium模拟鼠标下拉这样的操作是通过JS脚本来进行的： 123456789import timefrom selenium import webdriverbrowser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver")browser.get("https://www.oschina.net/blog")for i in range(3): browser.execute_script("window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;") time.sleep(3) 设置ChromeDriver不加载图片123456# 设置ChromeDriver不加载图片chrome_opt = webdriver.ChromeOptions()prefs = &#123;"profile.managed_default_content_settings.images": 2&#125;chrome_opt.add_experimental_option("prefs", prefs)browser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver", chrome_options=chrome_opt)browser.get("https://www.taobao.com") PhantomJS获取动态网页123456# phantomjs, 无界面的浏览器， 多进程情况下phantomjs性能会下降很严重browser = webdriver.PhantomJS(executable_path="/Users/lawtech/TempSpace/phantomjs-2.1.1-macosx/bin/phantomjs")browser.get("https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5")t_selector = Selector(text=browser.page_source)print(t_selector.css(".tm-promo-price .tm-price::text").extract())browser.quit() Selenium集成到Scrapy中123456789101112131415161718192021from selenium import webdriverfrom scrapy.http import HtmlResponseclass JSPageMiddleware(object): """ 通过Chrome动态请求网页 """ def __init__(self): self.browser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver") super(JSPageMiddleware, self).__init__() def process_request(self, request, spider): if spider.name == "jobbole": self.browser.get(request.url) import time time.sleep(3) print("访问&#123;0&#125;".format(request.url)) return HtmlResponse(url=self.browser.current_url, body=self.browser.page_source, encoding='utf-8') 在middlewares.py中添加如上代码之后，别忘了在settings.py中将其配置好： 12345DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'ArticleSpider.middlewares.JSPageMiddleware': 1,&#125; 其实我们可以把Chrome放到Spider中，此时就引入了信号量的问题： 12345678910111213141516171819202122from scrapy.xlib.pydispatch import dispatcherfrom scrapy import signalsfrom selenium import webdriverclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/all-posts/'] def __init__(self): self.browser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver") super(JobboleSpider, self).__init__() dispatcher.connect(self.spider_closed, signals.spider_closed) def spider_closed(self, spider): """ 当爬虫退出的时候关闭Chrome :param spider: :return: """ print("spider closed") self.browser.quit() 其余动态网页获取技术Chrome无界面运行首先安装pyvirtualdisplay：pip install pyvirtualdisplay -i https://pypi.douban.com/simple/ 主要代码如下： 1234567from pyvirtualdisplay import Displaydisplay = Display(visible=0, size=(800, 600))display.start()browser = webdriver.Chrome(executable_path="/Users/lawtech/TempSpace/chromedriver")browser.get("https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5")browser.quit() scrapy-splashhttps://github.com/scrapy-plugins/scrapy-splash 稳定性没有Chrome高 selenium-gridhttps://github.com/SeleniumHQ/selenium/wiki/Grid2 splinterhttps://github.com/cobrateam/splinter Scrapy的暂停与重启scrapy crawl lagou -s JOBDIR=jobinfo/001 上面这条命令即可完成lagouspider的暂停与重启，中途可以你可以使用ctrl+c终止采集程序的运行，恢复时，还是运行上面这条命令即可，连按两次ctrl+c就可以完全终止。 其中jobinfo/001 是一个保存采集列表状态的目录，千万不要同时开多个爬虫程序使用同一个目录，会导致混乱。 还有更简单的方法，就是在settings.py文件里加入下面的代码： JOBDIR=&#39;jobinfo/001&#39; 使用命令scrapy crawl lagou，就会自动生成一个jobinfo/001的目录，然后将工作列表放到这个文件夹里。 Scrapy url去重原理对url进行hash运算映射到某个地址，将该url和hash值当做键值对存放到hash表中，当需要检测新的url的时候，只需要对该url进行hash映射，如果得到的地址在hash表中已经存在，则说明已经被爬取过，则放弃爬取，否则，进行爬取并记录键值对。这样只需要维护一个hash表即可，需要考虑的问题是hash碰撞的问题，互联网上数据如瀚海般，如果hash函数设计不当，碰撞还是很容易发生的。scrapy框架下可以在pipeline中写一个Duplicates filter,背后采用的是hash值存储。 相关代码都在dupefilter.py中，其实就是做了一个哈希摘要，放在set中，去查新的url是否在set中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class RFPDupeFilter(BaseDupeFilter): """Request Fingerprint duplicates filter""" def __init__(self, path=None, debug=False): self.file = None self.fingerprints = set() self.logdupes = True self.debug = debug self.logger = logging.getLogger(__name__) if path: self.file = open(os.path.join(path, 'requests.seen'), 'a+') self.file.seek(0) self.fingerprints.update(x.rstrip() for x in self.file) @classmethod def from_settings(cls, settings): debug = settings.getbool('DUPEFILTER_DEBUG') return cls(job_dir(settings), debug) def request_seen(self, request): fp = self.request_fingerprint(request) if fp in self.fingerprints: return True self.fingerprints.add(fp) if self.file: self.file.write(fp + os.linesep) def request_fingerprint(self, request): return request_fingerprint(request) def close(self, reason): if self.file: self.file.close() def log(self, request, spider): if self.debug: msg = "Filtered duplicate request: %(request)s" self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) elif self.logdupes: msg = ("Filtered duplicate request: %(request)s" " - no more duplicates will be shown" " (see DUPEFILTER_DEBUG to show all duplicates)") self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) self.logdupes = False spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider) Scrapy telnet服务Scrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。 telnet仅仅是一个运行在Scrapy进程中的普通python终端。 telnet终端监听设置中定义的 TELNETCONSOLE_PORT ，默认为 6023 。 访问telnet请输入: 12telnet localhost 6023&gt;&gt;&gt; Scrapy官方文档对telnet做了更详尽的介绍。 Spider middleware 详解传送门：Spider Middleware 上图为Scrapy源码中spidermiddlewares的结构 depth.py:爬取深度的设置 httperror.py：状态的设置，比如是不是要把404的也抓取下来，等等。 Scrapy的数据收集传送门：数据收集 Scrapy信号详解传送门：信号 示例： 123456789101112131415161718192021222324# 收集伯乐在线所有404的url以及404页面数handle_httpstatus_list = [404]def __init__(self): self.fail_urls = [] super(JobboleSpider, self).__init__() dispatcher.connect(self.handle_spider_closed, signals.spider_closed)def handle_spider_closed(self, spider, reason): self.crawler.stats.set_value("failed_urls", ",".join(self.fail_urls)) passdef parse(self, response): """ 1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析 2. 获取下一页的url并交给scrapy进行下载 :param response: :return: """ if response.status == 404: self.fail_urls.append(response.url) self.crawler.stats.inc_value("failed_url") 调试结果： Scrapy扩展开发传送门：扩展]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——cookie禁用、自动限速、自定义Spider的settings]]></title>
    <url>%2F2017%2F06%2F12%2Fscrapy-cookies-settings%2F</url>
    <content type="text"><![CDATA[像cookie禁用、自动限速这样的设置都在settings.py文件中，下面我们就来简单介绍一下。 cookie禁用12# Disable cookies (enabled by default)COOKIES_ENABLED = True 默认设置为False。 自动限速自动限速是通过自动限速(AutoThrottle)扩展来实现的，该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。 限速算法算法根据以下规则调整下载延迟及并发数: spider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。 当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。 AutoThrottle 扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比DOWNLOAD_DELAY 更低的下载延迟或者比 CONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数 (或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。 设置下面是控制 AutoThrottle 扩展的设置: AUTOTHROTTLE_ENABLED AUTOTHROTTLE_START_DELAY AUTOTHROTTLE_MAX_DELAY AUTOTHROTTLE_DEBUG CONCURRENT_REQUESTS_PER_DOMAIN CONCURRENT_REQUESTS_PER_IP DOWNLOAD_DELAY AUTOTHROTTLE_ENABLED默认: False 启用AutoThrottle扩展。 AUTOTHROTTLE_START_DELAY默认: 5.0 初始下载延迟(单位:秒)。 AUTOTHROTTLE_MAX_DELAY默认: 60.0 在高延迟情况下最大的下载延迟(单位秒)。 AUTOTHROTTLE_DEBUG默认: False 起用AutoThrottle调试(debug)模式，展示每个接收到的response。 您可以通过此来查看限速参数是如何实时被调整的。 自定义Spider的settings每个Spider可以定义自己的设置，这些设置将优先覆盖项目目录中的设置，可以通过设置 custom_settings 属性来实现。 例如，我们的项目中，zhihu.py 中需要设置开启cookie，那么只需要在该文件中如下设置即可： 123custom_settings = &#123; "COOKIES_ENABLED": True&#125; settings.py详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# -*- coding: utf-8 -*-import os# Scrapy settings for ArticleSpider project## For simplicity, this file contains only settings considered important or# commonly used. You can find more settings consulting the documentation:## http://doc.scrapy.org/en/latest/topics/settings.html# http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html# http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.htmlBOT_NAME = 'ArticleSpider' #Scrapy项目的名字这将用来构造默认User-Agent，同时也用来log，当您使用startproject命令创建项目时其也被自动赋值。SPIDER_MODULES = ['ArticleSpider.spiders'] #Scrapy搜索spider的模块列表 默认: [xxx.spiders]NEWSPIDER_MODULE = 'ArticleSpider.spiders' #使用genspider命令创建新spider的模块 默认: 'xxx.spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agent# USER_AGENT = 'ArticleSpider (+http://www.yourdomain.com)' #爬取的默认User-Agent，除非被覆盖# Obey robots.txt rulesROBOTSTXT_OBEY = False #如果启用,Scrapy将会采用robots.txt策略 # Configure maximum concurrent requests performed by Scrapy (default: 16)#Scrapy downloader并发请求(concurrent requests)的最大值，默认: 16# CONCURRENT_REQUESTS = 32# Configure a delay for requests for the same website (default: 0)# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docs#为同一网站的请求配置延迟（默认值：0）下载器在下载同一个网站下一个页面前需要等待的时间,该选项可以用来限制爬取速度,减轻服务器压力。同时也支持小数:0.25 以秒为单位 DOWNLOAD_DELAY = 10# The download delay setting will honor only one of:#下载延迟设置只有一个有效# CONCURRENT_REQUESTS_PER_DOMAIN = 16 对单个网站进行并发请求的最大值。# CONCURRENT_REQUESTS_PER_IP = 16 对单个IP进行并发请求的最大值。如果非0,则忽略 CONCURRENT_REQUESTS_PER_DOMAIN 设定,使用该设定。 也就是说,并发限制将针对IP,而不是网站。该设定也影响 DOWNLOAD_DELAY: 如果 CONCURRENT_REQUESTS_PER_IP 非0,下载延迟应用在IP而不是网站上。 # Disable cookies (enabled by default)#禁用Cookie（默认情况下启用）COOKIES_ENABLED = False# Disable Telnet Console (enabled by default)#禁用Telnet控制台（默认启用）# TELNETCONSOLE_ENABLED = False# Override the default request headers:#覆盖默认请求头：# DEFAULT_REQUEST_HEADERS = &#123;# 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',# 'Accept-Language': 'en',# &#125;# Enable or disable spider middlewares# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html#启用或禁用爬虫中间件# SPIDER_MIDDLEWARES = &#123;# 'ArticleSpider.middlewares.ArticlespiderSpiderMiddleware': 543,# &#125;# Enable or disable downloader middlewares# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#启用或禁用下载器中间件DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,&#125;# Enable or disable extensions# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html#启用或禁用扩展程序# EXTENSIONS = &#123;# 'scrapy.extensions.telnet.TelnetConsole': None,# &#125;# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; # 'ArticleSpider.pipelines.JsonExporterPipeline': 2, # # 'scrapy.pipelines.images.ImagesPipeline': 1, # 'ArticleSpider.pipelines.ArticleImagePipeline': 1, 'ArticleSpider.pipelines.MysqlTwistedPipeline': 1,&#125;IMAGES_URLS_FIELD = "front_image_url"project_dir = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_dir, 'images')# 设置搜索路径import osimport sysBASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))sys.path.insert(0, os.path.join(BASE_DIR, 'ArticleSpider'))USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4"RANDOM_UA_TYPE = 'random'# IMAGES_MIN_HEIGHT = 100# IMAGES_MIN_WIDTH = 100# Enable and configure the AutoThrottle extension (disabled by default)# See http://doc.scrapy.org/en/latest/topics/autothrottle.html#启用和配置AutoThrottle扩展（默认情况下禁用）AUTOTHROTTLE_ENABLED = True# The initial download delay# AUTOTHROTTLE_START_DELAY = 5# The maximum download delay to be set in case of high latencies# AUTOTHROTTLE_MAX_DELAY = 60# The average number of requests Scrapy should be sending in parallel to# each remote server# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# Enable showing throttling stats for every response received:# AUTOTHROTTLE_DEBUG = False# Enable and configure HTTP caching (disabled by default)# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings#启用和配置HTTP缓存（默认情况下禁用）# HTTPCACHE_ENABLED = True# HTTPCACHE_EXPIRATION_SECS = 0# HTTPCACHE_DIR = 'httpcache'# HTTPCACHE_IGNORE_HTTP_CODES = []# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'MYSQL_HOST = "127.0.0.1"MYSQL_DBNAME = "article_spider"MYSQL_USER = "root"MYSQL_PASSWORD = "123"SQL_DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S"SQL_DATE_FORMAT = "%Y-%m-%d"]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——云打码实现验证码识别]]></title>
    <url>%2F2017%2F06%2F12%2Fscrapy-yundama%2F</url>
    <content type="text"><![CDATA[验证码识别大致有如下几种方式： 编码实现（Tesseract-OCR） 在线打码 人工打码 这里我们简单介绍一下在线打码，选择的打码平台为：云打码 具体方式查看调用示例即可。 下面给出代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import jsonimport requestsclass YDMHttp(object): apiurl = 'http://api.yundama.com/api.php' username = '' password = '' appid = '' appkey = '' def __init__(self, username, password, appid, appkey): self.username = username self.password = password self.appid = str(appid) self.appkey = appkey def balance(self): data = &#123;'method': 'balance', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print("获取剩余积分", ret_data["balance"]) return ret_data["balance"] else: return None def login(self): data = &#123;'method': 'login', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print("登录成功", ret_data["uid"]) return ret_data["uid"] else: return None def decode(self, filename, codetype, timeout): data = &#123;'method': 'upload', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey, 'codetype': str(codetype), 'timeout': str(timeout)&#125; files = &#123;'file': open(filename, 'rb')&#125; response_data = requests.post(self.apiurl, files=files, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print("识别成功", ret_data["text"]) return ret_data["text"] else: return Nonedef ydm(file_path): username = 'da_ge_da1' # 密码 password = 'da_ge_da' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = 3129 # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '40d5ad41c047179fc797631e3b9c3025' # 图片文件 filename = 'image/captcha.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 yundama = YDMHttp(username, password, appid, appkey) if (username == 'username'): print('请设置好相关参数再测试') else: # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 return yundama.decode(file_path, codetype, timeout);if __name__ == "__main__": # 用户名 username = 'da_ge_da1' # 密码 password = 'da_ge_da' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = 3129 # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '40d5ad41c047179fc797631e3b9c3025' # 图片文件 filename = 'image/captcha.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 if (username == 'username'): print('请设置好相关参数再测试') else: # 初始化 yundama = YDMHttp(username, password, appid, appkey) # 登陆云打码 uid = yundama.login() print('uid: %s' % uid) # 登陆云打码 uid = yundama.login() print('uid: %s' % uid) # 查询余额 balance = yundama.balance() print('balance: %s' % balance) # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 text = yundama.decode(filename, codetype, timeout)]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy核心架构分析]]></title>
    <url>%2F2017%2F06%2F11%2Fscrapy-framework%2F</url>
    <content type="text"><![CDATA[概览首先看一下Scrapy的架构图： 核心组件Scrapy有以下几大组件： Scrapy Engine：核心引擎，负责控制和调度各个组件，保证数据流转； Scheduler：负责管理任务、过滤任务、输出任务的调度器，存储、去重任务都在此控制； Downloader：下载器，负责在网络上下载网页数据，输入待下载URL，输出下载结果； Spiders：用户自己编写的爬虫脚本，可自定义抓取意图； Item Pipeline：负责输出结构化数据，可自定义输出位置； 除此之外，还有两大中间件组件： Downloader middlewares：介于引擎和下载器之间，可以在网页在下载前、后进行逻辑处理； Spider middlewares：介于引擎和爬虫之间，可以在调用爬虫输入下载结果和输出请求/数据时进行逻辑处理 数据流转按照架构图的序号，数据流转大概是这样的： 引擎从自定义爬虫中获取初始化请求（也叫种子URL）； 引擎把该请求放入调度器中，同时引擎向调度器获取一个待下载的请求（这两部是异步执行的）； 调度器返回给引擎一个待下载的请求； 引擎发送请求给下载器，中间会经过一系列下载器中间件； 这个请求通过下载器下载完成后，生成一个响应对象，返回给引擎，这中间会再次经过一系列下载器中间件； 引擎接收到下载返回的响应对象后，然后发送给爬虫，执行自定义爬虫逻辑，中间会经过一系列爬虫中间件； 爬虫执行对应的回调方法，处理这个响应，完成用户逻辑后，会生成结果对象或新的请求对象给引擎，再次经过一系列爬虫中间件； 引擎把爬虫返回的结果对象交由结果处理器处理，把新的请求对象通过引擎再交给调度器； 从1开始重复执行，直到调度器中没有新的请求处理； 核心组件交互图 这里需要说明一下图中的Scrapyer，其实这也是在源码的一个核心类，但官方架构图中没有展示出来，这个类其实是处于Engine、Spiders、Pipeline之间，是连通这3个组件的桥梁。 核心类图涉及到的一些核心类如下： 其中没有样式的黑色文字是类的核心属性，黄色样式的文字都是核心方法。 可以看到，Scrapy的核心类，其实主要包含5大组件、4大中间件管理器、爬虫类和爬虫管理器、请求、响应对象和数据解析类这几大块。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Requests，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy随机更换User-Agent和实现IP代理池]]></title>
    <url>%2F2017%2F06%2F11%2Fscrapy-useragent-proxyip%2F</url>
    <content type="text"><![CDATA[之前有一节用于介绍Request和Response，还是官方文档介绍的比较详尽，所以就不做笔记了。 这一节用于介绍随机更换User-Agent和实现IP代理池的方法，首先来看一下现在网站中所做的反爬虫工作。 常见的反爬虫和应对方法一般网站从三个方面反爬虫：用户请求的Headers，用户行为，网站目录和数据加载方式。前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，这样增大了爬取的难度。 通过Headers反爬虫从用户请求的Headers反爬虫是最常见的反爬虫策略。很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）。如果遇到了这类反爬虫机制，可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中；或者将Referer值修改为目标网站域名。对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。 针对这样的反爬虫方法，我们在Scrapy中实现随机更换User-Agent就很有必要了。 基于用户行为反爬虫还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。 大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。这样的代理ip爬虫经常会用到，最好自己准备一个。有了大量代理ip后可以每请求几次更换一个ip，这在requests或者urllib2中很容易做到，这样就能很容易的绕过第一种反爬虫。 对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。 对于这样的反爬虫方法，我们可以在Scrapy中实现IP代理池，问题便迎刃而解。 动态页面的反爬虫上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。首先用Firebug或者HttpFox对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests或者urllib2模拟ajax请求，对响应的json进行分析得到需要的数据。 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。我这几天爬的那个网站就是这样，除了加密ajax参数，它还把一些基本的功能都封装了，全部都是在调用自己的接口，而接口参数都是加密的。遇到这样的网站，我们就不能用上面的方法了，我用的是selenium+phantomJS框架，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利用 selenium+phantomJS能干很多事情，例如识别点触式（12306）或者滑动式的验证码，对页面表单进行暴力破解等等。它在自动化渗透中还 会大展身手，以后还会提到这个。 随机更换User-Agent首先将下面的代码添加到settings.py文件，替换默认的user-agent处理模块： 1234DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,&#125; 在middlewares.py文件中自定义User-Agent处理模块： 123456789101112131415161718192021from fake_useragent import UserAgentclass RandomUserAgentMiddleware(object): """ 随机更换User-Agent """ def __init__(self, crawler): super(RandomUserAgentMiddleware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get('RANDOM_UA_TYPE', 'random') @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) 其中，fake_useragent这个第三方库维护了大量的User-Agent，我们就没必要自己去维护了，直接使用就好。 下图为其的安装和使用方法： 实现IP代理池我们用西刺免费代理IP网站来实现这个IP代理池。我们选择爬取这个网站，然后将获取的数据写入数据库以供我们自己使用。 调试之后，用CSS选择器来获取我们所需的内容，并将其保存到数据库中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import requestsfrom scrapy.selector import Selectorimport MySQLdbconn = MySQLdb.connect(host="127.0.0.1", user="root", passwd="123", db="article_spider", charset="utf8")cursor = conn.cursor()def crawl_ips(): """ 爬取西刺网的免费代理IP """ headers = &#123; 'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4" &#125; for i in range(2093): re = requests.get("http://www.xicidaili.com/nn/&#123;0&#125;".format(i), headers=headers) selector = Selector(text=re.text) all_trs = selector.css("#ip_list tr") ip_list = [] for tr in all_trs[1:]: speed_str = tr.css(".bar::attr(title)").extract()[0] if speed_str: speed = float(speed_str.split("秒")[0]) ip = tr.css("td:nth-child(2)::text").extract_first() port = tr.css("td:nth-child(3)::text").extract_first() proxy_type = tr.css("td:nth-child(6)::text").extract_first() ip_list.append((ip, port, proxy_type, speed)) for ip_info in ip_list: cursor.execute( "insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', '&#123;2&#125;', '&#123;3&#125;')".format( ip_info[0], ip_info[1], ip_info[3], ip_info[2] ) ) conn.commit()class GetIP(object): def delete_ip(self, ip): """ 从数据库中删除无效的IP """ delete_sql = """ DELETE FROM proxy_ip WHERE ip='&#123;0&#125;' """.format(ip) cursor.execute(delete_sql) conn.commit() return True def judge_ip(self, ip, port): """ 判断IP是否可用 """ http_url = "http://www.baidu.com" proxy_url = "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) try: proxy_dict = &#123; "http": proxy_url, &#125; response = requests.get(http_url, proxies=proxy_dict) except Exception as e: print("invalid ip and port") self.delete_ip(ip) return False else: code = response.status_code if code &gt;= 200 and code &lt; 300: print("effective ip") return True else: print("invalid ip and port") self.delete_ip(ip) return False def get_random_ip(self): """ 从数据库中随机获取一个可用的IP """ random_sql = """ SELECT ip, port FROM proxy_ip ORDER BY RAND() LIMIT 1 """ result = cursor.execute(random_sql) for ip_info in cursor.fetchall(): ip = ip_info[0] port = ip_info[1] judge_re = self.judge_ip(ip, port) if judge_re: return "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) else: return self.get_random_ip()if __name__ == '__main__': get_ip = GetIP() get_ip.get_random_ip() 最后，在middlewares.py文件中写入我们用于设置IP代理的逻辑即可： 1234567class RandomProxyMiddleware(object): """ 动态设置IP代理 """ def process_request(self, request, spider): get_ip = GetIP() request.meta["proxy"] = get_ip.get_random_ip() 可参考的第三方库 scrapy-crawlera scrapy-proxies]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取拉勾网]]></title>
    <url>%2F2017%2F06%2F09%2Fscrapy-crawlspider-lagou%2F</url>
    <content type="text"><![CDATA[之前的Spider都是默认根据basic的templates创建，现在我们要用crawl的方式创建Spider，以爬取拉勾网整站信息。 CrawlSpider源码解析Spider基本上能做很多事情了，但是如果你想爬取知乎或者是简书全站的话，你可能需要一个更强大的武器。CrawlSpider基于Spider，但是可以说是为全站爬取而生。 简要说明CrawlSpider是爬取那些具有一定规则网站的常用的爬虫，它基于Spider并有一些独特属性 rules: 是Rule对象的集合，用于匹配目标网站并排除干扰 parse_start_url: 用于爬取起始响应，必须要返回Item，Request中的一个。 因为rules是Rule对象的集合，所以这里也要介绍一下Rule。它有几个参数：link_extractor、callback=None、cb_kwargs=None、follow=None、process_links=None、process_request=None其中的link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。 allow_domains：会被提取的链接的domains。 deny_domains：一定不会被提取链接的domains。 restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。还有一个类似的restrict_css 下面是官方提供的例子，我将从源代码的角度开始解读一些常见问题： 12345678910111213141516171819202122232425import scrapyfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'), ) def parse_item(self, response): self.logger.info('Hi, this is an item page! %s', response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id="item_id"]/text()').re(r'ID: (\d+)') item['name'] = response.xpath('//td[@id="item_name"]/text()').extract() item['description'] = response.xpath('//td[@id="item_description"]/text()').extract() return item 问题：CrawlSpider如何工作的？因为CrawlSpider继承了Spider，所以具有Spider的所有函数。首先由 start_requests 对 start_urls 中的每一个url发起请求（ make_requests_from_url )，这个请求会被parse接收。在Spider里面的parse需要我们定义，但CrawlSpider定义 parse 去解析响应（ self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True) ）_parse_response根据有无 callback ， follow 和 self.follow_links 执行不同的操作 1234567891011def _parse_response(self, response, callback, cb_kwargs, follow=True):##如果传入了callback，使用这个callback解析页面并获取解析得到的reques或item if callback: cb_res = callback(response, **cb_kwargs) or () cb_res = self.process_results(response, cb_res) for requests_or_item in iterate_spider_output(cb_res): yield requests_or_item## 其次判断有无follow，用_requests_to_follow解析响应是否有符合要求的link。 if follow and self._follow_links: for request_or_item in self._requests_to_follow(response): yield request_or_item 其中_requests_to_follow又会获取link_extractor（这个是我们传入的LinkExtractor）解析页面得到的link（link_extractor.extract_links(response)）,对url进行加工（process_links，需要自定义），对符合的link发起Request。使用.process_request(需要自定义）处理响应。 问题：CrawlSpider如何获取rules？CrawlSpider类会在__init__方法中调用_compile_rules方法，然后在其中浅拷贝rules中的各个Rule获取要用于回调(callback)，要进行处理的链接（process_links）和要进行的处理请求（process_request) 123456789101112def _compile_rules(self): def get_method(method): if callable(method): return method elif isinstance(method, six.string_types): return getattr(self, method, None) self._rules = [copy.copy(r) for r in self.rules] for rule in self._rules: rule.callback = get_method(rule.callback) rule.process_links = get_method(rule.process_links) rule.process_request = get_method(rule.process_request) 那么Rule是怎么样定义的呢？ 123456789101112class Rule(object): def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity): self.link_extractor = link_extractor self.callback = callback self.cb_kwargs = cb_kwargs or &#123;&#125; self.process_links = process_links self.process_request = process_request if follow is None: self.follow = False if callback else True else: self.follow = follow 因此LinkExtractor会传给link_extractor。 有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？由上面的讲解可以发现_parse_response会处理有callback的（响应）respons。cb_res = callback(response, **cb_kwargs) or ()而_requests_to_follow会将self._response_downloaded传给callback用于对页面中匹配的url发起请求（request）。r = Request(url=link.url, callback=self._response_downloaded) 如何在CrawlSpider进行模拟登陆因为CrawlSpider和Spider一样，都要使用start_requests发起请求，用从Andrew_liu大神借鉴的代码说明如何模拟登陆： 123456789101112131415161718192021222324252627##替换原来的start_requests，callback为def start_requests(self): return [Request("http://www.zhihu.com/#signin", meta = &#123;'cookiejar' : 1&#125;, callback = self.post_login)]def post_login(self, response): print 'Preparing login' #下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单 xsrf = Selector(response).xpath('//input[@name="_xsrf"]/@value').extract()[0] print xsrf #FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单 #登陆成功后, 会调用after_login回调函数 return [FormRequest.from_response(response, #"http://www.zhihu.com/login", meta = &#123;'cookiejar' : response.meta['cookiejar']&#125;, headers = self.headers, formdata = &#123; '_xsrf': xsrf, 'email': '1527927373@qq.com', 'password': '321324jia' &#125;, callback = self.after_login, dont_filter = True )]#make_requests_from_url会调用parse，就可以与CrawlSpider的parse进行衔接了def after_login(self, response) : for url in self.start_urls : yield self.make_requests_from_url(url) 源码及注释123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899class CrawlSpider(Spider): rules = () def __init__(self, *a, **kw): super(CrawlSpider, self).__init__(*a, **kw) self._compile_rules() def parse(self, response): """ 首先调用parse()来处理start_urls中返回的response对象 parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url() 设置了跟进标志位True parse将返回item和跟进了的Request对象 """ return self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=True) def parse_start_url(self, response): """ 处理start_url中返回的response，需要重写 """ return [] def process_results(self, response, results): return results def _requests_to_follow(self, response): """ 从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回 """ if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): # 抽取之内的所有链接，只要通过任意一个'规则'，即表示合法 links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen] if links and rule.process_links: # 使用用户指定的process_links处理每个连接 links = rule.process_links(links) for link in links: # 将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded() seen.add(link) # 构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数 r = Request(url=link.url, callback=self._response_downloaded) r.meta.update(rule=n, link_text=link.text) # 对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request. yield rule.process_request(r) def _response_downloaded(self, response): """ 处理通过rule提取出的连接，并返回item以及request """ rule = self._rules[response.meta['rule']] return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow) def _parse_response(self, response, callback, cb_kwargs, follow=True): """ 解析response对象，会用callback解析处理他，并返回request或Item对象 首先判断是否设置了回调函数。（该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数） 如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象， 然后再交给process_results处理。返回cb_res的一个列表 """ if callback: # 如果是parse调用的，则会解析成Request对象 # 如果是rule callback，则会解析成Item cb_res = callback(response, **cb_kwargs) or () cb_res = self.process_results(response, cb_res) for requests_or_item in iterate_spider_output(cb_res): yield requests_or_item if follow and self._follow_links: #如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象 for request_or_item in self._requests_to_follow(response): yield request_or_item def _compile_rules(self): def get_method(method): if callable(method): return method elif isinstance(method, six.string_types): return getattr(self, method, None) self._rules = [copy.copy(r) for r in self.rules] for rule in self._rules: rule.callback = get_method(rule.callback) rule.process_links = get_method(rule.process_links) rule.process_request = get_method(rule.process_request) @classmethod def from_crawler(cls, crawler, *args, **kwargs): spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs) spider._follow_links = crawler.settings.getbool( 'CRAWLSPIDER_FOLLOW_LINKS', True) return spider def set_crawler(self, crawler): super(CrawlSpider, self).set_crawler(crawler) self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True) 创建CrawlSpider 出现问题：ImportError: No module named &#39;utils&#39; 原因：我们之前将项目目录下的 ArticleSpider 文件夹Mark为 Sources Root 导致。 解决办法：自己在 Settings.py 中设置搜索路径。 123456# 设置搜索路径import osimport sysBASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))sys.path.insert(0, os.path.join(BASE_DIR, 'ArticleSpider')) 设置之后重新创建spider 1$ scrapy genspider -t crawl lagou www.lagou.com 数据表结构及items设计 完整代码逻辑items.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def remove_splash(value): """ 去除工作城市的斜杠 """ return value.replace("/", "")def handle_jobaddr(value): addr_list = value.split("\n") addr_list = [item.strip() for item in addr_list if item.strip() != "查看地图"] return "".join(addr_list)class LagouJobItemLoader(ItemLoader): default_output_processor = TakeFirst()class LagouJobItem(scrapy.Item): """ 拉勾网职位信息 """ title = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() salary = scrapy.Field() job_city = scrapy.Field( input_processor=MapCompose(remove_splash), ) work_years = scrapy.Field( input_processor=MapCompose(remove_splash), ) degree_need = scrapy.Field( input_processor=MapCompose(remove_splash), ) job_type = scrapy.Field() publish_time = scrapy.Field() job_advantage = scrapy.Field() job_desc = scrapy.Field() job_addr = scrapy.Field( input_processor=MapCompose(remove_tags, handle_jobaddr), ) company_name = scrapy.Field() company_url = scrapy.Field() tags = scrapy.Field( input_processor=Join(",") ) crawl_time = scrapy.Field() def get_insert_sql(self): insert_sql = """ insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need, job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url, tags, crawl_time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc) """ params = ( self["title"], self["url"], self["url_object_id"], self["salary"], self["job_city"], self["work_years"], self["degree_need"], self["job_type"], self["publish_time"], self["job_advantage"], self["job_desc"], self["job_addr"], self["company_name"], self["company_url"], self["tags"], self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params lagou.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom datetime import datetimefrom items import LagouJobItemLoader, LagouJobItemfrom utils.common import get_md5class LagouSpider(CrawlSpider): name = 'lagou' allowed_domains = ['www.lagou.com'] start_urls = ['https://www.lagou.com/'] rules = ( Rule(LinkExtractor(allow=('zhaopin/.*',)), ), Rule(LinkExtractor(allow=('gongsi/j\d+.html',)), ), Rule(LinkExtractor(allow=r'jobs/\d+.html'), callback='parse_job', follow=True), ) def parse_job(self, response): """ 解析拉勾网的职位 """ item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response) item_loader.add_css("title", ".job-name::attr(title)") item_loader.add_value("url", response.url) item_loader.add_value("url_object_id", get_md5(response.url)) item_loader.add_css("salary", ".job_request .salary::text") item_loader.add_xpath("job_city", "//*[@class='job_request']/p/span[2]/text()") item_loader.add_xpath("work_years", "//*[@class='job_request']/p/span[3]/text()") item_loader.add_xpath("degree_need", "//*[@class='job_request']/p/span[4]/text()") item_loader.add_xpath("job_type", "//*[@class='job_request']/p/span[5]/text()") item_loader.add_css("publish_time", ".publish_time::text") item_loader.add_css("job_advantage", ".job-advantage p::text") item_loader.add_css("job_desc", ".job_bt div") item_loader.add_css("job_addr", ".work_addr") item_loader.add_css("company_name", "#job_company dt a img::attr(alt)") item_loader.add_css("company_url", "#job_company dt a::attr(href)") item_loader.add_css("tags", ".position-label li::text") item_loader.add_value("crawl_time", datetime.now()) job_item = item_loader.load_item() return job_item]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Requests，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取知乎]]></title>
    <url>%2F2017%2F06%2F07%2Fscrapy-zhihu%2F</url>
    <content type="text"><![CDATA[在完成了Scrapy模拟登录知乎后，下一步要进行的就是进行对知乎页面，问题以及答案等内容的爬取工作了。 通过Scrapy Shell进行调试在使用shell调试时，直接通过 scrapy shell https://www.zhihu.com/question/58765535 会出现error 500错误。这是因为没有加headers的原因。正确添加headers的方法是：scrapy -s USER_AGENT=&quot;任意的User Agent&quot; 。此时，就可以在shell中进行分析了。 获得要分析的链接在登录完成进入首页之后，通过深度优先算法获得首页需要的链接，然后打开这些链接再次获得里面的链接，不断重复，获得所有内容。 1234567from urllib import parse def parse(self, response): # 因为没有具体的入口，采用深度优先的算法 all_urls = response.css("a::attr(href)").extract() all_urls = [parse.urljoin(response.url, url) for url in all_urls] for url in all_urls: pass 在分析页面内容之后，设计我们所需的数据表zhihu_question和zhihu_answer 完整代码 zhihu.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179# -*- coding: utf-8 -*-import scrapyimport reimport jsonimport datetimefrom scrapy.loader import ItemLoaderfrom items import ZhihuAnswerItem, ZhihuQuestionItemtry: import urlparse as parseexcept: from urllib import parseclass ZhihuSpider(scrapy.Spider): name = "zhihu" allowed_domains = ["www.zhihu.com"] start_urls = ['https://www.zhihu.com/'] # question的第一页answer的请求url start_answer_url = "https://www.zhihu.com/api/v4/questions/&#123;0&#125;/answers?sort_by=default&amp;include=data%5B%2A%5D.is_normal%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccollapsed_counts%2Creviewing_comments_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Crelationship.is_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.is_blocking%2Cis_blocked%2Cis_followed%2Cvoteup_count%2Cmessage_thread_token%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=&#123;1&#125;&amp;offset=&#123;2&#125;" headers = &#123; "Host": "www.zhihu.com", "Referer": "https://www.zhihu.com/", 'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4" &#125; def parse(self, response): """ 提取出html页面中的所有url 并跟踪这些url进行一步爬取 如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数 """ all_urls = response.css("a::attr(href)").extract() all_urls = [parse.urljoin(response.url, url) for url in all_urls] all_urls = filter(lambda x: True if x.startswith("https") else False, all_urls) for url in all_urls: match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", url) if match_obj: # 如果提取到question相关的页面则下载后交由提取函数进行提取 request_url = match_obj.group(1) yield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) else: # 如果不是question页面则直接进一步跟踪 yield scrapy.Request(url, headers=self.headers, callback=self.parse) def parse_question(self, response): """ 处理question页面， 从页面中提取出具体的question item """ # 处理question页面， 从页面中提取出具体的question item if "QuestionHeader-title" in response.text: # 处理新版本 match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) item_loader.add_css("title", "h1.QuestionHeader-title::text") item_loader.add_css("content", ".QuestionHeader-detail") item_loader.add_value("url", response.url) item_loader.add_value("zhihu_id", question_id) item_loader.add_css("answer_num", ".List-headerText span::text") item_loader.add_css("comments_num", ".QuestionHeader-Comment button::text") item_loader.add_css("watch_user_num", ".NumberBoard-value::text") item_loader.add_css("topics", ".QuestionHeader-topics .Popover div::text") question_item = item_loader.load_item() else: # 处理老版本页面的item提取 match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) # item_loader.add_css("title", ".zh-question-title h2 a::text") item_loader.add_xpath("title", "//*[@id='zh-question-title']/h2/a/text()|//*[@id='zh-question-title']/h2/span/text()") item_loader.add_css("content", "#zh-question-detail") item_loader.add_value("url", response.url) item_loader.add_value("zhihu_id", question_id) item_loader.add_css("answer_num", "#zh-question-answer-num::text") item_loader.add_css("comments_num", "#zh-question-meta-wrap a[name='addcomment']::text") # item_loader.add_css("watch_user_num", "#zh-question-side-header-wrap::text") item_loader.add_xpath("watch_user_num", "//*[@id='zh-question-side-header-wrap']/text()|//*[@class='zh-question-followers-sidebar']/div/a/strong/text()") item_loader.add_css("topics", ".zm-tag-editor-labels a::text") question_item = item_loader.load_item() yield scrapy.Request(self.start_answer_url.format(question_id, 20, 0), headers=self.headers, callback=self.parse_answer) yield question_item def parse_answer(self, reponse): """ 处理question的answer """ ans_json = json.loads(reponse.text) is_end = ans_json["paging"]["is_end"] next_url = ans_json["paging"]["next"] # 提取answer的具体字段 for answer in ans_json["data"]: answer_item = ZhihuAnswerItem() answer_item["zhihu_id"] = answer["id"] answer_item["url"] = answer["url"] answer_item["question_id"] = answer["question"]["id"] answer_item["author_id"] = answer["author"]["id"] if "id" in answer["author"] else None answer_item["content"] = answer["content"] if "content" in answer else None answer_item["praise_num"] = answer["voteup_count"] answer_item["comments_num"] = answer["comment_count"] answer_item["create_time"] = answer["created_time"] answer_item["update_time"] = answer["updated_time"] answer_item["crawl_time"] = datetime.datetime.now() yield answer_item if not is_end: yield scrapy.Request(next_url, headers=self.headers, callback=self.parse_answer) def start_requests(self): return [scrapy.Request('https://www.zhihu.com/#signin', headers=self.headers, callback=self.login)] def login(self, response): response_text = response.text match_obj = re.match('.*name="_xsrf" value="(.*?)"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = (match_obj.group(1)) if xsrf: post_url = "https://www.zhihu.com/login/phone_num" post_data = &#123; "_xsrf": xsrf, "phone_num": "18251556927", "password": "lawtech0301520", "captcha": "" &#125; import time t = str(int(time.time() * 1000)) captcha_url = "https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login".format(t) yield scrapy.Request(captcha_url, headers=self.headers, meta=&#123;"post_data": post_data&#125;, callback=self.login_after_captcha) def login_after_captcha(self, response): with open("captcha.jpg", "wb") as f: f.write(response.body) f.close() from PIL import Image try: im = Image.open('captcha.jpg') im.show() im.close() except: pass captcha = input("输入验证码\n&gt;") post_data = response.meta.get("post_data", &#123;&#125;) post_url = "https://www.zhihu.com/login/phone_num" post_data["captcha"] = captcha return [scrapy.FormRequest( url=post_url, formdata=post_data, headers=self.headers, callback=self.check_login )] def check_login(self, response): """ 验证服务器的返回数据判断是否成功 """ text_json = json.loads(response.text) if "msg" in text_json and text_json["msg"] == "登录成功": for url in self.start_urls: yield scrapy.Request(url, dont_filter=True, headers=self.headers) 为了用同一个Pipeline处理所有的数据库存储操作，因此将操作都放入items中，再有Pipeline进行统一处理。 items.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class ZhihuQuestionItem(scrapy.Item): """ 知乎问题Item """ zhihu_id = scrapy.Field() topics = scrapy.Field() url = scrapy.Field() title = scrapy.Field() content = scrapy.Field() answer_num = scrapy.Field() comments_num = scrapy.Field() watch_user_num = scrapy.Field() click_num = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): # 插入知乎question表的sql语句 insert_sql = """ insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) """ zhihu_id = self["zhihu_id"][0] topics = ",".join(self["topics"]) url = self["url"][0] title = "".join(self["title"]) content = "".join(self["content"]) answer_num = extract_num("".join(self["answer_num"])) comments_num = extract_num("".join(self["comments_num"])) if len(self["watch_user_num"]) == 2: watch_user_num = int(self["watch_user_num"][0]) click_num = int(self["watch_user_num"][1]) else: watch_user_num = int(self["watch_user_num"][0]) click_num = 0 crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT) params = (zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time) return insert_sql, paramsclass ZhihuAnswerItem(scrapy.Item): """ 知乎回答Item """ zhihu_id = scrapy.Field() url = scrapy.Field() question_id = scrapy.Field() author_id = scrapy.Field() content = scrapy.Field() praise_num = scrapy.Field() comments_num = scrapy.Field() create_time = scrapy.Field() update_time = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): # 插入知乎question表的sql语句 insert_sql = """ insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, praise_num, comments_num, create_time, update_time, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), praise_num=VALUES(praise_num), update_time=VALUES(update_time) """ create_time = datetime.datetime.fromtimestamp(self['create_time']).strftime(SQL_DATETIME_FORMAT) update_time = datetime.datetime.fromtimestamp(self['update_time']).strftime(SQL_DATETIME_FORMAT) params = ( self["zhihu_id"], self["url"], self["question_id"], self["author_id"], self["content"], self["praise_num"], self["comments_num"], create_time, update_time, self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params Pipelines.py 12345678910111213141516171819202122232425262728293031323334class MysqlTwistedPipeline(object): # 采用异步的机制写入mysql def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparms = dict( host=settings["MYSQL_HOST"], db=settings["MYSQL_DBNAME"], user=settings["MYSQL_USER"], passwd=settings["MYSQL_PASSWORD"], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool("MySQLdb", **dbparms) return cls(dbpool) def process_item(self, item, spider): # 使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) # 处理异常 def handle_error(self, failure, item, spider): # 处理异步插入的异常 print(failure) def do_insert(self, cursor, item): # 执行具体的插入 # 根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params)]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Requests，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy模拟登录知乎]]></title>
    <url>%2F2017%2F05%2F11%2Fscrapy-login-zhihu%2F</url>
    <content type="text"><![CDATA[Scrapy登录知乎要解决两个问题 session的传递，保证处理登录是同一个状态。 首个登录页面的改变，由直接爬取的页面变为登录页面，再去爬取页面。 话不多说，直接上代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# -*- coding: utf-8 -*-import scrapyimport reimport jsonimport timefrom PIL import Imageclass ZhihuSpider(scrapy.Spider): name = "zhihu" allowed_domains = ["www.zhihu.com"] start_urls = ['http://www.zhihu.com/'] headers = &#123; "Host": "www.zhihu.com", "Referer": "https://www.zhihu.com/", 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36' &#125; def parse(self, response): pass def start_requests(self): return [scrapy.Request("https://www.zhihu.com/#signin", headers=self.headers, callback=self.login)] def login(self, response): """ 登录 :param response: :return: """ response_text = response.text match_obj = re.match('.*name="_xsrf" value="(.*?)"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = match_obj.group(1) if xsrf: post_data = &#123; "_xsrf": xsrf, "phone_num": "18951855817", "password": "tracy584563542" &#125; t = str(int(time.time() * 1000)) captcha_url = 'https://www.zhihu.com/captcha.gif?r=' + t + "&amp;type=login" yield scrapy.Request(captcha_url, headers=self.headers, meta=&#123;"post_data": post_data&#125;, callback=self.login_after_captcha) def login_after_captcha(self, response): with open("captcha.jpg", "wb") as f: f.write(response.body) im = Image.open('captcha.jpg') im.show() im.close() captcha = input("请输入验证码：\n") post_data = response.meta.get("post_data", &#123;&#125;) post_data["captcha"] = captcha post_url = "https://www.zhihu.com/login/phone_num" return [scrapy.FormRequest( url=post_url, formdata=post_data, headers=self.headers, callback=self.check_login )] def check_login(self, response): """ 验证服务器的返回数据判断登录是否成功 :param response: :return: """ text_json = json.loads(response.text) if 'msg' in text_json and text_json['msg'] == '登陆成功': # 从继承的Spider类中拿的内容，恢复到正确执行 for url in self.start_urls: yield scrapy.Request(url, dont_filter=True, headers=self.headers) 首先对 scrapy.Spider 类中的 start_requests(self) 进行重载，改变首先要处理的页面为登录页面。得到登录页面后，获得xsrf，并下载验证码，通过 scrapy.FormRequest构造登录数据，通过check_login回调函数判断登录是否成功。在代码的最后一行转回正常的登录流程。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Requests模拟登录知乎]]></title>
    <url>%2F2017%2F05%2F11%2Fscrapy-requests-zhihu-login%2F</url>
    <content type="text"><![CDATA[Requests 是以 PEP 20 的箴言为中心开发的 Beautiful is better than ugly.(美丽优于丑陋) Explicit is better than implicit.(直白优于含蓄) Simple is better than complex.(简单优于复杂) Complex is better than complicated.(复杂优于繁琐) Readability counts.(可读性很重要) 常见状态码 表达式 说明 200 请求被正确执行 301/302 永久性重定向/临时性重定向 403 没有权限访问 404 没有资源访问 500 服务器错误 503 服务器停机或正在维护 登录分析在登录界面，输入手机号和密码，返回的地址为 Request URL:https://www.zhihu.com/login/phone_num当输入email地址后返回的地址为 Request URL:https://www.zhihu.com/login/email并且在formdata中出现 _xsrf:a71f46d549979fa192c09e11e4a463b5 这样的字符串。 抓取xsrf的值正则匹配抓取xsrf需要使用header头来进行源代码的获取： 1234567891011def get_xsrf(): """ 获取xsrf code :return: xsrf code """ response = requests.get("https://www.zhihu.com", headers=headers) match_obj = re.match('.*name="_xsrf" value="(.*?)"', response.text) if match_obj: print(match_obj.group(1)) else: return "" 验证码获取123456789101112def get_captcha(): t = str(int(time.time() * 1000)) captcha_url = 'https://www.zhihu.com/captcha.gif?r=' + t + "&amp;type=login" r = session.get(captcha_url, headers=headers) with open('captcha.jpg', 'wb') as f: f.write(r.content) f.close() im = Image.open('captcha.jpg') im.show() im.close() captcha = input("请输入验证码：\n") return captcha 登录逻辑1234567891011121314151617181920212223242526272829303132333435363738394041def zhihu_login(account, password): """ 知乎登录 :param account: :param password: :return: """ if re.match("^1\d&#123;10&#125;$", account): print("手机号码登录 \n") post_url = "https://www.zhihu.com/login/phone_num" post_data = &#123; "_xsrf": get_xsrf(), "phone_num": account, "password": password &#125; else: if "@" in account: print("邮箱登录 \n") else: print("你的账号输入有问题，请重新登录") return 0 post_url = 'https://www.zhihu.com/login/email' post_data = &#123; '_xsrf': get_xsrf(), 'password': password, 'email': account &#125; # 不需要验证码直接登录成功 login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() if login_code['r'] == 1: # 不输入验证码登录失败 # 使用需要输入验证码的方式登录 post_data["captcha"] = get_captcha() login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() print(login_code['msg']) # 保存 cookies 到文件， # 下次可以使用 cookie 直接登录，不需要输入账号和密码 session.cookies.save() 以上代码是通过引入requests库，使用它的session方法，进行连接，构造post_data，把自己的用户名密码等信息发送到网站，并通过正则判断发送的是邮箱或是手机进行登录。引入import http.cookiejar as cookielib，通过session.cookies.save()，对cookie进行保存。 通过Cookie登录12345678910111213141516171819# 使用登录cookie信息session = requests.session()session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")try: session.cookies.load(ignore_discard=True)except: print("Cookie未能加载") def is_login(): """ 通过查看用户个人信息来判断是否已经登录 :return: """ url = "https://www.zhihu.com/settings/profile" response = session.get(url, headers=headers, allow_redirects=False) if response.status_code == 200: return True else: return False 登录只能一次，如果再次登录，可以直接通过查看cookie来判断是否为登录状态。 首先把cookie通过session.cookies.load装载进来，执行is_login()函数，如果成功可以访问inbox_url页面，则状态码为200表示成功。这里一定要注意allow_redirects=False，当不允许且登录时会自动跳转到登录页面，则状态码是301或者302。 完整代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124# _*_ coding: utf-8 _*_"""__author__ = 'lawtech'__date__ = '2017/5/9 下午3:18'"""import reimport requestsimport timefrom PIL import Imagetry: import cookielibexcept: import http.cookiejar as cookielib# 构造requests headersagent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36'headers = &#123; "Host": "www.zhihu.com", "Referer": "https://www.zhihu.com/", 'User-Agent': agent&#125;# 使用登录cookie信息session = requests.session()session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")try: session.cookies.load(ignore_discard=True)except: print("Cookie未能加载")def get_xsrf(): """ 获取xsrf code :return: xsrf code """ response = requests.get("https://www.zhihu.com", headers=headers) match_obj = re.match('.*name="_xsrf" value="(.*?)"', response.text) if match_obj: print(match_obj.group(1)) else: return ""def get_captcha(): t = str(int(time.time() * 1000)) captcha_url = 'https://www.zhihu.com/captcha.gif?r=' + t + "&amp;type=login" r = session.get(captcha_url, headers=headers) with open('captcha.jpg', 'wb') as f: f.write(r.content) f.close() im = Image.open('captcha.jpg') im.show() im.close() captcha = input("请输入验证码：\n") return captchadef zhihu_login(account, password): """ 知乎登录 :param account: :param password: :return: """ if re.match("^1\d&#123;10&#125;$", account): print("手机号码登录 \n") post_url = "https://www.zhihu.com/login/phone_num" post_data = &#123; "_xsrf": get_xsrf(), "phone_num": account, "password": password &#125; else: if "@" in account: print("邮箱登录 \n") else: print("你的账号输入有问题，请重新登录") return 0 post_url = 'https://www.zhihu.com/login/email' post_data = &#123; '_xsrf': get_xsrf(), 'password': password, 'email': account &#125; # 不需要验证码直接登录成功 login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() if login_code['r'] == 1: # 不输入验证码登录失败 # 使用需要输入验证码的方式登录 post_data["captcha"] = get_captcha() login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() print(login_code['msg']) # 保存 cookies 到文件， # 下次可以使用 cookie 直接登录，不需要输入账号和密码 session.cookies.save()def is_login(): """ 通过查看用户个人信息来判断是否已经登录 :return: """ url = "https://www.zhihu.com/settings/profile" response = session.get(url, headers=headers, allow_redirects=False) if response.status_code == 200: return True else: return Falseif __name__ == '__main__': if is_login(): print("您已经登录！") else: account = input("请输入用户名：\n") password = input("请输入密码：\n") zhihu_login(account, password)]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Requests，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——理解Session和Cookie机制]]></title>
    <url>%2F2017%2F05%2F09%2Fscrapy-session-cookie%2F</url>
    <content type="text"><![CDATA[Cookie 机制Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。 具体来说cookie机制采用的是在客户端保持状态的方案。它是在用户端的会话状态的存贮机制，他需要用户打开客户端的cookie支持。cookie的作用就是为了解决HTTP协议无状态的缺陷所作的努力。 正统的cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示浏览器按照指示生成相应的cookie。然而纯粹的客户端脚本如JavaScript也可以生成cookie。而cookie的使用是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的cookie，如果某个cookie所声明的作用范围大于等于将要请求的资源所在的位置，则把该cookie附在请求资源的HTTP请求头上发送给服务器。 cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。若不设置过期时间，则表示这个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式。 而session机制采用的是一种在服务器端保持状态的解决方案。同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的。而session提供了方便管理全局变量的方式 。 session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，这个值也可能设置为由get来返回给服务器。 就安全性来说：当你访问一个使用session 的站点，同时在自己机子上建立一个cookie，建立在服务器端的session机制更安全些，因为它不会任意读取客户存储的信息。 Session 机制session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。 当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个session标识（称为session id），如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来使用（检索不到，会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个session id将被在本次响应中返回给客户端保存。 保存这个session id的方式可以采用cookie，这样在交互过程中浏览器可以自动的按照规则把这个标识发挥给服务器。一般这个cookie的名字都是类似于SEEESIONID。但cookie可以被人为的禁止，则必须有其他机制以便在cookie被禁止时仍然能够把session id传递回服务器。 经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路径的后面。还有一种技术叫做表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器。 Cookie与Session都能够进行会话跟踪，但是完成的原理不太一样。普通状况下二者均能够满足需求，但有时分不能够运用Cookie，有时分不能够运用Session。 两者比较存取方式不同Cookie中只能保管ASCII字符串，假如需求存取Unicode字符或者二进制数据，需求先进行编码。Cookie中也不能直接存取Java对象。若要存储略微复杂的信息，运用Cookie是比拟艰难的。 而Session中能够存取任何类型的数据，包括而不限于String、Integer、List、Map等。Session中也能够直接保管Java Bean乃至任何Java类，对象等，运用起来十分便当。能够把Session看做是一个Java容器类。 隐私策略不同Cookie存储在客户端阅读器中，对客户端是可见的，客户端的一些程序可能会窥探、复制以至修正Cookie中的内容。而Session存储在服务器上，对客户端是透明的，不存在敏感信息泄露的风险。 假如选用Cookie，比较好的方法是，敏感的信息如账号密码等尽量不要写到Cookie中。最好是像Google、Baidu那样将Cookie信息加密，提交到服务器后再进行解密，保证Cookie中的信息只要本人能读得懂。而假如选择Session就省事多了，反正是放在服务器上，Session里任何隐私都能够有效的保护。 服务器压力不同Session是保管在服务器端的，每个用户都会产生一个Session。假如并发访问的用户十分多，会产生十分多的Session，耗费大量的内存。因而像Google、Baidu、Sina这样并发访问量极高的网站，是不太可能运用Session来追踪客户会话的。 而Cookie保管在客户端，不占用服务器资源。假如并发阅读的用户十分多，Cookie是很好的选择。关于Google、Baidu、Sina来说，Cookie或许是唯一的选择。 浏览器支持不同Cookie是需要客户端浏览器支持的。假如客户端禁用了Cookie，或者不支持Cookie，则会话跟踪会失效。关于WAP上的应用，常规的Cookie就派不上用场了。 假如客户端浏览器不支持Cookie，需要运用Session以及URL地址重写。需要注意的是一切的用到Session程序的URL都要进行URL地址重写，否则Session会话跟踪还会失效。关于WAP应用来说，Session+URL地址重写或许是它唯一的选择。 假如客户端支持Cookie，则Cookie既能够设为本浏览器窗口以及子窗口内有效（把过期时间设为–1），也能够设为一切阅读器窗口内有效（把过期时间设为某个大于0的整数）。但Session只能在本阅读器窗口以及其子窗口内有效。假如两个浏览器窗口互不相干，它们将运用两个不同的Session。（IE8下不同窗口Session相干） 跨域支持不同Cookie支持跨域名访问，例如将domain属性设置为“.biaodianfu.com”，则以“.biaodianfu.com”为后缀的一切域名均能够访问该Cookie。跨域名Cookie如今被普遍用在网络中，例如Google、Baidu、Sina等。而Session则不会支持跨域名访问。Session仅在他所在的域名内有效。 综述仅运用Cookie或者仅运用Session可能完成不了理想的效果。这时应该尝试一下同时运用Cookie与Session。Cookie与Session的搭配运用在实践项目中会完成很多意想不到的效果。 Python Django 中实现两种机制Cookie 设置以下是Cookie设置的详细流程： 客户端发起一个请求连接（如HTTP GET） 服务器在http响应头上加上Set-Cookie，里面存放字符串的键值对 客户端随后的http请求头加上Cookie首部，它包含了之前服务器响应中设置cookie的信息。 根据这个Cookie首部的信息，服务器便能“记住”当前用户的信息。 下面就来看看Python中如何设置Cookie： 12345678910111213141516171819202122from BaseHTTPServer import HTTPServerfrom SimpleHTTPServer import SimpleHTTPRequestHandlerimport Cookieclass MyRequestHandler(SimpleHTTPRequestHandler): def do_GET(self): content = "&lt;html&gt;&lt;body&gt;Path is: %s&lt;/body&gt;&lt;/html&gt;" % self.path self.send_response(200) self.send_header('Content-type', 'text/html') self.send_header('Content-length', str(len(content))) cookie = Cookie.SimpleCookie() cookie['id'] = 'some_value_42' self.wfile.write(cookie.output()) self.wfile.write('\r\n') self.end_headers() self.wfile.write(content)server = HTTPServer(('', 59900), MyRequestHandler)server.serve_forever() 查看服务器端的http响应头，会发现以下字段： 1Set-Cookie: id=some_value_42 在Django中，可以用如下的方式获取或设置Cookie： 12345678def test_cookie(request): if 'id' in request.COOKIES: cookie_id = request.COOKIES['id'] return HttpResponse('Got cookie with id=%s' % cookie_id) else: resp = HttpResponse('No id cookie! Sending cookie to client') resp.set_cookie('id', 'some_value_99') return resp Django通过一系列的包装使得封装Cookie的操作变得更加简单，那么它在其中是怎么实现cookie的读取的呢，下面来窥探原理： 1234def _get_cookies(self): if not hasattr(self, '_cookies'): self._cookies = http.parse_cookie(self.environ.get('HTTP_COOKIE', '')) return self._cookies 可以看出，获取cookie的操作用了Lazy initialization（延迟加载）的技术，因为如果客户端不需要用到cookie，这个过程只会浪费不必要的操作。 再来看parse_cookie的实现： 12345678910111213141516def parse_cookie(cookie): if cookie == '': return &#123;&#125; if not isinstance(cookie, Cookie.BaseCookie): try: c = SimpleCookie() c.load(cookie, ignore_parse_errors=True) except Cookie.CookieError: # 无效cookie return &#123;&#125; else: c = cookie cookiedict = &#123;&#125; for key in c.keys(): cookiedict[key] = c.get(key).value return cookiedict 它负责解析Cookie并把结果集成到一个dict（字典）对象中，并返回字典。而设置cookie的操作则会被WSGIHandler执行。 注：Django的底层实现了WSGI的接口（如WSGIRequest，WSGIServer等）。 Session 应用下面看一个简单的session应用例子： 1234567def test_count_session(request): if 'count' in request.session: request.session['count'] += 1 return HttpResponse('new count=%s' % request.session['count']) else: request.session['count'] = 1 return HttpResponse('No count in session. Setting to 1') 它用session实现了一个计数器，当每一个请求到来时，就为计数器加一，把新的结果更新到session中。 查看http的响应头，会得到类似下面的信息。 1234Set-Cookie:sessionid=a92d67e44a9b92d7dafca67e507985c0; expires=Thu, 07-Jul-2011 04:16:28 GMT; Max-Age=1209600; Path=/ 里面包含了session_id以及过期时间等信息。 那么服务器端是如何保存session的呢？ 在django中，默认会把session保存在setting指定的数据库中，除此之外，也可以通过指定session engine，使session保存在文件(file)，内存(cache)中。 如果保存在数据库中，django会在数据库中创建一个如下的session表。 12345CREATE TABLE &quot;django_session&quot; ( &quot;session_key&quot; varchar(40) NOT NULL PRIMARY KEY, &quot;session_data&quot; text NOT NULL, &quot;expire_date&quot; datetime NOT NULL); session_key是放置在cookie中的id，它是唯一的，而session_data则存放序列化后的session数据字符串。 通过session_key可以在数据库中取得这条session的信息： 12345from django.contrib.sessions.models import Session#...sess = Session.objects.get(pk='a92d67e44a9b92d7dafca67e507985c0')print(sess.session_data)print(sess.get_decoded()) 输出： 12ZmEyNDVhNTBhMTk2ZmRjNzVlYzQ4NTFjZDk2Y2UwODc3YmVjNWVjZjqAAn1xAVUFY291bnRxAksGcy4=&#123;'count': 6&#125; 回看第一个例子，我们是通过request.session来获取session的，为什么请求对象会附带一个session对象呢，这其中做了什么呢？ 这就引出了下面要说的django里的中间件技术 Session middleware。 关于中间件，&lt;&lt;the Django Book&gt;&gt;是这样解释的： Django的中间件框架，是django处理请求和响应的一套钩子函数的集合。 我们看传统的django视图模式一般是这样的：http请求-&gt;view-&gt;http响应，而加入中间件框架后，则变为：http请求-&gt;中间件处理-&gt;app-&gt;中间件处理-&gt;http响应。而在django中这两个处理分别对应process_request和process_response函数，这两个钩子函数将会在特定的时候被触发。 直接看SessionMiddleware可能更清晰一些： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class SessionMiddleware(object): def __init__(self): engine = import_module(settings.SESSION_ENGINE) self.SessionStore = engine.SessionStore def process_request(self, request): session_key = request.COOKIES.get(settings.SESSION_COOKIE_NAME) request.session = self.SessionStore(session_key) def process_response(self, request, response): """ If request.session was modified, or if the configuration is to save the session every time, save the changes and set a session cookie or delete the session cookie if the session has been emptied. """ try: accessed = request.session.accessed modified = request.session.modified empty = request.session.is_empty() except AttributeError: pass else: # First check if we need to delete this cookie. # The session should be deleted only if the session is entirely empty if settings.SESSION_COOKIE_NAME in request.COOKIES and empty: response.delete_cookie(settings.SESSION_COOKIE_NAME, domain=settings.SESSION_COOKIE_DOMAIN) else: if accessed: patch_vary_headers(response, ('Cookie',)) if (modified or settings.SESSION_SAVE_EVERY_REQUEST) and not empty: if request.session.get_expire_at_browser_close(): max_age = None expires = None else: max_age = request.session.get_expiry_age() expires_time = time.time() + max_age expires = cookie_date(expires_time) # Save the session data and refresh the client cookie. # Skip session save for 500 responses, refs #3881. if response.status_code != 500: try: request.session.save() except UpdateError: # The user is now logged out; redirecting to same # page will result in a redirect to the login page # if required. return redirect(request.path) response.set_cookie(settings.SESSION_COOKIE_NAME, request.session.session_key, max_age=max_age, expires=expires, domain=settings.SESSION_COOKIE_DOMAIN, path=settings.SESSION_COOKIE_PATH, secure=settings.SESSION_COOKIE_SECURE or None, httponly=settings.SESSION_COOKIE_HTTPONLY or None) return response 在请求到来后，SessionMiddleware的process_request在请求取出session_key，并把一个新的session对象赋给request.session，而在返回响应时，process_response则判断session是否被修改或过期，来更新session的信息。 Django 用户认证中的 Session在django中，用下面的方法来验证用户是否登录是常见的事情。 123456def test_user(request): user_str = str(request.user) if request.user.is_authenticated(): return HttpResponse('%s is logged in' % user_str) else: return HttpResponse('%s is not logged in' % user_str) 其实request.user的实现也借助到了session。 在这个例子中，成功登录后，session表会保存类似下面的信息，里面记录了用户的id，以后进行验证时，便会到这个表中获取用户的信息。 1&#123;'_auth_user_id': 1, '_auth_user_backend': 'django.contrib.auth.backends.ModelBackend'&#125; 跟上面提到的Session中间件相似，用户验证也有一个中间件：AuthenticationMiddleware，在process_request中，通过request.class.user = LazyUser()在request设置了一个全局的可缓存的用户对象。 1234567891011class LazyUser(object): def __get__(self, request, obj_type=None): if not hasattr(request, '_cached_user'): from django.contrib.auth import get_user request._cached_user = get_user(request) return request._cached_userclass AuthenticationMiddleware(object): def process_request(self, request): request.__class__.user = LazyUser() return None 在get_user里，会在检查session中是否存放了当前用户对应的user_id，如果有，则通过id在model查找相应的用户返回，否则返回一个匿名的用户对象(AnonymousUser)。 12345678910def get_user(request): from django.contrib.auth.models import AnonymousUser try: user_id = request.session[SESSION_KEY] backend_path = request.session[BACKEND_SESSION_KEY] backend = load_backend(backend_path) user = backend.get_user(user_id) or AnonymousUser() except KeyError: user = AnonymousUser() return user Django中的Session实现Django使用的Session默认都继承于SessionBase类里，这个类实现了一些session操作方法，以及hash，decode，encode等方法。 123456789101112class SessionBase(object): """ Base class for all Session classes. """ TEST_COOKIE_NAME = 'testcookie' TEST_COOKIE_VALUE = 'worked' def __init__(self, session_key=None): self._session_key = session_key self.accessed = False self.modified = False self.serializer = import_string(settings.SESSION_SERIALIZER) 说的更直白一些，其实django中的session就是一个模拟dict的对象，并实现了一系列的hash和序列化方法，默认持久化在数据库中（有时候也可能由于为了提高性能，用redis之类的内存数据库来缓存session）。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Django，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Loaders机制介绍与实例]]></title>
    <url>%2F2017%2F05%2F08%2Fscrapy-item-loader%2F</url>
    <content type="text"><![CDATA[Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。 Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。 介绍使用 Item Loaders 来填充 Items要使用 Item Loader, 你必须先将它实例化。你可以使用类似字典的对象(例如: Item or dict)来进行实例化，或者不使用对象也可以，当不用对象进行实例化的时候，Item 会自动使用 ItemLoader.default\_item_class 属性中指定的 Item 类在 Item Loader constructor 中实例化。 然后，你开始收集数值到 Item Loader 时，通常使用 Selectors。你可以在同一个 item field 里面添加多个数 值；Item Loader 将知道如何用合适的处理函数来“添加”这些数值。 下面是在 Spider 中典型的 Item Loader 的用法，使用 Items chapter 中声明的 Product item ： 1234567891011from scrapy.contrib.loader import ItemLoader from myproject.items import Productdef parse(self, response): l = ItemLoader(item=Product(), response=response) l.add_xpath('name', '//div[@class="product_name"]') l.add_xpath('name', '//div[@class="product_title"]') l.add_xpath('price', '//p[@id="price"]') l.add_css('stock', 'p#stock]') l.add_value('last_updated', 'today') # you can also use literal values return l.load_item() 快速查看这些代码之后，我们可以看到发现 name 字段被从页面中两个不同的 XPath 位置提取： //div[@class=&quot;product_name&quot;] //div[@class=&quot;product_title&quot;] 换言之,数据通过用 add_xpath() 的方法，把从两个不同的 XPath 位置提取的数据收集起来。这是将在以后分配给 name 字段中的数据? 之后，类似的请求被用于 price 和 stock 字段 （后者使用 CSS selector 和 add_css() 方法）， 最后使用不同的方法 add_value() 对 last_update 填充文本值( today )。 最终，当所有数据被收集起来之后，调用 ItemLoader.load_item() 方法，实际上填充并且返回了之前通过调用 add_xpath()，add_css() ，add_value() 所提取和收集到的数据的 Item。 输入和输出处理器Item Loader 在每个（Item）字段中都包含了一个输入处理器和一个输出处理器。输入处理器收到数据时立刻提取数据 （通过 add_xpath()， add_css() 或者 add_value()方法）之后输入处理器的结果被收集起来并且保存在ItemLoader内。收集到所有的数据后，调用 ItemLoader.load_item() 方法来填充，并得到填充后的 Item 对象。这是当输出处理器被和之前收集到的数据（和用输入处理器处理的）被调用。输出处理器的结果是被分配到Item的最终值。 让我们看一个例子来说明如何输入和输出处理器被一个特定的字段调用（同样适用于其他field）： 123456l = ItemLoader(Product(), some_selector)l.add_xpath('name', xpath1) # (1)l.add_xpath('name', xpath2) # (2)l.add_css('name', css) # (3)l.add_value('name', 'test') # (4)return l.load_item() # (5) 发生了这些事情: 从 xpath1 提取出的数据,传递给 输入处理器 的 name 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡ 从 xpath2 提取出来的数据,传递给(1)中使用的相同的 输入处理器 .输入处理器的结果被附加到在(1)中收集的数据(如果有的话) ｡ 和之前相似，只不过这里的数据是通过 css CSS selector抽取，之后传输到在(1)和(2)使用 的input processor 中。最终输入处理器的结果被附加到在(1)和(2)中收集的数据之后 (如果存在数据的话)。 这里的处理方式也和之前相似，但是此处的值是通过add_value直接赋予的， 而不是利用XPath表达式或CSS selector获取。得到的值仍然是被传送到输入处理器。 在这里例程中，因为得到的值并非可迭代，所以在传输到输入处理器之前需要将其 转化为可迭代的单个元素，这才是它所接受的形式。 在之前步骤中所收集到的数据被传送到 output processor 的 name field中。 输出处理器的结果就是赋到item中 name field的值。 理解： 就是在使用Item Loader 时候，会有一个输入处理器，一个输出处理器，首先是收集好同一个字段的结果，传入到输入处理器当中，然后收集完后，会传递给输出处理器进行处理。输出处理器的处理结果，就是填充到item的结果。 需要注意的是：输入处理器的返回值会是内部收集的，然后被传递给输出处理器，来填充fields。 Scrapy 内部的处理器Scrapy内部，已经有一些设置好的内置处理器 Identity这是最简单的一个处理器，实际上就是什么都不做，传入多少个字段，就存储多少个字段，以list形式。&gt;&gt;&gt; from scrapy.contrib.loader.processor import Identity \&gt;&gt;&gt; proc = Identity() \&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;] TakeFirst从接受到的list中返回第一个非null/非空的值，&gt;&gt;&gt; from scrapy.contrib.loader.processor import TakeFirst \&gt;&gt;&gt; proc = TakeFirst() \&gt;&gt;&gt; proc([&#39;&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) &#39;one&#39; Join返回用分隔符（separator）作为间隔的连接形成的字符串。若不传入separator，则默认使用’ ‘（空格）。&gt;&gt;&gt; from scrapy.contrib.loader.processor import Join \&gt;&gt;&gt; proc = Join() \&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) u&#39;one two three&#39; \&gt;&gt;&gt; proc = Join(&#39;&lt;br&gt;&#39;) \&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) u&#39;one&lt;br&gt;two&lt;br&gt;three&#39; 另外还有Compose以及MapCompose，这里不一一详述。 声明 Item Loaders声明ItemLoaders 和声明Item类似，使用Class语法，例子： 12345678910111213from scrapy.contrib.loader import ItemLoaderfrom scrapy.contrib.loader.processor import TakeFirst, MapCompose, Joinclass ProductLoader(ItemLoader): default_output_processor = TakeFirst() name_in = MapCompose(unicode.title) name_out = Join() price_in = MapCompose(unicode.strip) # ... 上述代码中: 输出处理器，被声明为 _in 前缀，而输出处理器被声明为 _out 前缀。 设置默认处理器 ItemLoader.default_input_processor and ItemLoader.default_output_processor 声明输入、输出处理器输入、输出可以被如上方那样被声明，这也是最正常的方式。另外，我们也可以在另外的一个地方去声明输入和输出处理器：在item Field，元数据中。 1234567891011121314151617import scrapyfrom scrapy.contrib.loader.processor import Join, MapCompose, TakeFirstfrom w3lib.html import remove_tagsdef filter_price(value): if value.isdigit(): return valueclass Product(scrapy.Item): name = scrapy.Field( input_processor=MapCompose(remove_tags), output_processor=Join(), ) price = scrapy.Field( input_processor=MapCompose(remove_tags, filter_price), output_processor=TakeFirst(), ) 输出和输出处理器的优先级如下： Item Loader field 指定的field_in 和 field_out（最优先） Field 元数据中(input_processor 和 output_processor key) item loader 默认。 ItemLoader.default_input_processor() andItemLoader.default_output_processor() (least precedence) 最后，给出Item Loader的官方说明API： ItemLoader objects 实例通过 Item loader 加载 Item首先在 jobbole.py 中引入 from scrapy.loader import ItemLoader 代码如下： 1234567891011121314item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)item_loader.add_css("title", ".entry-header h1::text")item_loader.add_value("url", response.url)item_loader.add_value("url_object_id", get_md5(response.url))item_loader.add_css("create_date", "p.entry-meta-hide-on-mobile::text")front_image_url = response.meta.get("front_image_url", "") # 文章封面图item_loader.add_value("front_image_url", [front_image_url])item_loader.add_css("praise_nums", ".vote-post-up h10::text")item_loader.add_css("comment_nums", "a[href='#article-comment'] span::text")item_loader.add_css("fav_nums", ".bookmark-btn::text")item_loader.add_css("tags", "p.entry-meta-hide-on-mobile a::text")item_loader.add_css("content", "div.entry")article_item = item_loader.load_item() 其中第一行中 JobBoleArticleItem() 为在 items.py 中声明的实例，response 为返回的响应。这属于固定写法。add_css()中第一个值为 items.py 中定义的值，第二个值为css选择器规则，类似的方法还有 add_xpath()，根据场景进行选择。 同理，add_value()为添加确定值的方法。这里通过值传递附给 front_image_url 再通过add_value的方法，加入到最终的item中。 最后通过调用 load_item() 方法对结果进行解析，所有的结果都是一个list并保存到 article_item 中。 断点调试结果如图： 发现获取到的所有值都是一个list，这样很不方便，但使得代码可读性更高，可维护性更强。 通过 items.py 处理数据在 items.py 中引入 from scrapy.loader.processors import MapCompose ，然后可以在定义 scrapy.Field() 时可以加入处理函数（可以使匿名函数），例如： 在 MapCompose() 中可以加入多个函数，在 jobbole.py 中断点调试结果如图： 在title的结果后面出现了我们想要的后缀。 另外，可以看到，结果都是 list，我们每次都需要提取第一个值。Scrapy给我们提供了 TakeFirst 方法。 同样引入 from scrapy.loader.processors import MapCompose,TakeFirst ，修改代码如下： 1234title = scrapy.Field( input_processor = MapCompose(lambda x:x+"-jobbole", add_jobbole), output_processor = TakeFirst() ) 即可以得到第一个值。由于每一个结果都是取第一个值，每个值全部调用这个方法重复代码过多，可以通过自定义Item loader重载的方法解决。引入 from scrapy.loader import ItemLoader ，这个类提供了以下方法： 123456class ItemLoader(object): default_item_class = Item default_input_processor = Identity() default_output_processor = Identity() default_selector_class = Selector 我们自定义的Item loader需要继承这个类： 12class ArticleItemLoader(ItemLoader): default_output_processor = TakeFirst() 然后在 jobbole.py 文件中，把 item_loader = ItemLoader(item=JobBoleArticleItem(), response=response) 中的 ItemLoader 变为 ArticleItemLoader，即： 1item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response) 这样得到的结果就是一个str而不是list了。 不过在上图是可以看到，它的tags也取了第一个值，但实际上它的值是三个，不满足我们的需要。引入Join方法 from scrapy.loader.processors import MapCompose, TakeFirst, Join，同时不使用自定义的item loader即可。 123tags = scrapy.Field( output_processor=Join(','), ) 和前面一样，有时候tags会有 评论 的不符合要求的tags，还需要自定义函数把相应的字段去掉。 12345def remove_comment(value): if '评论' in value: return '' else: return value 在处理图片时，使用pipelines需要传递的是一个列表，这里经过处理后，变成了str。可以通过一个默认函数不让默认的TakeFirst处理即可。 12def return_value(value): return value 调用方法是： 123front_image_url = scrapy.Field( output_processor=MapCompose(return_value), ) 最后，我们在之前还用正则表达式来清洗点赞数，收藏数，评论数这些数据，在item loader中我们也可以用函数处理： 123456789101112131415161718def get_nums(value): match_re = re.match(".*?(\d+).*?", value) if match_re: nums = int(match_re.group(1)) else: nums = 0 return numspraise_nums = scrapy.Field( input_processor=MapCompose(get_nums), ) comment_nums = scrapy.Field( input_processor=MapCompose(get_nums), ) fav_nums = scrapy.Field( input_processor=MapCompose(get_nums), ) 调试结果中str就变成int类型了：]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——通过Pipeline保存数据到MySQL]]></title>
    <url>%2F2017%2F05%2F07%2Fscrapy-item-mysql%2F</url>
    <content type="text"><![CDATA[将数据保存到MySQL数据库，需要用到 mysqlclient 模块，需要在我们的虚拟环境中用 pip 进行安装。 设计数据表需要根据之前Item来设计我们的数据表 jobbole_article ，数据库取名为 article_spider。 123456789101112class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field() front_image_path = scrapy.Field() praise_nums = scrapy.Field() comment_nums = scrapy.Field() fav_nums = scrapy.Field() tags = scrapy.Field() content = scrapy.Field() 初步设计的数据表如下，在后面使用时还会进行必要的改动： 采用同步机制写入MySQL首先在 pipelines.py 中引入数据库连接模块 import MySQLdb ，然后完善 MysqlPipeline 类的代码： 1234567891011121314class MysqlPipeline(object): # 采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('127.0.0.1', 'root', '12', 'article_spider', charset='utf8', use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ self.cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) self.conn.commit() __init__ 方法是对数据进行初始化，定义连接信息如host，数据库用户名、密码、数据库名称、数据库编码在 process_item 方法中进行插入数据操作，格式都是固定的。 最后在 settings.py 中把 MysqlPipeline() 加入到 ITEM_PIPELINES 的配置中。 采用异步机制写入MySQL在上面的同步机制写入数据库中，我们把连接信息 MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;12&#39;, &#39;article_spider&#39;, charset=&#39;utf8&#39;, use_unicode=True) 直接定义在函数中，如果不经常改动的话，可以把相关信息放到 settings.py 中进行调用。 1234MYSQL_HOST = '127.0.0.1'MYSQL_DBNAME = 'article_spider'MYSQL_USER = 'root'MYSQL_PASSWORD = '12' 在 pipelines.py 中新建 MysqlTwistedPipeline ，写入如下代码： 12345class MysqlTwistedPipeline(object): @classmethod def from_settings(cls, settings): host = settings['MYSQL_HOST'] pass 在 from_settings 这个类方法中，我们获取到了settings配置中的 MYSQL_HOST ，这个方法在Scrapy初始化的时候就会被调用，会将Scrapy的settings对象传递进来，我们在这里进行断点调试，查看是否获取到了这个对象： 发现在settings的attributes这个字典中，确实有我们定义的各种属性。 我们的异步操作需要引入twisted，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940from twisted.enterprise import adbapiimport MySQLdbimport MySQLdb.cursorsclass MysqlTwistedPipeline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparams = dict( host=settings['MYSQL_HOST'], db=settings['MYSQL_DBNAME'], user=settings['MYSQL_USER'], passwd=settings['MYSQL_PASSWORD'], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool("MySQLdb", **dbparams) return cls(dbpool) def process_item(self, item, spider): # 使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error) # 处理异常 def handle_error(self, failure): # 处理异步插入异常 print(failure) def do_insert(self, cursor, item): # 执行具体的插入 insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) 我们在使用时，绝大部分代码无须变动，只要修改 do_insert 方法中的插入内容，以及自己的信息即可。 在数据量不大时，用同步插入即可。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——保存item到json文件]]></title>
    <url>%2F2017%2F05%2F07%2Fscrapy-item-json%2F</url>
    <content type="text"><![CDATA[在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。 使用系统 exporter 导出为 JSON 文件12345678910111213141516from scrapy.exporters import JsonItemExporterclass JsonExporterPipeline(object): # 调用Scrapy提供的json exporter导出json文件 def __init__(self): self.file = open('article.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item 自定义 Pipeline 导出为 JSON 文件123456789101112class JsonWithEncodingPipeline(object): # 自定义json文件的导出 def __init__(self): self.file = codecs.open('article.json', 'w', encoding="utf-8") def process_item(self, item, spider): lines = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(lines) return item def spider_closed(self, spider): self.file.close() 函数说明codecs ：避免打开文件时出现编码错误。json.dumps ：dict转成strjson.loads ：str转成dictensure_ascii=False ：避免处理英文以外语言时出错return item ：交给下一个pipeline处理]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Pipeline]]></title>
    <url>%2F2017%2F05%2F06%2Fscrapy-item-pipeline%2F</url>
    <content type="text"><![CDATA[当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。 每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。 以下是 item pipeline 的一些典型应用： 清理 HTML 数据 验证爬取的数据（检查 item 包含某些字段） 查重（并丢弃） 将爬取结果保存到数据库中 编写自定义的 Pipeline定义一个Python类，然后实现方法 process_item(self, item, spider) 即可，返回一个字典或Item，或者抛出 DropItem 异常丢弃这个Item。 除此之外，还可以实现以下几个方法： open_spider(self, spider) ：当spider被开启时，这个方法被调用 close_spider(self, spider) ：当spider被关闭时，这个方法被调用 from_crawler(cls, crawler) ： 可访问核心组件比如配置和信号，并注册钩子函数到Scrapy中 Item Pipeline示例价格验证让我们来看一下以下这个假设的 pipeline，它为那些不含税（price_excludes_vat 属性）的item调整了price属性，同时丢弃了那些没有价格item： 12345678910111213from scrapy.exceptions import DropItemclass PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item['price']: if item['price_excludes_vat']: item['price'] = item['price'] * self.vat_factor return item else: raise DropItem("Missing price in %s" % item) 将item写入Json文件下面的这个Pipeline将所有的item写入到一个单独的json文件，，每行包含一个序列化 为 JSON 格式的 item: 1234567891011import jsonclass JsonWriterPipeline(object): def __init__(self): self.file = open('items.jl', 'wb') def process_item(self, item, spider): line = json.dumps(dict(item)) + "\n" self.file.write(line) return item JsonWriterPipeline 的目的只是为了介绍怎样编写 item pipeline，如果你想要将所有爬取的 item 都保存到同 一个 JSON 文件， 你需要使用 Feed exports 。 将item存储到MongoDB中这个例子使用pymongo来演示怎样讲item保存到MongoDB中。 MongoDB的地址和数据库名在配置 settings.py 中指定，这个例子主要是向你展示怎样使用from_crawler()方法，以及如何清理资源。 123456789101112131415161718192021222324252627import pymongoclass MongoPipeline(object): collection_name = 'scrapy_items' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'items') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert(dict(item)) return item 去重一个用于去重的过滤器，丢弃那些已经被处理过的 item。让我们假设我们的 item 有一个唯一的 id，但是我们 sp ider 返回的多个 item 中包含有相同的 id: 12345678910111213from scrapy.exceptions import DropItemclass DuplicatesPipeline(object): def __init__(self): self.ids_seen = set() def process_item(self, item, spider): if item['id'] in self.ids_seen: raise DropItem("Duplicate item found: %s" % item) else: self.ids_seen.add(item['id']) return item 启用一个 Item Pipeline 组件为了启用一个 Item Pipeline 组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子: 1234ITEM_PIPELINES = &#123; 'myproject.pipelines.PricePipeline': 300, 'myproject.pipelines.JsonWriterPipeline': 800, &#125; 分配给每个类的整型值，确定了他们运行的顺序，item 按数字从低到高的顺序，通过 pipeline，通常将这些数字 定义在 0-1000 范围内。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Feed Exports]]></title>
    <url>%2F2017%2F05%2F06%2Fscrapy-feed-exports%2F</url>
    <content type="text"><![CDATA[实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。 Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。 序列化方式（serialization format）feed 输出使用到了 Item exporters 。其自带支持的类型有: JSON JSON lines CSV XML 也可以通过 FEED_EXPORTERS 设置扩展支持的属性。 在 exporters.py 中可以看到所有的 Item exporters： 下表对主要的 Item exporters进行简要的介绍： 类型 FEED_FORMAT 使用的 exporter JSON json JsonItemExporter JSON lines jsonlines JsonLinesItemExporter CSV csv CsvItemExporter XML xml XmlItemExporter Pickle pickle PickleItemExporter Marshal marshal MarshalItemExporter 存储（Storages）使用 feed 输出时您可以通过使用 URI（通过 FEED_URI 设置）来定义存储端。feed 输出支持 URI 方式支持的多种存储后端类型。 自带支持的存储后端有： 本地文件系统 FTP S3（需要 boto） 标准输出 有些存储后端会因所需的外部库未安装而不可用。例如，S3 只有在 boto 库安装的情况下才可使用。 存储 URI 参数存储 URI 也包含参数。当 feed 被创建时这些参数可以被覆盖： %(time)s - 当 feed 被创建时被 timestamp 覆盖 %(name)s - 被 spider 的名字覆盖 其他命名的参数会被 spider 同名的属性所覆盖。例如， 当 feed 被创建时，%(site_id)s 将会被 spider.site_id 属性所覆盖。 下面用一些例子来说明: 存储在 FTP，每个 spider 一个目录: ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json 存储在 S3，每一个 spider 一个目录: s3://mybucket/scraping/feeds/%(name)s/%(time)s.json 存储后端（Storage backends）本地文件系统将 feed 存储在本地系统。 URI scheme: file URI 样例: file:///tmp/export.csv 需要的外部依赖库: none 注意: (只有)存储在本地文件系统时，您可以指定一个绝对路径 /tmp/export.csv 并忽略协议(scheme)。不过这 仅仅只能在 Unix 系统中工作。 FTP将 feed 存储在 FTP 服务器。 URI scheme: ftp URI 样例: ftp://user:pass@ftp.example.com/path/to/export.csv 需要的外部依赖库: none S3将 feed 存储在 Amazon S3 。 URI scheme: s3 URI 样例: s3://mybucket/path/to/export.csv s3://aws_key:aws_secret@mybucket/path/to/export.csv 需要的外部依赖库: boto 您可以通过在 URI 中传递 user/pass 来完成 AWS 认证，或者也可以通过下列的设置来完成: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY 标准输出feed 输出到 Scrapy 进程的标准输出。 URI scheme: stdout URI 样例: stdout 需要的外部依赖库: none 设定（Settings）这些是配置 feed 输出的设定: FEED_URI (必须) FEED_FORMAT FEED_STORAGES FEED_EXPORTERS FEED_STORE_EMPTY FEED_URIDefault: None 输出 feed 的 URI。支持的 URI 协议请参见存储后端。 为了启用 feed 输出，该设定是必须的。 FEED_FORMAT输出 feed 的序列化格式。可用的值请参见序列化方式（Serialization formats）。 FEED_STORE_EMPTYDefault: False 是否输出空 feed（没有 item 的 feed）。 FEED_STORAGESDefault: {} 包含项目支持的额外 feed 存储端的字典。 字典的键（key）是 URI 协议（scheme），值是存储类（storage class）的路径。 FEED_STORAGES_BASEDefault: 1234567&#123;'': 'scrapy.contrib.feedexport.FileFeedStorage', 'file': 'scrapy.contrib.feedexport.FileFeedStorage', 'stdout': 'scrapy.contrib.feedexport.StdoutFeedStorage', 's3': 'scrapy.contrib.feedexport.S3FeedStorage', 'ftp': 'scrapy.contrib.feedexport.FTPFeedStorage',&#125; 包含 Scrapy 内置支持的 feed 存储端的字典。 FEED_EXPORTERSDefault: {} 包含项目支持的额外输出器（exporter）的字典。 该字典的键（key）是 URI 协议（scheme），值是 Item 输出器（exp orter）类的路径。 FEED_EXPORTERS_BASEDefault: 1234567FEED_EXPORTERS_BASE = &#123; 'json': 'scrapy.contrib.exporter.JsonItemExporter', 'jsonlines': 'scrapy.contrib.exporter.JsonLinesItemExporter', 'csv': 'scrapy.contrib.exporter.CsvItemExporter', 'xml': 'scrapy.contrib.exporter.XmlItemExporter', 'marshal': 'scrapy.contrib.exporter.MarshalItemExporter',&#125; 包含 Scrapy 内置支持的 feed 输出器（exporter）的字典。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Selectors]]></title>
    <url>%2F2017%2F05%2F05%2Fscrapy-selectors%2F</url>
    <content type="text"><![CDATA[当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： BeautifulSoup 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 lxml 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。 Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。 XPath 是一门用来在 XML 文件中选择节点的语言，也可以用在 HTML 上。 CSS 是一门将 HTML 文档样式化的语言。选择器由它定义，并与特定的 HTML 元素的样式相关连。 Scrapy 选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。 本文解释了选择器如何工作，并描述了相应的 API。不同于 lxml API 的臃肿，该 API 短小而简洁。这是因为 lxml 库除了用来选择标记化文档外，还可以用到许多任务上。 使用选择器构造选择器Scrapy selectors是 Selector 类的实例，通过传入 text 或 TextResponse 来创建，它自动根据传入的类型选择解析规则（XML or HTML）： 12&gt;&gt;&gt; from scrapy.selector import Selector &gt;&gt;&gt; from scrapy.http import HtmlResponse 以文字构造（都以 xpath 和 css 两种方法解析字段内容，加深理解）： 12345&gt;&gt;&gt; body = '&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;'&gt;&gt;&gt; Selector(text=body).xpath("//span/text()").extract()['good']&gt;&gt;&gt; Selector(text=body).css("html body span::text").extract()['good'] 以 response 构造： 12345&gt;&gt;&gt; response = HtmlResponse(url='http://example.com', body=body, encoding='utf-8')&gt;&gt;&gt; Selector(response=response).xpath('//span/text()').extract()['good']&gt;&gt;&gt; Selector(response=response).css("html body span::text").extract()['good'] response 对象以 .selector 属性提供了一个 selector ， 可以随时使用该快捷方法: 1234&gt;&gt;&gt; response.selector.xpath('//span/text()').extract()['good']&gt;&gt;&gt; response.selector.css("html body span::text").extract()['good'] 使用选择器我们将使用 Scrapy shell （提供交互测试）和位于 Scrapy 文档服务器的一个样例页面，来解释如何使用选择器： http://doc.scrapy.org/en/latest/_static/selectors-sample1.html 该页面源码如下： 123456789101112131415&lt;html&gt; &lt;head&gt; &lt;base href='http://example.com/' /&gt; &lt;title&gt;Example website&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id='images'&gt; &lt;a href='image1.html'&gt;Name: My image 1 &lt;br /&gt;&lt;img src='image1_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image2.html'&gt;Name: My image 2 &lt;br /&gt;&lt;img src='image2_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image3.html'&gt;Name: My image 3 &lt;br /&gt;&lt;img src='image3_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image4.html'&gt;Name: My image 4 &lt;br /&gt;&lt;img src='image4_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image5.html'&gt;Name: My image 5 &lt;br /&gt;&lt;img src='image5_thumb.jpg' /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 首先，打开 scrapy shell， 1scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html 当 shell 载入后，您将获得名为 response 的 shell 变量，其为响应的 response，并且在其 response.selector 属性上绑定了一个 selector。 因为我们处理的是 HTML，选择器将自动使用 HTML 语法分析。 那么，通过查看该页面的源码，我们构建一个 XPath 来选择 title 标签内的文字: 12&gt;&gt;&gt; response.selector.xpath("//title/text()")&gt;&gt;&gt; [&lt;Selector xpath='//title/text()' data='Example website'&gt;] 由于在 response 中使用 XPath、CSS 查询十分普遍，因此，Scrapy 提供了两个实用的快捷方式：response.xpath() 及 response.css() ： 12345&gt;&gt;&gt; response.xpath("//title/text()")&gt;&gt;&gt; [&lt;Selector xpath='//title/text()' data='Example website'&gt;]&gt;&gt;&gt; response.css("title::text")&gt;&gt;&gt; [&lt;Selector xpath='descendant-or-self::title/text()' data='Example website'&gt;] 现在我们将得到根 URL（base URL）和一些图片链接: 1234567891011121314151617&gt;&gt;&gt; response.xpath('//base/@href').extract() ['http://example.com/']&gt;&gt;&gt; response.css('base::attr(href)').extract() ['http://example.com/']&gt;&gt;&gt; response.xpath('//a[contains(@href, "image")]/@href').extract() ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']&gt;&gt;&gt; response.css('a[href*=image]::attr(href)').extract() ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']&gt;&gt;&gt; response.xpath('//a[contains(@href, "image")]/img/@src').extract() ['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg']&gt;&gt;&gt; response.css('a[href*=image] img::attr(src)').extract() ['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg'] 嵌套选择器选择器方法（ .xpath() or .css() ）返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子： 123456789101112131415161718&gt;&gt;&gt; links = response.xpath("//a[contains(@href,'image')]")&gt;&gt;&gt; links.extract()['&lt;a href="image1.html"&gt;Name: My image 1 &lt;br&gt;&lt;img src="image1_thumb.jpg"&gt;&lt;/a&gt;', '&lt;a href="image2.html"&gt;Name: My image 2 &lt;br&gt;&lt;img src="image2_thumb.jpg"&gt;&lt;/a&gt;', '&lt;a href="image3.html"&gt;Name: My image 3 &lt;br&gt;&lt;img src="image3_thumb.jpg"&gt;&lt;/a&gt;', '&lt;a href="image4.html"&gt;Name: My image 4 &lt;br&gt;&lt;img src="image4_thumb.jpg"&gt;&lt;/a&gt;', '&lt;a href="image5.html"&gt;Name: My image 5 &lt;br&gt;&lt;img src="image5_thumb.jpg"&gt;&lt;/a&gt;']&gt;&gt;&gt; for index, link in enumerate(links):...: args = (index, link.xpath('@href').extract(), link.xpath('img/@src').extract())...: print('Link number %d points to url %s and image %s' % args)...:Link number 0 points to url ['image1.html'] and image ['image1_thumb.jpg']Link number 1 points to url ['image2.html'] and image ['image2_thumb.jpg']Link number 2 points to url ['image3.html'] and image ['image3_thumb.jpg']Link number 3 points to url ['image4.html'] and image ['image4_thumb.jpg']Link number 4 points to url ['image5.html'] and image ['image5_thumb.jpg'] 结合正则表达式使用选择器Selector 也有一个 .re() 方法，用来通过正则表达式来提取数据。然而，不同于使用 .xpath() 或者 .css() 方法，.re()方法返回 unicode 字符串的列表。所以你无法构造嵌套式的 .re() 调用。 下面是一个例子，从上面的 html 源码中提取图像名字： 12&gt;&gt;&gt; response.xpath("//a[contains(@href, 'image')]/text()").re(r'Name:\s*(.*)')['My image 1 ', 'My image 2 ', 'My image 3 ', 'My image 4 ', 'My image 5 '] 使用相对 XPaths记住如果你使用嵌套的选择器，并使用起始为 / 的 XPath，那么该 XPath 将对文档使用绝对路径，而且对于你调用的 Selector 不是相对路径。 比如，假设你想提取在 &lt;div&gt; 元素中的所有 &lt;p&gt; 元素。首先，你将先得到所有的 &lt;div&gt; 元素： 1&gt;&gt;&gt; divs = response.xpath("//div") 开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 &lt;div&gt; 元素内部提取所有的 &lt;p&gt; 元素： 12&gt;&gt;&gt; for p in divs.xpath('//p'): # this is wrong - gets all &lt;p&gt; from the whole document ... print p.extract() 下面是比较合适的处理方法(注意 .//p XPath 的点前缀)： 12&gt;&gt;&gt; for p in divs.xpath('.//p'): # extracts all &lt;p&gt; inside ... print p.extract() 另一种常见的情况将是提取所有直系 &lt;p&gt; 的结果： 12&gt;&gt;&gt; for p in divs.xpath('p'): ... print p.extract() 使用 EXSLT 扩展因建于 lxml 之上，Scrapy 选择器也支持一些 EXSLT 扩展，可以在 XPath 表达式中使用这些预先制定的命名空间： 前缀 命名空间 用途 re http://exslt.org/regular-expressions 正则表达式 set http://exslt.org/sets 集合操作 正则表达式例如在XPath的 starts-with() 或 contains() 无法满足需求时， test() 函数可以非常有用。 例如在列表中选择有”class”元素且结尾为一个数字的链接： 1234567891011121314151617&gt;&gt;&gt; from scrapy import Selector&gt;&gt;&gt; doc = """... &lt;div&gt;... &lt;ul&gt;... &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;... &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;... &lt;/ul&gt;... &lt;/div&gt;... """&gt;&gt;&gt; sel = Selector(text=doc, type="html")&gt;&gt;&gt; sel.xpath("//li//@href").extract()['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']&gt;&gt;&gt; sel.xpath("//li[re:test(@class, 'item-\d$')]//@href").extract()['link1.html', 'link2.html', 'link4.html', 'link5.html'] 注意：C语言库 libxslt 不原生支持EXSLT正则表达式，因此 lxml 在实现时使用了Python re 模块的钩子。 因此，在 XPath 表达式中使用 regexp 函数可能会牺牲少量的性能。 集合操作集合操作可以方便地用于在提取文字元素前从文档树中去除一些部分。 例如使用 itemscopes 组和对应的 itemprops 来提取微数据（来自 http://schema.org/Product 的样本内容）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&gt;&gt;&gt; doc = """... &lt;div itemscope itemtype="http://schema.org/Product"&gt;... &lt;span itemprop="name"&gt;Kenmore White 17" Microwave&lt;/span&gt;... ![](kenmore-microwave-17in.jpg)... &lt;div itemprop="aggregateRating"... itemscope itemtype="http://schema.org/AggregateRating"&gt;... Rated &lt;span itemprop="ratingValue"&gt;3.5&lt;/span&gt;/5... based on &lt;span itemprop="reviewCount"&gt;11&lt;/span&gt; customer reviews... &lt;/div&gt;...... &lt;div itemprop="offers" itemscope itemtype="http://schema.org/Offer"&gt;... &lt;span itemprop="price"&gt;$55.00&lt;/span&gt;... &lt;link itemprop="availability" href="http://schema.org/InStock" /&gt;In stock... &lt;/div&gt;...... Product description:... &lt;span itemprop="description"&gt;0.7 cubic feet countertop microwave.... Has six preset cooking categories and convenience features like... Add-A-Minute and Child Lock.&lt;/span&gt;...... Customer reviews:...... &lt;div itemprop="review" itemscope itemtype="http://schema.org/Review"&gt;... &lt;span itemprop="name"&gt;Not a happy camper&lt;/span&gt; -... by &lt;span itemprop="author"&gt;Ellie&lt;/span&gt;,... &lt;meta itemprop="datePublished" content="2011-04-01"&gt;April 1, 2011... &lt;div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating"&gt;... &lt;meta itemprop="worstRating" content = "1"&gt;... &lt;span itemprop="ratingValue"&gt;1&lt;/span&gt;/... &lt;span itemprop="bestRating"&gt;5&lt;/span&gt;stars... &lt;/div&gt;... &lt;span itemprop="description"&gt;The lamp burned out and now I have to replace... it. &lt;/span&gt;... &lt;/div&gt;...... &lt;div itemprop="review" itemscope itemtype="http://schema.org/Review"&gt;... &lt;span itemprop="name"&gt;Value purchase&lt;/span&gt; -... by &lt;span itemprop="author"&gt;Lucas&lt;/span&gt;,... &lt;meta itemprop="datePublished" content="2011-03-25"&gt;March 25, 2011... &lt;div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating"&gt;... &lt;meta itemprop="worstRating" content = "1"/&gt;... &lt;span itemprop="ratingValue"&gt;4&lt;/span&gt;/... &lt;span itemprop="bestRating"&gt;5&lt;/span&gt;stars... &lt;/div&gt;... &lt;span itemprop="description"&gt;Great microwave for the price. It is small and... fits in my apartment.&lt;/span&gt;... &lt;/div&gt;... ...... &lt;/div&gt;... """&gt;&gt;&gt; sel = Selector(text=doc, type="html")&gt;&gt;&gt; for scope in sel.xpath('//div[@itemscope]'):... print "current scope:", scope.xpath('@itemtype').extract()... props = scope.xpath('''... set:difference(./descendant::*/@itemprop,... .//*[@itemscope]/*/@itemprop)''')... print " properties:", props.extract()... printcurrent scope: [u'http://schema.org/Product'] properties: [u'name', u'aggregateRating', u'offers', u'description', u'review', u'review']current scope: [u'http://schema.org/AggregateRating'] properties: [u'ratingValue', u'reviewCount']current scope: [u'http://schema.org/Offer'] properties: [u'price', u'availability']current scope: [u'http://schema.org/Review'] properties: [u'name', u'author', u'datePublished', u'reviewRating', u'description']current scope: [u'http://schema.org/Rating'] properties: [u'worstRating', u'ratingValue', u'bestRating']current scope: [u'http://schema.org/Review'] properties: [u'name', u'author', u'datePublished', u'reviewRating', u'description']current scope: [u'http://schema.org/Rating'] properties: [u'worstRating', u'ratingValue', u'bestRating'] 这里我们先迭代 itemscope 元素，对于每一个元素，我们寻找所有 itemprops 元素，并排除那些在另一个元素内部的元素 itemscope 。 内置选择器参考Selector 实例1class scrapy.selector.Selector(response=None, text=None, type=None) 一个实例Selector是一个包装器响应来选择其内容的某些部分。 response是一个HtmlResponse或一个XmlResponse将被用于选择和提取的数据对象。 text是一个unicode字符串或utf-8编码的文本，当一个 response不可用时。使用text和response一起是未定义的行为。 type定义选择器类型，它可以是&quot;html&quot;，&quot;xml&quot;或None（默认）。 如果type是None，选择器将根据response类型（见下文）自动选择最佳类型，或者默认&quot;html&quot;情况下与选项一起使用text。 如果type是None和response传递，选择器类型从响应类型推断如下： &quot;html&quot;对于HtmlResponse类型 &quot;xml&quot;对于XmlResponse类型 &quot;html&quot;为任何其他 否则，如果type设置，选择器类型将被强制，并且不会发生检测。 xpath（查询）查找与xpath匹配的节点query，并将结果作为 SelectorList实例将所有元素展平。列表元素也实现Selector接口。 query 是一个包含要应用的XPATH查询的字符串。 注意 为了方便起见，这种方法可以称为 response.xpath() css（查询）应用给定的CSS选择器并返回一个SelectorList实例。 query 是一个包含要应用的CSS选择器的字符串。 在后台，CSS查询使用cssselect库和run .xpath()方法转换为XPath查询 。 注意 为了方便起见，该方法可以称为 response.css() extract（）序列化并返回匹配的节点作为unicode字符串列表。编码内容的百分比未引用。 re（regex）应用给定的正则表达式并返回一个包含匹配项的unicode字符串的列表。 regex可以是编译的正则表达式或将被编译为正则表达式的字符串 re.compile(regex) 注意 注意，re()和re_first()解码HTML实体（除\&lt;和\&amp;）。 register_namespace（prefix，uri）注册在此使用的给定命名空间Selector。如果不注册命名空间，则无法从非标准命名空间中选择或提取数据。参见下面的例子。 remove_namespaces（）删除所有命名空间，允许使用无命名空间的xpaths遍历文档。参见下面的例子。 nonzero（）返回True如果有选择或任何实际的内容False 除外。换句话说，a的布尔值Selector由它选择的内容给出。 SelectorList对象class scrapy.selector.SelectorList 本SelectorList类是内置的一个子list 类，它提供了几个方法。 xpath（查询）调用.xpath()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。 query 是同一个参数 Selector.xpath() css（查询）调用.css()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。 query 是同一个参数 Selector.css() extract（）调用.extract()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。 re（）调用.re()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。 nonzero（）如果列表不为空，则返回True，否则返回False。 HTML响应的选择器示例这里有几个Selector例子来说明几个概念。在所有情况下，我们假设已经Selector实例化了一个HtmlResponse对象，如下： 1sel = Selector(html_response) &lt;h1&gt;从HTML响应主体中选择所有元素，返回Selector对象列表 （即SelectorList对象）： 1sel.xpath(&quot;//h1&quot;) &lt;h1&gt;从HTML响应正文中提取所有元素的文本，返回unicode字符串 12sel.xpath(&quot;//h1&quot;).extract() # this includes the h1 tagsel.xpath(&quot;//h1/text()&quot;).extract() # this excludes the h1 tag 迭代所有&lt;p&gt;标签并打印其类属性： 12for node in sel.xpath(&quot;//p&quot;): print node.xpath(&quot;@class&quot;).extract() XML响应的选择器示例这里有几个例子来说明几个概念。在这两种情况下，我们假设已经Selector实例化了一个 XmlResponse对象，像这样： 1sel = Selector(xml_response) 从XML响应主体中选择所有元素，返回Selector对象列表（即SelectorList对象）： 1sel.xpath(&quot;//product&quot;) 从需要注册命名空间的Google Base XML Feed中提取所有价格： 12sel.register_namespace(&quot;g&quot;, &quot;http://base.google.com/ns/1.0&quot;)sel.xpath(&quot;//g:price&quot;).extract() 删除名称空间当处理抓取项目时，通常很方便地完全删除命名空间，只需处理元素名称，编写更简单/方便的XPath。你可以使用的 Selector.remove_namespaces()方法。 让我们展示一个例子，用GitHub博客atom feed来说明这一点。 首先，我们打开shell和我们想要抓取的url： 1$ scrapy shell https://github.com/blog.atom 一旦在shell中，我们可以尝试选择所有对象，并看到它不工作（因为Atom XML命名空间模糊了这些节点）： 12&gt;&gt;&gt; response.xpath(&quot;//link&quot;)[] 但是一旦我们调用该Selector.remove_namespaces()方法，所有节点都可以直接通过他们的名字访问： 12345&gt;&gt;&gt; response.selector.remove_namespaces()&gt;&gt;&gt; response.xpath(&quot;//link&quot;)[&lt;Selector xpath=&apos;//link&apos; data=u&apos;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&apos;&gt;, &lt;Selector xpath=&apos;//link&apos; data=u&apos;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&apos;&gt;, ... 如果你想知道为什么默认情况下不调用命名空间删除过程，而不是手动调用它，这是因为两个原因，按照相关性的顺序： 删除命名空间需要迭代和修改文档中的所有节点，这对于Scrapy爬取的所有文档来说是一个相当昂贵的操作 可能有一些情况下，实际上需要使用命名空间，以防某些元素名称在命名空间之间冲突。这些情况非常罕见。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——items设计]]></title>
    <url>%2F2017%2F05%2F05%2Fscrapy-items-design%2F</url>
    <content type="text"><![CDATA[Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 scrapy.Field() 。 定义 items由于需要添加一个封面图，对上面的爬虫添加一个 front_image_url 字段对 parse 函数进行修改： 1234567891011121314def parse(self, response): """ 1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析 2. 获取下一页的url并交给scrapy进行下载 :param response: :return: """ # 解析列表页中的所有文章url并交给解析函数进行具体字段的解析 post_nodes = response.css("#archive .floated-thumb .post-thumb a") for post_node in post_nodes: image_url = post_node.css("img::attr(src)").extract_first("") post_url = post_node.css("::attr(href)").extract_first("") yield Request(url=parse.urljoin(response.url, post_url), meta=&#123;"front_image_url": image_url&#125;, callback=self.parse_detail) 其中的 meta 字段是传递值的方法。在调试时返回的 response 中会出现 meta 的内容，它是一个字典，故在传递时可以直接通过 response.meta[&#39;front_image_url&#39;] 进行引用（也可以使用get的方法，附默认值防止出现异常）： 在 items.py 文件中，定义一个item并声明其字段： 123456789101112class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field() front_image_path = scrapy.Field() praise_nums = scrapy.Field() comment_nums = scrapy.Field() fav_nums = scrapy.Field() tags = scrapy.Field() content = scrapy.Field() 在 jobbole.py 中添加 from ArticleSpider.items import JobBoleArticleItem 对item进行引用，然后在 parse_detail 中进行初始化 article_item = JobBoleArticleItem() ，之后将获取到的字段内容存入初始化的item中，最终代码如下： 123456789101112131415161718192021222324252627282930313233343536def parse_detail(self, response): article_item = JobBoleArticleItem() # 通过css选择器提取字段 front_image_url = response.meta.get("front_image_url", "") # 文章封面图 title = response.css(".entry-header h1::text").extract()[0] create_date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·", "").strip() praise_nums = int(response.css(".vote-post-up h10::text").extract()[0]) fav_nums = response.css(".bookmark-btn::text").extract()[0] match_re = re.match(".*?(\d+).*?", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css("a[href='#article-comment'] span::text").extract()[0] match_re = re.match(".*?(\d+).*?", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css("div.entry").extract()[0] tag_list = response.css("p.entry-meta-hide-on-mobile a::text").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list) article_item["title"] = title article_item["url"] = response.url article_item["create_date"] = create_date article_item["front_image_url"] = front_image_url article_item["praise_nums"] = praise_nums article_item["comment_nums"] = comment_nums article_item["fav_nums"] = fav_nums article_item["content"] = content article_item["tags"] = tags yield article_item 其中 yield article_item 会自动提交到 settings 中的 ITEM_PIPELINES 进行处理。此时在 pipelines.py 中设置断点调试，可以看到 article_item 中的值已经传递到这里了。 自定义 Pipelines在 settings.py 中有一个ITEM_PIPELINES的选项，把它的注释去掉增加下载图片的代码： 123456789# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; 'ArticleSpider.pipelines.ArticlespiderPipeline': 300, 'scrapy.pipelines.images.ImagesPipeline': 1,&#125;IMAGES_URLS_FIELD = "front_image_url"project_dir = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_dir, 'images') 上面的代码启用了下载图片piplines，并定义了存储地址及想要存储的图片地址。在settings.py同级目录下建立文件夹 images 用来保存图片。当运行爬虫时，图片就会自动下载图片并保存到本地。如果想要得到存储的图片路径的话，需要自定义pipelines。 首先，在 pipeines.py 中引入 from scrapy.pipelines.images import ImagesPipeline ， 然后自定义一个pipeline对ImagesPipeline进行重载： 123class ArticleImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): pass 进行断点调试，查看results中的信息： 调试结果中，results是一个list，第一个值是一个bool值表示图片是否获取成功，第二个值是一个字典，保存了图片路径，图片地址等信息。 最终自定义的pipeline代码如下： 123456class ArticleImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): for ok, value in results: image_file_path = value["path"] item["front_image_path"] = image_file_path return item 上面代码得到image_path保存到 item[&quot;front_image_path&quot;] 中并返回，这时会根据pipelines的顺序进行下一个pipelines进行处理。通过断点调试可以得到想要的结果。 完善 items 获取对之前定义的items中的 url_object_id 字段，需要对url进行md5处理，因此在 items.py 同级目录下新建一个名为 utils 的 python package，新建 common.py ，代码如下： 123456789import hashlibdef get_md5(url): if isinstance(url, str): url = url.encode("utf-8") m = hashlib.md5() m.update(url) return m.hexdigest() 然后在 jobbole.py 下引入 from ArticleSpider.utils.common import get_md5 ，在item内容填充时加上 article_item[&quot;url_object_id&quot;] = get_md5(response.url) 即可。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——编写Spider爬取伯乐在线所有文章]]></title>
    <url>%2F2017%2F04%2F26%2Fscrapy-jobbole-spider%2F</url>
    <content type="text"><![CDATA[仍然是以 http://blog.jobbole.com/all-posts/ 页面为例 提取文章列表页首页使用CSS选择器获取页面中的文章url列表： 1post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() 123456789101112131415161718192021&gt;&gt;&gt; response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract()&gt;&gt;&gt;['http://blog.jobbole.com/111005/', 'http://blog.jobbole.com/108468/', 'http://blog.jobbole.com/110975/', 'http://blog.jobbole.com/110986/', 'http://blog.jobbole.com/110957/', 'http://blog.jobbole.com/110976/', 'http://blog.jobbole.com/110923/', 'http://blog.jobbole.com/110962/', 'http://blog.jobbole.com/110958/', 'http://blog.jobbole.com/110140/', 'http://blog.jobbole.com/110939/', 'http://blog.jobbole.com/110941/', 'http://blog.jobbole.com/110931/', 'http://blog.jobbole.com/110934/', 'http://blog.jobbole.com/110929/', 'http://blog.jobbole.com/110835/', 'http://blog.jobbole.com/110906/', 'http://blog.jobbole.com/110916/', 'http://blog.jobbole.com/110913/', 'http://blog.jobbole.com/110903/'] 先在Spider头部引入from scrapy.http import Request，使用Request进行对文章url列表获取函数的调用 123456789101112def parse(self, response): """ 1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析 2. 获取下一页的url并交给scrapy进行下载 :param response: :return: """ # 解析列表页中的所有文章url并交给解析函数进行具体字段的解析 post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() for post_url in post_urls: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 其中，最后 yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 是对每个url调用parse_detail方法进行字段解析，这里url的参数是带有完整域名的格式，如果不是完整域名，则需要对域名进行拼接成完成域名进行解析。首先要引入 from urllib import parse ，通过parse自带的 parse.urljoin() 进行拼接，代码为： yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 。 循环获取下一个列表页 每一个列表页都有“下一页”链接，我们通过CSS选择器来获取下一页的链接，然后交给parse函数进行循环解析。 1234# 提取下一页并交给scrapy进行下载next_url = response.css(".next.page-numbers::attr(href)").extract_first()if next_url: yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) 其中，extract_first()方法与extract()[0]用法相同，都是提取第一个字符串元素。 解析函数如下： 12345678910111213141516171819202122def parse_detail(self, response): # 通过css选择器提取字段 title = response.css(".entry-header h1::text").extract()[0] create_date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·", "").strip() praise_nums = int(response.css(".vote-post-up h10::text").extract()[0]) fav_nums = response.css(".bookmark-btn::text").extract()[0] match_re = re.match(".*?(\d+).*?", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css("a[href='#article-comment'] span::text").extract()[0] match_re = re.match(".*?(\d+).*?", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css("div.entry").extract()[0] tag_list = response.css("p.entry-meta-hide-on-mobile a::text").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list) print(title, create_date, fav_nums, comment_nums, tags) 运行结果]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——CSS选择器]]></title>
    <url>%2F2017%2F04%2F25%2Fcss-selector%2F</url>
    <content type="text"><![CDATA[CSS选择器的用法CSS选择器简介在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。 常用CSS选择器介绍 表达式 说明 * 选择所有节点 #container 选择id为container的节点 .container 选择所有class包含container的节点 li a 选取所有li下的所有a节点 ul + p 选择ul后面的第一个p元素 div#container &gt; ul 选取id为container的div的第一个ul子元素 ul ~ p 选取与ul相邻的所有p元素 a[title] 选取所有有title属性的a元素 a[href=”http://163.com“] 选取所有href属性为163的a元素 a[href*=”163”] 选取所有href属性包含163的a元素 a[href^=”http”] 选取所有href属性以http开头的a元素 a[href$=”.jpg”] 选取所有href以.jpg结尾的a元素 input[type=radio]:checked 选择选中的radio的元素 div:not(#container) 选取所有id非container的div属性 li:nth-child(3) 选取第三个li元素 tr:nth-child(2n) 第偶数个tr Scrapy中CSS选择器用法示例仍然是用Xpath用法示例中的例子来进行测试 获取标题 12&gt;&gt;&gt; response.css(".entry-header h1::text").extract()[0]'2016 腾讯软件开发面试题（部分）' 注意：这里获取文字内容的方法为::text，而不是text()。 获取文章发布时间 12&gt;&gt;&gt; response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·", "").strip()'2017/02/18' 获取点赞数、收藏数、评论数 1234567891011# 点赞数&gt;&gt;&gt; response.css(".vote-post-up h10::text").extract()[0]'2'# 收藏数，获取之后需要用正则表达式进行清洗&gt;&gt;&gt; response.css(".bookmark-btn::text").extract()[0]' 23 收藏'# 评论数，获取之后需要用正则表达式进行清洗&gt;&gt;&gt; response.css("a[href='#article-comment'] span::text").extract()[0]' 7 评论' 正则表达式清洗收藏数，评论数的逻辑如下 123456789fav_nums = response.css(".bookmark-btn::text").extract()[0]match_re = re.match(".*?(\d+).*?", fav_nums)if match_re: fav_nums = int(match_re.group(1)) comment_nums = response.css("a[href='#article-comment'] span::text").extract()[0]match_re = re.match(".*?(\d+).*?", comment_nums)if match_re: comment_nums = int(match_re.group(1)) 获取正文1&gt;&gt;&gt; response.css("div.entry").extract()[0] 获取tags 12&gt;&gt;&gt; response.css("p.entry-meta-hide-on-mobile a::text").extract()['职场', ' 7 评论 ', '面试'] 然后需要对数据进行清洗 123tag_list = response.css("p.entry-meta-hide-on-mobile a::text()").extract()tag_list = [element for element in tag_list if not element.strip().endswith("评论")]tags = ",".join(tag_list)]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，CSS，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Spiders]]></title>
    <url>%2F2017%2F04%2F22%2Fscrapy-spiders%2F</url>
    <content type="text"><![CDATA[Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。 对 spider 来说，爬取的流程如下： 先初始化请求URL列表，并指定下载后处理response的回调函数。初次请求URL通过 start_urls 指定，调用 start_requests() 产生 Request 对象，然后注册 parse 方法作为回调 在parse回调中解析response并返回字典, Item 对象, Request 对象或它们的迭代对象。 Request 对象还会包含回调函数，之后Scrapy下载完后会被这里注册的回调函数处理。 在回调函数里面，你通过使用选择器（同样可以使用BeautifulSoup,lxml或其他工具）解析页面内容，并生成解析后的结果Item。 最后返回的这些Item通常会被持久化到数据库中(使用Item Pipeline)或者使用Feed exports将其保存到文件中。 虽然该循环对任何类型的 spider 都（多少）适用，但 Scrapy 仍然为了不同的需求提供了多种默认 spider。 之后将讨论这些 spider。 Spider 参数Spider 可以通过接受参数来修改其功能。 spider 参数一般用来定义初始 URL 或者指定限制爬取网站的部分。 您也可以使用其来配置 spider 的任何功能。 在运行 crawl 时添加 -a 可以传递 Spider 参数： 1scrapy crawl myspider -a category=electronics Spider 在构造器（constructor）中获取参数： 123456789import scrapyclass MySpider(scrapy.Spider): name = 'myspider' def __init__(self, category=None, *args, **kwargs): super(MySpider, self).__init__(*args, **kwargs) self.start_urls = ['http://www.example.com/categories/%s' % category] # ... Spider 参数也可以通过 Scrapyd 的 schedule.json API 来传递。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Items]]></title>
    <url>%2F2017%2F04%2F21%2Fscrapy-items%2F</url>
    <content type="text"><![CDATA[爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。 Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。 声明 ItemItem 使用简单的 class 定义语法以及 Field 对象来声明。例如: 1234567import scrapyclass ProductItem(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) Scrapy Item 的定义方式与 Django Models 很类似，但是没有 Django 那么多不同的字段类型（Field Type）。 Item 字段（Item Fields）Field 对象指明了每个字段的元数据（metadata）。例如上面例子中 last_updated 中指明了该字段的序列化函数。 您可以为每个字段指明任何类型的元数据。Field 对象对接受的值没有任何限制。也正是因为这个原因，文档也无法提供所有可用的元数据的键（key）参考列表。Field 对象中保存的每个键可以由多个组件使用，并且只有这些组件知道这个键的存在。您可以根据自己的需求，定义使用其他的 Field 键。 设置 Field 对象的主要目的就是在一个地方定义好所有的元数据。一般来说，那些依赖某个字段的组件肯定使用了特定的键（key）。您必须查看组件相关的文档，查看其用了哪些元数据键（metadata key）。 需要注意的是，用来声明 item 的 Field 对象并没有被赋值为 class 的属性。不过您可以通过 Item.fields 属性进 行访问。 与 Item 配合在API这里的操作，和 dict API 非常的相似。 创建 item12345678910&gt;&gt;&gt; import scrapy&gt;&gt;&gt; class ProductItem(scrapy.Item):... name = scrapy.Field()... price = scrapy.Field()... stock = scrapy.Field()... last_updated = scrapy.Field(serializer=str)...&gt;&gt;&gt; product = ProductItem(name='Desktop', price=1000)&gt;&gt;&gt; print(product)&#123;'name': 'Desktop', 'price': 1000&#125; 获取字段的值两种方式： 键值对 get() 123456789101112131415161718192021222324252627282930313233343536373839404142# 已设定key和value值&gt;&gt;&gt; product['name']'Desktop'&gt;&gt;&gt; product.get('name')'Desktop'# 未设定key和value值&gt;&gt;&gt; product['last_updated']Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py", line 59, in __getitem__ return self._values[key]KeyError: 'last_updated' # 默认返回空值&gt;&gt;&gt; product.get('last_updated')# 设定返回值&gt;&gt;&gt; product.get('last_updated', 'not set value')'not set value'# 未声明的字段&gt;&gt;&gt; product['lala']Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py", line 59, in __getitem__ return self._values[key]KeyError: 'lala'&gt;&gt;&gt; product.get('lala', 'not exist')'not exist'# 字段是否被赋值&gt;&gt;&gt; 'name' in productTrue&gt;&gt;&gt; 'last_updated' in productFalse# 字段是否被声明&gt;&gt;&gt; 'last_updated' in product.fieldsTrue&gt;&gt;&gt; 'lala' in product.fieldsFalse 设置字段的值123456789101112# 已经声明的字段&gt;&gt;&gt; product['last_updated'] = 'today'&gt;&gt;&gt; product['last_updated']'today'# 未声明的字段无法赋值&gt;&gt;&gt; product['lala'] = 'test'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py", line 66, in __setitem__ (self.__class__.__name__, key))KeyError: 'ProductItem does not support field: lala' 获取所有能够获取到的值可以使用 dict API 来获取所有的值: 1234&gt;&gt;&gt; product.keys()dict_keys(['last_updated', 'price', 'name'])&gt;&gt;&gt; product.items()ItemsView(&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125;) 复制 item12345678&gt;&gt;&gt; product2 = ProductItem(product)&gt;&gt;&gt; print(product2)&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125;# 推荐使用第二种方法&gt;&gt;&gt; product3 = product2.copy()&gt;&gt;&gt; print(product3)&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125; 根据 item 创建字典(dict)12&gt;&gt;&gt; dict(product)&#123;'price': 1000, 'last_updated': 'today', 'name': 'Desktop'&#125; 根据字典(dict)创建 item12345678910&gt;&gt;&gt; ProductItem(&#123;'name':'laptop pc', 'price':1500&#125;)&#123;'name': 'laptop pc', 'price': 1500&#125;&gt;&gt;&gt; ProductItem(&#123;'name':'laptop pc', 'lala':1500&#125;)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py", line 56, in __init__ self[k] = v File "/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py", line 66, in __setitem__ (self.__class__.__name__, key))KeyError: 'ProductItem does not support field: lala' 扩展 Item可以通过继承原始的 Item 来扩展 item(添加更多的字段或者修改某些字段的元数据)。 添加新的字段 123class DiscountedProductItem(Product): discount_percent = scrapy.Field(serializer=str) discount_expiration_date = scrapy.Field() 使用原字段的元数据 123class SpecificProduct(Product): name = scrapy.Field(Product.fields['name'], serializer=my_serializer)#my_serializer 指序列化的类型 上述代码，在保留了原始的元数据值的情况下，添加（或覆盖）了 name 字段的 serializer 。 存在及覆盖，不存在即添加。 Item 对象class scrapy.item.Item([arg]) 返回一个根据给定的参数可选初始化的 item 。 Item 复制了标准化的 dict API ，包括初始化函数也是一样。除此之外，唯一添加的额外属性就是 fields 。 fields 是一个包含了 item 所有声明的字段的字典，而不仅仅是获取到的字段。该字典的 key 是字段（field）的名字，值是 Item 声明中使用到的 Field 对象。 字段（Field）对象class scrapy.item.Field([arg]) Field仅仅是内置的 dict 类的一个别名（继承于 dict ），并没有提供额外的方法或属性。说白了，Field就是完完全全的Python字典，被用来基于类属性的方法支持 Item 声明语法。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记(六)：数据安全与性能保障——处理系统故障]]></title>
    <url>%2F2017%2F04%2F20%2FRedis-6%2F</url>
    <content type="text"><![CDATA[如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。 验证快照文件和 AOF 文件无论时快照持久化还是AOF持久化，都提供了在遇到系统故障时进行数据回复的工具。Redis提供了两个命令行程序 redis-check-aof 和 redis-check-rdb(redis-check-dump was renamed to redis-check-rdb in redis version 3.2) ，它们可以在系统故障发生之后，检查AOF文件和快照文件的状态，并在有需要的情况下对文件进行修复。 在不给定任何参数的情况下运行这两个程序，就可以看见它们的基本使用方法： 1234$ redis-check-rdbUsage: redis-check-rdb &lt;rdb-file-name&gt;$ redis-check-dumpUsage: redis-check-dump &lt;dump.rdb&gt; 如果运行 redis-check-aof 程序时给了 --fix 参数，那么会对AOF文件进行修复。修复方法非常简单：扫描给定的 AOF 文件，寻找不正确或不完整的命令，当发现第一个出错命令的时候，程序会删除出错的命令以及位于出错命令之后的所有命令。在大多数情况下，被删除的都是 AOF 文件末尾的不完整的写命令。遗憾的是，目前没有办法修复出错的快照文件。尽管发现快照文件首个出现错误的地方是有可能的，但因为快照文件本身经过了压缩，而出现在快照文件中间的错误有可能会导致快照文件的剩余部分无法读取。因此，最好为重要的快照文件保留多个备份，并在进行数据恢复时，通过计算快照文件的 SHA1 散列值和 SHA256 散列值来对内容进行验证。 更换故障主服务器我们来看一下在拥有一个主服务器和一个从服务器的情况下，更换主服务器的具体步骤。假设A、B两台机器都运行着 Redis ，机器A为 master ，机器B为 slave 。机器A因为暂时无法修复的故障而断开了连接，因此决定将同样安装了 Redis 的机器 C 用作新的主服务器。 更换服务器的计划非常简单：首先向机器B发送一个 SAVE 命令，让它创建一个新的快照文件，接着将这个快照文件发送给机器C，并在机器 C 上面启动 Redis 。最后，让B成为机器C的从服务器。由于环境有限，就在同一台机器上用不同的端口进行测试，下面进行演示： 先进入 Redis 安装位置，再安装两个 Redis 服务并分别修改配置文件 redis.conf 中的 port 为6380和6381 12345678$ cd /usr/local$ sudo cp -r redis redis6380Password:$ sudo chmod -R 777 redis6380$ vim redis6380/redis.conf$ sudo cp -r redis redis6381$ sudo chmod -R 777 redis6381$ vim redis6381/redis.conf 启动机器A端口为6379，机器B端口为6380，并让B成为A的从服务器 123456789101112# 启动A$ cd redis$ ./src/redis-server redis.conf# 启动B$ cd redis6380$ ./src/redis-server redis.conf# 让B成为A的从服务器$ $ redis-cli -h localhost -p 6380localhost:6380&gt; SLAVEOF localhost 6379OK 停止机器A的 Redis 服务，此时只剩 Redis 从服务器B在运行 向机器B发送 SAVE 命令 12localhost:6380&gt; SAVEOK 将机器B的快照文件复制到机器C的对应目录，并启动 Redis 服务 123$ cp -f /usr/local/redis6380/dump.rdb /usr/local/redis6381$ cd /usr/local/redis6381$ ./src/redis-server redis.conf 让机器B成为机器C的从服务器 12localhost:6380&gt; SLAVEOF localhost 6381OK 测试机器B是否能从机器C同步数据 123456$ redis-cli -h localhost -p 6381localhost:6381&gt; set key new-masterOK$ redis-cli -h localhost -p 6380localhost:6380&gt; get key'new-master' 另一种创建新的主服务器的方法，就是将从服务器升级（turn）为主服务器，并为升级后的主服务器创建从服务器。 以上两种方法都可以让 Redis 回到之前的一个主服务器和一个从服务器的状态，而用户接下来需要做的就是更新客户端的配置，让它们去读写正确的服务器。除此之外，如果用户需要重启 Redis 的话，那么可能还需要对服务器的持久化配置进行更新。 Redis Sentinel可以监视指定的Redis主服务器及其下属的从服务器，并在主服务器下线时自动进行故障转移(failover)。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis, Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy命令行工具]]></title>
    <url>%2F2017%2F04%2F20%2Fscrapy-command-line-tools%2F</url>
    <content type="text"><![CDATA[Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。 使用 scrapy 工具创建项目一般来说，使用 scrapy 工具的第一件事就是创建 Scrapy 项目： 1scrapy startproject myproject 该命令将会在 myproject 目录中创建一个 Scrapy 项目。 接下来，进入到项目目录中: 1cd myproject 这时候就可以使用 scrapy 命令来管理和控制项目了。 控制项目创建一个新的 spider： 1scrapy genspider mydomain mydomain.com Scrapy 提供了两种类型的命令。一种必须在 Scrapy 项目中运行(针对项目(Project-specific)的命令)，另外一种则不需要(全局命令)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。 123456789101112131415161718# 全局命令(不需要在项目中运行)startprojectsettingsrunspidershellfetchviewversion# 项目(Project-only)命令(必须在项目中运行)crawlchecklisteditparsegenspiderdeploybench 工具命令介绍我们可以通过运行命令来获取关于每个命令的详细内容： 1scrapy &lt;command&gt; -h 也可以查看所有的命令： 1scrapy -h 下面就对这些命令进行介绍。 startproject 语法：scrapy startproject &lt;project_name&gt; 全局命令 在 project_name 文件夹下创建一个名为 project_name 的 Scrapy 项目。 genspider 语法：scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt; 项目命令 在当前项目中创建 spider。这仅仅是创建 spider 的一种快捷方法。该方法可以使用提前定义好的模板来生成 spider。您也可以自己创建 spider 的源码文件。 例子： 12345678910111213141516171819202122232425# 查看模板$ scrapy genspider -lAvailable templates: basic crawl csvfeed xmlfeed# 编辑模板$ scrapy genspider -d basic# -*- coding: utf-8 -*-import scrapyclass $classname(scrapy.Spider): name = "$name" allowed_domains = ["$domain"] start_urls = ['http://$domain/'] def parse(self, response): pass # 根据模板来生成spider$ scrapy genspider -t basic example example.com Created spider 'example' using template 'basic' in module: tutorial.spiders.example crawl 语法：scrapy crawl myspider 项目命令 使用 spider 进行爬取。 例子： 12$ scrapy crawl myspider [ ... myspider starts crawling ... ] check 语法：scrapy check [-l] &lt;spider&gt; 项目命令 运行 contract 检查。 例子： 1234567891011121314$ scrapy check -l first_spider * parse * parse_item second_spider * parse * parse_item$ scrapy check [FAILED] first_spider:parse_item &gt;&gt;&gt; 'RetailPricex' field is missing[FAILED] first_spider:parse &gt;&gt;&gt; Returned 92 requests, expected 0..4 list 语法：scrapy list 项目命令 列出当前项目中所有可用的 spider。每行输出一个 spider。 例子： 123$ scrapy list spider1 spider2 edit 语法：scrapy edit &lt;spider&gt; 项目命令 使用 EDITOR 中设定的编辑器编辑给定的 spider 该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者 IDE 来编写调试 spider。 fetch 语法：scrapy fetch &lt;url&gt; 全局命令 使用 Scrapy 下载器(downloader)下载给定的 URL，并将获取到的内容送到标准输出。 该命令以 spider 下载页面的方式获取页面。例如，如果 spider 有 USER_AGENT 属性修改了 User Agen t，该命令将会使用该属性。 因此，您可以使用该命令来查看 spider 如何获取某个特定页面。 该命令如果非项目中运行则会使用默认 Scrapy downloader 设定。 例子： 12345678910111213$ scrapy fetch --nolog http://www.example.com/some/page.html [ ... html content here ... ]$ scrapy fetch --nolog --headers http://www.example.com/ &#123;'Accept-Ranges': ['bytes'],'Age': ['1263 '], 'Connection': ['close '], 'Content-Length': ['596'], 'Content-Type': ['text/html; charset=UTF-8'], 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'], 'Etag': ['"573c1-254-48c9c87349680"'], 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'], 'Server': ['Apache/2.2.3 (CentOS)']&#125; view 语法：scrapy view &lt;url&gt; 全局命令 在浏览器中打开给定的 URL，并以 Scrapy spider 获取到的形式展现。 有些时候 spider 获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查 spider 所获取到的页面，并确认这是您所期望的。 例子： 12$ scrapy view http://www.example.com/some/page.html [ ... browser starts ... ] shell 语法：scrapy shell [url] 全局命令 以给定的 URL(如果给出)或者空(没有给出 URL)启动 Scrapy shell。查看 Scrapy 终端(Scrapy shell) 获取更多信息。 例子： 12$ scrapy shell http://www.example.com/some/page.html [ ... scrapy shell starts ... ] parse 语法：scrapy parse &lt;url&gt; [options] 项目命令 获取给定的 URL 并使用相应的 spider 分析处理。如果提供 --callback 选项，则使用 spider 的该方法处理，否则使用 parse 。 settings 语法：scrapy settings [option] 全局命令 获取 Scrapy 的设定 在项目中运行时，该命令将会输出项目的设定值，否则输出 Scrapy 默认设定。 例子： 1234$ scrapy settings --get BOT_NAME scrapybot $ scrapy settings --get DOWNLOAD_DELAY 0 runspider 语法：scrapy runspider &lt;spider_file.py&gt; 全局命令 在未创建项目的情况下，运行一个编写在 Python 文件中的 spider。 例子： 12$ scrapy runspider myspider.py [ ... spider starts crawling ... ] version 语法：scrapy version [-v] 全局命令 输出 Scrapy 版本。配合 -v 运行时，该命令同时输出 Python，Twisted 以及平台的信息，方便 bug 提交。 例子： 1234567891011$ scrapy version -vScrapy : 1.3.3lxml : 3.7.3.0libxml2 : 2.9.4cssselect : 1.0.1parsel : 1.1.0w3lib : 1.17.0Twisted : 17.1.0Python : 3.5.2 (default, Oct 11 2016, 04:59:56) - [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)]pyOpenSSL : 16.2.0 (OpenSSL 1.1.0e 16 Feb 2017)Platform : Darwin-16.6.0-x86_64-i386-64bit deploy 语法：scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ] 项目命令 将项目部署到 Scrapyd 服务。 bench 语法：scrapy bench 全局命令 运行 benchmark 测试。Benchmarking。 自定义项目命令您也可以通过 COMMANDS_MODULE 来添加您自己的项目命令。您可以以 scrapy/commands 中 Scrapy commands 为例来了解如何实现您的命令。 COMMANDS_MODULE1Default: '' (empty string) 用于查找添加自定义 Scrapy 命令的模块。 例子： 1COMMANDS_MODULE = 'mybot.commands']]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法示例]]></title>
    <url>%2F2017%2F04%2F16%2Fxpath-example%2F</url>
    <content type="text"><![CDATA[Scrapy 中 XPath 获取相应内容为了方便调试，在终端下输入以下命令进入Scrapy shell： 1scrapy shell 'http://blog.jobbole.com/110287' 获取标题 1234&gt;&gt;&gt; response.xpath("//div[@class='entry-header']/h1/text()").extract()[0]'2016 腾讯软件开发面试题（部分）'&gt;&gt;&gt; response.xpath('//*[@id="post-110287"]/div[1]/h1/text()').extract()[0]'2016 腾讯软件开发面试题（部分）' 以上两种方法都可以得到文章标题，第一种方法通过标题class的属性得到，第二种方法通过确定id，然后通过列表切片得到标题字符串。 获得文章发布时间 1234&gt;&gt;&gt; response.xpath("//p[@class='entry-meta-hide-on-mobile']/text()").extract()[0]'\r\n\r\n 2017/02/18 · '&gt;&gt;&gt; response.xpath("//p[@class='entry-meta-hide-on-mobile']/text()").extract()[0].strip().replace('·', '').strip()'2017/02/18' 第一条命令只能获取p标签中的内容，还需要对获取的数据用 strip() 和 replace() 方法进行清洗。 获取点赞数、收藏数、评论数 对于含有多个属性的class如：class=&quot; btn-bluet-bigger href-style vote-post-up register-user-only &quot;，若只使用其中的一个属性得到值，可以使用contains。 获取点赞数12&gt;&gt;&gt; int(response.xpath("//span[contains(@class, 'vote-post-up')]/h10/text()").extract()[0])2 获取收藏数、评论数12&gt;&gt;&gt; response.xpath("//span[contains(@class, 'bookmark-btn')]/text()").extract()[0]' 23 收藏' 得到的内容为’ 23 收藏’，需要使用正则表达式进行数据清洗。 1234fav_nums = response.xpath("//span[contains(@class, 'bookmark-btn')]/text()").extract()[0]match_re = re.match(".*?(\d+).*?", fav_nums)if match_re: fav_nums = int(match_re.group(1)) 同样的，评论数的获取也需要正则表达式的帮忙。 1234comment_nums = response.xpath("//a[@href='#article-comment']/span/text()").extract()[0]match_re = re.match(".*?(\d+).*?", comment_nums)if match_re: comment_nums = int(match_re.group(1)) 获取正文1content = response.xpath("//div[@class='entry']").extract()[0] 获取tags 所有的tag都在a标签下，类似获得日期的方式，增加一个a标签路径即可。 12&gt;&gt;&gt; response.xpath("//p[@class='entry-meta-hide-on-mobile']/a/text()").extract()['职场', ' 7 评论 ', '面试'] 现在需要对数据进行清洗，去除评论标签。 123tag_list = response.xpath("//p[@class='entry-meta-hide-on-mobile']/a/text()").extract()tag_list = [element for element in tag_list if not element.strip().endswith("评论")]tags = ",".join(tag_list) 总结最后我们构造的spider文件如下： 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import scrapyimport reclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/110287'] def parse(self, response): title = response.xpath("//div[@class='entry-header']/h1/text()").extract()[0] create_date = response.xpath("//p[@class='entry-meta-hide-on-mobile']/text()").extract()[0].strip().replace("·", "").strip() praise_nums = int(response.xpath("//span[contains(@class, 'vote-post-up')]/h10/text()").extract()[0]) fav_nums = response.xpath("//span[contains(@class, 'bookmark-btn')]/text()").extract()[0] match_re = re.match(".*?(\d+).*?", fav_nums) if match_re: fav_nums = int(match_re.group(1)) comment_nums = response.xpath("//a[@href='#article-comment']/span/text()").extract()[0] match_re = re.match(".*?(\d+).*?", comment_nums) if match_re: comment_nums = int(match_re.group(1)) content = response.xpath("//div[@class='entry']").extract()[0] tag_list = response.xpath("//p[@class='entry-meta-hide-on-mobile']/a/text()").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list)]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，XPath，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy简单入门]]></title>
    <url>%2F2017%2F04%2F16%2Fscrapy-simple-intro%2F</url>
    <content type="text"><![CDATA[创建项目开始爬取前，首先需要创建一个新的Scrapy项目 1scrapy startproject tutorial 该命令将会创建包含下列内容的 tutorial 目录: 12345678910tutorial/ scrapy.cfg tutorial/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... 这些文件分别是: scrapy.cfg：项目的配置文件 tutorial/：该项目的 python 模块，之后我们将在此加入代码。 tutorial/items.py：项目中的 item 文件。 tutorial/pipelines.py：项目中的 pipelines 文件。 tutorial/settings.py：项目的设置文件。 tutorial/spiders/：放置 spider 代码的目录。 定义 ItemItem 是保存爬取到的数据的容器；其使用方法和 python 字典类似， 并且提供了额外保护机制来避免拼写错误导 致的未定义字段错误。 类似在 ORM 中做的一样，您可以通过创建一个scrapy.Item类， 并且定义类型为scrapy.Field的类属性来定义一个 Item。 (如果不了解 ORM, 不用担心，您会发现这个步骤非常简单) 首先根据需要从 dmoz.org 获取到的数据对 item 进行建模。 我们需要从 dmoz 中获取名字，url，以及网站的描 述。 对此，在 item 中定义相应的字段。编辑tutorial目录中的items.py文件: 123456import scrapyclass DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() 可能一开始这有些复杂，但是通过定义 item， 我们可以很方便的使用 Scrapy 的其他方法，而这些方法需要知道我们的 item 的定义。 编写第一个爬虫Spider 是用户编写用于从单个网站(或者一些网站)爬取数据的类。 其包含了一个用于下载的初始 URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方 法。 为了创建一个 Spider，我们必须继承scrapy.Spider类， 且定义以下三个属性: name : 用于区别 Spider。 该名字必须是唯一的，您不可以为不同的 Spider 设定相同的名字。 start_urls : 包含了 Spider 在启动时进行爬取的 url 列表。 因此，第一个被获取到的页面将是其中之一。 后续的 URL 则从初始的 URL 获取到的数据中提取。 parse() : spider 的一个方法。 被调用时，每个初始 URL 完成下载后生成的Response对象将会作为 唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成 item)以及生成需 要进一步处理的 URL 的Request对象。 以下为我们的第一个 Spider 代码，保存在tutorial/spiders目录下的dmoz_spider.py文件中: 1234567891011121314import scrapyclass DmozSpider(scrapy.Spider): name = "dmoz" allow_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/" ] def parse(self, response): filename = response.url.split("/")[-2] with open(filename, 'wb') as f: f.write(response.body) 其中，allow_domains是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页。 爬取进入项目的根目录，执行以下命令启动spider 1scrapy crawl dmoz crawl dmoz启动用于爬取dmoz.org的 spider，可以得到如下输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152532017-04-15 21:51:39 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)2017-04-15 21:51:39 [scrapy.utils.log] INFO: Overridden settings: &#123;'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders']&#125;2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled extensions:['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.logstats.LogStats', 'scrapy.extensions.telnet.TelnetConsole']2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware', 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats']2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware']2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled item pipelines:[]2017-04-15 21:51:39 [scrapy.core.engine] INFO: Spider opened2017-04-15 21:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2017-04-15 21:51:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;: HTTP status code is not handled or not allowed2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt;: HTTP status code is not handled or not allowed2017-04-15 21:51:41 [scrapy.core.engine] INFO: Closing spider (finished)2017-04-15 21:51:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:&#123;'downloader/request_bytes': 734, 'downloader/request_count': 3, 'downloader/request_method_count/GET': 3, 'downloader/response_bytes': 3525, 'downloader/response_count': 3, 'downloader/response_status_count/403': 3, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2017, 4, 15, 13, 51, 41, 968931), 'log_count/DEBUG': 4, 'log_count/INFO': 9, 'response_received_count': 3, 'scheduler/dequeued': 2, 'scheduler/dequeued/memory': 2, 'scheduler/enqueued': 2, 'scheduler/enqueued/memory': 2, 'start_time': datetime.datetime(2017, 4, 15, 13, 51, 39, 764494)&#125;2017-04-15 21:51:41 [scrapy.core.engine] INFO: Spider closed (finished) 查看包含[dmoz]的输出，可以看到输出的 log 中包含定义在start_urls的初始 URL，并且与 spider 中是一 一对应的。在 log 中可以看到其没有指向其他页面( (referer:None) )。 除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含 url 所对应的内容的文件被创建 了: Book，Resources 。 发生了什么？Scrapy 为 Spider 的start_urls属性中的每个 URL 创建了scrapy.Request对象，并将parse方法作为回调函数(callback)赋值给了Request。 Request对象经过调度，执行生成scrapy.http.Response对象并送回给spider parse()方法。 提取 ItemSelectors 选择器简介从网页中提取数据有很多方法。Scrapy 使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors。关于 selector 和其他提取机制的信息请参考Selector文档。 关于Xpath的简单使用方法，可以查看之前的一篇博客Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法 为了配合 XPath，Scrapy 除了提供了Selector之外，还提供了方法来避免每次从 response 中提取数据时生成 selector 的麻烦。 Selector 有四个基本的方法： xpath()：传入 xpath 表达式，返回该表达式所对应的所有节点的 selector list 列表 。 css()：传入 CSS 表达式，返回该表达式所对应的所有节点的 selector list 列表。 extract()：序列化该节点为 unicode 字符串并返回 list。 re()：根据传入的正则表达式对数据进行提取，返回 unicode 字符串 list 列表。 在 Shell 中尝试 Selector 选择器为了介绍 Selector 的使用方法，接下来我们将要使用内置的 Scrapy shell。Scrapy Shell 需要我们预装好 IPython(一个扩展的 Python 终端)。 我们需要进入项目的根目录，执行下列命令来启动 shell: 1scrapy shell "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/" Shell 的输出类似于： 12345678910111213141516172017-04-15 22:04:22 [scrapy.core.engine] INFO: Spider opened2017-04-15 22:04:23 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)2017-04-15 22:04:24 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)[s] Available Scrapy objects:[s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s] crawler &lt;scrapy.crawler.Crawler object at 0x109728ac8&gt;[s] item &#123;&#125;[s] request &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;[s] response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;[s] settings &lt;scrapy.settings.Settings object at 0x10a2a0a58&gt;[s] spider &lt;DefaultSpider 'default' at 0x10a4dc3c8&gt;[s] Useful shortcuts:[s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)[s] fetch(req) Fetch a scrapy.Request and update local objects[s] shelp() Shell help (print this help)[s] view(response) View response in a browser&gt;&gt;&gt; 当 shell 载入后，我们将得到一个包含 response 数据的本地 response 变量。输入 response.body 将输出 resp onse 的包体，输出 response.headers 可以看到 response 的包头。 更为重要的是，当输入 response.selector 时， 我们将获取到一个可以用于查询返回数据的 selector(选择器)， 以及映射到 response.selector.xpath() 、response.selector.css() 的 快捷方法(shortcut): response.xpat h() 和 response.css() 。 下面就来试试： 12345678&gt;&gt;&gt; response.xpath('//title')[&lt;Selector xpath='//title' data='&lt;title&gt;DMOZ&lt;/title&gt;'&gt;]&gt;&gt;&gt; response.xpath('//title').extract()['&lt;title&gt;DMOZ&lt;/title&gt;']&gt;&gt;&gt; response.xpath('//title/text()')[&lt;Selector xpath='//title/text()' data='DMOZ'&gt;]&gt;&gt;&gt; response.xpath('//title/text()').extract()['DMOZ'] 提取数据现在，我们来尝试从这些页面中提取些有用的数据。 我们可以在终端中输入 response.body 来观察 HTML 源码并确定合适的 XPath 表达式。不过，这任务非常无聊且不易。您可以考虑使用 Firefox 的 Firebug 扩展来使得工作更为轻松。 在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 元素中。 我们可以通过这段代码选择该页面中网站列表里所有 元素: 1response.xpath('//ul/li') 网站的描述： 1response.xpath('//ul/li/text()').extract() 网站的标题： 1response.xpath('//ul/li/a/text()').extract() 以及网站的链接： 1response.xpath('//ul/li/a/@href').extract() 之前提到过，每个 .xpath() 调用返回 selector 组成的 list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性: 12345for response in response.xpath('//ul/li'): title = response.xpath('a/text()').extract() link = response.xpath('a/@href').extract() desc = response.xpath('text()').extract() print(title, link, desc) 在我们的 spider 中加入如下代码： 123456789101112131415import scrapyclass DmozSpider(scrapy.Spider): name = "dmoz" allow_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"] def parse(self, response): for response in response.xpath('//ul/li'): title = response.xpath('a/text()').extract() link = response.xpath('a/@href').extract() desc = response.xpath('text()').extract() print(title, link, desc) 现在尝试再次爬取 dmoz.org，您将看到爬取到的网站信息被成功输出: 1scrapy crawl dmoz 使用 ItemItem 对象是自定义的 python 字典。您可以使用标准的字典语法来获取到其每个字段的值。(字段就是我们之前用 Field 赋值的属性): 1234&gt;&gt;&gt; item = DmozItem() &gt;&gt;&gt; item['title'] = 'Example title' &gt;&gt;&gt; item['title'] 'Example title' 一般来说，Spider 将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是: 1234567891011121314151617181920import scrapyfrom tutorial.items import DmozItemclass DmozSpider(scrapy.Spider): name = "dmoz" allow_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/" ] def parse(self, response): for response in response.xpath('//ul/li'): item = DmozItem() item['title'] = response.xpath('a/text()').extract() item['link'] = response.xpath('a/@href').extract() item['desc'] = response.xpath('text()').extract() yield item 现在对 dmoz.org 进行爬取将会产生 DmozItem 对象: 1[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author webs 'link': [u'http://gnosis.cx/TPiP/'], 'title': [u'Text Processing in Python']&#125; [dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applic 'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'], 'title': [u'XML Processing with Python']&#125; 保存爬取到的数据最简单存储爬取的数据的方式是使用 Feed exports : 1scrapy crawl dmoz -o items.json 该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。 在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的 item 做更多更为复杂的 操作，您可以编写 Item Pipeline 。 类似于我们在创建项目时对 Item 做的，用于您编写自己的 tutorial/pipelines.py 也被创建。 不过如果您仅仅想要保存 item，您不需要实现任何的 pipeline。]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3+Django配置mysql连接]]></title>
    <url>%2F2017%2F04%2F14%2Fdjango-py3-mysql%2F</url>
    <content type="text"><![CDATA[之前一直使用的是Django1.9和Python2.7，现在使用Python3和Django1.10，发现Mysql-Python一直无法安装 这是因为mysql官网上的版本只支持Python3.4的数据库驱动，所以Python3.5是安装不上相应的驱动的，可以使用pymysql。在虚拟环境下pip install pymysql就可以了，然后在项目目录下的__init__.py文件中添加 12import pymysqlpymysql.install_as_mysqldb() 就可以代替Django默认使用的MySQLdb了。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django，Mysql，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Python字符编码]]></title>
    <url>%2F2017%2F04%2F12%2Fpython-str-encode%2F</url>
    <content type="text"><![CDATA[Python字符编码字符编码是计算机编程中不可回避的问题，不管你用 Python2 还是 Python3，亦或是 C++, Java 等，我都觉得非常有必要理清计算机中的字符编码概念。 基本概念 字符（Character） 在电脑和电信领域中，字符是一个信息单位，它是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。比如，一个汉字，一个英文字母，一个标点符号等都是一个字符。 字符集（Character set） 字符集是字符的集合。字符集的种类较多，每个字符集包含的字符个数也不同。比如，常见的字符集有 ASCII 字符集、GB2312 字符集、Unicode 字符集等，其中，ASCII 字符集共有 128 个字符，包含可显示字符（比如英文大小写字符、阿拉伯数字）和控制字符（比如空格键、回车键）；GB2312 字符集是中国国家标准的简体中文字符集，包含简化汉字、一般符号、数字等；Unicode 字符集则包含了世界各国语言中使用到的所有字符。 字符编码（Character encoding） 字符编码，是指对于字符集中的字符，将其编码为特定的二进制数，以便计算机处理。常见的字符编码有 ASCII 编码，UTF-8 编码，GBK 编码等。一般而言，字符集和字符编码往往被认为是同义的概念，比如，对于字符集 ASCII，它除了有「字符的集合」这层含义外，同时也包含了「编码」的含义，也就是说，ASCII 既表示了字符集也表示了对应的字符编码。 下面我们用一个表格做下总结： 概念 描述 实例 字符 一个信息单位，各种文字和符号的总称 ‘中’, ‘a’, ‘1’, ‘$’, ‘￥’, … 字符集 字符的集合 ASCII 字符集, GB2312 字符集, Unicode 字符集 字符编码 将字符集中的字符，编码为特定的二进制数 ASCII 编码，GB2312 编码，Unicode 编码 字节 计算机中存储数据的单元，一个 8 位（bit）的二进制数 0x01, 0x45, … 常见字符编码简介常见的字符编码有 ASCII 编码，GBK 编码，Unicode 编码和 UTF-8 编码等等。这里，我们主要介绍 ASCII、Unicode 和 UTF-8。 ASCII 计算机是在美国诞生的，人家用的是英语，而在英语的世界里，不过就是英文字母，数字和一些普通符号的组合而已。 在 20 世纪 60 年代，美国制定了一套字符编码方案，规定了英文字母，数字和一些普通符号跟二进制的转换关系，被称为 ASCII (American Standard Code for Information Interchange，美国信息互换标准编码) 码。 比如，大写英文字母 A 的二进制表示是 01000001（十进制 65），小写英文字母 a 的二进制表示是 01100001 （十进制 97），空格 SPACE 的二进制表示是 00100000（十进制 32）。 Unicode ASCII 码只规定了 128 个字符的编码，这在美国是够用的。可是，计算机后来传到了欧洲，亚洲，乃至世界各地，而世界各国的语言几乎是完全不一样的，用 ASCII 码来表示其他语言是远远不够的，所以，不同的国家和地区又制定了自己的编码方案，比如中国大陆的 GB2312 编码 和 GBK 编码等，日本的 Shift_JIS 编码等等。 虽然各个国家和地区可以制定自己的编码方案，但不同国家和地区的计算机在数据传输的过程中就会出现各种各样的乱码（mojibake），这无疑是个灾难。 怎么办？想法也很简单，就是将全世界所有的语言统一成一套编码方案，这套编码方案就叫 Unicode，它为每种语言的每个字符设定了独一无二的二进制编码，这样就可以跨语言，跨平台进行文本处理了，是不是很棒！ Unicode 1.0 版诞生于 1991 年 10 月，至今它仍在不断增修，每个新版本都会加入更多新的字符，目前最新的版本为 2016 年 6 月 21 日公布的 9.0.0。 Unicode 标准使用十六进制数字，而且在数字前面加上前缀 U+，比如，大写字母「A」的 unicode 编码为 U+0041，汉字「严」的 unicode 编码为 U+4E25。更多的符号对应表，可以查询 unicode.org，或者专门的汉字对应表。 UTF-8 Unicode 看起来已经很完美了，实现了大一统。但是，Unicode 却存在一个很大的问题：资源浪费。 为什么这么说呢？原来，Unicode 为了能表示世界各国所有文字，一开始用两个字节，后来发现两个字节不够用，又用了四个字节。比如，汉字「严」的 unicode 编码是十六进制数 4E25，转换成二进制有十五位，即 100111000100101，因此至少需要两个字节才能表示这个汉字，但是对于其他的字符，就可能需要三个或四个字节，甚至更多。 这时，问题就来了，如果以前的 ASCII 字符集也用这种方式来表示，那岂不是很浪费存储空间。比如，大写字母「A」的二进制编码为 01000001，它只需要一个字节就够了，如果 unicode 统一使用三个字节或四个字节来表示字符，那「A」的二进制编码的前面几个字节就都是 0，这是很浪费存储空间的。 为了解决这个问题，在 Unicode 的基础上，人们实现了 UTF-16, UTF-32 和 UTF-8。下面只说一下 UTF-8。 UTF-8 (8-bit Unicode Transformation Format) 是一种针对 Unicode 的可变长度字符编码，它使用一到四个字节来表示字符，例如，ASCII 字符继续使用一个字节编码，阿拉伯文、希腊文等使用两个字节编码，常用汉字使用三个字节编码，等等。 因此，我们说，UTF-8 是 Unicode 的实现方式之一，其他实现方式还包括 UTF-16（字符用两个或四个字节表示）和 UTF-32（字符用四个字节表示）。 Python的默认编码Python2 的默认编码是 ascii，Python3 的默认编码是 utf-8，可以通过下面的方式获取： Python2 123456Python 2.7.12 (default, Oct 11 2016, 05:20:59)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()'ascii' Python3 123456Python 3.5.2 (default, Oct 11 2016, 04:59:56)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()'utf-8' Python3编码问题Python3 最重要的一项改进之一就是解决了 Python2 中字符串与字符编码遗留下来的这个大坑。 Python2 字符串设计上的一些缺陷： 使用 ASCII 码作为默认编码方式，对中文处理很不友好。 把字符串的牵强地分为 unicode 和 str 两种类型，误导开发者 首先，Python3 把系统默认编码设置为 UTF-8，然后，文本字符和二进制数据区分得更清晰，分别用 str 和 bytes 表示。文本字符全部用 str 类型表示，str 能表示 Unicode 字符集中所有字符，而二进制字节数据用一种全新的数据类型，用 bytes 来表示。 str12345678910&gt;&gt;&gt; a = "a"&gt;&gt;&gt; a'a'&gt;&gt;&gt; type(a)&lt;class 'str'&gt;&gt;&gt;&gt; b = "禅"&gt;&gt;&gt; b'禅'&gt;&gt;&gt; type(b)&lt;class 'str'&gt; bytesPython3 中，在字符引号前加‘b’，明确表示这是一个 bytes 类型的对象，实际上它就是一组二进制字节序列组成的数据，bytes 类型可以是 ASCII范围内的字符和其它十六进制形式的字符数据，但不能用中文等非ASCII字符表示。 12345678910111213&gt;&gt;&gt; c = b'a'&gt;&gt;&gt; cb'a'&gt;&gt;&gt; type(c)&lt;class 'bytes'&gt;&gt;&gt;&gt; d = b'\xe7\xa6\x85'&gt;&gt;&gt; db'\xe7\xa6\x85'&gt;&gt;&gt; type(d)&lt;class 'bytes'&gt;&gt;&gt;&gt; e = b'禅' File "&lt;stdin&gt;", line 1SyntaxError: bytes can only contain ASCII literal characters. bytes 类型提供的操作和 str 一样，支持分片、索引、基本数值运算等操作。但是 str 与 bytes 类型的数据不能执行 + 操作，尽管在py2中是可行的。 123456789101112&gt;&gt;&gt; b'a'+b'c'b'ac'&gt;&gt;&gt; b'a'*2b'aa'&gt;&gt;&gt; b"abcdef\xd6"[1:]b'bcdef\xd6'&gt;&gt;&gt; b"abcdef\xd6"[-1]214&gt;&gt;&gt; b"a" + "b"Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: can't concat bytes to str python2 与 python3 字节与字符的对应关系 python2 python3 表现 转换 作用 str bytes 字节 encode 存储 unicode str 字符 decode 显示 encode与decodestr 与 bytes 之间的转换可以用 encode 和 decode 方法。 encode 负责字符到字节的编码转换。默认使用 UTF-8 编码转换。 12345&gt;&gt;&gt; s = "Python之禅"&gt;&gt;&gt; s.encode()b'Python\xe4\xb9\x8b\xe7\xa6\x85'&gt;&gt;&gt; s.encode('gbk')b'Python\xd6\xae\xec\xf8' decode 负责字节到字符的解码转换，通用使用 UTF-8 编码格式进行转换。 1234&gt;&gt;&gt; b'Python\xe4\xb9\x8b\xe7\xa6\x85'.decode()'Python之禅'&gt;&gt;&gt; b'Python\xd6\xae\xec\xf8'.decode("gbk")'Python之禅']]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Unicode，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy安装及调试]]></title>
    <url>%2F2017%2F04%2F11%2Fscrapy-install%2F</url>
    <content type="text"><![CDATA[环境搭建 12$ mkvirtualenv article_spider$ pip install -i https://pypi.douban.com/simple/ scrapy 创建项目12345678910111213$ workon article_spider$ scrapy startproject ArticleSpiderNew Scrapy project 'ArticleSpider', using template directory '/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/templates/project', created in: /Users/lawtech/PycharmProjects/ArticleSpiderYou can start your first spider with: cd ArticleSpider scrapy genspider example example.com $ cd ArticleSpider$ scrapy genspider jobbole blog.jobbole.comCreated spider 'jobbole' using template 'basic' in module: ArticleSpider.spiders.jobbole 项目目录介绍首先先要回答一个问题。 问：把网站装进爬虫里，总共分几步？答案很简单，四步： 新建项目 (Project)：新建一个新的爬虫项目 明确目标（Items）：明确你想要抓取的目标 制作爬虫（Spider）：制作爬虫开始爬取网页 存储内容（Pipeline）：设计管道存储爬取内容 创建好AticleSpider项目之后，可以看到将会创建一个AticleSpider文件夹，目录结构如下： 12345678910ArticleSpider/ scrapy.cfg ArticleSpider/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py jobbole.py 下面来简单介绍一下各个文件的作用： scrapy.cfg：项目的配置文件 ArticleSpider/：项目的 Python 模块，将会从这里引用代码 ArticleSpider/items.py：项目的 items 文件 ArticleSpider/pipelines.py：项目的 pipelines 文件 ArticleSpider/settings.py：项目的设置文件 ArticleSpider/spiders/：存储爬虫的目录 pycharm 调试 scrapy 执行流程在项目根目录下新建 main.py 文件，添加如下代码 123456789from scrapy.cmdline import executeimport sysimport os# __file__ 表示当前py文件sys.path.append(os.path.dirname(os.path.abspath(__file__)))# 将实际命令拆分execute(["scrapy", "crawl", "spiders文件夹下的py文件名称"])]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Xpath教程]]></title>
    <url>%2F2017%2F04%2F11%2Fxpath-usage%2F</url>
    <content type="text"><![CDATA[XPath的用法XPath简介 XPath 使用路径表达式在 XML / HTML 文档中进行导航 XPath 包含一个标准函数库 XPath 是 XSLT 中的主要元素 XPath 是一个 W3C 标准 XPath术语节点（Node）在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。XML 文档是被作为节点树来对待的。树的根被称为文档节点或者根节点。 请看下面这个 XML 文档： 123456789101112&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang="en"&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 上面的XML文档中的节点例子： 123&lt;bookstore&gt; （文档节点）&lt;author&gt;J K. Rowling&lt;/author&gt; （元素节点）lang="en" （属性节点） 基本值（或称原子值，Atomic value）基本值是无父或无子的节点。 基本值的例子： 12J K. Rowling"en" 项目（Item）项目是基本值或者节点。 节点关系参考示例： 12345678910&lt;bookstore&gt;&lt;book&gt; &lt;title&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 父节点（Parent） 每个元素以及属性都有一个父节点。 在上面的例子中，book 元素是 title、author、year 以及 price 元素的父节点 子节点（Children） 元素节点可有零个、一个或多个子节点。 在上面的例子中，title、author、year 以及 price 元素都是 book 元素的子节点 同胞节点（Sibling） 拥有相同的父节点的节点 在上面的例子中，title、author、year 以及 price 元素都是同胞节点 先辈节点（Ancestor） 某节点的父节点、父节点的父节点，等等。 在上面的例子中，title 元素的先辈节点是 book 元素和 bookstore 元素 后代节点（Descendant） 某个节点的子节点，子节点的子节点，等等。 在上面的例子中，bookstore 的后代节点是 book、title、author、year 以及 price 元素 XPath语法参考示例： 123456789101112131415&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang="eng"&gt;Harry Potter&lt;/title&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;book&gt; &lt;title lang="eng"&gt;Learning XML&lt;/title&gt; &lt;price&gt;39.95&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 选取节点XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。下面列出了最有用的路径表达式： 表达式 描述 nodename 选取此节点的所有子节点 / 从根节点选取 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 . 选取当前节点 .. 选取当前节点的父节点。 @ 选取属性。 实例： 路径表达式 结果 bookstore 选取bookstore元素的所有子节点 /bookstore 选取根元素bookstore 注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ bookstore/book 选取属于bookstore的子元素的所有book元素 //book 选取所有book元素，而不考虑它们的位置 bookstore//book 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置 //@lang 选取名为 lang 的所有属性 谓语（Predicates）谓语用来查找某个特定的节点或者包含某个指定的值的节点。 谓语被嵌在方括号中。 实例 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 路径表达式 结果 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素 /bookstore/book[postion()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素 //title[@lang=’eng’] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性 /bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00 /bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00 选取未知节点XPath 通配符可用来选取未知的 XML 元素 通配符 描述 * 匹配任何元素节点 @* 匹配任何属性节点 node() 匹配任何节点 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 /bookstore/* 选取 bookstore 元素的所有子元素 //* 选取文档中的所有元素 //title[@*] 选取所有带有属性的 title 元素 选取若干路径通过在路径表达式中使用“|”运算符，您可以选取若干个路径。 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 //book/title &#124; //book/price 选取 book 元素的所有 title 和 price 元素 //title &#124; //price 选取文档中的所有 title 和 price 元素 /bookstore/book/title &#124; //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，XPath，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——Python正则表达式]]></title>
    <url>%2F2017%2F04%2F11%2Fregex%2F</url>
    <content type="text"><![CDATA[Python正则表达式简介正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。 re 模块使 Python 语言拥有全部的正则表达式功能。 compile 函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。 re 模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串做为它们的第一个参数。 正则表达式模式（常用）模式字符串使用特殊的语法来表示一个正则表达式：字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。多数字母和数字前加一个反斜杠时会拥有不同的含义。标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。反斜杠本身需要使用反斜杠转义。由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’/t’，等价于’//t’)匹配相应的特殊字符。 下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。 模式 描述 ^ 匹配字符串的开头 $ 匹配字符串的末尾 . 匹配任意字符，除了换行符”\n”，当re.DOTALL标记被指定时，可以匹配包含换行符的任意字符 […] 用来表示一组字符，单独列出：[amk]匹配’a’，’m’或’k’ [^…] 不在[]中的字符： [ ^abc ]匹配除了a,b,c之外的字符 re* 匹配0个或多个的表达式 re+ 匹配1个或多个的表达式 re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 re{ n,} 精确匹配n个前面表达式 re{n,m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a &#124; b 匹配a或b (re) 匹配括号内的表达式，也表示一个组 (?imx) 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域 (?-imx) 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域 \w 匹配字母数字及下划线 \W 匹配非字母数字及下划线 \s 匹配任意空白字符，等价于 [\t\n\r\f]. \S 匹配任意非空字符 \d 匹配任意数字，等价于 [0-9]. \D 匹配任意非数字 \A 匹配字符串开始 \Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串 \z 匹配字符串结束 \G 匹配最后匹配完成的位置 \b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’ \B 匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’ \n,\t,等 匹配一个换行符。匹配一个制表符。等 \1…\9 匹配第n个分组的子表达式。 \10 匹配第n个分组的子表达式，如果它经匹配。否则指的是八进制字符码的表达式 正则表达式实例字符匹配 实例 描述 python 匹配”python” 字符类 实例 描述 [Pp]ython 匹配 “Python” 或 “python” rub[ye] 匹配 “ruby” 或 “rube” [lawtech] 匹配中括号内的任意一个字母 [0-9] 匹配任何数字。类似于 [0123456789] [a-z] 匹配任何小写字母 [A-Z] 匹配任何大写字母 [a-zA-Z0-9] 匹配任何字母及数字 [^lawtech] 除了lawtech字母以外的所有字符 [^0-9] 匹配除了数字外的字符 特殊字符类 实例 描述 . 匹配除 “\n” 之外的任何单个字符，要匹配包括 ‘\n’ 在内的任何字符，请使用像’[.\n]’ 的模式 \d 匹配一个数字字符，等价于 [0-9] \D 匹配一个非数字字符，等价于 [ ^0-9 ] \s 匹配任何空白字符，包括空格、制表符、换页符等等，等价于[\f\n\r\t\v] \S 匹配任何非空白字符。等价于 [ ^\f\n\r\t\v ] \w 匹配包括下划线的任何单词字符，等价于[A-Za-z0-9_] \W 匹配任何非单词字符等价于 [ ^A-Za-z0-9_ ] 正则表达式修饰符 - 可选标志正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志： 修饰符 描述 全拼 re.I 使匹配对大小写不敏感 IGNORECASE re.L 做本地化识别（locale-aware）匹配 LOCALE re.M 多行匹配，影响 ^ 和 $ MULTILINE re.S 使 . 匹配包括换行在内的所有字符 DOTALL re.U 根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B. UNICODE re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 VERBOSE re模块能够处理正则表达式的操作生成正则表达式对象 生成正则表达式对象 compile(pattern, flags=0)构建一个正则表达式，返回该正则表达式对象 12import repattern = re.compile('re') 进行匹配 match() 确定正则表达式是否匹配字符串的开头 search() 扫描字符串以查找匹配 findall() 找到所有正则表达式匹配的子字符串，并把它们作为一个列表返回 finditer() 找到所有正则表达式匹配的子字符串，并把它们以迭代器的形式返回 group() 返回通过正则表达式匹配到的字符串 start() 返回成功匹配开始位置 end() 返回成功匹配结束位置 span() 返回包含成功匹配开始和结束位置的元组 re.match函数re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回None。 函数语法： 1re.match(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 匹配成功re.match方法返回一个匹配的对象（match object），否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组 groups() 返回一个包含所有小组字符串的元组，从1到所含的小组号 实例1： 12345# -*- coding: UTF-8 -*- import reprint(re.match('www', 'www.lawtech0902.com').span()) # 在起始位置匹配print(re.match('com', 'www.lawtech0902.com')) # 不在起始位置匹配 运行结果： 12(0, 3)None 实例2： 12345678910111213# -*- coding: UTF-8 -*- import reline = "Cats are smarter than dogs"matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)if matchObj: print("matchObj.group() : ", matchObj.group()) print("matchObj.group(1) : ", matchObj.group(1)) print("matchObj.group(2) : ", matchObj.group(2))else: print("No match!!") 运行结果： 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter re.search方法re.search 扫描整个字符串并返回第一个成功的匹配。 函数语法： 1re.search(pattern, string, flags=0) 函数参数说明与re.match方法相同。 匹配成功re.search方法返回一个匹配的对象，否则返回None。 实例1： 12345# -*- coding: UTF-8 -*- import reprint(re.search('www', 'www.lawtech0902.com').span()) # 在起始位置匹配print(re.search('com', 'www.lawtech0902.com').span()) # 不在起始位置匹配 运行结果： 12(0, 3)(16, 19) 实例2： 12345678910111213# -*- coding: UTF-8 -*- import reline = "Cats are smarter than dogs"searchObj = re.search( r'(.*) are (.*?) .*', line, re.M|re.I)if searchObj: print("searchObj.group() : ", searchObj.group()) print("searchObj.group(1) : ", searchObj.group(1)) print("searchObj.group(2) : ", searchObj.group(2))else: print("Nothing found!!") 运行结果： 123searchObj.group() : Cats are smarter than dogssearchObj.group(1) : CatssearchObj.group(2) : smarter re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 实例： 12345678910111213141516# -*- coding: UTF-8 -*- import reline = "Cats are smarter than dogs"matchObj = re.match( r'dogs', line, re.M|re.I)if matchObj: print("match --&gt; matchObj.group() : ", matchObj.group())else: print("No match!!")matchObj = re.search( r'dogs', line, re.M|re.I)if matchObj: print("search --&gt; matchObj.group() : ", matchObj.group())else: print("No match!!") 运行结果： 12No match!!search --&gt; matchObj.group() : dogs 检索和替换Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。 语法： 1re.sub(pattern, repl, string, count=0, flags=0) 参数： pattern : 正则中的模式字符串。 repl : 替换的字符串，也可为一个函数。 string : 要被查找替换的原始字符串。 count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 实例： 12345678910111213# -*- coding: UTF-8 -*-import rephone = "2004-959-559 # 这是一个国外电话号码"# 删除字符串中的 Python注释 num = re.sub(r'#.*$', "", phone)print("电话号码是: ", num)# 删除非数字(-)的字符串 num = re.sub(r'\D', "", phone)print("电话号码是 : ", num) 运行结果： 12电话号码是: 2004-959-559电话号码是 : 2004959559 repl参数是一个函数以下实例中将字符串中的匹配的数字乘于 2： 1234567891011# -*- coding: UTF-8 -*-import re# 将匹配的数字乘于 2def double(matched): value = int(matched.group('value')) return str(value * 2)s = 'A23G4HFD567'print(re.sub('(?P&lt;value&gt;\d+)', double, s)) 运行结果为： 1A46G8HFD1134]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy，Regular Expression，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python分布式爬虫打造搜索引擎项目学习笔记——爬虫基础知识回顾]]></title>
    <url>%2F2017%2F04%2F11%2Fscrapy-project-basic%2F</url>
    <content type="text"><![CDATA[基础知识技术选型：scrapy vs requests + beautifulsoup requests和beautifulsoup都是库，scrapy是框架 scrapy框架中可以加入requests和beautifulsoup scrapy基于twisted，性能是最大的优势 scrapy方便扩展，提供了很多内置的功能 scrapy内置的css和xpath selector非常方便，beautifulsoup最大的缺点就是慢 网页分类 静态网页 动态网页 webservice（restapi） 爬虫能做什么爬虫的作用 搜索引擎——百度、谷歌、垂直领域搜索引擎 推荐引擎——今日头条 机器学习的数据样本 数据分析、舆情分析等 正则表达式详见另一篇博客 网站url结构 深度优先遍历和广度优先遍历123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# _*_ coding: utf-8 _*_"""__author__ = 'lawtech'__date__ = '2017/4/10 下午1:41'""""""深度优先遍历和广度优先遍历"""class Graph(object): def __init__(self, *args, **kwargs): self.node_neighbors = &#123;&#125; self.visited = &#123;&#125; def add_nodes(self, nodelist): for node in nodelist: self.add_node(node) def add_node(self, node): if node not in self.nodes(): self.node_neighbors[node] = [] def add_edge(self, edge): u, v = edge if (v not in self.node_neighbors[u]) and (u not in self.node_neighbors[v]): self.node_neighbors[u].append(v) if u != v: self.node_neighbors[v].append(u) def nodes(self): return self.node_neighbors.keys() def depth_first_search(self, root=None): """ 队列 :param root: :return: """ order = [] def dfs(node): self.visited[node] = True order.append(node) for n in self.node_neighbors[node]: if n not in self.visited: dfs(n) if root: dfs(root) for node in self.nodes(): if node not in self.visited: dfs(node) print(order) return order def breadth_first_search(self, root=None): """ 递归 :param root: :return: """ queue = [] order = [] def bfs(): while len(queue) &gt; 0: node = queue.pop(0) self.visited[node] = True for n in self.node_neighbors[node]: if (n not in self.visited) and (n not in queue): queue.append(n) order.append(n) if root: queue.append(root) order.append(root) bfs() for node in self.nodes(): if node not in self.visited: queue.append(node) order.append(node) bfs() print(order) return orderif __name__ == '__main__': g = Graph() g.add_nodes(nodelist=[i + 1 for i in range(8)]) g.add_edge((1, 2)) g.add_edge((1, 3)) g.add_edge((2, 4)) g.add_edge((2, 5)) g.add_edge((4, 8)) g.add_edge((5, 8)) g.add_edge((3, 6)) g.add_edge((3, 7)) g.add_edge((6, 7)) print("nodes:", g.nodes()) g.breadth_first_search(1) g.depth_first_search(1) 运行结果： ​ (‘nodes:’, [1, 2, 3, 4, 5, 6, 7, 8]) ​ [1, 2, 3, 4, 5, 6, 7, 8] ​ [1, 2, 4, 8, 5, 3, 6, 7] 爬虫去重策略 将访问过的url保存到数据库中 将访问过的url保存到set中，只需要O(1)的代价就可以查询url url经过md5等方法哈希后保存到set中 用bitmap方法，将访问过的url通过hash函数映射到某一位 bloomfilter方法对bitmap进行改进，多重hash函数降低冲突 Python字符串编码详见另一篇博客]]></content>
      <categories>
        <category>Basic knowledge</category>
      </categories>
      <tags>
        <tag>Scrapy，Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记(五)：数据安全与性能保障——复制]]></title>
    <url>%2F2017%2F04%2F10%2FRedis-5%2F</url>
    <content type="text"><![CDATA[复制（replication），可以让其他服务器拥有一个不断更新的数据副本，从而使得拥有数据副本的服务器可以用于处理客户端发送的读请求。对于高负载应用来说，复制是不可或缺的一个特性。 关系型数据库通常会使用一个主服务器（master）向多个从服务器（slave）发送更新，并使用从服务器来处理所有读请求。Redis也采用了同样的方法来实现自己的复制特性，并将其用作扩展性能的一种手段。 复制相关配置选项当从服务器连接主服务器时，主服务器会执行BGSAVE操作，为了正确地使用复制特性，用户需要保证服务器已经正确地设置了dir选项和dirname选项。 配置slaveof host port选项即可连接主服务器。 下面将演示怎么实现一个简单的复制系统。我们在一台机器上起两个Redis实例，监听不同的端口，其中一个作为主库，另外一个作为从库。首先不加任何参数来启动一个Redis实例作为主数据库： 主库默认监听6379端口。 接着新建一个终端，加上slaveof参数启动另一个Redis实例作为从库，并且监听6380端口： 从控制台输出中可以看到，从库已经连接到主库：126.0.0.1:6379了，我们可以分别在主库和从库中使用info replication命令看一看当前实例在复制系统中的相关信息 现在可以测试一下主从库的数据同步了： 12345678$ redis-cli -p 6379127.0.0.1:6379&gt; set test-replicate lawtechOK$ redis-cli -p 6380127.0.0.1:6380&gt; get test-replicate"lawtech"127.0.0.1:6380&gt; set x y(error) READONLY You can't write against a read only slave. 可以看到，在主库中添加的数据确实同步到了从库中。但是，我们在向从库中写入数据时报错了，这是因为在默认情况下，从库是只读的。我们可以在从库的配置文件中加上如下的配置项允许从库写数据： 1slave-read-only no 但是，因为从库中修改的数据不会被同步到任何其他数据库，并且一旦主库修改了数据，从库的数据就会因为自动同步被覆盖，所以一般情况下，不建议将从库设置为可写。 相同的道理，配置多台从库也使用相同的方法，都在从库的配置文件中加上slaveof参数即可。 此外，我们可以在客户端使用命令 1SLAVEOF 新主库地址 新主库端口 来修改当前数据库的主库，如果当前数据库已经是其他库的从库， 则当前数据库会停止和原来的数据库的同步而和新的数据库同步。 最后，从数据库还可以通过运行命令： 1SLAVEOF NO ONE 来停止接受来自其他数据库的同步而升级成为主库。 Redis复制的启动过程从服务器连接主服务器时，主服务器会创建一个快照文件并将其发送至从服务器，但这只是主从复制执行过程的其中一步，下表列举出复制过程主从服务器执行的所有操作： 步骤 主服务器操作 从服务器操作 1 （ 等待命令进入） 连接(或者重连接)主服务器，发送SYNC命令 2 开始执行BGSAVE，并使用缓冲区记录BGSAVE之后执行的所有写命令 根据配置选项来决定时继续使用现在的数据来处理客户端命令，还是向发送请求的客户端返回错误 3 BGSAVE执行完毕，向从服务器发送快照文件，并在发送期间继续使用缓冲区记录被执行的写命令 丢弃所有旧的数据，开始载入主服务器发来的快照文件 4 快照文件发送完毕，开始向从服务器发送存储在缓冲区里面的写命令 完成对快照文件的解释操作，像往常一样开始接受命令请求 5 缓冲区存储的写命令发送完毕；从现在开始，没执行一个写命令，就像从服务器发送相同的写命令 执行主服务器发来的所有存储在缓冲区里面的写命令；从现在开始，接收并执行主服务器传来的每个写命令 由上述步骤可以看出，有必要给Redis主服务器留30%~45%的内存用于执行BGSAVE命令和创建记录写命令的缓冲区。另外，从服务器还有一点需要注意的是，从服务器在进行同步时，会清空自己的所有数据，因为第3步中，从服务器会丢弃所有旧数据。 警告：Redis不支持主主复制（master-master replication） 当多个从服务器尝试连接同一个主服务器的时候，就会出现下表所示的两种情况中的其中一种： 当有新的从服务器连接主服务器时 主服务器的操作 上述步骤3尚未执行 所有从服务器都会接收相同的快照文件和相同的缓冲区写命令 上述步骤3正在执行或者已经执行 当主服务器与较早进行连接的从服务器执行完复制所需的5个步骤之后，主服务器会与新连接的从服务器执行一次新的步骤1至步骤5 由此可以看出多个从服务器的同步对网络的开销挺大的，有可能会影响到主服务器接收写命令，甚至是与主服务器位于同一网络中的其他硬件。 主从链创建多个从服务器可能造成网络不可用，此时可以使用另外一个解决方案，从服务器拥有自己的从服务器，并由此形成主从链（master/slave chaining）。 从服务器对从服务器进行复制在操作上和从服务器对主服务器进行复制的唯一区别在于。如果从服务器X拥有从服务器Y，那么当从服务器X在执行启动过程表中步骤4时，X将断开与Y的连接，导致Y需要重新连接并重新同步（resync）。 当读请求的重要性明显高于写请求的重要性，并且读请求的数量需求远远超出一台Redis服务器可以处理的范围时，用户就需要添加新的从服务器来处理读请求，随着负载不断上升，主服务器可能会无法快速地更新所有从服务器。 为了缓解这个问题，可以创建一个由Redis主/从节点(master/slave node)组成的中间层来分担主服务器的复制工作，如下图所示： 上面这个示例中，树的中层有3个帮助开展复制工作的服务器，底层有9个从服务器。其中，只有3台从服务器和主服务器通信，其他都向从服务器同步数据，从而降低了系统的负载。 检验硬盘写入为了将数据保存在多台机器中，用户首先需要为主服务器设置多个从服务器，然后对每个从服务器设置appendonly yes选项和appendfsync everysec选项（如有需要，也可以对主服务器这样设置），但这只是第一步：因为用户还需要等待主服务器发送的写命令到达从服务器，并且在执行后续操作前，检查数据是否已经被写入了硬盘中。 整个操作分两个环节： 验证主服务器是否已经将写数据发送至从服务器：用户需要在向主服务器写入真正的数据之后，再向主服务器写入一个唯一的虚构值（unique dummy value），然后通过检查虚构值是否存在于从服务器来判断数据是否已经到达从服务器。 判断数据是否已经被保存到硬盘中：检查INFO命令的输出结果中aof_pending_bio_fsync属性的值是否为0，如果是，则数据已经被保存到了硬盘中。 在向主服务器写入数据后，用户可以将主服务器和从服务器的连接作为参数调用下面的代码来自动进行上述操作： 1234567891011121314151617181920212223242526272829# _*_ coding: utf-8 _*_import uuidimport timedef wait_for_sync(mconn, sconn): identifier = str(uuid.uuid4()) # 将令牌添加至主服务器 mconn.zadd('sync:wait', identifier, time.time()) # 如果有必要的话，等待从服务器完成同步 while not sconn.info()['master_link_status'] != 'up': time.sleep(.001) # 等待从服务器接收数据更新 while not sconn.zscore('sync:wait', identifier): time.sleep(.001) # 最多只等待1秒 deadline = time.time() + 1.01 # 检查数据更新是否已经被同步到了硬盘 while time.time() &lt; deadline: if sconn.info()['aof_pending_bio_fsync'] == 0: break time.sleep(.001) # 清理刚刚创建的新令牌以及之前可能留下的旧令牌 mconn.zrem('sync:wait', identifier) mconn.zremrangebyscore('sync:wait', 0, time.time() - 900) 为了确保操作可以正确执行，wait_for_sync()函数会首先确认从服务器已经连接上主服务器，然后检查自己添加到等待同步有序集合（sync wait ZSET）里面的值是否已经存在于从服务器，在发现值存在后，等待从服务器将缓冲区的所有数据写入硬盘里。最后，确认数据已经被保存到硬盘之后，函数会执行一些清理操作。 通过同时使用复制和AOF持久化，用户可以增强Redis对于系统崩溃的抵抗能力。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis, Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记(四)：数据安全与性能保障——持久化]]></title>
    <url>%2F2017%2F04%2F09%2FRedis-4%2F</url>
    <content type="text"><![CDATA[什么是持久化？持久化（Persistence），即把数据（如内存中的对象）保存到可永久保存的存储设备中（如磁盘）。持久化的主要应用是将内存中的对象存储在数据库中，或者存储在磁盘文件中、XML数据文件中等等。 持久化是将程序数据在持久状态和瞬时状态间转换的机制。 JDBC就是一种持久化机制。文件IO也是一种持久化机制。 我们这样理解：在一定周期内保持不变就是持久化，持久化是针对时间来说的。数据库中的数据就是持久化了的数据，只要你不去删除或修改。 持久化选项Redis提供了两种不同的持久化方法来将数据存储到硬盘中，保证数据在Redis重启后仍然存在： RDB持久化：在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot），也就是将存在于某一时刻的所有数据都写入硬盘里面，所以也叫作快照持久化。 AOF持久化：全称是 append-only file（只追加文件）， 它记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 这两种持久化的方式既可以单独使用，也可以同时使用，具体选择哪种方式需要根据用户的数据及应用进行选择。 下面的代码示例展示了Redis对于两种持久化选项提供的配置选项 1234567891011121314# 快照持久化选项save 60 1000 # 60秒内有1000次写入操作的时候执行快照的创建stop-writes-on-bgsave-error no # 创建快照失败的时候是否仍然继续执行写命令rdbcompression yes # 是否对快照文件进行压缩dbfilename dump.rdb # 如何命名硬盘上的快照文件dir ./ # 快照所保存的位置# AOF持久化选项appendonly no # 是否使用AOF持久化appendfsync everysec # 多久才将写入的内容同步到硬盘no-appendfsync-on-rewrite no # 在对AOF进行压缩(compaction)的时候能否执行同步操作auto-aof-rewrite-percentage 100 # 多久执行一次AOF压缩auto-aof-rewrite-min-size 64mb # 多久执行一次AOF压缩dir ./ # AOF所保存的位置 快照持久化（RDB）创建快照的办法 客户端通过向Redis发送BGSAVE命令来创建快照。 如果平台支持（除了Windows），那么Redis会调用fork来创建一个子进程，然后子进程负责将快照写到硬盘中，而父进程则继续处理命令请求。 使用场景： 如果用户使用了save设置，例如：save 60 1000 ,那么从Redis最近一次创建快照之后开始计算，当“60秒之内有1000次写入操作”这个条件满足的时候，Redis就会自动触发BGSAVE命令。 如果用户使用了多个save设置，那么当任意一个save配置满足条件的时候，Redis都会触发一次BGSAVE命令。 客户端通过向Redis发SAVE命令来创建快照。 接收到SAVE命令的Redis服务器在快照创建完毕之前将不再响应任何其他命令的请求。SAVE命令并不常用，我们通常只在没有足够的内存去执行BGSAVE命令的时候才会使用SAVE命令，或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令。 使用场景： 当Redis通过SHUTDOWN命令接收到关闭服务器的请求时，或者接收到标准的TERM信号时，会执行一次SAVE命令，阻塞所有的客户端，不再执行客户端发送的任何命令，并且在执行完SAVE命令之后关闭服务器。 优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次快照文件，并且在每个月的每一天，也备份一个快照文件。 这样的话，即使遇到问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心。 RDB 可以最大化 Redis 的性能：父进程在保存快照文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 缺点 如果在新的快照文件创建好之前，Redis、系统、硬件三者中任意一个发生崩溃，那么Redis将丢失最近一次创建快照之后写入的所有数据。如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork()可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOF持久化AOF持久化会将被执行的写命令写到AOF文件的末尾，以此来记录数据发生的变化。这样，我们在恢复数据的时候，只需要从头到尾的执行一下AOF文件即可恢复数据。 我们可以通过appendonly yes命令来打开AOF持久化选项 appendfsync同步频率下表展示了appendfsync选项对AOF文件的同步频率的影响 选项 同步频率 always 每个Redis写命令都要同步写入硬盘。这样做会严重降低Redis的速度 everysec 每秒执行一次同步，显示地将多个写命令同步到硬盘 no 让操作系统来决定应该何时进行同步 always的方式固然可以对没一条数据进行很好的保存，但是这种同步策略需要对硬盘进行大量的写操作，所以Redis处理命令的速度会受到硬盘性能的限制。 普通的硬盘每秒钟只能处理大约200个写命令，使用固态硬盘SSD每秒可以处理几万个写命令，但是每次只写一个命令，这种只能怪不断地写入很少量的数据的做法有可能引发严重的写入放大问题，这种情况下降严重影响固态硬盘的使用寿命。 everysec的方式，Redis以每秒一次的频率大队AOF文件进行同步。这样的话既可以兼顾数据安全也可以兼顾写入性能。 Redis以每秒同步一次AOF文件的性能和不使用任何持久化特性时的性能相差无几，使用每秒更新一次 的方式，可以保证，即使出现故障，丢失的数据也在一秒之内产生的数据。 no的方式，Redis将不对AOF文件执行任何显示的同步操作，而是由操作系统来决定应该何时对AOF文件进行同步。 这个命令一般不会对Redis的性能造成多大的影响，但是当系统出现故障的时候使用这种选项的Redis服务器丢失不定数量的数据。 另外，当用户的硬盘处理写入操作的速度不够快的话，那么缓冲区被等待写入硬盘的数据填满时，Redis的写入操作将被阻塞，并导致Redis处理命令请求的速度变慢，因为这个原因，一般不推荐使用这个选项。 重写/压缩AOF文件随着数据量的增大，AOF的文件可能会很大，这样在每次进行数据恢复的时候就会进行很长的时间，为了解决日益增大的AOF文件，用户可以向Redis发送BGREWRITEAOF命令，这个命令会通过移除AOF文件中的冗余命令来重写AOF文件，是AOF文件的体积变得尽可能的小。 BGREWRITEAOF的工作原理和BGSAVE的原理很像：Redis会创建一个子进程，然后由子进程负责对AOF文件的重写操作。 因为AOF文件重写的时候会创建子进程，所以快照持久化因为创建子进程而导致的性能和内存占用问题同样会出现在AOF文件重写的时候。 跟快照持久化通过save选项来自动执行BGSAVE一样，AOF通过设置auto-aof-rewrite-percentage和auto-aof-rewrite-min-size选项来自动执行BGREWRITEAOF。 如下配置 12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 表示当前AOF的文件体积大于64MB，并且AOF文件的体积比上一次重写之后的体积变大了至少一倍（100%）的时候，Redis将执行重写BGREWRITEAOF命令。 如果AOF重写执行的过于频繁的话，可以将auto-aof-rewrite-percentage选项的值设置为100以上，这种最偶发就可以让Redis在AOF文件的体积变得更大之后才执行重写操作，不过，这也使得在进行数据恢复的时候执行的时间变得更加长一些。 优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 如何选择RDB和AOF？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 RDB 和 AOF 之间的相互作用BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE 。 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户，BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis, Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MxOnline项目学习总结]]></title>
    <url>%2F2017%2F04%2F07%2FMxOnline-summary%2F</url>
    <content type="text"><![CDATA[拖拖拉拉地学完了imooc的”强力django+杀手级xadmin打造上线标准的在线教育平台”课程，记录一下每一章学习的内容概要。 MxOnline项目学习总结第一、二章 课程介绍 开发环境搭建 第三章 django基础回顾 settings.py 设置 urls.py 配置 models.py 设计 views.py 编码 templates 模板编码 第四章 数据库设计 users app model 设计 organization app model 设计 course app model 设计 operation app model 设计 第五章 后台管理系统开发 django admin 介绍 xadmin 安装和 model 注册 xadmin 全局配置 第六章 登录、注册、找回密码 登录（ session 和 cookie 机制） 注册（ form 表单提交、图片验证码，发送邮件 ） 找回密码（邮件发送） 第七章 课程机构功能实现 机构列表（分页，筛选、排序） 机构详情页（收藏，富文本展示） 咨询提交（ modelform 验证和保存） 第八章 课程功能实现 课程列表（分页、排序） 课程详情页（收藏，章节展示、资源展示、评论） 第九章 讲师功能实现 讲师列表（分页、排序） 讲师详情（收藏） 第十章 个人中心功能实现 用户信息修改（修改密码、头像、邮箱、基本信息） 导航栏全局搜索功能 我的课程 我的收藏（删除收藏） 我的消息 第十一章 全局功能实现 全局404和500页面配置 首页开发 点击数和收藏数修改和退出功能 第十二章 常见 web 攻击 sql 注入攻击 xss 攻击 csrf 攻击 第十三章 xadmin 进阶开发 userprofile 注册和设置 xadmin 常见功能设置 inlinemodel 注册、proxy 代理注册 django ueditor 富文本编辑器继承 excel 导入插件集成]]></content>
      <categories>
        <category>Django, Python</category>
      </categories>
      <tags>
        <tag>Django, Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记(三)：Redis命令补充]]></title>
    <url>%2F2017%2F04%2F06%2FRedis-3%2F</url>
    <content type="text"><![CDATA[在之前的学习笔记中，还有许多Redis的命令没有涉及，这一篇主要用来简要地补充，当然，详细的命令还得参考Redis的官方命令文档。 键值相关命令下表展示了Redis提供的一些键值(KEY-VALUE)相关的常用命令及其redis-py API 命令 用例 描述 redis-py API KEYS KEYS pattern 查找所有符合给定模式pattern(正则表达式)的key keys(pattern=’*’) EXISTS EXISTS key 检查给定key是否存在 exists(name) EXPIRE EXPIRE key seconds 为给定key设置生存时间，当key过期时(生存时间为0)，它会被自动删除 expire(name, time) MOVE MOVE key db 将当前数据库的key移动到给定的数据库db当中 move(name, db) PERSIST PERSIST key 移除给定key的生存时间，将这个key从『易失的』(带生存时间key)转换成『持久的』(一个不带生存时间、永不过期的key) persist(name) RANDOMKEY RANDOMKEY 从当前数据库返回一个随机的key randomkey() RENAME RENAME key newkey 将key重命名为newkey，如果key与newkey相同，将返回一个错误。如果newkey已经存在，则值将被覆盖 rename(src, dst) TYPE TYPE key 返回key所存储的value的数据结构类型，它可以返回string, list, set, zset和hash等不同的类型 type(name) TTL TTL key 返回key剩余的过期时间(单位：秒) ttl(name) 下面这个交互示例展示了Redis中关于键的过期时间相关的命令的使用方法 12345678910111213&gt;&gt;&gt; r.set('key', 'value')True&gt;&gt;&gt; r.get('key')b'value'&gt;&gt;&gt; r.expire('key', 2)True&gt;&gt;&gt; time.sleep(2)&gt;&gt;&gt; r.get('key')&gt;&gt;&gt; r.set('key', 'value2')True&gt;&gt;&gt; r.expire('key', 100); r.ttl('key')True100 发布与订阅发布订阅(pub/sub)是一种消息通信模式，主要的目的是解耦消息发布者和消息订阅者之间的耦合，这点和设计模式中的观察者模式比较相似。pub/sub不仅仅解决发布者和订阅者之间代码级别耦合也解决两者在物理部署上的耦合。Redis作为一个pub/sub的server，在订阅者和发布者之间起到了消息路由的功能。订阅者可以通过subscribe和psubscribe命令向 redis server订阅自己感兴趣的消息类型，redis将消息类型称为通道(channel)。当发布者通过publish命令向 redis server发送二进制字符串消息(binary string message)时，订阅该消息类型的全部client都会收到此消息。这里消息的传递是多对多的，一个client可以订阅多个channel，也可以向多个channel发送消息。 下表展示了Redis提供的发布与订阅命令及其redis-py API 命令 用例 描述 redis-py API SUBSCRIBE SUBSCRIBE channel [channel …] 订阅给定的频道 subscribe(args, *kwargs) UNSUBSCRIBE UNSUBSCRIBE [channel [channel …]] 退订给定的频道，如果没有给定任何频道，则退订所有频道 unsubscribe(*args) PUBLISH PUBLISH channel message 将信息message发送到指定的频道channel publish(channel, message) PSUBSCRIBE PSUBSCRIBE pattern [pattern …] 订阅与给定模式相关的频道 psubscribe(args, *kwargs) PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern …]] 退订给定的模式，如果没有给定，则退订所有模式 PUNSUBSCRIBE [pattern [pattern …]] PUBSUB PUBSUB subcommand [argument [argument …]] PUBSUB命令是一个introspection命令，允许检查Pub/Sub子系统的状态，它由单独记录的子命令组成 pubsub(**kwargs) 考虑到PUBLISH命令和SUBSCRIBE命令在Python客户端的实现方式，一个比较简单的延时发布与订阅的方法，就是像如下代码那样用辅助线程(helper thread)来执行PUBLISH命令 1234567891011121314151617181920212223242526import redisimport timeimport threadingpool = redis.ConnectionPool(host='localhost', port=6379, db=0)r = redis.StrictRedis(connection_pool=pool)def publisher(n): time.sleep(1) for i in range(n): r.publish('channel', i)def run_pubsub(): threading.Thread(target=publisher, args=(3,)).start() pubsub = r.pubsub() pubsub.subscribe(['channel']) count=0 for item in pubsub.listen(): print(item) count += 1 if count == 4: pubsub.unsubscribe() if count == 5: break publisher函数在刚开始执行时会先休眠，让订阅者有足够的时间来连接服务器并监听消息。在发布消息之后进行短暂的休眠，让消息可以一条接一条地出现。 run_pubsub函数启动发送者线程，让它发送三条消息。随后创建发布与订阅对象，并让它订阅给定的频道。通过遍历函数pubsub.listen()的执行结果来监听订阅消息。在接收到一条订阅反馈消息和三条发布者发送的消息之后，执行退订操作，停止监听新消息。客户端在接收到退订反馈消息之后，就不再接收消息。 实际运行函数并观察它们的行为123456&gt;&gt;&gt; run_pubsub()&#123;'type': 'subscribe', 'channel': b'channel', 'data': 1, 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'0', 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'1', 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'2', 'pattern': None&#125;&#123;'type': 'unsubscribe', 'channel': b'channel', 'data': 0, 'pattern': None&#125; 以上这些结构就是我们在遍历pubsub.listen()函数时得到的元素。 在刚开始订阅一个频道的时候，客户端会接收到一条关于被订阅频道的反馈消息。在退订频道时，客户端会接收到一条反馈消息，告知被退订的是哪一个频道，以及客户端目前仍在订阅的频道数量。 其他命令排序Redis中负责执行排序操作的SORT命令可以根据字符串、列表、集合、有序集合、散列这5中键里面存储的数据，对列表、集合以及有序集合进行排序，可以将SORT命令看作是SQL语言中的order by子句。 下表展示了SORT命令的定义及其redis-py API 命令 用例 描述 redis-py API SORT SORT key [BY pattern][LIMIT offset count] [GET pattern][ASC\ DESC] [ALPHA] destination 返回或存储key的list、set或sorted set中的元素。默认是按照数值类型排序的，并且按照两个元素的双精度浮点数类型值进行比较 sort(name, start=None, num=None, by=None, get=None, desc=False, alpha=False, store=None, groups=False) 下面展示了SORT命令的一些简单的用法123456789101112131415161718&gt;&gt;&gt; r.rpush('sort-input', 23, 15, 110, 7)4&gt;&gt;&gt; r.sort('sort-input')[b'7', b'15', b'23', b'110']&gt;&gt;&gt; r.sort('sort-input', alpha=True)[b'110', b'15', b'23', b'7']&gt;&gt;&gt; r.hset('d-7', 'field', 5)1&gt;&gt;&gt; r.hset('d-15', 'field', 1)1&gt;&gt;&gt; r.hset('d-23', 'field', 9)1&gt;&gt;&gt; r.hset('d-110', 'field', 3)1&gt;&gt;&gt; r.sort('sort-input', by='d-*-&gt;field')[b'15', b'110', b'7', b'23']&gt;&gt;&gt; r.sort('sort-input', by='d-*-&gt;field', get='d-*-&gt;field')[b'1', b'3', b'5', b'9'] SORT命令不仅可以对列表进行排序，还可以对集合进行排序，然后返回一个列表形式的排序结果。上述代码除了展示如何使用alpha关键字(根据元素字母表顺序，默认根据大小)参数对元素进行字符串排序之外，还展示了如何基于外部数据对元素进行排序，以及如何获取并返回外部数据。 尽管SORT是Redis中唯一一个可以同时处理3种不同类型的数据的命令，但是事务同样可以让我们在一连串不间断执行的命令里面操作不同类型的数据。 基本的Redis事务Redis中的事务(transaction)是一组命令的集合。MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部执行，要么全不执行。 事务的应用非常普遍，比如银行汇款过程中A向B汇款，系统先从A的账户中将钱划走，然后向B的账户中增加相应的金额。这两个步骤必须属于同一个事务，要么全部执行，要么全不执行。 Redis的基本事务(basic transaction)需要用到MULTI和EXEC命令。在Redis中，被MULTI和EXEC命令包围的所有命令会一个接一个地执行，直到所有命令都执行完毕为止。当一个事务执行完毕后，才会处理其他客户端的命令。 Redis中执行事务的步骤：首先需要执行MULTI命令，然后输入我们想要在事务里面执行的命令，最后再执行EXEC命令。MULTI命令用于开启一个事务，它总是返回OK 。MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。另一方面，通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务。EXEC命令的回复是一个数组，数组中的每个元素都是执行事务中的命令所产生的回复。其中，回复元素的先后顺序和命令发送的先后顺序一致。当客户端处于事务状态时，所有传入的命令都会返回一个内容为QUEUED的状态回复status reply，这些被入队的命令将在EXEC命令被调用时执行。 从语义上来说，Redis事务在Python客户端中是由管道(pipeline)实现的：对连接对象调用pipeline()方法将创建一个事务，在一切正常的情况下，客户端会自动地调用MULTI命令包裹用户输入的多个命令。此外，为了减少Redis与客户端之间的通信往返次数，提升执行多个命令的性能，Python的Redis客户端会存储起事务包含的多个命令，然后在事务执行时一次性将所有命令都发送给Redis。 要展示事务执行的结果，最简单的方法就是将事务放到线程里面执行，下面这个交互示例展示了在没有使用事务的情况下，执行并行(parallel)自增操作的结果1234567891011121314151617&gt;&gt;&gt; import redis&gt;&gt;&gt; import threading&gt;&gt;&gt; import time&gt;&gt;&gt; r = redis.StrictRedis(host='localhost', port=6379, db=0)&gt;&gt;&gt; def notrans():... print(r.incr('notrans:'))... time.sleep(.1)... r.incr('notrans:', -1)...&gt;&gt;&gt; if 1:... for i in range(3):... threading.Thread(target=notrans).start()... time.sleep(.5)...213 上述代码启动了3个线程来执行没有被事务包裹的自增、休眠和自减操作，正因为没有使用事务，所以三个线程都可以在执行自减操作前，对notrans:计数器执行自增操作。 下面这个交互示例就展示了如何使用事务处理命令的并行执行问题 123456789101112131415&gt;&gt;&gt; def trans():... pipeline = r.pipeline()... pipeline.incr('trans:')... time.sleep(.1)... pipeline.incr('trans:', -1)... print(pipeline.execute()[0])...&gt;&gt;&gt; if 1:... for i in range(3):... threading.Thread(target=trans).start()... time.sleep(.5)...111 首先在trans函数中创建一个事务型(transactional)管道对象，然后先把针对’tans:’计数器的自增操作放入队列，等待100ms后再将针对’tans:’计数器的自减操作放入队列，最后执行被事务包裹的命令，并打印自增操作的执行结果。最终在执行结果中可以看到，尽管自增和自减操作之间有一段延迟时间，但通过使用事务，各个线程都可以在不被其他线程打断的情况下，执行各自队列里面的命令。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis, Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记(二)：Redis命令及其Python API]]></title>
    <url>%2F2017%2F03%2F27%2FRedis-2%2F</url>
    <content type="text"><![CDATA[redis-py简介安装在之前的学习笔记(一)中已经安装过redis-py，我的Python版本是3.5.21$ pip3 install redis 快速开始123456&gt;&gt;&gt; import redis&gt;&gt;&gt; r = redis.StrictRedis(host='localhost', port=6379, db=0)&gt;&gt;&gt; r.set('key', 'value')True&gt;&gt;&gt; r.get('key')b'value' API参考Redis的官方命令文档很好地解释了每个命令的详细信息。 redis-py公开了实现这些命令的两个客户端类。 第一，StrictRedis类试图遵守官方命令语法， 但是有些一些例外： SELECT: 没有实现，考虑到线程安全的原因。 DEL: 由于del是python语法关键字，所用delete来代替。 CONFIG GET|SET: 分开用 config_get or config_set来代替 MULTI/EXEC: 事务作为Pipeline类的其中一部分的实现。Pipeline默认保证了MULTI,EXEC声明。但是你可以指定transaction=False来禁用这一行为。 SUBSCRIBE/LISTEN:PubSub作为一个独立的类来实现发布订阅机制。 SCAN/SSCAN/HSCAN/ZSCAN:每个命令都对应一个等价的迭代器方法scan_iter/sscan_iter/hscan_iter/zscan_iter methods for this behavior。 第二，Redis类是StrictRedis的子类，提供redis-py版本向后的兼容性。 关于StrictRedis与Redis的区别：(官方推荐使用StrictRedis.) 以下几个方法在StrictRedis和Redis类中的参数顺序不同。 LREM: 在Redis类中是这样的：lrem(self, name, value, num=0)在StrictRedis类中是这样的：lrem(self, name, count, value) ZADD: 在Redis类中是这样的：zadd(‘my-key’, ‘name1’, 1.1, ‘name2’, 2.2, name3=3.3, name4=4.4)在StrictRedis中是这样的：zadd(‘my-key’, 1.1, ‘name1’, 2.2, ‘name2’, name3=3.3, name4=4.4) SETEX: 在Redis类中是这样的：setex(self, name, value, time)而在StrictRedis中是这样的：setex(self, name, time, value) 连接池 redis-py使用connection pool来管理对一个redis server的所有连接，避免每次建立、释放连接的开销。默认情况下，每个Redis实例都会依次创建并维护一个自己的连接池。我们可以直接建立一个连接池，然后传递给Redis或StrictRedis连接命令作为参数，这样就可以实现多个Redis实例共享一个连接池，以实现客户端分片，或者对连接的管理方式进行更高精度的控制。 12&gt;&gt;&gt; pool = redis.ConnectionPool(host='localhost', port=6379, db=0)&gt;&gt;&gt; r = redis.StrictRedis(connection_pool=pool) 我们也可以创建自己的Connection子类，用于控制异步框架中的套接字行为，要使用自己的连接实例化客户端类，需要创建一个连接池，将类传递给connection_class参数。 1&gt;&gt;&gt; pool = redis.ConnectionPool(connection_class=YourConnectionClass,your_arg='...', ...) 释放连接回到连接池：可以使用Redis类的reset()方法，或者使用with上下文管理语法。 解析器：解析器控制如何解析Redis-server的响应内容，redis-py提供两种方式的解析器类支持PythonParser和HiredisParser(需要单独安装)。它优先选用HiredisParser,如果不存在，则选用PythonParser. Hiredis是redis核心团队开发的一个高性能c库，能够提高10x的解析速度。 响应回调：The client class使用一系列的callbacks来完成响应到对应python类型的映射。这些响应回调，定义在 Redis client class中的RESPONSE_CALLBACKS字典中。你可以使用set_response_callback 方法来添加自定义回调类。这个方法接受两个参数：一个命令名字，一个回调类。回调类接受至少一个参数：响应内容，关键字参数作为命令调用时的参数。 线程安全性Redis客户端实例可以安全地在线程之间共享。 在内部，连接实例只在命令执行期间从连接池检索，并在执行后直接返回到池中。 命令执行过程从不修改客户端实例上的状态。 但是，有一个警告：Redis SELECT命令。 SELECT命令允许您切换连接正在使用的数据库。 该数据库保持选中，直到选择另一个或连接关闭为止。 这会创建一个问题，因为可以将连接返回到连接到不同数据库的池。 因此，redis-py不会在客户端实例上实现SELECT命令。 如果在同一应用程序中使用多个Redis数据库，则应为每个数据库创建一个单独的客户机实例（也可能是单独的连接池）。 在线程之间传递PubSub或Pipeline对象是不安全的。 Redis命令及其对应redis-py API由于Redis官方命令文档很好地解释了每个命令的详细信息，所以我这里只对最常用的Redis命令进行整理，并给出其redis-py API。 字符串下表展示了对Redis字符串执行自增和自减操作的命令及其redis-py API。 命令 用例 描述 redis-py API INCR INCR key-name 将键存储的值加1 incr(name, amount=1) DECR DECR key-name 将键存储的值减1 decr(name, amount=1) INCRBY INCRBY key-name amount 将键存储的值加整数amount incr(name, amount=1) DECRBY DECRBY key-name amount 将键存储的值减整数amount decr(name, amount=1) INCRBYFLOAT INCRBYFLOAT key-name amount 将键存储的值加浮点数amount incrbyfloat(name, amount=1.0) 在redis-py内部，使用了INCRBY和DECRBY命令来实现incr()和decr()方法，并且第二个参数amount是可选的，默认为1。 下面这个交互示例展示了Redis的INCR和DECR操作12345678910111213&gt;&gt;&gt; r.get('key')&gt;&gt;&gt; r.incr('key')1&gt;&gt;&gt; r.incr('key', 15)16&gt;&gt;&gt; r.get('key')b'16'&gt;&gt;&gt; r.decr('key', 5)11&gt;&gt;&gt; r.set('key', 13)True&gt;&gt;&gt; r.incr('key')14 当用户将一个值存储到Redis字符串中时，如果这个值可以被解释(interpet)为十进制整数或者浮点数，那么Redis会允许用户对这个字符串执行各种INCR和DECR操作。如果用户对一个不存在的键或者一个保存了空串的键执行自增或自减操作，Redis会自动将这个键的值当作是0来处理。若非上述情况，则Redis将会返回一个错误。 除了自增和自减操作，Redis还可以对字节串进行读取和写入的操作。 下表展示了Redis用来处理字符串子串和二进制位的命令及其redis-py API。 命令 用例 描述 redis-py API APPEND APPEND key-name value 将值value追加到给定键key-name当前存储的值的末尾 append(key, value) GETRANGE GETRANGE key-name start end 获取一个偏移量从start到end的子串，包含start和end getrange(key, start, end) SETRANGE SETRANGE key-name offset value 将从start开始的子串设置为给定值 setrange(name, offset, value) GETBIT GETBIT key-name offset 将字节串看作是二进制位串，并返回位串中偏移量为offset的二进制位的值 getbit(name, offset) SETBIT SETBIT key-name offset value 将字节串看作是二进制位串，并将位串中偏移量为offset的二进制位的值设为value setbit(name, offset, value) BITCOUNT BITCOUNT key-name [start end] 统计字符串被设置为1的bit数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行 bitcount(key, start=None, end=None) BITOP BITOP operation dest-key key-name [key-name …] 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 bitop(operation, dest, *keys) 在执行SETRANGE或者SETBIT命令时，如果offset比当前key对应string还要长，那这个string后面就补空字节(null)以达到offset。使用GETRANGE时超出字符串末尾的数据会被认为是空字符串，而使用GETBIT时超出字符串末尾的二进制位会被视为是0。 下面这个交互示例展示了Redis的子串操作和二进制位操作123456789101112131415161718192021222324&gt;&gt;&gt; r.append('new-string-key', 'hello ')6&gt;&gt;&gt; r.append('new-string-key', 'world!')12&gt;&gt;&gt; r.substr('new-string-key', 3, 7)b'lo wo'&gt;&gt;&gt; r.getrange('new-string-key', 3, 7)b'lo wo'&gt;&gt;&gt; r.setrange('new-string-key', 0, 'H')12&gt;&gt;&gt; r.get('new-string-key')b'Hello world!'&gt;&gt;&gt; r.setrange('new-string-key', 11, ', how are you?')25&gt;&gt;&gt; r.get('new-string-key')b'Hello world, how are you?'&gt;&gt;&gt; r.setbit('another-key', 2, 1)0&gt;&gt;&gt; r.setbit('another-key', 7, 1)0&gt;&gt;&gt; r.getbit('another-key', 1)0&gt;&gt;&gt; r.get('another-key')b'!' Redis现在的GETRANGE命令是由以前的SUBSTR命令改名而来，所以现在redis-py中两者仍然都可以使用，但是最好还是使用getrange()方法来获取子串。 列表下表展示了一些之前介绍过的常用列表命令 命令 用例 描述 redis-py API RPUSH RPUSH key value [value …] 向存于key的列表的尾部插入所有指定的值 rpush(name, *values) LPUSH LPUSH key value [value …] 将所有指定的值插入到存于key的列表的头部 lpush(name, *values) RPOP RPOP key 移除并返回key对应的list的最后一个元素 rpop(name) LPOP LPOP key 移除并返回key对应的list的第一个元素 lpop(name) LINDEX LINDEX key index 返回列表索引位置的元素 lindex(name, index) LRANGE LRANGE key start stop 返回存储在key的列表里指定范围内的元素 lrange(name, start, end) LTRIM LTRIM key start stop 修剪(trim)一个已存在的list，这样list就会只包含指定范围的指定元素 ltrim(name, start, end) 下面这个交互示例展示了Redis列表的推入和弹出操作12345678910111213141516171819202122&gt;&gt;&gt; r.rpush('list-key', 'last')1&gt;&gt;&gt; r.lpush('list-key', 'first')2&gt;&gt;&gt; r.rpush('list-key', 'new last')3&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'first', b'last', b'new last']&gt;&gt;&gt; r.lpop('list-key')b'first'&gt;&gt;&gt; r.lpop('list-key')b'last'&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'new last']&gt;&gt;&gt; r.rpush('list-key', 'a', 'b', 'c')4&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'new last', b'a', b'b', b'c']&gt;&gt;&gt; r.ltrim('list-key', 2, -1)True&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'b', b'c'] 还有几个列表命令能将元素从一个列表移动到另一个列表，或者阻塞(block)执行命令的客户端直到有其他客户端给列表添加元素为止。 下表列出了这些阻塞弹出命令以及列表之间移动元素的命令 命令 用例 描述 redis-py API BLPOP BLPOP key [key …] timeout 弹出第一个非空列表的头元素，或在timeout秒内阻塞并等待可弹出的元素出现 blpop(keys, timeout=0) BRPOP BRPOP key [key …] timeout 弹出第一个非空列表的末尾元素，或在timeout秒内阻塞并等待可弹出的元素出现 brpop(keys, timeout=0) RPOPLPUSH RPOPLPUSH source destination 原子性地返回并移除存储在source的列表的最后一个元素(列表尾部元素)， 并把该元素放入存储在destination的列表的第一个元素位置(列表头部) rpoplpush(src, dst) BRPOPLPUSH BRPOPLPUSH source destination timeout BRPOPLPUSH 是 RPOPLPUSH 的阻塞版本。 当 source 包含元素的时候，这个命令表现得跟 RPOPLPUSH 一模一样。 当 source 是空的时候，Redis将会阻塞这个连接，直到另一个客户端 push 元素进入或者达到 timeout 时限。 brpoplpush(src, dst, timeout=0) 注：原子性是指命令正在都区或者修改数据的时候，其他客户端不能读取或修改相同的数据。 下面这个交互示例展示了Redis列表的阻塞弹出命令以及元素移动命令1234567891011121314151617181920&gt;&gt;&gt; r.rpush('list', 'item1')1&gt;&gt;&gt; r.rpush('list', 'item2')2&gt;&gt;&gt; r.rpush('list2', 'item3')1&gt;&gt;&gt; r.brpoplpush('list2', 'list', 1)b'item3'&gt;&gt;&gt; r.brpoplpush('list2', 'list', 1)&gt;&gt;&gt; r.lrange('list', 0, -1)[b'item3', b'item1', b'item2']&gt;&gt;&gt; r.brpoplpush('list', 'list2', 1)b'item2'&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list', b'item3')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list', b'item1')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list2', b'item2')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1) 对于阻塞弹出命令和弹出并推入命令，最常见的用例就是消息传递(messaging)和任务队列(task queue)。 集合下表展示了一部分最常用的集合命令 命令 用例 描述 redis-py API SADD SADD key member [member …] 添加一个或多个指定的member元素到key集合中 sadd(name, *values) SREM SREM key member [member …] 在key集合中移除指定的元素 srem(name, *values) SISMEMBER SISMEMBER key member 返回成员member是否是存储的集合key的成员 sismember(name, value) SCARD SCARD key 返回集合包含元素的数量 scard(name) SMEMBERS SMEMBERS key 返回key集合所有的元素 smembers(name) SRANDMEMBER SRANDMEMBER key [count] 仅提供key参数,那么随机返回key集合中的一个元素，返回含有 count 个不同的元素的数组，对count分情况处理 srandmember(name, number=None) SPOP SPOP key [count] 从key对应集合中返回并删除一个或多个元素 spop(name) SMOVE SMOVE source destination member 将member从source集合移动到destination集合中 smove(src, dst, value) 下面这个交互示例展示了这些常用的集合命令12345678910111213141516&gt;&gt;&gt; r.sadd('set-key', 'a', 'b', 'c')3&gt;&gt;&gt; r.srem('set-key', 'c', 'd')1&gt;&gt;&gt; r.srem('set-key', 'c', 'd')0&gt;&gt;&gt; r.scard('set-key')2&gt;&gt;&gt; r.smembers('set-key')&#123;b'b', b'a'&#125;&gt;&gt;&gt; r.smove('set-key', 'set-key2', 'a')True&gt;&gt;&gt; r.smove('set-key', 'set-key2', 'c')False&gt;&gt;&gt; r.smembers('set-key2')&#123;b'a'&#125; 但是集合真正厉害的地方在于组合和关联多个集合，下表展示了相关的Redis命令 命令 用例 描述 redis-py API SDIFF SDIFF key [key …] 返回一个集合与给定集合的差集的元素 sdiff(keys, *args) SDIFFSTORE SDIFFSTORE destination key [key …] 类似于 SDIFF，不同之处在于该命令不返回结果集，而是将结果存放在destination集合中，如果destination已经存在, 则将其覆盖重写 sdiffstore(dest, keys, *args) SINTER SINTER key [key …] 返回指定所有的集合的成员的交集 sinter(keys, *args) SINTERSTORE SINTERSTORE destination key [key …] 与SINTER命令类似，但是它并不是直接返回结果集，而是将结果保存在 destination集合中，如果destination集合存在, 则会被重写 sinterstore(dest, keys, *args) SUNION SUNION key [key …] 返回给定的多个集合的并集中的所有成员 sunion(keys, *args) SUNIONSTORE SUNIONSTORE destination key [key …] 类似于SUNION命令，不同的是它并不返回结果集，而是将结果存储在destination集合中，如果destination已经存在，则将其覆盖. sunionstore(dest, keys, *args) 这些命令分别是并集运算、交集运算和差集运算这三个基本集合操作的“返回结果”版本和“存储结果”版本，下面这个交互示例展示了这些命令的基本使用12345678910&gt;&gt;&gt; r.sadd('skey1', 'a', 'b', 'c', 'd')4&gt;&gt;&gt; r.sadd('skey2', 'c', 'd', 'e', 'f')4&gt;&gt;&gt; r.sdiff('skey1', 'skey2')&#123;b'b', b'a'&#125;&gt;&gt;&gt; r.sinter('skey1', 'skey2')&#123;b'c', b'd'&#125;&gt;&gt;&gt; r.sunion('skey1', 'skey2')&#123;b'd', b'a', b'f', b'e', b'c', b'b'&#125; 和Python的集合相比，Redis的集合除了可以被多个客户端远程地进行访问之外，其他的语义和功能基本都是相同的。 散列首先介绍一些常用的添加和删除键值对的Redis散列命令 命令 用例 描述 redis-py API HMGET HMGET key field [field …] 返回key指定的散列中指定字段的值 hmget(name, keys, *args) HMSET HMSET key field value [field value …] 设置key指定的散列中指定字段的值，该命令将重写所有在散列中存在的字段，如果key指定的散列不存在，会创建一个新的散列并与key关联 hmset(name, mapping) HDEL HDEL key field [field …] 从key指定的散列中移除指定的域，在散列中不存在的域将被忽略，如果key指定的散列不存在，它将被认为是一个空的散列，该命令将返回0 hdel(name, *keys) HLEN HLEN key 返回key指定的散列包含的字段的数量 hlen(name) 其中，HDEL命令已经介绍过了，而HLEN以及用于一次读取或设置多个键的HMGET和HMSET则是新出现的命令。它们既可以给用户带来方便，又可以通过减少命令的调用次数以及客户端与Redis之间的通信往返次数来提升Redis的性能。 下面这个交互示例展示了这些命令的使用方法12345678&gt;&gt;&gt; r.hmset('hash-key', &#123;'k1':'v1','k2':'v2','k3':'v3'&#125;)True&gt;&gt;&gt; r.hmget('hash-key', ['k2', 'k3'])[b'v2', b'v3']&gt;&gt;&gt; r.hlen('hash-key')3&gt;&gt;&gt; r.hdel('hash-key', 'k1', 'k3')2 之前的学习笔记(一)介绍的HGET命令和HSET命令分别是HMGET和HMSET命令的单参数版本。因为HDEL已经可以同时删除多个键值对了，所以Redis没有实现HMDEL命令。 下表列出了散列的其他几个批量操作命令，以及一些和字符串操作类似的散列命令。 命令 用例 描述 redis-py API HEXISTS HEXISTS key field 检查给定键是否存在于散列中 hexists(name, key) HKEYS HKEYS key 返回散列包含的所有键 hkeys(name) HVALS HVALS key 返回散列包含的所有值 hvals(name) HGETALL HGETALL key 返回散列包含的所有键值对 hgetall(name) HINCRBY HINCRBY key field increment 将键存储的值加上整数increment hincrby(name, key, amount=1) HINCRBYFLOAT HINCRBYFLOAT key field increment 将键存储的值加上浮点数increment hincrbyfloat(name, key, amount=1.0) 下面这个交互示例展示了这些命令的使用方法12345678910&gt;&gt;&gt; r.hmset('hash-key2', &#123;'short':'hello', 'long':1000*1&#125;)True&gt;&gt;&gt; r.hkeys('hash-key2')[b'short', b'long']&gt;&gt;&gt; r.hexists('hash-key2', 'num')False&gt;&gt;&gt; r.hincrby('hash-key2', 'num')1&gt;&gt;&gt; r.hexists('hash-key2', 'num')True 在对散列进行处理时，如果键值对的值的体积非常大，那么用户可以先用HKEYS获取散列的所有键，然后只获取必要的值，这样可以有效地减少需要传输的数据量，避免服务器阻塞。 有序集合下表展示了一些常用的有序集合命令，大部分在第一章都有介绍 命令 用例 描述 redis-py API ZADD ZADD key score member [score member …] 将带有给定分值的成员添加到有序集合中 zadd(name, args, *kwargs) ZREM ZREM key member [member …] 移除给定的成员，并返回被移除成员的数量 zrem(name, *values) ZCARD ZCARD key 返回有序集合包含的成员数量 zcard(name) ZINCRBY ZINCRBY key increment member 将member成员的分值加上increment zincrby(name, value, amount=1) ZCOUNT ZCOUNT key min max 返回分值介于min和max之间的成员数量 zcount(name, min, max) ZRANK ZRANK key member 返回成员member在有序集合中的排名 zrank(name, value) ZSCORE ZSCORE key member 返回成员member的分值 zscore(name, value) ZRANGE ZRANGE key start stop [WITHSCORES] 返回排名介于start和stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回 zrange(name, start, end, desc=False, withscores=False, score_cast_func=) 下面这个交互示例展示了Redis中的一些常用的有序集合命令12345678910111213141516&gt;&gt;&gt; r.zadd('zset-key', 3, 'a', 2, 'b', 1, 'c')3&gt;&gt;&gt; r.zcard('zset-key')3&gt;&gt;&gt; r.zincrby('zset-key', 'c', 3)4.0&gt;&gt;&gt; r.zscore('zset-key', 'b')2.0&gt;&gt;&gt; r.zrank('zset-key', 'c')2&gt;&gt;&gt; r.zcount('zset-key', 0, 3)2&gt;&gt;&gt; r.zrem('zset-key', 'b')1&gt;&gt;&gt; r.zrange('zset-key', 0, -1, withscores=True)[(b'a', 3.0), (b'c', 4.0)] 其中在Python客户端用StrictRedis客户端类执行ZADD命令需要先输入分值，再输入成员，这也是Redis的标准，而Redis客户端类则截然相反。 下表展示了另外一下非常有用的有序集合命令 命令 用例 描述 redis-py API ZREVRANK ZREVRANK key member 返回有序集合里成员member的排名，成员按照分值从大到小排列 zrevrank(name, value) ZREVRANGE ZREVRANGE key start stop [WITHSCORES] 返回有序集合给定排名范围内的成员，成员按照分值从大到小排列 zrevrange(name, start, end, withscores=False, score_cast_func=) ZRANGEBYSCORE ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 返回有序集合中指定分数区间内的成员 zrangebyscore(name, min, max, start=None, num=None, withscores=False, score_cast_func=) ZREVRANGEBYSCORE ZREVRANGEBYSCORE key max min [WITHSCORES][LIMIT offset count] 返回有序集合中指定分数区间内的成员，分数由高到低排序。 zrevrangebyscore(name, max, min, start=None, num=None, withscores=False, score_cast_func=) ZREMRANGEBYRANK ZREMRANGEBYRANK key start stop 移除有序集key中，指定排名(rank)区间内的所有成员 zremrangebyrank(name, min, max) ZREMRANGEBYSCORE ZREMRANGEBYSCORE key min max 移除有序集key中，所有score值介于min和max之间(包括等于min或max)的成员 zremrangebyscore(name, min, max) ZINTERSTORE ZINTERSTORE destination numkeys key [key …] [WEIGHTS weight] [SUM MIN MAX] 计算给定的numkeys个有序集合的交集，并且把结果放到destination中 zinterstore(dest, keys, aggregate=None) ZUNIONSTORE ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight] [SUM MIN MAX] 计算给定的numkeys个有序集合的并集，并且把结果放到destination中。 zunionstore(dest, keys, aggregate=None) 其中有几个是没有介绍过的新命令，除了使用逆序来处理有序集合之外，ZREV*命令的工作方式和相对应的非逆序命令的工作方式完全一样(逆序就是指元素按照分值从大到小地排列)。 下面这个交互示例展示了ZINTERSTORE和ZUNIONSTORE命令的用法123456789101112131415161718&gt;&gt;&gt; r.zadd('zset-1', 1, 'a', 2, 'b', 3, 'c')3&gt;&gt;&gt; r.zadd('zset-2', 4, 'b', 1, 'c', 0, 'd')3&gt;&gt;&gt; r.zinterstore('zset-i', ['zset-1', 'zset-2'])2&gt;&gt;&gt; r.zrange('zset-i', 0, -1, withscores=True)[(b'c', 4.0), (b'b', 6.0)]&gt;&gt;&gt; r.zunionstore('zset-u', ['zset-1', 'zset-2'], aggregate='min')4&gt;&gt;&gt; r.zrange('zset-u', 0, -1, withscores=True)[(b'd', 0.0), (b'a', 1.0), (b'c', 1.0), (b'b', 2.0)]&gt;&gt;&gt; r.sadd('set-1', 'a', 'd')2&gt;&gt;&gt; r.zunionstore('zset-u2', ['zset-1', 'zset-2', 'set-1'])4&gt;&gt;&gt; r.zrange('zset-u2', 0, -1, withscores=True)[(b'd', 1.0), (b'a', 2.0), (b'c', 4.0), (b'b', 6.0)] 用户可以在执行交并运算时传入不同的聚合函数，共有sum、min、max三种可选；用户还可以把集合作为输入传给ZINTERSTORE和ZUNIONSTORE，命令会将集合看作是成员分值全为1的有序集合来处理。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis, Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记(一)：初识Redis]]></title>
    <url>%2F2017%2F03%2F25%2FRedis-1%2F</url>
    <content type="text"><![CDATA[Redis简介Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。 Redis与memcached及其他类型数据库对比Redis经常被拿来与memcached进行比较，两者都可用于存储键值映射，性能也相差无几，但是Redis能够自动以两种不同的方式将数据写入硬盘，而且Redis除了能存储普通的字符串键，还能存储其他4种数据结构，使得Redis可以用于解决更为广泛的问题，并且即可以作为主数据库使用，又可以作为其他存储系统的辅助数据库。 下表展示了Redis与memcached，MySQL以及MongoDB的特性与功能。 名称 类型 数据存储选项 查询类型 附加功能 Redis 使用内存存储的非关系数据库 字符串、列表、集合、散列表、有序集合 每种数据类型专属的命令，以及批量操作和不完全的事务支持 发布与订阅，主从复制，持久化，脚本 memcached 使用内存存储的键值缓存 键值之间的映射 创建、读取、删除、更新等命令 多线程服务器，用于提升性能 MySQL 关系数据库 每个数据库可以包含多个表，每个表可以包含多个行；可以处理多个表的视图；支持空间和第三方扩展 SELECT、INSERT、UPDATE、DELETE、函数、存储过程 支持ACID性质(需要使用InnoDB)，主从复制，主主复制 MongoDB 使用硬盘存储(on-disk)的非关系文档存储 每个数据库可以包含多个表，每个表可以包含多个无schema的BSON文档 创建、读取、更新、删除、条件查询等命令 支持map-reduce操作，主从复制，分片，空间索引 Redis安装(mac)首先下载用于安装Rudix的引导脚本，并安装Rudix12$ curl -O http://rudix.google.code.com/hg/Ports/rudix/rudix.Py$ sudo python rudix.py install rudix 然后使用命令Rudix安装Redis，若能成功启动Redis服务器则安装成功12$ sudo rudix install redis$ redis-server 最后用pip为Python安装Redis客户端库1$ sudo pip install redis Redis数据结构简介Redis可以存储键与5种不同数据结构类型之间的映射，分别是STRING(字符串)、LIST(列表)、SET(集合)、HASH(散列)、ZSET(有序集合)。有一部分命令对于这5种数据结构是通用的，如DEL、TYPE、RENAME等；但也有一部分命令只能对特定的一种或者两种结构使用。 下表从结构存储的值及读写能力对比了Redis的5种数据结构。 结构类型 结构存储的值 结构的读写能力 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作；对整数和浮点数进行自增或自减操作 LIST 一个链表，链表上的每个节点都包含了一个字符串 从链表两端推入或弹出元素；根据偏移量对链表进行修剪(trim)；读取单个或多个元素；根据值查找或移除元素 SET 包含字符串的无序收集器，并且被包含的每个字符串互不相同 添加、获取、移除单个元素；检查一个元素是否存在于集合中；计算交集、并集、差集；从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对；获取所有键值对 ZSET 字符串成员(member)与浮点数分值(score)之间的有序映射，元素的排列顺序由分值的大小决定 添加、获取、删除单个元素；根据分值范围(range)或者成员来获取元素 Redis字符串下表展示了3种简单的字符串命令： 命令 行为 GET 获取存储在给定键中的值 SET 设置存储在给定键中的值 DEL 删除存储在给定键中的值(该命令可用于所有类型) SET、GET、DEL的使用示例：12345678910$ redis-cli127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; get hello"world"127.0.0.1:6379&gt; del hello(integer) 1127.0.0.1:6379&gt; get hello(nil)127.0.0.1:6379&gt; SET命令在执行成功时返回OK，Python客户端会将这个OK转换为True；DEL命令在执行成功时将会返回被成功删除的值的数量；GET命令在尝试得到不存在的值时，将会返回一个nil，Python客户端会将这个nil转换为None。 Redis列表下表展示了6种简单的列表命令： 命令 行为 LPUSH(RPUSH) 将给定值推入列表的左端(右端) LPOP(RPOP) 从列表的左端(右端)弹出一个值，并返回被弹出的值 LINDEX 获取列表在给定位置上的单个值 LRANGE 获取列表在给定范围上的所有值 RPUSH、LRANGE、LINDEX、LPOP的使用示例12345678910111213141516171819$ redis-cli127.0.0.1:6379&gt; rpush list-key item(integer) 1127.0.0.1:6379&gt; rpush list-key item2(integer) 2127.0.0.1:6379&gt; rpush list-key item(integer) 3127.0.0.1:6379&gt; lrange list-key 0 -11) "item"2) "item2"3) "item"127.0.0.1:6379&gt; lindex list-key 1"item2"127.0.0.1:6379&gt; lpop list-key "item"127.0.0.1:6379&gt; lrange list-key 0 -11) "item2"2) "item"127.0.0.1:6379&gt; RPUSH和LPUSH命令在执行成功后会返回当前列表的长度；列表索引范围从0开始，到-1结束，可以取出列表包含的所有元素；使用LINDEX可以从列表中取出单个元素。 Redis集合Redis的集合和列表都可以存储多个字符串，它们的不同之处在于，列表可以存储多个相同的字符串，而集合则通过散列表来保证自己存储的每个字符串都是不同的(这些散列表只有键)。 下表展示了6种简单的集合命令： 命令 行为 SADD 将给定元素添加到集合 SMEMBERS 返回集合包含的所有元素 SISMEMBER 检查给定元素是否存在于集合中 SREM 如果给定的元素存在于集合中，那么移除这个元素 SADD、SMEMBERS、SISMEMBER、SREM的使用示例12345678910111213141516171819202122232425$ redis-cli127.0.0.1:6379&gt; sadd set-key item(integer) 1127.0.0.1:6379&gt; sadd set-key item2(integer) 1127.0.0.1:6379&gt; sadd set-key item3(integer) 1127.0.0.1:6379&gt; sadd set-key item(integer) 0127.0.0.1:6379&gt; smembers set-key1) "item2"2) "item3"3) "item"127.0.0.1:6379&gt; sismember set-key item4(integer) 0127.0.0.1:6379&gt; sismember set-key item(integer) 1127.0.0.1:6379&gt; srem set-key item2(integer) 1127.0.0.1:6379&gt; srem set-key item2(integer) 0127.0.0.1:6379&gt; smembers set-key1) "item3"2) "item"127.0.0.1:6379&gt; SADD命令返回1表示成功添加到集合中，返回0表示该元素已存在于集合中；SMEMBERS命令获取到的元素组成的序列将会被Python客户端转换为Python集合；Python客户端会返回一个布尔值来表示SISMEMBER命令的检查结果；SREM命令会返回被移除元素的数量。 Redis散列Redis的散列就像一个微型Redis，它可以存储多个键值对之间的映射。和字符串一样，散列存储的值既可以是字符串也可以是数值。可以将散列看做文档数据库里面的文档，还可以看做是关系数据库里面的行，因为散列、文档和行都允许用户同时访问或修改一个或多个域(field)。 下表展示了4种简单的列表命令： 命令 行为 HSET 在散列里面关联给定的键值对 HGET 获取指定散列键的值 HGETALL 获取散列包含的所有键值对 HDEL 如果给定键存在于散列里面，那么移除这个键 HSET、HGET、HGETALL、HDEL的使用示例12345678910111213141516171819202122$ redis-cli127.0.0.1:6379&gt; hset hash-key sub-key1 value1(integer) 1127.0.0.1:6379&gt; hset hash-key sub-key2 value2(integer) 1127.0.0.1:6379&gt; hset hash-key sub-key1 value1(integer) 0127.0.0.1:6379&gt; hgetall hash-key1) "sub-key1"2) "value1"3) "sub-key2"4) "value2"127.0.0.1:6379&gt; hdel hash-key sub-key2(integer) 1127.0.0.1:6379&gt; hdel hash-key sub-key2(integer) 0127.0.0.1:6379&gt; hget hash-key sub-key1"value1"127.0.0.1:6379&gt; hgetall hash-key1) "sub-key1"2) "value1"127.0.0.1:6379&gt; HSET返回一个值来表示给定的键是否已经存在于散列里面；Python客户端会把HGETALL命令获取的整个散列转换为一个Python字典；HDEL命令执行后会返回一个值来表示给定的键在移除之前是否存在于散列里面。 Redis有序集合有序集合和散列一样，都用于存储键值对：其中有序集合的每个键称为成员（member），都是独一无二的，而有序集合的每个值称为分值（score），都必须是浮点数。有序集合是Redis里面唯一既可以根据成员访问元素（这一点和散列一样），又可以根据分值以及分值的排列顺序来访问元素的结构。 下表展示了4种简单的有序集合命令： 命令 行为 ZADD 将一个带有给定分值的成员添加到有序集合里面 ZRANGE 根据元素在有序排列中所处的位置，从有序集合里获取多个元素 ZRANGEBYSCORE 获取有序集合在给定分值范围内的所有元素 ZREM 如果给定成员存在于有序集合，那么移除这个成员 ZADD、ZRANGE、ZRANGEBYSCORE、ZREM的使用示例1234567891011121314151617181920212223$ redis-cli127.0.0.1:6379&gt; zadd zset-key 728 member1(integer) 1127.0.0.1:6379&gt; zadd zset-key 982 member0(integer) 1127.0.0.1:6379&gt; zadd zset-key 982 member0(integer) 0127.0.0.1:6379&gt; zrange zset-key 0 -1 withscores1) "member1"2) "728"3) "member0"4) "982"127.0.0.1:6379&gt; zrangebyscore zset-key 0 800 withscores1) "member1"2) "728"127.0.0.1:6379&gt; zrem zset-key member1(integer) 1127.0.0.1:6379&gt; zrem zset-key member1(integer) 0127.0.0.1:6379&gt; zrange zset-key 0 -1 withscores1) "member0"2) "982"127.0.0.1:6379&gt; 在尝试向有序集合添加元素的时候，ZADD命令会返回新添加元素的数量；ZRANGE命令获取有序集合包含的所有元素，这些元素会按照分值进行排序，Python客户端会将这些分值转换成浮点数；ZRANGEBYSCORE命令也可以根据分值来获取有序集合的其中一部分元素；ZREM命令在移除有序集合元素的时候，命令会返回被移除元素的数量。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis, Python</tag>
      </tags>
  </entry>
</search>
