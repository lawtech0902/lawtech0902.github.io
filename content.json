{"meta":{"title":"LawTech's Blog","subtitle":"不破不立","description":"破邮python爱好者。","author":"LawTech.","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-03-26T06:40:18.000Z","updated":"2017-03-26T06:40:18.000Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":""},{"title":"关于","date":"2017-03-26T08:09:29.000Z","updated":"2017-03-26T08:36:08.000Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"About个人简介 lawtech0902— Python爱好者 学生党一枚~ 本科：南邮信息安全 硕士：南邮软件工程 Email：584563542@qq.com 关于这个博客 写着玩儿~ 其他： 皇马球迷，我罗加油；"},{"title":"分类","date":"2017-03-26T05:59:19.000Z","updated":"2017-03-26T06:00:37.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-03-26T05:59:01.000Z","updated":"2017-03-26T06:00:05.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Mac配置Privoxy设置go get代理","slug":"mac-privoxy","date":"2017-10-17T05:18:54.000Z","updated":"2017-10-17T05:49:25.110Z","comments":true,"path":"2017/10/17/mac-privoxy/","link":"","permalink":"http://yoursite.com/2017/10/17/mac-privoxy/","excerpt":"","text":"安装Privoxy地址：https://www.privoxy.org/sf-download-mirror/ 配置Privoxy12345cd /Applications/Privoxy# 监听8118端口echo 'listen-address 0.0.0.0:8118' &gt;&gt; /usr/local/etc/privoxy/config# 设置转发socks5服务器echo 'forward-socks5 / localhost:1080 .' &gt;&gt; /usr/local/etc/privoxy/config 发现提示错误： 1-bash: /usr/local/etc/privoxy/config: Permission denied 解决方法： 12# 修改/usr/local目录的所有者与组sudo chown -R \"$USER\":admin /usr/local 配置HTTP代理1234567891011# 如果你使用bashvim ~/.bashrc# 如果你使用zshvim ~/.zshrc# 加入export http_proxy=http://127.0.0.1:8118/# 保存退出# source 使其即刻生效source ~/.bashrc# orsource ~/.zshrc 测试Privoxy1234567lawtech@lawdeMacBook-Pro:/Applications/Privoxy$ curl www.google.com&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"&gt;&lt;TITLE&gt;302 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;&lt;H1&gt;302 Moved&lt;/H1&gt;The document has moved&lt;A HREF=\"http://www.google.co.jp/?gfe_rd=cr&amp;amp;dcr=0&amp;amp;ei=G5flWZmXKa_o8Ae9j7PADw\"&gt;here&lt;/A&gt;.&lt;/BODY&gt;&lt;/HTML&gt; 终于可以开心地go get了！","categories":[{"name":"Go","slug":"Go","permalink":"http://yoursite.com/categories/Go/"}],"tags":[{"name":"Go，Mac，Privoxy","slug":"Go，Mac，Privoxy","permalink":"http://yoursite.com/tags/Go，Mac，Privoxy/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——scrapyd部署scrapy爬虫","slug":"scrapyd","date":"2017-07-27T12:18:54.000Z","updated":"2017-07-27T15:22:43.000Z","comments":true,"path":"2017/07/27/scrapyd/","link":"","permalink":"http://yoursite.com/2017/07/27/scrapyd/","excerpt":"在完成scrapy项目之后，就要进入项目实际部署环节。","text":"在完成scrapy项目之后，就要进入项目实际部署环节。 传送门：scrapyd 首先在我们之前爬虫项目的虚拟环境article_spider中安装scrapyd： 1(article_spider) lawtech@lawdeMacBook-Pro-2:~$ pip install scrapyd lawtech@lawdeMacBook-Pro-2:~$ pip install scrapyd-client scrapyd-client就没必要在虚拟环境中安装了。 在scrapy项目中，有一个文件scrapy.cfg： 1234567891011# Automatically created by: scrapy startproject## For more information about the [deploy] section see:# https://scrapyd.readthedocs.org/en/latest/deploy.html[settings]default = ArticleSpider.settings[deploy:lawtech]url = http://localhost:6800/project = ArticleSpider 其中的deploy就是为scrapyd服务的。 要部署项目，首先要启动scrapyd： 12345678910lawtech@lawdeMacBook-Pro-2:~$ workon article_spider(article_spider) lawtech@lawdeMacBook-Pro-2:~$ scrapyd2017-07-27T23:14:51+0800 [-] Loading /Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapyd/txapp.py...2017-07-27T23:14:52+0800 [-] Scrapyd web console available at http://127.0.0.1:6800/2017-07-27T23:14:52+0800 [-] Loaded.2017-07-27T23:14:52+0800 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 17.1.0 (/Users/lawtech/myvirtualenvs/article_spider/bin/python3.5 3.5.2) starting up.2017-07-27T23:14:52+0800 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.selectreactor.SelectReactor.2017-07-27T23:14:52+0800 [-] Site starting on 68002017-07-27T23:14:52+0800 [twisted.web.server.Site#info] Starting factory &lt;twisted.web.server.Site object at 0x107a97198&gt;2017-07-27T23:14:52+0800 [Launcher] Scrapyd 1.2.0 started: max_proc=16, runner=&apos;scrapyd.runner&apos; 部署项目： 12345(article_spider) lawtech@lawdeMacBook-Pro-2:~/PycharmProjects/ArticleSpider$ scrapyd-deploy lawtech -p ArticleSpiderPacking version 1501168592Deploying to project &quot;ArticleSpider&quot; in http://localhost:6800/addversion.jsonServer response (200):&#123;&quot;status&quot;: &quot;ok&quot;, &quot;version&quot;: &quot;1501168592&quot;, &quot;spiders&quot;: 3, &quot;project&quot;: &quot;ArticleSpider&quot;, &quot;node_name&quot;: &quot;lawdeMacBook-Pro-2.local&quot;&#125; 现在只是将项目部署到目标地址，但是没有调度爬虫，调度爬虫需要用到curl命令，在http://localhost:6800有提示如下： 1curl http://localhost:6800/schedule.json -d project=default -d spider=somespider 只需要改动一下即可 12lawtech@lawdeMacBook-Pro-2:~/PycharmProjects/ArticleSpider$ curl http://localhost:6800/schedule.json -d project=ArticleSpider -d spider=jobbole&#123;&quot;status&quot;: &quot;ok&quot;, &quot;jobid&quot;: &quot;2466c76872df11e7846da45e60ba3bb7&quot;, &quot;node_name&quot;: &quot;lawdeMacBook-Pro-2.local&quot;&#125; 然后即可在http://127.0.0.1:6800/jobs查看调度结果了。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python，scrapyd","slug":"Scrapy，Python，scrapyd","permalink":"http://yoursite.com/tags/Scrapy，Python，scrapyd/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——总结","slug":"scrapy-summary","date":"2017-07-27T12:18:54.000Z","updated":"2017-07-27T15:25:55.000Z","comments":true,"path":"2017/07/27/scrapy-summary/","link":"","permalink":"http://yoursite.com/2017/07/27/scrapy-summary/","excerpt":"","text":"开发环境搭建 技术选型 伯乐在线爬取 知乎爬虫 拉勾网站整站爬虫 爬虫与反爬虫 scrapy进阶开发 scrapy-redis分布式开发 elasticsearch的基础知识 django搭建搜索网站 scrapyd部署scrapy项目","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python，ElasticSearch","slug":"Scrapy，Python，ElasticSearch","permalink":"http://yoursite.com/tags/Scrapy，Python，ElasticSearch/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Django搭建搜索网站","slug":"django-elasticsearch","date":"2017-07-20T12:18:54.000Z","updated":"2017-07-27T14:26:24.000Z","comments":true,"path":"2017/07/20/django-elasticsearch/","link":"","permalink":"http://yoursite.com/2017/07/20/django-elasticsearch/","excerpt":"在完成Scrapy和Elasticsearch的基本学习之后，下面就要利用Django开始搭建我们的搜索网站。","text":"在完成Scrapy和Elasticsearch的基本学习之后，下面就要利用Django开始搭建我们的搜索网站。 es完成搜索建议-搜索建议字段保存传送门：Completion Suggester 为了完成搜索时自动补全的功能，我们在es_types.py中加入一个字段suggest = Completion(analyzer=ik_analyzer)，由于源码冲突问题，我们需要自定义一个analyzer： 123456789101112from elasticsearch_dsl.analysis import CustomAnalyzer as _CustomAnalyzerclass CustomAnalyzer(_CustomAnalyzer): \"\"\" 自定义Analyzer \"\"\" def get_analysis_definition(self): return &#123;&#125;ik_analyzer = CustomAnalyzer(\"ik_max_word\", filter=[\"lowercase\"]) 然后重新生成索引： 我们需要为article生成搜索建议词，所以在save_to_es()函数中加入article.suggest = [{&quot;input&quot;:[], &quot;weight&quot;:2}]，然后我们需要自己写一个方法通过之前的analyze接口生成搜索建议： 123456789101112131415161718192021222324from elasticsearch_dsl.connections import connections# 生成es的实例es = connections.create_connection(ArticleType._doc_type.using)def gen_suggests(index, info_tuple): \"\"\" 根据字符串生成搜索建议数组 \"\"\" used_words = set() suggests = [] for text, weight in info_tuple: if text: # 调用es的analyze接口分析字符串 words = es.indices.analyze(index=index, analyzer=\"ik_max_word\", params=&#123;\"filter\": [\"lowercase\"]&#125;, body=text) analyzed_words = set(r[\"token\"] for r in words[\"tokens\"] if len(r[\"token\"]) &gt; 1) new_words = analyzed_words - used_words else: new_words = set() if new_words: suggests.append(&#123;\"input\": list(new_words), \"weight\": weight&#125;) return suggests 所以之前的代码就可以修改为 123456789101112131415161718def save_to_es(self): article = ArticleType() article.title = self['title'] article.create_date = self[\"create_date\"] article.content = remove_tags(self[\"content\"]) article.front_image_url = self[\"front_image_url\"] if \"front_image_path\" in self: article.front_image_path = self[\"front_image_path\"] article.praise_nums = self[\"praise_nums\"] article.fav_nums = self[\"fav_nums\"] article.comment_nums = self[\"comment_nums\"] article.url = self[\"url\"] article.tags = self[\"tags\"] article.meta.id = self[\"url_object_id\"] article.suggest = gen_suggests(ArticleType._doc_type.index, ((article.title,10),(article.tags, 7))) article.save() return 这样我们就完成了suggest字段的准备。 Django实现Elasticsearch的搜索建议不想写了，没耐心了，巴拉巴拉~~ 直接传送门走起：LcvSearch","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python，Django，Elasticsearch","slug":"Scrapy，Python，Django，Elasticsearch","permalink":"http://yoursite.com/tags/Scrapy，Python，Django，Elasticsearch/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Elasticsearch搜索引擎的使用","slug":"elasticsearch-usage","date":"2017-07-09T12:18:54.000Z","updated":"2017-07-09T15:55:44.000Z","comments":true,"path":"2017/07/09/elasticsearch-usage/","link":"","permalink":"http://yoursite.com/2017/07/09/elasticsearch-usage/","excerpt":"Elasticsearch介绍ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 我们建立一个网站或应用程序，并要添加搜索功能，但是想要完成搜索工作的创建是非常困难的。我们希望搜索解决方案要运行速度快，我们希望能有一个零配置和一个完全免费的搜索模式，我们希望能够简单地使用JSON通过HTTP来索引数据，我们希望我们的搜索服务器始终可用，我们希望能够从一台开始并扩展到数百台，我们要实时搜索，我们要简单的多租户，我们希望建立一个云的解决方案。因此我们利用Elasticsearch来解决所有这些问题以及可能出现的更多其它问题。","text":"Elasticsearch介绍ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 我们建立一个网站或应用程序，并要添加搜索功能，但是想要完成搜索工作的创建是非常困难的。我们希望搜索解决方案要运行速度快，我们希望能有一个零配置和一个完全免费的搜索模式，我们希望能够简单地使用JSON通过HTTP来索引数据，我们希望我们的搜索服务器始终可用，我们希望能够从一台开始并扩展到数百台，我们要实时搜索，我们要简单的多租户，我们希望建立一个云的解决方案。因此我们利用Elasticsearch来解决所有这些问题以及可能出现的更多其它问题。 Elasticsearch安装 Elasticsearch-RTF安装 什么是Elasticsearch-RTF？ RTF是Ready To Fly的缩写，在航模里面，表示无需自己组装零件即可直接上手即飞的航空模型，Elasticsearch-RTF是针对中文的一个发行版，即使用最新稳定的elasticsearch版本，并且帮你下载测试好对应的插件，如中文分词插件等，目的是让你可以下载下来就可以直接的使用（虽然es已经很简单了，但是很多新手还是需要去花时间去找配置，中间的过程其实很痛苦），当然等你对这些都熟悉了之后，你完全可以自己去diy了，跟linux的众多发行版是一个意思。 当前版本 Elasticsearch 5.1.1 传送门：Elasticsearch-RTF head插件和kibana的安装 传送门：elasticsearch-head，kibana 安装方式在github地址中都有详细说明。 注意：在elsticsearch.yml中添加如下设置，使得elasticsearch-head能够连接到elasticsearch。 12345# security policy configuration, allowing third-party plugins connection.http.cors.enabled: truehttp.cors.allow-origin: \"*\"http.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETEhttp.cors.allow-headers: \"X-Requested-With, Content-Type, Content-Length, X-User\" Elasticsearch的基本概念传送门：基本概念 倒排索引倒排索引（英语：Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。 有两种不同的反向索引形式： 一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。 一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。 后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。 传送门：倒排索引 这个例子很好，所以搬来： 以英文为例，下面是要被索引的文本： T0=&quot;it is what it is&quot; T1=&quot;what is it&quot; T2=”it is a banana” 我们就能得到下面的反向文件索引： 12345&quot;a&quot;: &#123;2&#125;&quot;banana&quot;: &#123;2&#125;&quot;is&quot;: &#123;0, 1, 2&#125;&quot;it&quot;: &#123;0, 1, 2&#125;&quot;what&quot;: &#123;0, 1&#125; 检索的条件”what”, “is” 和 “it” 将对应这个集合：{0,1}∩{0,1,2}∩{0,1,2}={0,1}。 对相同的文字，我们得到后面这些完全反向索引，有文档数量和当前查询的单词结果组成的的成对数据。 同样，文档数量和当前查询的单词结果都从零开始。所以，”banana”: {(2, 3)} 就是说 “banana”在第三个文档里 (T2)，而且在第三个文档的位置是第四个单词(地址为 3)。 12345&quot;a&quot;: &#123;(2, 2)&#125;&quot;banana&quot;: &#123;(2, 3)&#125;&quot;is&quot;: &#123;(0, 1), (0, 4), (1, 1), (2, 1)&#125;&quot;it&quot;: &#123;(0, 0), (0, 3), (1, 2), (2, 0)&#125; &quot;what&quot;: &#123;(0, 2), (1, 0)&#125; 如果我们执行短语搜索”what is it” 我们得到这个短语的全部单词各自的结果所在文档为文档0和文档1。但是这个短语检索的连续的条件仅仅在文档1得到。 Elasticsearch基本的索引和文档CRUD操作我们在Kibana Dev Tools的Console中进行这些操作。 创建索引 添加索引完成后，elasticsearch-head中显示如下： 实际上，在head中也可以添加索引： 这两种方式是等效的，只是在Kibana中我们使用的是Rest API。 获取settings 12345# 获取settingsGET lagou/_settingsGET _all/_settingsGET .kibana,lagou/_settingsGET _settings 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# GET lagou/_settings&#123; &quot;lagou&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499158041414&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;BhByVdsdTx65H4xnL4TlWQ&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;lagou&quot; &#125; &#125; &#125;&#125;# GET _all/_settings&#123; &quot;lagou&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499158041414&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;BhByVdsdTx65H4xnL4TlWQ&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;lagou&quot; &#125; &#125; &#125;, &quot;.kibana&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499073856161&quot;, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;g_cP7qZERXiVeKEwjbNE1g&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;.kibana&quot; &#125; &#125; &#125;&#125;# GET .kibana,lagou/_settings&#123; &quot;lagou&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499158041414&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;BhByVdsdTx65H4xnL4TlWQ&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;lagou&quot; &#125; &#125; &#125;, &quot;.kibana&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499073856161&quot;, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;g_cP7qZERXiVeKEwjbNE1g&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;.kibana&quot; &#125; &#125; &#125;&#125;# GET _settings&#123; &quot;lagou&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499158041414&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;BhByVdsdTx65H4xnL4TlWQ&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;lagou&quot; &#125; &#125; &#125;, &quot;.kibana&quot;: &#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499073856161&quot;, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;g_cP7qZERXiVeKEwjbNE1g&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;.kibana&quot; &#125; &#125; &#125;&#125; 修改settings 12345678910# 修改settingsPUT lagou/_settings&#123; &quot;number_of_replicas&quot;: 2&#125;PUT lagou/_settings&#123; &quot;number_of_shards&quot;: 2&#125; 运行结果： 12345678910111213141516171819# PUT lagou/_settings&#123; &quot;acknowledged&quot;: true&#125;# PUT lagou/_settings&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;can&apos;t change the number of shards for an index&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;can&apos;t change the number of shards for an index&quot; &#125;, &quot;status&quot;: 400&#125; 由于shards一旦设置就不能更改，所以第二个操作失败。 获取索引信息 123# 获取索引信息GET _allGET lagou 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# GET _all&#123; &quot;lagou&quot;: &#123; &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123;&#125;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499158041414&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;2&quot;, &quot;uuid&quot;: &quot;BhByVdsdTx65H4xnL4TlWQ&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;lagou&quot; &#125; &#125; &#125;, &quot;.kibana&quot;: &#123; &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123; &quot;index-pattern&quot;: &#123; &quot;properties&quot;: &#123; &quot;fieldFormatMap&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;fields&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;intervalName&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;notExpandable&quot;: &#123; &quot;type&quot;: &quot;boolean&quot; &#125;, &quot;sourceFilters&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;timeFieldName&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;, &quot;config&quot;: &#123; &quot;properties&quot;: &#123; &quot;buildNum&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;, &quot;timelion-sheet&quot;: &#123; &quot;properties&quot;: &#123; &quot;description&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;hits&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;kibanaSavedObjectMeta&quot;: &#123; &quot;properties&quot;: &#123; &quot;searchSourceJSON&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;, &quot;timelion_chart_height&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;timelion_columns&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;timelion_interval&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;timelion_other_interval&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;timelion_rows&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;timelion_sheet&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;version&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;, &quot;server&quot;: &#123; &quot;properties&quot;: &#123; &quot;uuid&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499073856161&quot;, &quot;number_of_shards&quot;: &quot;1&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;g_cP7qZERXiVeKEwjbNE1g&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;.kibana&quot; &#125; &#125; &#125;&#125;# GET lagou&#123; &quot;lagou&quot;: &#123; &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123;&#125;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1499158041414&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;2&quot;, &quot;uuid&quot;: &quot;BhByVdsdTx65H4xnL4TlWQ&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;5010199&quot; &#125;, &quot;provided_name&quot;: &quot;lagou&quot; &#125; &#125; &#125;&#125; 保存文档 1234567891011121314151617181920212223242526# 保存文档PUT lagou/job/1&#123; &quot;title&quot;:&quot;python分布式爬虫开发&quot;, &quot;salary_min&quot;:15000, &quot;city&quot;:&quot;北京&quot;, &quot;company&quot;:&#123; &quot;name&quot;:&quot;百度&quot;, &quot;company_addr&quot;:&quot;北京市软件园&quot; &#125;, &quot;publish_date&quot;:&quot;2017-4-16&quot;, &quot;comments&quot;:15&#125;POST lagou/job/&#123; &quot;title&quot;:&quot;python django 开发工程师&quot;, &quot;salary_min&quot;:30000, &quot;city&quot;:&quot;上海&quot;, &quot;company&quot;:&#123; &quot;name&quot;:&quot;美团科技&quot;, &quot;company_addr&quot;:&quot;北京市软件园A区&quot; &#125;, &quot;publish_date&quot;:&quot;2017-4-16&quot;, &quot;comments&quot;:20&#125; 运行结果： 1234567891011121314151617181920212223242526272829# PUT lagou/job/1&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125;# POST lagou/job/&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0MzKkUnOriGeBA_nYi&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 在elasticsearch-head中显示如下： 分别采用PUT和POST方法保存两个文档，采用POST方法时未指定id仍然保存成功，id为系统分配的uuid。 获取文档 12345# 获取文档GET lagou/job/1GET lagou/job/1?_source=titleGET lagou/job/1?_source=title,cityGET lagou/job/1?_source 运行结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# GET lagou/job/1&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python分布式爬虫开发&quot;, &quot;salary_min&quot;: 15000, &quot;city&quot;: &quot;北京&quot;, &quot;company&quot;: &#123; &quot;name&quot;: &quot;百度&quot;, &quot;company_addr&quot;: &quot;北京市软件园&quot; &#125;, &quot;publish_date&quot;: &quot;2017-4-16&quot;, &quot;comments&quot;: 15 &#125;&#125;# GET lagou/job/1?_source=title&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python分布式爬虫开发&quot; &#125;&#125;# GET lagou/job/1?_source=title,city&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;city&quot;: &quot;北京&quot;, &quot;title&quot;: &quot;python分布式爬虫开发&quot; &#125;&#125;# GET lagou/job/1?_source&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python分布式爬虫开发&quot;, &quot;salary_min&quot;: 15000, &quot;city&quot;: &quot;北京&quot;, &quot;company&quot;: &#123; &quot;name&quot;: &quot;百度&quot;, &quot;company_addr&quot;: &quot;北京市软件园&quot; &#125;, &quot;publish_date&quot;: &quot;2017-4-16&quot;, &quot;comments&quot;: 15 &#125;&#125; 修改文档 12345678910111213141516171819# 修改文档PUT lagou/job/1&#123; &quot;title&quot;:&quot;python分布式爬虫开发&quot;, &quot;salary_min&quot;:15000, &quot;company&quot;:&#123; &quot;name&quot;:&quot;百度&quot;, &quot;company_addr&quot;:&quot;北京市软件园&quot; &#125;, &quot;publish_date&quot;:&quot;2017-4-16&quot;, &quot;comments&quot;:15&#125;POST lagou/job/1/_update&#123; &quot;doc&quot;:&#123; &quot;comments&quot;:20 &#125;&#125; 运行结果： 12345678910111213141516171819202122232425262728# PUT lagou/job/1&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: false&#125;# POST lagou/job/1/_update&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 3, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 修改文档有两种方式，一种是PUT覆盖更新方式，一种是POST增量更新方式。 删除 1234567# 删除# 删除文档DELETE lagou/job/1# 删除索引DELETE lagou/ 运行结果： 12345678910111213141516171819# DELETE lagou/job/1&#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 4, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125;# DELETE lagou/&#123; &quot;acknowledged&quot;: true&#125; Elasticsearch的mget和bulk批量操作mget像Elasticsearch一样，检索多个文档依旧非常快。合并多个请求可以避免每个请求单独的网络开销。如果你需要从Elasticsearch中检索多个文档，相对于一个一个的检索，更快的方式是在一个请求中使用multi-get或者mget API。 mget API参数是一个docs数组，数组的每个节点定义一个文档的_index、_type、_id元数据。如果你只想检索一个或几个确定的字段，也可以定义一个_source参数。 为了演示mget操作，我们新建了一个索引testdb，其下文档如下： 第一种： 12345678910111213141516# 查询job1 id为1的数据，job2 id为2的数据GET _mget&#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;:&quot;testdb&quot;, &quot;_type&quot;:&quot;job1&quot;, &quot;_id&quot;:1 &#125;, &#123; &quot;_index&quot;:&quot;testdb&quot;, &quot;_type&quot;:&quot;job2&quot;, &quot;_id&quot;:2 &#125; ]&#125; 运行结果： 123456789101112131415161718192021222324&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;testdb&quot;, &quot;_type&quot;: &quot;job1&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;job1_1&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;testdb&quot;, &quot;_type&quot;: &quot;job2&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;job2_2&quot; &#125; &#125; ]&#125; 第二种方法，在url里传递索引名称： 1234567891011121314# 第二种方法，在url中传递索引名称GET testdb/_mget&#123; &quot;docs&quot;:[ &#123; &quot;_type&quot;:&quot;job1&quot;, &quot;_id&quot;:1 &#125;, &#123; &quot;_type&quot;:&quot;job2&quot;, &quot;_id&quot;:2 &#125; ]&#125; 运行结果都是一样的。 如果查询的数据index和type都相同，则可以将type也传入到url中： 123456789101112# 查询的数据索引名称相同，type也相同GET testdb/job1/_mget&#123; &quot;docs&quot;:[ &#123; &quot;_id&quot;:1 &#125;, &#123; &quot;_id&quot;:2 &#125; ]&#125; 另外还有一种简写方法： 12345# 只需要传递id即可GET testdb/job1/_mget&#123; &quot;ids&quot;:[1,2]&#125; 两者结果相同。 bulk就像mget允许我们一次性检索多个文档一样，bulk API允许我们使用单一请求来实现多个文档的create、index、update或delete。这对索引类似于日志活动这样的数据流非常有用，它们可以以成百上千的数据为一个批次按序进行索引。 传送门：更新时的批量操作 测试： 123456# bulk操作POST _bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;lagou&quot;, &quot;_type&quot;:&quot;job&quot;, &quot;_id&quot;:1&#125;&#125;&#123;&quot;title&quot;:&quot;python分布式爬虫开发&quot;,&quot;salary_min&quot;:15000,&quot;city&quot;:&quot;北京&quot;,&quot;company&quot;:&#123;&quot;name&quot;:&quot;百度&quot;,&quot;company_addr&quot;:&quot;北京市软件园&quot;&#125;,&quot;publish_date&quot;:&quot;2017-4-16&quot;,&quot;comments&quot;:15&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;lagou&quot;, &quot;_type&quot;:&quot;job2&quot;, &quot;_id&quot;:2&#125;&#125;&#123;&quot;title&quot;:&quot;python django开发&quot;,&quot;salary_min&quot;:30000,&quot;city&quot;:&quot;成都&quot;,&quot;company&quot;:&#123;&quot;name&quot;:&quot;阿里巴巴&quot;,&quot;company_addr&quot;:&quot;北京市软件园B区&quot;&#125;,&quot;publish_date&quot;:&quot;2017-4-18&quot;,&quot;comments&quot;:50&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738&#123; &quot;took&quot;: 462, &quot;errors&quot;: false, &quot;items&quot;: [ &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job2&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true, &quot;status&quot;: 201 &#125; &#125; ]&#125; Elasticsearch的mapping映射管理为了能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理成全文本（Full-text）或精确的字符串值，Elasticsearch需要知道每个字段里面都包含了什么类型。这些类型和字段的信息存储（包含）在映射（mapping）中。 索引中每个文档都有一个类型(type)。 每个类型拥有自己的映射(mapping)或者模式定义(schema definition)。一个映射定义了字段类型，每个字段的数据类型，以及字段被Elasticsearch处理的方式。映射还用于设置关联到类型上的元数据。 传送门：映射 测试： 创建映射： 123456789101112131415161718192021222324252627282930313233343536373839# 创建索引PUT lagou&#123; &quot;mappings&quot;: &#123; &quot;job&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;salary_min&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;city&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;company&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;company_addr&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;employee_count&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;, &quot;publish_date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot; &#125;, &quot;comments&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125; &#125;&#125; 放入数据： 1234567891011121314# 放入数据PUT lagou/job/1&#123; &quot;title&quot;: &quot;python分布式爬虫开发&quot;, &quot;salary_min&quot;: 15000, &quot;city&quot;: &quot;北京&quot;, &quot;company&quot;: &#123; &quot;name&quot;: &quot;百度&quot;, &quot;company_addr&quot;: &quot;北京市软件园&quot;, &quot;employee_count&quot;: 50 &#125;, &quot;publish_date&quot;: &quot;2017-4-18&quot;, &quot;comments&quot;: 15&#125; 运行结果： 12345678910111213&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 可以在elasticsearch-head中查看结果： 如果我们对放入的数据稍做修改，将&quot;salary_min&quot;:15000改为&quot;salary_min&quot;:&quot;15000&quot;，操作还是可以完成的，这是因为mappings会对其进行类型转换得到我们所要的integer类型。 如果将&quot;salary_min&quot;:15000改为&quot;salary_min&quot;:&quot;abc&quot;，那就会出错： 1234567891011121314151617&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;mapper_parsing_exception&quot;, &quot;reason&quot;: &quot;failed to parse [salary_min]&quot; &#125; ], &quot;type&quot;: &quot;mapper_parsing_exception&quot;, &quot;reason&quot;: &quot;failed to parse [salary_min]&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;number_format_exception&quot;, &quot;reason&quot;: &quot;For input string: \\&quot;abc\\&quot;&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 获取mapping： 1234GET lagou/_mappingGET lagou/mapping/jobGET _all/_mappingGET _all/_mapping/job Elasticsearch的简单查询查询分类： 基本查询：使用Elasticsearch内置查询条件进行查询 组合查询：把多个查询组合在一起进行复合查询 过滤：查询同时，通过filter条件下在不影响打分的情况下筛选数据 测试，首先添加映射： 1234567891011121314151617181920212223242526272829# 添加映射PUT lagou&#123; &quot;mappings&quot;: &#123; &quot;job&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;store&quot;: true, &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;company_name&quot;: &#123; &quot;store&quot;: true, &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;desc&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;comments&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;add_time&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot; &#125; &#125; &#125; &#125;&#125; 注：”ik_max_word”是分析器类型的一种，会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合。 传送门：elasticsearch-analysis-ik 然后添加我们准备好的数据： 123456789101112131415161718192021222324252627282930313233343536# 添加数据POST lagou/job&#123; &quot;title&quot;: &quot;python django 开发工程师&quot;, &quot;company_name&quot;: &quot;美团科技有限公司&quot;, &quot;desc&quot;: &quot;对django的概念熟悉，熟悉python基础知识&quot;, &quot;comments&quot;: 20, &quot;add_time&quot;: &quot;2017-4-1&quot;&#125;POST lagou/job&#123; &quot;title&quot;: &quot;python scrapy redis分布式爬虫基本&quot;, &quot;company_name&quot;: &quot;百度科技有限公司&quot;, &quot;desc&quot;: &quot;对scrapy的概念熟悉，熟悉redis的基本操作&quot;, &quot;comments&quot;: 5, &quot;add_time&quot;: &quot;2017-4-15&quot;&#125;POST lagou/job&#123; &quot;title&quot;: &quot;elasticsearch打造搜索引擎&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉数据结构算法，熟悉python的基本开发&quot;, &quot;comments&quot;: 15, &quot;add_time&quot;: &quot;2017-6-20&quot;&#125;POST lagou/job&#123; &quot;title&quot;: &quot;python打造推荐引擎系统&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉推荐引擎的原理以及算法，掌握C语言&quot;, &quot;comments&quot;: 60, &quot;add_time&quot;: &quot;2017-10-20&quot;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# POST lagou/job&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWQ-nOriGeBA_nYp&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125;# POST lagou/job&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWS6nOriGeBA_nYq&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125;# POST lagou/job&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWUJnOriGeBA_nYr&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125;# POST lagou/job&#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWVXnOriGeBA_nYs&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 接下来就可以测试查询操作了。 match查询， 123456789# match查询GET lagou/job/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Python&quot; &#125; &#125;&#125; 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&#123; &quot;took&quot;: 200, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.25811607, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWVXnOriGeBA_nYs&quot;, &quot;_score&quot;: 0.25811607, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python打造推荐引擎系统&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉推荐引擎的原理以及算法，掌握C语言&quot;, &quot;comments&quot;: 60, &quot;add_time&quot;: &quot;2017-10-20&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWQ-nOriGeBA_nYp&quot;, &quot;_score&quot;: 0.19944568, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python django 开发工程师&quot;, &quot;company_name&quot;: &quot;美团科技有限公司&quot;, &quot;desc&quot;: &quot;对django的概念熟悉，熟悉python基础知识&quot;, &quot;comments&quot;: 20, &quot;add_time&quot;: &quot;2017-4-1&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWS6nOriGeBA_nYq&quot;, &quot;_score&quot;: 0.1383129, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python scrapy redis分布式爬虫基本&quot;, &quot;company_name&quot;: &quot;百度科技有限公司&quot;, &quot;desc&quot;: &quot;对scrapy的概念熟悉，熟悉redis的基本操作&quot;, &quot;comments&quot;: 5, &quot;add_time&quot;: &quot;2017-4-15&quot; &#125; &#125; ] &#125;&#125; 如果把&quot;title&quot;:&quot;python&quot;改为&quot;title&quot;:&quot;Python&quot;，依然能得到和上面一样的搜索结果，因为ik的分词器为自动地进行大小写转换。 term查询： 123456789# term查询GET lagou/job/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;python&quot; &#125; &#125;&#125; 发现查询的结果和match查询是一样的，但两者是有区别的，match查询经过ik分词，而term查询是把整个词拿去匹配的，就好像type是keyword一样，不对查询词做任何处理。 terms查询 123456789# terms查询GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;title&quot;: [&quot;工程师&quot;, &quot;django&quot;, &quot;系统&quot;] &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;took&quot;: 11, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.5164987, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWQ-nOriGeBA_nYp&quot;, &quot;_score&quot;: 1.5164987, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python django 开发工程师&quot;, &quot;company_name&quot;: &quot;美团科技有限公司&quot;, &quot;desc&quot;: &quot;对django的概念熟悉，熟悉python基础知识&quot;, &quot;comments&quot;: 20, &quot;add_time&quot;: &quot;2017-4-1&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWVXnOriGeBA_nYs&quot;, &quot;_score&quot;: 0.25811607, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python打造推荐引擎系统&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉推荐引擎的原理以及算法，掌握C语言&quot;, &quot;comments&quot;: 60, &quot;add_time&quot;: &quot;2017-10-20&quot; &#125; &#125; ] &#125;&#125; terms查询的特点是可以在查询时传入一个列表。 控制查询的返回数量 1234567891011# 控制查询的返回数量GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;python&quot; &#125; &#125;, &quot;from&quot;: 1, &quot;size&quot;: 2&#125; 通过from和size来控制结果的返回数量。 match_all查询 1234567# match_all查询GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; match_all查询会返回所有结果。 match_phrase查询 123456789101112# match_phrase短语查询GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;python系统&quot;, &quot;slop&quot;: 6 &#125; &#125; &#125;&#125; 运行结果： 12345678910111213141516171819202122232425262728&#123; &quot;took&quot;: 82, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.1133824, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWVXnOriGeBA_nYs&quot;, &quot;_score&quot;: 0.1133824, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python打造推荐引擎系统&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉推荐引擎的原理以及算法，掌握C语言&quot;, &quot;comments&quot;: 60, &quot;add_time&quot;: &quot;2017-10-20&quot; &#125; &#125; ] &#125;&#125; slop值表示两个词之间的最小距离。 multi_match查询 123456789101112# multi_match查询# 比如可以指定多个字段# 比如查询title和desc这两个字段里面包含python的关键词文档GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;python&quot;, &quot;fields&quot;: [&quot;title^3&quot;,&quot;desc&quot;] &#125; &#125;&#125; &quot;title^3&quot;的意思是为title设置较高的权重，对最后结果的排序有影响。 指定返回字段 123456789GET lagou/_search&#123; &quot;stored_fields&quot;: [&quot;title&quot;,&quot;company_name&quot;], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;python&quot; &#125; &#125;&#125; 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; &quot;took&quot;: 13, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.25811607, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWVXnOriGeBA_nYs&quot;, &quot;_score&quot;: 0.25811607, &quot;fields&quot;: &#123; &quot;title&quot;: [ &quot;python打造推荐引擎系统&quot; ], &quot;company_name&quot;: [ &quot;阿里巴巴科技有限公司&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWQ-nOriGeBA_nYp&quot;, &quot;_score&quot;: 0.19944568, &quot;fields&quot;: &#123; &quot;title&quot;: [ &quot;python django 开发工程师&quot; ], &quot;company_name&quot;: [ &quot;美团科技有限公司&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWS6nOriGeBA_nYq&quot;, &quot;_score&quot;: 0.1383129, &quot;fields&quot;: &#123; &quot;title&quot;: [ &quot;python scrapy redis分布式爬虫基本&quot; ], &quot;company_name&quot;: [ &quot;百度科技有限公司&quot; ] &#125; &#125; ] &#125;&#125; 通过sort把结果排序 1234567891011121314# 通过sort把结果排序GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;comments&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 运行结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&#123; &quot;took&quot;: 21, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWVXnOriGeBA_nYs&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python打造推荐引擎系统&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉推荐引擎的原理以及算法，掌握C语言&quot;, &quot;comments&quot;: 60, &quot;add_time&quot;: &quot;2017-10-20&quot; &#125;, &quot;sort&quot;: [ 60 ] &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWQ-nOriGeBA_nYp&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python django 开发工程师&quot;, &quot;company_name&quot;: &quot;美团科技有限公司&quot;, &quot;desc&quot;: &quot;对django的概念熟悉，熟悉python基础知识&quot;, &quot;comments&quot;: 20, &quot;add_time&quot;: &quot;2017-4-1&quot; &#125;, &quot;sort&quot;: [ 20 ] &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWUJnOriGeBA_nYr&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;elasticsearch打造搜索引擎&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉数据结构算法，熟悉python的基本开发&quot;, &quot;comments&quot;: 15, &quot;add_time&quot;: &quot;2017-6-20&quot; &#125;, &quot;sort&quot;: [ 15 ] &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWS6nOriGeBA_nYq&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python scrapy redis分布式爬虫基本&quot;, &quot;company_name&quot;: &quot;百度科技有限公司&quot;, &quot;desc&quot;: &quot;对scrapy的概念熟悉，熟悉redis的基本操作&quot;, &quot;comments&quot;: 5, &quot;add_time&quot;: &quot;2017-4-15&quot; &#125;, &quot;sort&quot;: [ 5 ] &#125; ] &#125;&#125; range查询 1234567891011121314# 查询范围# range查询GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;comments&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 20, &quot;boost&quot;: 2.0 &#125; &#125; &#125;&#125; 查询10&lt;=comments&lt;=20的结果。 运行结果： 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;took&quot;: 37, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 2, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWQ-nOriGeBA_nYp&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;python django 开发工程师&quot;, &quot;company_name&quot;: &quot;美团科技有限公司&quot;, &quot;desc&quot;: &quot;对django的概念熟悉，熟悉python基础知识&quot;, &quot;comments&quot;: 20, &quot;add_time&quot;: &quot;2017-4-1&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;job&quot;, &quot;_id&quot;: &quot;AV0RoWUJnOriGeBA_nYr&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;elasticsearch打造搜索引擎&quot;, &quot;company_name&quot;: &quot;阿里巴巴科技有限公司&quot;, &quot;desc&quot;: &quot;熟悉数据结构算法，熟悉python的基本开发&quot;, &quot;comments&quot;: 15, &quot;add_time&quot;: &quot;2017-6-20&quot; &#125; &#125; ] &#125;&#125; 也可以对时间进行range查询： 1234567891011GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;add_time&quot;: &#123; &quot;gte&quot;: &quot;2017-04-01&quot;, &quot;lte&quot;: &quot;now&quot; &#125; &#125; &#125;&#125; wildcard查询 123456789101112# wildcard查询GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;pyth*n&quot;, &quot;boost&quot;: 2.0 &#125; &#125; &#125;&#125; wildcard查询支持通配符。 Elasticsearch的bool组合查询首先使用bulk操作建立测试数据： 1234567891011121314151617181920# bool查询# 老版本的filtered已经被bool替换# 用bool包括must should must_not filter来完成，格式如下：# bool:&#123;# &quot;filter&quot;:[],# &quot;must&quot;:[],# &quot;should&quot;:[],# &quot;must_not&quot;:&#123;&#125;,# &#125;# 建立测试数据POST lagou/testjob/_bulk&#123;&quot;index&quot;:&#123;&quot;_id&quot;:1&#125;&#125;&#123;&quot;salary&quot;:10, &quot;title&quot;:&quot;Python&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:2&#125;&#125;&#123;&quot;salary&quot;:20, &quot;title&quot;:&quot;Scrapy&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:3&#125;&#125;&#123;&quot;salary&quot;:30, &quot;title&quot;:&quot;Django&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:4&#125;&#125;&#123;&quot;salary&quot;:30, &quot;title&quot;:&quot;Elasticsearch&quot;&#125; 接下来介绍简单的过滤查询： 最简单的filter查询 1234567891011121314151617# 薪资为20k的工作# select * from testjob where salary=20GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;salary&quot;: &quot;20&quot; &#125; &#125; &#125; &#125;&#125; 运行结果： 12345678910111213141516171819202122232425&#123; &quot;took&quot;: 43, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;testjob&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;salary&quot;: 20, &quot;title&quot;: &quot;Scrapy&quot; &#125; &#125; ] &#125;&#125; 也可以指定多个值 12345678910111213141516# 也可以指定多个值GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;salary&quot;:[10,20] &#125; &#125; &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;took&quot;: 39, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;testjob&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;salary&quot;: 20, &quot;title&quot;: &quot;Scrapy&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;testjob&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;salary&quot;: 10, &quot;title&quot;: &quot;Python&quot; &#125; &#125; ] &#125;&#125; 职位查询 12345678910111213141516# select * from testjob where title=&quot;Python&quot;GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Python&quot; &#125; &#125; &#125; &#125;&#125; 或者把&quot;Python&quot;变为小写&quot;python&quot;那么也可以用term。 查看分析器解析的结果 12345GET _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;Python网络开发工程师&quot;&#125; 运行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;python&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 6, &quot;type&quot;: &quot;ENGLISH&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;网络&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 8, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;络&quot;, &quot;start_offset&quot;: 7, &quot;end_offset&quot;: 8, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;开发&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 10, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;发&quot;, &quot;start_offset&quot;: 9, &quot;end_offset&quot;: 10, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 4 &#125;, &#123; &quot;token&quot;: &quot;工程师&quot;, &quot;start_offset&quot;: 10, &quot;end_offset&quot;: 13, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 5 &#125;, &#123; &quot;token&quot;: &quot;工程&quot;, &quot;start_offset&quot;: 10, &quot;end_offset&quot;: 12, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 6 &#125;, &#123; &quot;token&quot;: &quot;师&quot;, &quot;start_offset&quot;: 12, &quot;end_offset&quot;: 13, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 7 &#125; ]&#125; 如果用ik_smart会怎样呢？ 1234567891011121314151617181920212223242526272829303132&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;python&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 6, &quot;type&quot;: &quot;ENGLISH&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;网络&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 8, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;开发&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 10, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;工程师&quot;, &quot;start_offset&quot;: 10, &quot;end_offset&quot;: 13, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 3 &#125; ]&#125; bool过滤查询，可以做组合过滤查询 12345678910111213141516# select * from testjob where (salary=20 or title=Python) AND (salary != 30)# 查询薪资等于20k或者工作为python的工作，排除价格为30k的GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;:&#123;&quot;salary&quot;:20&#125;&#125;, &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;python&quot;&#125;&#125; ], &quot;must_not&quot;: &#123; &quot;term&quot;:&#123;&quot;price&quot;:30&#125; &#125; &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;testjob&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;salary&quot;: 20, &quot;title&quot;: &quot;Scrapy&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;testjob&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;salary&quot;: 10, &quot;title&quot;: &quot;Python&quot; &#125; &#125; ] &#125;&#125; 嵌套查询 1234567891011121314151617# select * from testjob where title=&quot;python&quot; or (title=&quot;elasticsearch&quot; AND salary=30)GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;python&quot;&#125;&#125;, &#123;&quot;bool&quot;:&#123; &quot;must&quot;: [ &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;elasticsearch&quot;&#125;&#125;, &#123;&quot;term&quot;:&#123;&quot;salary&quot;:30&#125;&#125; ] &#125;&#125; ] &#125; &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;took&quot;: 16, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.6931472, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;testjob&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.6931472, &quot;_source&quot;: &#123; &quot;salary&quot;: 30, &quot;title&quot;: &quot;Elasticsearch&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;lagou&quot;, &quot;_type&quot;: &quot;testjob&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;salary&quot;: 10, &quot;title&quot;: &quot;Python&quot; &#125; &#125; ] &#125;&#125; 下面需要介绍过滤空和非空的方法 123456789101112131415161718192021222324252627282930313233343536373839404142# 过滤空和非空# 建立测试数据POST lagou/testjob2/_bulk&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;tags&quot;:[&quot;search&quot;]&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&#123;&quot;tags&quot;:[&quot;search&quot;,&quot;python&quot;]&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;3&quot;&#125;&#125;&#123;&quot;other_field&quot;:[&quot;some data&quot;]&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;4&quot;&#125;&#125;&#123;&quot;tags&quot;:null&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;5&quot;&#125;&#125;&#123;&quot;tags&quot;:[&quot;search&quot;, null]&#125;# 处理null空值的方法# select tags from testjob2 where tags is not NULLGET lagou/testjob2/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;tags&quot; &#125; &#125; &#125; &#125;&#125;# 过滤非空值GET lagou/testjob2/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;tags&quot; &#125; &#125; &#125; &#125;&#125; scrapy写入数据到Elasticsearch中我们在scrapy中依靠pipeline来将数据写入到Elasticsearch中，这里需要介绍一个es的python接口elasticsearch-dsl-py。 传送门：elasticsearch-dsl-py 安装：pip install elasticsearch-dsl 在根目录下新建一个models包，在包内建立es_types.py文件： 1234567891011121314151617181920212223242526272829303132from datetime import datetimefrom elasticsearch_dsl import DocType, Date, Nested, Boolean, \\ analyzer, InnerObjectWrapper, Completion, Keyword, Text, Integerfrom elasticsearch_dsl.connections import connections# 连接本机esconnections.create_connection(hosts=[\"localhost\"])class ArticleType(DocType): \"\"\" 伯乐在线文章类型 \"\"\" title = Text(analyzer=\"ik_max_word\") create_date = Date() url = Keyword() url_object_id = Keyword() front_image_url = Keyword() front_image_path = Keyword() praise_nums = Integer() comment_nums = Integer() fav_nums = Integer() tags = Text(analyzer=\"ik_max_word\") content = Text(analyzer=\"ik_max_word\") class Meta: index = \"jobbole\" doc_type = \"article\"if __name__ == '__main__': ArticleType.init() 这样就可以很方便地在es中建立我们所需要的mappings，和Django中的models十分相似。 运行后我们可以再es-head中查看： pipelines.py中代码如下： 123456789101112131415161718192021222324252627from models.es_types import ArticleTypefrom w3lib.html import remove_tagsclass ElasticsearchPipeline(object): \"\"\" 将数据写入es中 \"\"\" def process_item(self, item, spider): # 将item转换为es的数据 article = ArticleType() article.title = item[\"title\"] article.create_date = item[\"create_date\"] article.content = remove_tags(item[\"content\"]) article.front_image_url = item[\"front_image_url\"] if \"front_image_path\" in item: article.front_image_path = item[\"front_image_path\"] article.praise_nums = item[\"praise_nums\"] article.fav_nums = item[\"fav_nums\"] article.comment_nums = item[\"comment_nums\"] article.url = item[\"url\"] article.tags = item[\"tags\"] article.meta.id = item[\"url_object_id\"] article.save() return item 调试之后发现数据顺利地进入到了es中： 为了进行对所有数据进行统一处理，我们可以把数据转换的逻辑拿到items.py中： 1234567891011121314151617def save_to_es(self): article = ArticleType() article.title = self[\"title\"] article.create_date = self[\"create_date\"] article.content = remove_tags(self[\"content\"]) article.front_image_url = self[\"front_image_url\"] if \"front_image_path\" in self: article.front_image_path = self[\"front_image_path\"] article.praise_nums = self[\"praise_nums\"] article.fav_nums = self[\"fav_nums\"] article.comment_nums = self[\"comment_nums\"] article.url = self[\"url\"] article.tags = self[\"tags\"] article.meta.id = self[\"url_object_id\"] article.save() return pipelines.py： 12345678910class ElasticsearchPipeline(object): \"\"\" 将数据写入es中 \"\"\" def process_item(self, item, spider): # 将item转换为es的数据 item.save_to_es() return item 其他的知乎拉勾的爬虫数据转换也基本类似。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python, Elasticsearch","slug":"Scrapy，Python-Elasticsearch","permalink":"http://yoursite.com/tags/Scrapy，Python-Elasticsearch/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——scrapy-redis分布式爬虫","slug":"scrapy-redis","date":"2017-07-01T12:18:54.000Z","updated":"2017-07-03T07:07:17.000Z","comments":true,"path":"2017/07/01/scrapy-redis/","link":"","permalink":"http://yoursite.com/2017/07/01/scrapy-redis/","excerpt":"分布式爬虫要点 分布式爬虫的优点 充分利用多机器的宽带加速爬取 充分利用多机的IP加速爬取速度 问题：为什么scrapy不支持分布式？ 答：在scrapy中scheduler是运行在队列中的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理，所以scrapy不支持分布式。 分布式爬虫需要解决的问题 requests队列集中管理 去重集中管理 综上，我们需要使用Redis来解决这些问题。","text":"分布式爬虫要点 分布式爬虫的优点 充分利用多机器的宽带加速爬取 充分利用多机的IP加速爬取速度 问题：为什么scrapy不支持分布式？ 答：在scrapy中scheduler是运行在队列中的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理，所以scrapy不支持分布式。 分布式爬虫需要解决的问题 requests队列集中管理 去重集中管理 综上，我们需要使用Redis来解决这些问题。 Redis基础知识Redis的基础知识在我早前的文章中已经学习过了，在这里就不介绍了，直接看之前的文章就行。 传送门：Redis学习笔记 scrapy-redis编写分布式爬虫代码传送门：1.scapy-redis Github 2.scrapy-redis 文档 其实大部分的逻辑是一样的，只需要在spider中加入redis_key = &#39;spidername:start_urls&#39;，以及修改一些settings.py中配置即可。 scrapy-redis源码解析 项目结构connection.py 负责根据setting中配置实例化redis连接。被dupefilter和scheduler调用，总之涉及到redis存取的都要使用到这个模块。 dupefilter.py 负责执行requst的去重，实现的很有技巧性，使用redis的set数据结构。但是注意scheduler并不使用其中用于在这个模块中实现的dupefilter键做request的调度，而是使用queue.py模块中实现的queue。 当request不重复时，将其存入到queue中，调度时将其弹出。 queue.py 其作用如II所述，但是这里实现了三种方式的queue： FIFO的SpiderQueue，SpiderPriorityQueue，以及LIFI的SpiderStack。默认使用的是第二中，这也就是出现之前文章中所分析情况的原因（链接：）。 pipelines.py 这是是用来实现分布式处理的作用。它将Item存储在redis中以实现分布式处理。 另外可以发现，同样是编写pipelines，在这里的编码实现不同于文章（链接：）中所分析的情况，由于在这里需要读取配置，所以就用到了from_crawler()函数。 scheduler.py 此扩展是对scrapy中自带的scheduler的替代（在settings的SCHEDULER变量中指出），正是利用此扩展实现crawler的分布式调度。其利用的数据结构来自于queue中实现的数据结构。 scrapy-redis所实现的两种分布式：爬虫分布式以及item处理分布式就是由模块scheduler和模块pipelines实现。上述其它模块作为为二者辅助的功能模块。 spider.py 设计的这个spider从redis中读取要爬的url,然后执行爬取，若爬取过程中返回更多的url，那么继续进行直至所有的request完成。之后继续从redis中读取url，循环这个过程。 分析：在这个spider中通过connect signals.spider_idle信号实现对crawler状态的监视。当idle时，返回新的make_requests_from_url(url)给引擎，进而交给调度器调度。 架构解析Scrapy架构： scrapy-redis架构： 如上图所示，scrapy-redis在scrapy的架构上增加了redis，基于redis的特性拓展了如下组件： 调度器（Scheduler）：scrapy-redis调度器通过redis的set不重复的特性，巧妙的实现了Duplication Filter去重（DupeFilter set存放爬取过的request）。Spider新生成的request，将request的指纹到redis的DupeFilter set检查是否重复，并将不重复的request push写入redis的request队列。调度器每次从redis的request队列里根据优先级pop出一个request, 将此request发给spider处理。 Item Pipeline：将Spider爬取到的Item给scrapy-redis的Item Pipeline，将爬取到的Item存入redis的items队列。可以很方便的从items队列中提取item，从而实现items processes 集群 集成bloomfilter到scrapy-redis中传送门：bloomfilter算法详解及实例 算法实现：bloomfilter_imooc dupefilter.py： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import loggingimport timefrom scrapy.dupefilters import BaseDupeFilterfrom scrapy.utils.request import request_fingerprintfrom . import defaultsfrom .connection import get_redis_from_settingsfrom ScrapyRedisTest.utils.bloomfilter import PyBloomFilter, connlogger = logging.getLogger(__name__)# TODO: Rename class to RedisDupeFilter.class RFPDupeFilter(BaseDupeFilter): \"\"\"Redis-based request duplicates filter. This class can also be used with default Scrapy's scheduler. \"\"\" logger = logger def __init__(self, server, key, debug=False): \"\"\"Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. \"\"\" self.server = server self.key = key self.debug = debug self.logdupes = True self.bf = PyBloomFilter(conn=conn, key=key) @classmethod def from_settings(cls, settings): \"\"\"Returns an instance from given settings. This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as it needs to pass the spider name in the key. Parameters ---------- settings : scrapy.settings.Settings Returns ------- RFPDupeFilter A RFPDupeFilter instance. \"\"\" server = get_redis_from_settings(settings) # XXX: This creates one-time key. needed to support to use this # class as standalone dupefilter with scrapy's default scheduler # if scrapy passes spider on open() method this wouldn't be needed # TODO: Use SCRAPY_JOB env as default and fallback to timestamp. key = defaults.DUPEFILTER_KEY % &#123;'timestamp': int(time.time())&#125; debug = settings.getbool('DUPEFILTER_DEBUG') return cls(server, key=key, debug=debug) @classmethod def from_crawler(cls, crawler): \"\"\"Returns instance from crawler. Parameters ---------- crawler : scrapy.crawler.Crawler Returns ------- RFPDupeFilter Instance of RFPDupeFilter. \"\"\" return cls.from_settings(crawler.settings) def request_seen(self, request): \"\"\"Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool \"\"\" fp = self.request_fingerprint(request) if self.bf.is_exist(fp): return True else: self.bf.add(fp) return False # This returns the number of values added, zero if already exists. # added = self.server.sadd(self.key, fp) # return added == 0 def request_fingerprint(self, request): \"\"\"Returns a fingerprint for a given request. Parameters ---------- request : scrapy.http.Request Returns ------- str \"\"\" return request_fingerprint(request) def close(self, reason=''): \"\"\"Delete data on close. Called by Scrapy's scheduler. Parameters ---------- reason : str, optional \"\"\" self.clear() def clear(self): \"\"\"Clears fingerprints data.\"\"\" self.server.delete(self.key) def log(self, request, spider): \"\"\"Logs given request. Parameters ---------- request : scrapy.http.Request spider : scrapy.spiders.Spider \"\"\" if self.debug: msg = \"Filtered duplicate request: %(request)s\" self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) elif self.logdupes: msg = (\"Filtered duplicate request %(request)s\" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\") self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) self.logdupes = False","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python，Redis","slug":"Scrapy，Python，Redis","permalink":"http://yoursite.com/tags/Scrapy，Python，Redis/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy 进阶开发","slug":"scrapy-advanced-dev","date":"2017-06-29T06:18:54.000Z","updated":"2017-07-04T06:14:51.000Z","comments":true,"path":"2017/06/29/scrapy-advanced-dev/","link":"","permalink":"http://yoursite.com/2017/06/29/scrapy-advanced-dev/","excerpt":"本篇主要介绍selenium的使用、其余的一些动态网页获取技术以及scrapy的一些进阶知识。","text":"本篇主要介绍selenium的使用、其余的一些动态网页获取技术以及scrapy的一些进阶知识。 Selenium的使用Selenium介绍Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本。 Selenium安装 Selenium安装完成之后，还需要下载浏览器对应的webdriver才能开始使用，我们这里选择Chrome的ChromeDriver。 Selenium动态网页请求123456789101112from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\")browser.get(\"https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5\")t_selector = Selector(text=browser.page_source)print(t_selector.css(\".tm-promo-price .tm-price::text\").extract())browser.quit() 我们用Selenium请求一个天猫商品的动态网页，并用Scrapy Selector来获取对应的商品价格信息。 Selenium模拟登录知乎 调试观察之后，采用Selenium自带的选择器方法来模拟输入账号密码并且点击登录。 12345678from selenium import webdriverbrowser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\")browser.get(\"https://www.zhihu.com/#signin\")browser.find_element_by_css_selector(\".view-signin input[name='account']\").send_keys(\"your_username\")browser.find_element_by_css_selector(\".view-signin input[name='password']\").send_keys(\"your_password\")browser.find_element_by_css_selector(\".view_signin button.sign-button\").click() Selenium模拟登录微博 首先调试观察微博登录页面 1234567from selenium import webdriverbrowser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\")browser.get(\"https://www.weibo.com\")browser.find_element_by_css_selector(\"#loginname\").send_keys(\"your_username\")browser.find_element_by_css_selector(\".info_list.password input[node-type='password']\").send_keys(\"your_password\")browser.find_element_by_css_selector(\".info_list.login_btn a[node-type='submitBtn']\").click() 发现如下错误 原因：我们在页面还没有请求完成时就进行了下一步操作，导致元素获取不到。 在请求发出之后，休眠一段时间等待页面加载完成即可。 12345678910import timefrom selenium import webdriverbrowser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\")browser.get(\"https://www.weibo.com\")time.sleep(15)browser.find_element_by_css_selector(\"#loginname\").send_keys(\"584563542@qq.com\")browser.find_element_by_css_selector(\".info_list.password input[node-type='password']\").send_keys(\"tracy584563542\")browser.find_element_by_css_selector(\".info_list.login_btn a[node-type='submitBtn']\").click() Selenium模拟鼠标下拉这样的操作是通过JS脚本来进行的： 123456789import timefrom selenium import webdriverbrowser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\")browser.get(\"https://www.oschina.net/blog\")for i in range(3): browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;\") time.sleep(3) 设置ChromeDriver不加载图片123456# 设置ChromeDriver不加载图片chrome_opt = webdriver.ChromeOptions()prefs = &#123;\"profile.managed_default_content_settings.images\": 2&#125;chrome_opt.add_experimental_option(\"prefs\", prefs)browser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\", chrome_options=chrome_opt)browser.get(\"https://www.taobao.com\") PhantomJS获取动态网页123456# phantomjs, 无界面的浏览器， 多进程情况下phantomjs性能会下降很严重browser = webdriver.PhantomJS(executable_path=\"/Users/lawtech/TempSpace/phantomjs-2.1.1-macosx/bin/phantomjs\")browser.get(\"https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5\")t_selector = Selector(text=browser.page_source)print(t_selector.css(\".tm-promo-price .tm-price::text\").extract())browser.quit() Selenium集成到Scrapy中123456789101112131415161718192021from selenium import webdriverfrom scrapy.http import HtmlResponseclass JSPageMiddleware(object): \"\"\" 通过Chrome动态请求网页 \"\"\" def __init__(self): self.browser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\") super(JSPageMiddleware, self).__init__() def process_request(self, request, spider): if spider.name == \"jobbole\": self.browser.get(request.url) import time time.sleep(3) print(\"访问&#123;0&#125;\".format(request.url)) return HtmlResponse(url=self.browser.current_url, body=self.browser.page_source, encoding='utf-8') 在middlewares.py中添加如上代码之后，别忘了在settings.py中将其配置好： 12345DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'ArticleSpider.middlewares.JSPageMiddleware': 1,&#125; 其实我们可以把Chrome放到Spider中，此时就引入了信号量的问题： 12345678910111213141516171819202122from scrapy.xlib.pydispatch import dispatcherfrom scrapy import signalsfrom selenium import webdriverclass JobboleSpider(scrapy.Spider): name = \"jobbole\" allowed_domains = [\"blog.jobbole.com\"] start_urls = ['http://blog.jobbole.com/all-posts/'] def __init__(self): self.browser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\") super(JobboleSpider, self).__init__() dispatcher.connect(self.spider_closed, signals.spider_closed) def spider_closed(self, spider): \"\"\" 当爬虫退出的时候关闭Chrome :param spider: :return: \"\"\" print(\"spider closed\") self.browser.quit() 其余动态网页获取技术Chrome无界面运行首先安装pyvirtualdisplay：pip install pyvirtualdisplay -i https://pypi.douban.com/simple/ 主要代码如下： 1234567from pyvirtualdisplay import Displaydisplay = Display(visible=0, size=(800, 600))display.start()browser = webdriver.Chrome(executable_path=\"/Users/lawtech/TempSpace/chromedriver\")browser.get(\"https://detail.tmall.com/item.htm?spm=a230r.1.14.13.bgHDMS&amp;id=539418030842&amp;cm_id=140105335569ed55e27b&amp;abbucket=5\")browser.quit() scrapy-splashhttps://github.com/scrapy-plugins/scrapy-splash 稳定性没有Chrome高 selenium-gridhttps://github.com/SeleniumHQ/selenium/wiki/Grid2 splinterhttps://github.com/cobrateam/splinter Scrapy的暂停与重启scrapy crawl lagou -s JOBDIR=jobinfo/001 上面这条命令即可完成lagouspider的暂停与重启，中途可以你可以使用ctrl+c终止采集程序的运行，恢复时，还是运行上面这条命令即可，连按两次ctrl+c就可以完全终止。 其中jobinfo/001 是一个保存采集列表状态的目录，千万不要同时开多个爬虫程序使用同一个目录，会导致混乱。 还有更简单的方法，就是在settings.py文件里加入下面的代码： JOBDIR=&#39;jobinfo/001&#39; 使用命令scrapy crawl lagou，就会自动生成一个jobinfo/001的目录，然后将工作列表放到这个文件夹里。 Scrapy url去重原理对url进行hash运算映射到某个地址，将该url和hash值当做键值对存放到hash表中，当需要检测新的url的时候，只需要对该url进行hash映射，如果得到的地址在hash表中已经存在，则说明已经被爬取过，则放弃爬取，否则，进行爬取并记录键值对。这样只需要维护一个hash表即可，需要考虑的问题是hash碰撞的问题，互联网上数据如瀚海般，如果hash函数设计不当，碰撞还是很容易发生的。scrapy框架下可以在pipeline中写一个Duplicates filter,背后采用的是hash值存储。 相关代码都在dupefilter.py中，其实就是做了一个哈希摘要，放在set中，去查新的url是否在set中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class RFPDupeFilter(BaseDupeFilter): \"\"\"Request Fingerprint duplicates filter\"\"\" def __init__(self, path=None, debug=False): self.file = None self.fingerprints = set() self.logdupes = True self.debug = debug self.logger = logging.getLogger(__name__) if path: self.file = open(os.path.join(path, 'requests.seen'), 'a+') self.file.seek(0) self.fingerprints.update(x.rstrip() for x in self.file) @classmethod def from_settings(cls, settings): debug = settings.getbool('DUPEFILTER_DEBUG') return cls(job_dir(settings), debug) def request_seen(self, request): fp = self.request_fingerprint(request) if fp in self.fingerprints: return True self.fingerprints.add(fp) if self.file: self.file.write(fp + os.linesep) def request_fingerprint(self, request): return request_fingerprint(request) def close(self, reason): if self.file: self.file.close() def log(self, request, spider): if self.debug: msg = \"Filtered duplicate request: %(request)s\" self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) elif self.logdupes: msg = (\"Filtered duplicate request: %(request)s\" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\") self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) self.logdupes = False spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider) Scrapy telnet服务Scrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。 telnet仅仅是一个运行在Scrapy进程中的普通python终端。 telnet终端监听设置中定义的 TELNETCONSOLE_PORT ，默认为 6023 。 访问telnet请输入: 12telnet localhost 6023&gt;&gt;&gt; Scrapy官方文档对telnet做了更详尽的介绍。 Spider middleware 详解传送门：Spider Middleware 上图为Scrapy源码中spidermiddlewares的结构 depth.py:爬取深度的设置 httperror.py：状态的设置，比如是不是要把404的也抓取下来，等等。 Scrapy的数据收集传送门：数据收集 Scrapy信号详解传送门：信号 示例： 123456789101112131415161718192021222324# 收集伯乐在线所有404的url以及404页面数handle_httpstatus_list = [404]def __init__(self): self.fail_urls = [] super(JobboleSpider, self).__init__() dispatcher.connect(self.handle_spider_closed, signals.spider_closed)def handle_spider_closed(self, spider, reason): self.crawler.stats.set_value(\"failed_urls\", \",\".join(self.fail_urls)) passdef parse(self, response): \"\"\" 1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析 2. 获取下一页的url并交给scrapy进行下载 :param response: :return: \"\"\" if response.status == 404: self.fail_urls.append(response.url) self.crawler.stats.inc_value(\"failed_url\") 调试结果： Scrapy扩展开发传送门：扩展","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——cookie禁用、自动限速、自定义Spider的settings","slug":"scrapy-cookies-settings","date":"2017-06-12T06:18:54.000Z","updated":"2017-06-22T08:29:40.000Z","comments":true,"path":"2017/06/12/scrapy-cookies-settings/","link":"","permalink":"http://yoursite.com/2017/06/12/scrapy-cookies-settings/","excerpt":"像cookie禁用、自动限速这样的设置都在settings.py文件中，下面我们就来简单介绍一下。","text":"像cookie禁用、自动限速这样的设置都在settings.py文件中，下面我们就来简单介绍一下。 cookie禁用12# Disable cookies (enabled by default)COOKIES_ENABLED = True 默认设置为False。 自动限速自动限速是通过自动限速(AutoThrottle)扩展来实现的，该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。 限速算法算法根据以下规则调整下载延迟及并发数: spider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。 当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。 AutoThrottle 扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比DOWNLOAD_DELAY 更低的下载延迟或者比 CONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数 (或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。 设置下面是控制 AutoThrottle 扩展的设置: AUTOTHROTTLE_ENABLED AUTOTHROTTLE_START_DELAY AUTOTHROTTLE_MAX_DELAY AUTOTHROTTLE_DEBUG CONCURRENT_REQUESTS_PER_DOMAIN CONCURRENT_REQUESTS_PER_IP DOWNLOAD_DELAY AUTOTHROTTLE_ENABLED默认: False 启用AutoThrottle扩展。 AUTOTHROTTLE_START_DELAY默认: 5.0 初始下载延迟(单位:秒)。 AUTOTHROTTLE_MAX_DELAY默认: 60.0 在高延迟情况下最大的下载延迟(单位秒)。 AUTOTHROTTLE_DEBUG默认: False 起用AutoThrottle调试(debug)模式，展示每个接收到的response。 您可以通过此来查看限速参数是如何实时被调整的。 自定义Spider的settings每个Spider可以定义自己的设置，这些设置将优先覆盖项目目录中的设置，可以通过设置 custom_settings 属性来实现。 例如，我们的项目中，zhihu.py 中需要设置开启cookie，那么只需要在该文件中如下设置即可： 123custom_settings = &#123; \"COOKIES_ENABLED\": True&#125; settings.py详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# -*- coding: utf-8 -*-import os# Scrapy settings for ArticleSpider project## For simplicity, this file contains only settings considered important or# commonly used. You can find more settings consulting the documentation:## http://doc.scrapy.org/en/latest/topics/settings.html# http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html# http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.htmlBOT_NAME = 'ArticleSpider' #Scrapy项目的名字这将用来构造默认User-Agent，同时也用来log，当您使用startproject命令创建项目时其也被自动赋值。SPIDER_MODULES = ['ArticleSpider.spiders'] #Scrapy搜索spider的模块列表 默认: [xxx.spiders]NEWSPIDER_MODULE = 'ArticleSpider.spiders' #使用genspider命令创建新spider的模块 默认: 'xxx.spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agent# USER_AGENT = 'ArticleSpider (+http://www.yourdomain.com)' #爬取的默认User-Agent，除非被覆盖# Obey robots.txt rulesROBOTSTXT_OBEY = False #如果启用,Scrapy将会采用robots.txt策略 # Configure maximum concurrent requests performed by Scrapy (default: 16)#Scrapy downloader并发请求(concurrent requests)的最大值，默认: 16# CONCURRENT_REQUESTS = 32# Configure a delay for requests for the same website (default: 0)# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docs#为同一网站的请求配置延迟（默认值：0）下载器在下载同一个网站下一个页面前需要等待的时间,该选项可以用来限制爬取速度,减轻服务器压力。同时也支持小数:0.25 以秒为单位 DOWNLOAD_DELAY = 10# The download delay setting will honor only one of:#下载延迟设置只有一个有效# CONCURRENT_REQUESTS_PER_DOMAIN = 16 对单个网站进行并发请求的最大值。# CONCURRENT_REQUESTS_PER_IP = 16 对单个IP进行并发请求的最大值。如果非0,则忽略 CONCURRENT_REQUESTS_PER_DOMAIN 设定,使用该设定。 也就是说,并发限制将针对IP,而不是网站。该设定也影响 DOWNLOAD_DELAY: 如果 CONCURRENT_REQUESTS_PER_IP 非0,下载延迟应用在IP而不是网站上。 # Disable cookies (enabled by default)#禁用Cookie（默认情况下启用）COOKIES_ENABLED = False# Disable Telnet Console (enabled by default)#禁用Telnet控制台（默认启用）# TELNETCONSOLE_ENABLED = False# Override the default request headers:#覆盖默认请求头：# DEFAULT_REQUEST_HEADERS = &#123;# 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',# 'Accept-Language': 'en',# &#125;# Enable or disable spider middlewares# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html#启用或禁用爬虫中间件# SPIDER_MIDDLEWARES = &#123;# 'ArticleSpider.middlewares.ArticlespiderSpiderMiddleware': 543,# &#125;# Enable or disable downloader middlewares# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#启用或禁用下载器中间件DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,&#125;# Enable or disable extensions# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html#启用或禁用扩展程序# EXTENSIONS = &#123;# 'scrapy.extensions.telnet.TelnetConsole': None,# &#125;# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; # 'ArticleSpider.pipelines.JsonExporterPipeline': 2, # # 'scrapy.pipelines.images.ImagesPipeline': 1, # 'ArticleSpider.pipelines.ArticleImagePipeline': 1, 'ArticleSpider.pipelines.MysqlTwistedPipeline': 1,&#125;IMAGES_URLS_FIELD = \"front_image_url\"project_dir = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_dir, 'images')# 设置搜索路径import osimport sysBASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))sys.path.insert(0, os.path.join(BASE_DIR, 'ArticleSpider'))USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4\"RANDOM_UA_TYPE = 'random'# IMAGES_MIN_HEIGHT = 100# IMAGES_MIN_WIDTH = 100# Enable and configure the AutoThrottle extension (disabled by default)# See http://doc.scrapy.org/en/latest/topics/autothrottle.html#启用和配置AutoThrottle扩展（默认情况下禁用）AUTOTHROTTLE_ENABLED = True# The initial download delay# AUTOTHROTTLE_START_DELAY = 5# The maximum download delay to be set in case of high latencies# AUTOTHROTTLE_MAX_DELAY = 60# The average number of requests Scrapy should be sending in parallel to# each remote server# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# Enable showing throttling stats for every response received:# AUTOTHROTTLE_DEBUG = False# Enable and configure HTTP caching (disabled by default)# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings#启用和配置HTTP缓存（默认情况下禁用）# HTTPCACHE_ENABLED = True# HTTPCACHE_EXPIRATION_SECS = 0# HTTPCACHE_DIR = 'httpcache'# HTTPCACHE_IGNORE_HTTP_CODES = []# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'MYSQL_HOST = \"127.0.0.1\"MYSQL_DBNAME = \"article_spider\"MYSQL_USER = \"root\"MYSQL_PASSWORD = \"123\"SQL_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"SQL_DATE_FORMAT = \"%Y-%m-%d\"","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——云打码实现验证码识别","slug":"scrapy-yundama","date":"2017-06-12T06:18:54.000Z","updated":"2017-06-22T07:19:19.000Z","comments":true,"path":"2017/06/12/scrapy-yundama/","link":"","permalink":"http://yoursite.com/2017/06/12/scrapy-yundama/","excerpt":"验证码识别大致有如下几种方式： 编码实现（Tesseract-OCR） 在线打码 人工打码","text":"验证码识别大致有如下几种方式： 编码实现（Tesseract-OCR） 在线打码 人工打码 这里我们简单介绍一下在线打码，选择的打码平台为：云打码 具体方式查看调用示例即可。 下面给出代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import jsonimport requestsclass YDMHttp(object): apiurl = 'http://api.yundama.com/api.php' username = '' password = '' appid = '' appkey = '' def __init__(self, username, password, appid, appkey): self.username = username self.password = password self.appid = str(appid) self.appkey = appkey def balance(self): data = &#123;'method': 'balance', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data[\"ret\"] == 0: print(\"获取剩余积分\", ret_data[\"balance\"]) return ret_data[\"balance\"] else: return None def login(self): data = &#123;'method': 'login', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data[\"ret\"] == 0: print(\"登录成功\", ret_data[\"uid\"]) return ret_data[\"uid\"] else: return None def decode(self, filename, codetype, timeout): data = &#123;'method': 'upload', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey, 'codetype': str(codetype), 'timeout': str(timeout)&#125; files = &#123;'file': open(filename, 'rb')&#125; response_data = requests.post(self.apiurl, files=files, data=data) ret_data = json.loads(response_data.text) if ret_data[\"ret\"] == 0: print(\"识别成功\", ret_data[\"text\"]) return ret_data[\"text\"] else: return Nonedef ydm(file_path): username = 'da_ge_da1' # 密码 password = 'da_ge_da' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = 3129 # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '40d5ad41c047179fc797631e3b9c3025' # 图片文件 filename = 'image/captcha.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 yundama = YDMHttp(username, password, appid, appkey) if (username == 'username'): print('请设置好相关参数再测试') else: # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 return yundama.decode(file_path, codetype, timeout);if __name__ == \"__main__\": # 用户名 username = 'da_ge_da1' # 密码 password = 'da_ge_da' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = 3129 # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '40d5ad41c047179fc797631e3b9c3025' # 图片文件 filename = 'image/captcha.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 if (username == 'username'): print('请设置好相关参数再测试') else: # 初始化 yundama = YDMHttp(username, password, appid, appkey) # 登陆云打码 uid = yundama.login() print('uid: %s' % uid) # 登陆云打码 uid = yundama.login() print('uid: %s' % uid) # 查询余额 balance = yundama.balance() print('balance: %s' % balance) # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 text = yundama.decode(filename, codetype, timeout)","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy随机更换User-Agent和实现IP代理池","slug":"scrapy-useragent-proxyip","date":"2017-06-11T06:18:54.000Z","updated":"2017-06-21T10:13:36.000Z","comments":true,"path":"2017/06/11/scrapy-useragent-proxyip/","link":"","permalink":"http://yoursite.com/2017/06/11/scrapy-useragent-proxyip/","excerpt":"之前有一节用于介绍Request和Response，还是官方文档介绍的比较详尽，所以就不做笔记了。 这一节用于介绍随机更换User-Agent和实现IP代理池的方法，首先来看一下现在网站中所做的反爬虫工作。","text":"之前有一节用于介绍Request和Response，还是官方文档介绍的比较详尽，所以就不做笔记了。 这一节用于介绍随机更换User-Agent和实现IP代理池的方法，首先来看一下现在网站中所做的反爬虫工作。 常见的反爬虫和应对方法一般网站从三个方面反爬虫：用户请求的Headers，用户行为，网站目录和数据加载方式。前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，这样增大了爬取的难度。 通过Headers反爬虫从用户请求的Headers反爬虫是最常见的反爬虫策略。很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）。如果遇到了这类反爬虫机制，可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中；或者将Referer值修改为目标网站域名。对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。 针对这样的反爬虫方法，我们在Scrapy中实现随机更换User-Agent就很有必要了。 基于用户行为反爬虫还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。 大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。这样的代理ip爬虫经常会用到，最好自己准备一个。有了大量代理ip后可以每请求几次更换一个ip，这在requests或者urllib2中很容易做到，这样就能很容易的绕过第一种反爬虫。 对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。 对于这样的反爬虫方法，我们可以在Scrapy中实现IP代理池，问题便迎刃而解。 动态页面的反爬虫上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。首先用Firebug或者HttpFox对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests或者urllib2模拟ajax请求，对响应的json进行分析得到需要的数据。 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。我这几天爬的那个网站就是这样，除了加密ajax参数，它还把一些基本的功能都封装了，全部都是在调用自己的接口，而接口参数都是加密的。遇到这样的网站，我们就不能用上面的方法了，我用的是selenium+phantomJS框架，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利用 selenium+phantomJS能干很多事情，例如识别点触式（12306）或者滑动式的验证码，对页面表单进行暴力破解等等。它在自动化渗透中还 会大展身手，以后还会提到这个。 随机更换User-Agent首先将下面的代码添加到settings.py文件，替换默认的user-agent处理模块： 1234DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,&#125; 在middlewares.py文件中自定义User-Agent处理模块： 123456789101112131415161718192021from fake_useragent import UserAgentclass RandomUserAgentMiddleware(object): \"\"\" 随机更换User-Agent \"\"\" def __init__(self, crawler): super(RandomUserAgentMiddleware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get('RANDOM_UA_TYPE', 'random') @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) 其中，fake_useragent这个第三方库维护了大量的User-Agent，我们就没必要自己去维护了，直接使用就好。 下图为其的安装和使用方法： 实现IP代理池我们用西刺免费代理IP网站来实现这个IP代理池。我们选择爬取这个网站，然后将获取的数据写入数据库以供我们自己使用。 调试之后，用CSS选择器来获取我们所需的内容，并将其保存到数据库中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import requestsfrom scrapy.selector import Selectorimport MySQLdbconn = MySQLdb.connect(host=\"127.0.0.1\", user=\"root\", passwd=\"123\", db=\"article_spider\", charset=\"utf8\")cursor = conn.cursor()def crawl_ips(): \"\"\" 爬取西刺网的免费代理IP \"\"\" headers = &#123; 'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4\" &#125; for i in range(2093): re = requests.get(\"http://www.xicidaili.com/nn/&#123;0&#125;\".format(i), headers=headers) selector = Selector(text=re.text) all_trs = selector.css(\"#ip_list tr\") ip_list = [] for tr in all_trs[1:]: speed_str = tr.css(\".bar::attr(title)\").extract()[0] if speed_str: speed = float(speed_str.split(\"秒\")[0]) ip = tr.css(\"td:nth-child(2)::text\").extract_first() port = tr.css(\"td:nth-child(3)::text\").extract_first() proxy_type = tr.css(\"td:nth-child(6)::text\").extract_first() ip_list.append((ip, port, proxy_type, speed)) for ip_info in ip_list: cursor.execute( \"insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', '&#123;2&#125;', '&#123;3&#125;')\".format( ip_info[0], ip_info[1], ip_info[3], ip_info[2] ) ) conn.commit()class GetIP(object): def delete_ip(self, ip): \"\"\" 从数据库中删除无效的IP \"\"\" delete_sql = \"\"\" DELETE FROM proxy_ip WHERE ip='&#123;0&#125;' \"\"\".format(ip) cursor.execute(delete_sql) conn.commit() return True def judge_ip(self, ip, port): \"\"\" 判断IP是否可用 \"\"\" http_url = \"http://www.baidu.com\" proxy_url = \"http://&#123;0&#125;:&#123;1&#125;\".format(ip, port) try: proxy_dict = &#123; \"http\": proxy_url, &#125; response = requests.get(http_url, proxies=proxy_dict) except Exception as e: print(\"invalid ip and port\") self.delete_ip(ip) return False else: code = response.status_code if code &gt;= 200 and code &lt; 300: print(\"effective ip\") return True else: print(\"invalid ip and port\") self.delete_ip(ip) return False def get_random_ip(self): \"\"\" 从数据库中随机获取一个可用的IP \"\"\" random_sql = \"\"\" SELECT ip, port FROM proxy_ip ORDER BY RAND() LIMIT 1 \"\"\" result = cursor.execute(random_sql) for ip_info in cursor.fetchall(): ip = ip_info[0] port = ip_info[1] judge_re = self.judge_ip(ip, port) if judge_re: return \"http://&#123;0&#125;:&#123;1&#125;\".format(ip, port) else: return self.get_random_ip()if __name__ == '__main__': get_ip = GetIP() get_ip.get_random_ip() 最后，在middlewares.py文件中写入我们用于设置IP代理的逻辑即可： 1234567class RandomProxyMiddleware(object): \"\"\" 动态设置IP代理 \"\"\" def process_request(self, request, spider): get_ip = GetIP() request.meta[\"proxy\"] = get_ip.get_random_ip() 可参考的第三方库 scrapy-crawlera scrapy-proxies","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy核心架构分析","slug":"scrapy-framework","date":"2017-06-11T06:18:54.000Z","updated":"2017-06-21T04:04:54.000Z","comments":true,"path":"2017/06/11/scrapy-framework/","link":"","permalink":"http://yoursite.com/2017/06/11/scrapy-framework/","excerpt":"概览首先看一下Scrapy的架构图：","text":"概览首先看一下Scrapy的架构图： 核心组件Scrapy有以下几大组件： Scrapy Engine：核心引擎，负责控制和调度各个组件，保证数据流转； Scheduler：负责管理任务、过滤任务、输出任务的调度器，存储、去重任务都在此控制； Downloader：下载器，负责在网络上下载网页数据，输入待下载URL，输出下载结果； Spiders：用户自己编写的爬虫脚本，可自定义抓取意图； Item Pipeline：负责输出结构化数据，可自定义输出位置； 除此之外，还有两大中间件组件： Downloader middlewares：介于引擎和下载器之间，可以在网页在下载前、后进行逻辑处理； Spider middlewares：介于引擎和爬虫之间，可以在调用爬虫输入下载结果和输出请求/数据时进行逻辑处理 数据流转按照架构图的序号，数据流转大概是这样的： 引擎从自定义爬虫中获取初始化请求（也叫种子URL）； 引擎把该请求放入调度器中，同时引擎向调度器获取一个待下载的请求（这两部是异步执行的）； 调度器返回给引擎一个待下载的请求； 引擎发送请求给下载器，中间会经过一系列下载器中间件； 这个请求通过下载器下载完成后，生成一个响应对象，返回给引擎，这中间会再次经过一系列下载器中间件； 引擎接收到下载返回的响应对象后，然后发送给爬虫，执行自定义爬虫逻辑，中间会经过一系列爬虫中间件； 爬虫执行对应的回调方法，处理这个响应，完成用户逻辑后，会生成结果对象或新的请求对象给引擎，再次经过一系列爬虫中间件； 引擎把爬虫返回的结果对象交由结果处理器处理，把新的请求对象通过引擎再交给调度器； 从1开始重复执行，直到调度器中没有新的请求处理； 核心组件交互图 这里需要说明一下图中的Scrapyer，其实这也是在源码的一个核心类，但官方架构图中没有展示出来，这个类其实是处于Engine、Spiders、Pipeline之间，是连通这3个组件的桥梁。 核心类图涉及到的一些核心类如下： 其中没有样式的黑色文字是类的核心属性，黄色样式的文字都是核心方法。 可以看到，Scrapy的核心类，其实主要包含5大组件、4大中间件管理器、爬虫类和爬虫管理器、请求、响应对象和数据解析类这几大块。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Requests，Python","slug":"Scrapy，Requests，Python","permalink":"http://yoursite.com/tags/Scrapy，Requests，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取拉勾网","slug":"scrapy-crawlspider-lagou","date":"2017-06-09T06:18:54.000Z","updated":"2017-06-09T08:31:25.000Z","comments":true,"path":"2017/06/09/scrapy-crawlspider-lagou/","link":"","permalink":"http://yoursite.com/2017/06/09/scrapy-crawlspider-lagou/","excerpt":"之前的Spider都是默认根据basic的templates创建，现在我们要用crawl的方式创建Spider，以爬取拉勾网整站信息。","text":"之前的Spider都是默认根据basic的templates创建，现在我们要用crawl的方式创建Spider，以爬取拉勾网整站信息。 CrawlSpider源码解析Spider基本上能做很多事情了，但是如果你想爬取知乎或者是简书全站的话，你可能需要一个更强大的武器。CrawlSpider基于Spider，但是可以说是为全站爬取而生。 简要说明CrawlSpider是爬取那些具有一定规则网站的常用的爬虫，它基于Spider并有一些独特属性 rules: 是Rule对象的集合，用于匹配目标网站并排除干扰 parse_start_url: 用于爬取起始响应，必须要返回Item，Request中的一个。 因为rules是Rule对象的集合，所以这里也要介绍一下Rule。它有几个参数：link_extractor、callback=None、cb_kwargs=None、follow=None、process_links=None、process_request=None其中的link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。 allow_domains：会被提取的链接的domains。 deny_domains：一定不会被提取链接的domains。 restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。还有一个类似的restrict_css 下面是官方提供的例子，我将从源代码的角度开始解读一些常见问题： 12345678910111213141516171819202122232425import scrapyfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'), ) def parse_item(self, response): self.logger.info('Hi, this is an item page! %s', response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)') item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').extract() item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').extract() return item 问题：CrawlSpider如何工作的？因为CrawlSpider继承了Spider，所以具有Spider的所有函数。首先由 start_requests 对 start_urls 中的每一个url发起请求（ make_requests_from_url )，这个请求会被parse接收。在Spider里面的parse需要我们定义，但CrawlSpider定义 parse 去解析响应（ self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True) ）_parse_response根据有无 callback ， follow 和 self.follow_links 执行不同的操作 1234567891011def _parse_response(self, response, callback, cb_kwargs, follow=True):##如果传入了callback，使用这个callback解析页面并获取解析得到的reques或item if callback: cb_res = callback(response, **cb_kwargs) or () cb_res = self.process_results(response, cb_res) for requests_or_item in iterate_spider_output(cb_res): yield requests_or_item## 其次判断有无follow，用_requests_to_follow解析响应是否有符合要求的link。 if follow and self._follow_links: for request_or_item in self._requests_to_follow(response): yield request_or_item 其中_requests_to_follow又会获取link_extractor（这个是我们传入的LinkExtractor）解析页面得到的link（link_extractor.extract_links(response)）,对url进行加工（process_links，需要自定义），对符合的link发起Request。使用.process_request(需要自定义）处理响应。 问题：CrawlSpider如何获取rules？CrawlSpider类会在__init__方法中调用_compile_rules方法，然后在其中浅拷贝rules中的各个Rule获取要用于回调(callback)，要进行处理的链接（process_links）和要进行的处理请求（process_request) 123456789101112def _compile_rules(self): def get_method(method): if callable(method): return method elif isinstance(method, six.string_types): return getattr(self, method, None) self._rules = [copy.copy(r) for r in self.rules] for rule in self._rules: rule.callback = get_method(rule.callback) rule.process_links = get_method(rule.process_links) rule.process_request = get_method(rule.process_request) 那么Rule是怎么样定义的呢？ 123456789101112class Rule(object): def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity): self.link_extractor = link_extractor self.callback = callback self.cb_kwargs = cb_kwargs or &#123;&#125; self.process_links = process_links self.process_request = process_request if follow is None: self.follow = False if callback else True else: self.follow = follow 因此LinkExtractor会传给link_extractor。 有callback的是由指定的函数处理，没有callback的是由哪个函数处理的？由上面的讲解可以发现_parse_response会处理有callback的（响应）respons。cb_res = callback(response, **cb_kwargs) or ()而_requests_to_follow会将self._response_downloaded传给callback用于对页面中匹配的url发起请求（request）。r = Request(url=link.url, callback=self._response_downloaded) 如何在CrawlSpider进行模拟登陆因为CrawlSpider和Spider一样，都要使用start_requests发起请求，用从Andrew_liu大神借鉴的代码说明如何模拟登陆： 123456789101112131415161718192021222324252627##替换原来的start_requests，callback为def start_requests(self): return [Request(\"http://www.zhihu.com/#signin\", meta = &#123;'cookiejar' : 1&#125;, callback = self.post_login)]def post_login(self, response): print 'Preparing login' #下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单 xsrf = Selector(response).xpath('//input[@name=\"_xsrf\"]/@value').extract()[0] print xsrf #FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单 #登陆成功后, 会调用after_login回调函数 return [FormRequest.from_response(response, #\"http://www.zhihu.com/login\", meta = &#123;'cookiejar' : response.meta['cookiejar']&#125;, headers = self.headers, formdata = &#123; '_xsrf': xsrf, 'email': '1527927373@qq.com', 'password': '321324jia' &#125;, callback = self.after_login, dont_filter = True )]#make_requests_from_url会调用parse，就可以与CrawlSpider的parse进行衔接了def after_login(self, response) : for url in self.start_urls : yield self.make_requests_from_url(url) 源码及注释123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899class CrawlSpider(Spider): rules = () def __init__(self, *a, **kw): super(CrawlSpider, self).__init__(*a, **kw) self._compile_rules() def parse(self, response): \"\"\" 首先调用parse()来处理start_urls中返回的response对象 parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url() 设置了跟进标志位True parse将返回item和跟进了的Request对象 \"\"\" return self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=True) def parse_start_url(self, response): \"\"\" 处理start_url中返回的response，需要重写 \"\"\" return [] def process_results(self, response, results): return results def _requests_to_follow(self, response): \"\"\" 从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回 \"\"\" if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): # 抽取之内的所有链接，只要通过任意一个'规则'，即表示合法 links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen] if links and rule.process_links: # 使用用户指定的process_links处理每个连接 links = rule.process_links(links) for link in links: # 将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded() seen.add(link) # 构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数 r = Request(url=link.url, callback=self._response_downloaded) r.meta.update(rule=n, link_text=link.text) # 对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request. yield rule.process_request(r) def _response_downloaded(self, response): \"\"\" 处理通过rule提取出的连接，并返回item以及request \"\"\" rule = self._rules[response.meta['rule']] return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow) def _parse_response(self, response, callback, cb_kwargs, follow=True): \"\"\" 解析response对象，会用callback解析处理他，并返回request或Item对象 首先判断是否设置了回调函数。（该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数） 如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象， 然后再交给process_results处理。返回cb_res的一个列表 \"\"\" if callback: # 如果是parse调用的，则会解析成Request对象 # 如果是rule callback，则会解析成Item cb_res = callback(response, **cb_kwargs) or () cb_res = self.process_results(response, cb_res) for requests_or_item in iterate_spider_output(cb_res): yield requests_or_item if follow and self._follow_links: #如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象 for request_or_item in self._requests_to_follow(response): yield request_or_item def _compile_rules(self): def get_method(method): if callable(method): return method elif isinstance(method, six.string_types): return getattr(self, method, None) self._rules = [copy.copy(r) for r in self.rules] for rule in self._rules: rule.callback = get_method(rule.callback) rule.process_links = get_method(rule.process_links) rule.process_request = get_method(rule.process_request) @classmethod def from_crawler(cls, crawler, *args, **kwargs): spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs) spider._follow_links = crawler.settings.getbool( 'CRAWLSPIDER_FOLLOW_LINKS', True) return spider def set_crawler(self, crawler): super(CrawlSpider, self).set_crawler(crawler) self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True) 创建CrawlSpider 出现问题：ImportError: No module named &#39;utils&#39; 原因：我们之前将项目目录下的 ArticleSpider 文件夹Mark为 Sources Root 导致。 解决办法：自己在 Settings.py 中设置搜索路径。 123456# 设置搜索路径import osimport sysBASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))sys.path.insert(0, os.path.join(BASE_DIR, 'ArticleSpider')) 设置之后重新创建spider 1$ scrapy genspider -t crawl lagou www.lagou.com 数据表结构及items设计 完整代码逻辑items.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def remove_splash(value): \"\"\" 去除工作城市的斜杠 \"\"\" return value.replace(\"/\", \"\")def handle_jobaddr(value): addr_list = value.split(\"\\n\") addr_list = [item.strip() for item in addr_list if item.strip() != \"查看地图\"] return \"\".join(addr_list)class LagouJobItemLoader(ItemLoader): default_output_processor = TakeFirst()class LagouJobItem(scrapy.Item): \"\"\" 拉勾网职位信息 \"\"\" title = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() salary = scrapy.Field() job_city = scrapy.Field( input_processor=MapCompose(remove_splash), ) work_years = scrapy.Field( input_processor=MapCompose(remove_splash), ) degree_need = scrapy.Field( input_processor=MapCompose(remove_splash), ) job_type = scrapy.Field() publish_time = scrapy.Field() job_advantage = scrapy.Field() job_desc = scrapy.Field() job_addr = scrapy.Field( input_processor=MapCompose(remove_tags, handle_jobaddr), ) company_name = scrapy.Field() company_url = scrapy.Field() tags = scrapy.Field( input_processor=Join(\",\") ) crawl_time = scrapy.Field() def get_insert_sql(self): insert_sql = \"\"\" insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need, job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url, tags, crawl_time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc) \"\"\" params = ( self[\"title\"], self[\"url\"], self[\"url_object_id\"], self[\"salary\"], self[\"job_city\"], self[\"work_years\"], self[\"degree_need\"], self[\"job_type\"], self[\"publish_time\"], self[\"job_advantage\"], self[\"job_desc\"], self[\"job_addr\"], self[\"company_name\"], self[\"company_url\"], self[\"tags\"], self[\"crawl_time\"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params lagou.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom datetime import datetimefrom items import LagouJobItemLoader, LagouJobItemfrom utils.common import get_md5class LagouSpider(CrawlSpider): name = 'lagou' allowed_domains = ['www.lagou.com'] start_urls = ['https://www.lagou.com/'] rules = ( Rule(LinkExtractor(allow=('zhaopin/.*',)), ), Rule(LinkExtractor(allow=('gongsi/j\\d+.html',)), ), Rule(LinkExtractor(allow=r'jobs/\\d+.html'), callback='parse_job', follow=True), ) def parse_job(self, response): \"\"\" 解析拉勾网的职位 \"\"\" item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response) item_loader.add_css(\"title\", \".job-name::attr(title)\") item_loader.add_value(\"url\", response.url) item_loader.add_value(\"url_object_id\", get_md5(response.url)) item_loader.add_css(\"salary\", \".job_request .salary::text\") item_loader.add_xpath(\"job_city\", \"//*[@class='job_request']/p/span[2]/text()\") item_loader.add_xpath(\"work_years\", \"//*[@class='job_request']/p/span[3]/text()\") item_loader.add_xpath(\"degree_need\", \"//*[@class='job_request']/p/span[4]/text()\") item_loader.add_xpath(\"job_type\", \"//*[@class='job_request']/p/span[5]/text()\") item_loader.add_css(\"publish_time\", \".publish_time::text\") item_loader.add_css(\"job_advantage\", \".job-advantage p::text\") item_loader.add_css(\"job_desc\", \".job_bt div\") item_loader.add_css(\"job_addr\", \".work_addr\") item_loader.add_css(\"company_name\", \"#job_company dt a img::attr(alt)\") item_loader.add_css(\"company_url\", \"#job_company dt a::attr(href)\") item_loader.add_css(\"tags\", \".position-label li::text\") item_loader.add_value(\"crawl_time\", datetime.now()) job_item = item_loader.load_item() return job_item","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Requests，Python","slug":"Scrapy，Requests，Python","permalink":"http://yoursite.com/tags/Scrapy，Requests，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy爬取知乎","slug":"scrapy-zhihu","date":"2017-06-07T06:18:54.000Z","updated":"2017-06-07T10:41:56.000Z","comments":true,"path":"2017/06/07/scrapy-zhihu/","link":"","permalink":"http://yoursite.com/2017/06/07/scrapy-zhihu/","excerpt":"在完成了Scrapy模拟登录知乎后，下一步要进行的就是进行对知乎页面，问题以及答案等内容的爬取工作了。","text":"在完成了Scrapy模拟登录知乎后，下一步要进行的就是进行对知乎页面，问题以及答案等内容的爬取工作了。 通过Scrapy Shell进行调试在使用shell调试时，直接通过 scrapy shell https://www.zhihu.com/question/58765535 会出现error 500错误。这是因为没有加headers的原因。正确添加headers的方法是：scrapy -s USER_AGENT=&quot;任意的User Agent&quot; 。此时，就可以在shell中进行分析了。 获得要分析的链接在登录完成进入首页之后，通过深度优先算法获得首页需要的链接，然后打开这些链接再次获得里面的链接，不断重复，获得所有内容。 1234567from urllib import parse def parse(self, response): # 因为没有具体的入口，采用深度优先的算法 all_urls = response.css(\"a::attr(href)\").extract() all_urls = [parse.urljoin(response.url, url) for url in all_urls] for url in all_urls: pass 在分析页面内容之后，设计我们所需的数据表zhihu_question和zhihu_answer 完整代码 zhihu.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179# -*- coding: utf-8 -*-import scrapyimport reimport jsonimport datetimefrom scrapy.loader import ItemLoaderfrom items import ZhihuAnswerItem, ZhihuQuestionItemtry: import urlparse as parseexcept: from urllib import parseclass ZhihuSpider(scrapy.Spider): name = \"zhihu\" allowed_domains = [\"www.zhihu.com\"] start_urls = ['https://www.zhihu.com/'] # question的第一页answer的请求url start_answer_url = \"https://www.zhihu.com/api/v4/questions/&#123;0&#125;/answers?sort_by=default&amp;include=data%5B%2A%5D.is_normal%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccollapsed_counts%2Creviewing_comments_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Crelationship.is_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.is_blocking%2Cis_blocked%2Cis_followed%2Cvoteup_count%2Cmessage_thread_token%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=&#123;1&#125;&amp;offset=&#123;2&#125;\" headers = &#123; \"Host\": \"www.zhihu.com\", \"Referer\": \"https://www.zhihu.com/\", 'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/603.2.4 (KHTML, like Gecko) Version/10.1.1 Safari/603.2.4\" &#125; def parse(self, response): \"\"\" 提取出html页面中的所有url 并跟踪这些url进行一步爬取 如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数 \"\"\" all_urls = response.css(\"a::attr(href)\").extract() all_urls = [parse.urljoin(response.url, url) for url in all_urls] all_urls = filter(lambda x: True if x.startswith(\"https\") else False, all_urls) for url in all_urls: match_obj = re.match(\"(.*zhihu.com/question/(\\d+))(/|$).*\", url) if match_obj: # 如果提取到question相关的页面则下载后交由提取函数进行提取 request_url = match_obj.group(1) yield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) else: # 如果不是question页面则直接进一步跟踪 yield scrapy.Request(url, headers=self.headers, callback=self.parse) def parse_question(self, response): \"\"\" 处理question页面， 从页面中提取出具体的question item \"\"\" # 处理question页面， 从页面中提取出具体的question item if \"QuestionHeader-title\" in response.text: # 处理新版本 match_obj = re.match(\"(.*zhihu.com/question/(\\d+))(/|$).*\", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) item_loader.add_css(\"title\", \"h1.QuestionHeader-title::text\") item_loader.add_css(\"content\", \".QuestionHeader-detail\") item_loader.add_value(\"url\", response.url) item_loader.add_value(\"zhihu_id\", question_id) item_loader.add_css(\"answer_num\", \".List-headerText span::text\") item_loader.add_css(\"comments_num\", \".QuestionHeader-Comment button::text\") item_loader.add_css(\"watch_user_num\", \".NumberBoard-value::text\") item_loader.add_css(\"topics\", \".QuestionHeader-topics .Popover div::text\") question_item = item_loader.load_item() else: # 处理老版本页面的item提取 match_obj = re.match(\"(.*zhihu.com/question/(\\d+))(/|$).*\", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) # item_loader.add_css(\"title\", \".zh-question-title h2 a::text\") item_loader.add_xpath(\"title\", \"//*[@id='zh-question-title']/h2/a/text()|//*[@id='zh-question-title']/h2/span/text()\") item_loader.add_css(\"content\", \"#zh-question-detail\") item_loader.add_value(\"url\", response.url) item_loader.add_value(\"zhihu_id\", question_id) item_loader.add_css(\"answer_num\", \"#zh-question-answer-num::text\") item_loader.add_css(\"comments_num\", \"#zh-question-meta-wrap a[name='addcomment']::text\") # item_loader.add_css(\"watch_user_num\", \"#zh-question-side-header-wrap::text\") item_loader.add_xpath(\"watch_user_num\", \"//*[@id='zh-question-side-header-wrap']/text()|//*[@class='zh-question-followers-sidebar']/div/a/strong/text()\") item_loader.add_css(\"topics\", \".zm-tag-editor-labels a::text\") question_item = item_loader.load_item() yield scrapy.Request(self.start_answer_url.format(question_id, 20, 0), headers=self.headers, callback=self.parse_answer) yield question_item def parse_answer(self, reponse): \"\"\" 处理question的answer \"\"\" ans_json = json.loads(reponse.text) is_end = ans_json[\"paging\"][\"is_end\"] next_url = ans_json[\"paging\"][\"next\"] # 提取answer的具体字段 for answer in ans_json[\"data\"]: answer_item = ZhihuAnswerItem() answer_item[\"zhihu_id\"] = answer[\"id\"] answer_item[\"url\"] = answer[\"url\"] answer_item[\"question_id\"] = answer[\"question\"][\"id\"] answer_item[\"author_id\"] = answer[\"author\"][\"id\"] if \"id\" in answer[\"author\"] else None answer_item[\"content\"] = answer[\"content\"] if \"content\" in answer else None answer_item[\"praise_num\"] = answer[\"voteup_count\"] answer_item[\"comments_num\"] = answer[\"comment_count\"] answer_item[\"create_time\"] = answer[\"created_time\"] answer_item[\"update_time\"] = answer[\"updated_time\"] answer_item[\"crawl_time\"] = datetime.datetime.now() yield answer_item if not is_end: yield scrapy.Request(next_url, headers=self.headers, callback=self.parse_answer) def start_requests(self): return [scrapy.Request('https://www.zhihu.com/#signin', headers=self.headers, callback=self.login)] def login(self, response): response_text = response.text match_obj = re.match('.*name=\"_xsrf\" value=\"(.*?)\"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = (match_obj.group(1)) if xsrf: post_url = \"https://www.zhihu.com/login/phone_num\" post_data = &#123; \"_xsrf\": xsrf, \"phone_num\": \"18251556927\", \"password\": \"lawtech0301520\", \"captcha\": \"\" &#125; import time t = str(int(time.time() * 1000)) captcha_url = \"https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login\".format(t) yield scrapy.Request(captcha_url, headers=self.headers, meta=&#123;\"post_data\": post_data&#125;, callback=self.login_after_captcha) def login_after_captcha(self, response): with open(\"captcha.jpg\", \"wb\") as f: f.write(response.body) f.close() from PIL import Image try: im = Image.open('captcha.jpg') im.show() im.close() except: pass captcha = input(\"输入验证码\\n&gt;\") post_data = response.meta.get(\"post_data\", &#123;&#125;) post_url = \"https://www.zhihu.com/login/phone_num\" post_data[\"captcha\"] = captcha return [scrapy.FormRequest( url=post_url, formdata=post_data, headers=self.headers, callback=self.check_login )] def check_login(self, response): \"\"\" 验证服务器的返回数据判断是否成功 \"\"\" text_json = json.loads(response.text) if \"msg\" in text_json and text_json[\"msg\"] == \"登录成功\": for url in self.start_urls: yield scrapy.Request(url, dont_filter=True, headers=self.headers) 为了用同一个Pipeline处理所有的数据库存储操作，因此将操作都放入items中，再有Pipeline进行统一处理。 items.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class ZhihuQuestionItem(scrapy.Item): \"\"\" 知乎问题Item \"\"\" zhihu_id = scrapy.Field() topics = scrapy.Field() url = scrapy.Field() title = scrapy.Field() content = scrapy.Field() answer_num = scrapy.Field() comments_num = scrapy.Field() watch_user_num = scrapy.Field() click_num = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): # 插入知乎question表的sql语句 insert_sql = \"\"\" insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) \"\"\" zhihu_id = self[\"zhihu_id\"][0] topics = \",\".join(self[\"topics\"]) url = self[\"url\"][0] title = \"\".join(self[\"title\"]) content = \"\".join(self[\"content\"]) answer_num = extract_num(\"\".join(self[\"answer_num\"])) comments_num = extract_num(\"\".join(self[\"comments_num\"])) if len(self[\"watch_user_num\"]) == 2: watch_user_num = int(self[\"watch_user_num\"][0]) click_num = int(self[\"watch_user_num\"][1]) else: watch_user_num = int(self[\"watch_user_num\"][0]) click_num = 0 crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT) params = (zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time) return insert_sql, paramsclass ZhihuAnswerItem(scrapy.Item): \"\"\" 知乎回答Item \"\"\" zhihu_id = scrapy.Field() url = scrapy.Field() question_id = scrapy.Field() author_id = scrapy.Field() content = scrapy.Field() praise_num = scrapy.Field() comments_num = scrapy.Field() create_time = scrapy.Field() update_time = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): # 插入知乎question表的sql语句 insert_sql = \"\"\" insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, praise_num, comments_num, create_time, update_time, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), praise_num=VALUES(praise_num), update_time=VALUES(update_time) \"\"\" create_time = datetime.datetime.fromtimestamp(self['create_time']).strftime(SQL_DATETIME_FORMAT) update_time = datetime.datetime.fromtimestamp(self['update_time']).strftime(SQL_DATETIME_FORMAT) params = ( self[\"zhihu_id\"], self[\"url\"], self[\"question_id\"], self[\"author_id\"], self[\"content\"], self[\"praise_num\"], self[\"comments_num\"], create_time, update_time, self[\"crawl_time\"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params Pipelines.py 12345678910111213141516171819202122232425262728293031323334class MysqlTwistedPipeline(object): # 采用异步的机制写入mysql def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparms = dict( host=settings[\"MYSQL_HOST\"], db=settings[\"MYSQL_DBNAME\"], user=settings[\"MYSQL_USER\"], passwd=settings[\"MYSQL_PASSWORD\"], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool(\"MySQLdb\", **dbparms) return cls(dbpool) def process_item(self, item, spider): # 使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) # 处理异常 def handle_error(self, failure, item, spider): # 处理异步插入的异常 print(failure) def do_insert(self, cursor, item): # 执行具体的插入 # 根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params)","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Requests，Python","slug":"Scrapy，Requests，Python","permalink":"http://yoursite.com/tags/Scrapy，Requests，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Requests模拟登录知乎","slug":"scrapy-requests-zhihu-login","date":"2017-05-11T06:18:54.000Z","updated":"2017-05-12T06:55:07.000Z","comments":true,"path":"2017/05/11/scrapy-requests-zhihu-login/","link":"","permalink":"http://yoursite.com/2017/05/11/scrapy-requests-zhihu-login/","excerpt":"Requests 是以 PEP 20 的箴言为中心开发的 Beautiful is better than ugly.(美丽优于丑陋) Explicit is better than implicit.(直白优于含蓄) Simple is better than complex.(简单优于复杂) Complex is better than complicated.(复杂优于繁琐) Readability counts.(可读性很重要)","text":"Requests 是以 PEP 20 的箴言为中心开发的 Beautiful is better than ugly.(美丽优于丑陋) Explicit is better than implicit.(直白优于含蓄) Simple is better than complex.(简单优于复杂) Complex is better than complicated.(复杂优于繁琐) Readability counts.(可读性很重要) 常见状态码 表达式 说明 200 请求被正确执行 301/302 永久性重定向/临时性重定向 403 没有权限访问 404 没有资源访问 500 服务器错误 503 服务器停机或正在维护 登录分析在登录界面，输入手机号和密码，返回的地址为 Request URL:https://www.zhihu.com/login/phone_num当输入email地址后返回的地址为 Request URL:https://www.zhihu.com/login/email并且在formdata中出现 _xsrf:a71f46d549979fa192c09e11e4a463b5 这样的字符串。 抓取xsrf的值正则匹配抓取xsrf需要使用header头来进行源代码的获取： 1234567891011def get_xsrf(): \"\"\" 获取xsrf code :return: xsrf code \"\"\" response = requests.get(\"https://www.zhihu.com\", headers=headers) match_obj = re.match('.*name=\"_xsrf\" value=\"(.*?)\"', response.text) if match_obj: print(match_obj.group(1)) else: return \"\" 验证码获取123456789101112def get_captcha(): t = str(int(time.time() * 1000)) captcha_url = 'https://www.zhihu.com/captcha.gif?r=' + t + \"&amp;type=login\" r = session.get(captcha_url, headers=headers) with open('captcha.jpg', 'wb') as f: f.write(r.content) f.close() im = Image.open('captcha.jpg') im.show() im.close() captcha = input(\"请输入验证码：\\n\") return captcha 登录逻辑1234567891011121314151617181920212223242526272829303132333435363738394041def zhihu_login(account, password): \"\"\" 知乎登录 :param account: :param password: :return: \"\"\" if re.match(\"^1\\d&#123;10&#125;$\", account): print(\"手机号码登录 \\n\") post_url = \"https://www.zhihu.com/login/phone_num\" post_data = &#123; \"_xsrf\": get_xsrf(), \"phone_num\": account, \"password\": password &#125; else: if \"@\" in account: print(\"邮箱登录 \\n\") else: print(\"你的账号输入有问题，请重新登录\") return 0 post_url = 'https://www.zhihu.com/login/email' post_data = &#123; '_xsrf': get_xsrf(), 'password': password, 'email': account &#125; # 不需要验证码直接登录成功 login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() if login_code['r'] == 1: # 不输入验证码登录失败 # 使用需要输入验证码的方式登录 post_data[\"captcha\"] = get_captcha() login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() print(login_code['msg']) # 保存 cookies 到文件， # 下次可以使用 cookie 直接登录，不需要输入账号和密码 session.cookies.save() 以上代码是通过引入requests库，使用它的session方法，进行连接，构造post_data，把自己的用户名密码等信息发送到网站，并通过正则判断发送的是邮箱或是手机进行登录。引入import http.cookiejar as cookielib，通过session.cookies.save()，对cookie进行保存。 通过Cookie登录12345678910111213141516171819# 使用登录cookie信息session = requests.session()session.cookies = cookielib.LWPCookieJar(filename=\"cookies.txt\")try: session.cookies.load(ignore_discard=True)except: print(\"Cookie未能加载\") def is_login(): \"\"\" 通过查看用户个人信息来判断是否已经登录 :return: \"\"\" url = \"https://www.zhihu.com/settings/profile\" response = session.get(url, headers=headers, allow_redirects=False) if response.status_code == 200: return True else: return False 登录只能一次，如果再次登录，可以直接通过查看cookie来判断是否为登录状态。 首先把cookie通过session.cookies.load装载进来，执行is_login()函数，如果成功可以访问inbox_url页面，则状态码为200表示成功。这里一定要注意allow_redirects=False，当不允许且登录时会自动跳转到登录页面，则状态码是301或者302。 完整代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124# _*_ coding: utf-8 _*_\"\"\"__author__ = 'lawtech'__date__ = '2017/5/9 下午3:18'\"\"\"import reimport requestsimport timefrom PIL import Imagetry: import cookielibexcept: import http.cookiejar as cookielib# 构造requests headersagent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36'headers = &#123; \"Host\": \"www.zhihu.com\", \"Referer\": \"https://www.zhihu.com/\", 'User-Agent': agent&#125;# 使用登录cookie信息session = requests.session()session.cookies = cookielib.LWPCookieJar(filename=\"cookies.txt\")try: session.cookies.load(ignore_discard=True)except: print(\"Cookie未能加载\")def get_xsrf(): \"\"\" 获取xsrf code :return: xsrf code \"\"\" response = requests.get(\"https://www.zhihu.com\", headers=headers) match_obj = re.match('.*name=\"_xsrf\" value=\"(.*?)\"', response.text) if match_obj: print(match_obj.group(1)) else: return \"\"def get_captcha(): t = str(int(time.time() * 1000)) captcha_url = 'https://www.zhihu.com/captcha.gif?r=' + t + \"&amp;type=login\" r = session.get(captcha_url, headers=headers) with open('captcha.jpg', 'wb') as f: f.write(r.content) f.close() im = Image.open('captcha.jpg') im.show() im.close() captcha = input(\"请输入验证码：\\n\") return captchadef zhihu_login(account, password): \"\"\" 知乎登录 :param account: :param password: :return: \"\"\" if re.match(\"^1\\d&#123;10&#125;$\", account): print(\"手机号码登录 \\n\") post_url = \"https://www.zhihu.com/login/phone_num\" post_data = &#123; \"_xsrf\": get_xsrf(), \"phone_num\": account, \"password\": password &#125; else: if \"@\" in account: print(\"邮箱登录 \\n\") else: print(\"你的账号输入有问题，请重新登录\") return 0 post_url = 'https://www.zhihu.com/login/email' post_data = &#123; '_xsrf': get_xsrf(), 'password': password, 'email': account &#125; # 不需要验证码直接登录成功 login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() if login_code['r'] == 1: # 不输入验证码登录失败 # 使用需要输入验证码的方式登录 post_data[\"captcha\"] = get_captcha() login_page = session.post(post_url, post_data, headers=headers) login_code = login_page.json() print(login_code['msg']) # 保存 cookies 到文件， # 下次可以使用 cookie 直接登录，不需要输入账号和密码 session.cookies.save()def is_login(): \"\"\" 通过查看用户个人信息来判断是否已经登录 :return: \"\"\" url = \"https://www.zhihu.com/settings/profile\" response = session.get(url, headers=headers, allow_redirects=False) if response.status_code == 200: return True else: return Falseif __name__ == '__main__': if is_login(): print(\"您已经登录！\") else: account = input(\"请输入用户名：\\n\") password = input(\"请输入密码：\\n\") zhihu_login(account, password)","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Requests，Python","slug":"Scrapy，Requests，Python","permalink":"http://yoursite.com/tags/Scrapy，Requests，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy模拟登录知乎","slug":"scrapy-login-zhihu","date":"2017-05-11T06:18:54.000Z","updated":"2017-05-12T08:12:46.000Z","comments":true,"path":"2017/05/11/scrapy-login-zhihu/","link":"","permalink":"http://yoursite.com/2017/05/11/scrapy-login-zhihu/","excerpt":"Scrapy登录知乎要解决两个问题 session的传递，保证处理登录是同一个状态。 首个登录页面的改变，由直接爬取的页面变为登录页面，再去爬取页面。","text":"Scrapy登录知乎要解决两个问题 session的传递，保证处理登录是同一个状态。 首个登录页面的改变，由直接爬取的页面变为登录页面，再去爬取页面。 话不多说，直接上代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# -*- coding: utf-8 -*-import scrapyimport reimport jsonimport timefrom PIL import Imageclass ZhihuSpider(scrapy.Spider): name = \"zhihu\" allowed_domains = [\"www.zhihu.com\"] start_urls = ['http://www.zhihu.com/'] headers = &#123; \"Host\": \"www.zhihu.com\", \"Referer\": \"https://www.zhihu.com/\", 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36' &#125; def parse(self, response): pass def start_requests(self): return [scrapy.Request(\"https://www.zhihu.com/#signin\", headers=self.headers, callback=self.login)] def login(self, response): \"\"\" 登录 :param response: :return: \"\"\" response_text = response.text match_obj = re.match('.*name=\"_xsrf\" value=\"(.*?)\"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = match_obj.group(1) if xsrf: post_data = &#123; \"_xsrf\": xsrf, \"phone_num\": \"18951855817\", \"password\": \"tracy584563542\" &#125; t = str(int(time.time() * 1000)) captcha_url = 'https://www.zhihu.com/captcha.gif?r=' + t + \"&amp;type=login\" yield scrapy.Request(captcha_url, headers=self.headers, meta=&#123;\"post_data\": post_data&#125;, callback=self.login_after_captcha) def login_after_captcha(self, response): with open(\"captcha.jpg\", \"wb\") as f: f.write(response.body) im = Image.open('captcha.jpg') im.show() im.close() captcha = input(\"请输入验证码：\\n\") post_data = response.meta.get(\"post_data\", &#123;&#125;) post_data[\"captcha\"] = captcha post_url = \"https://www.zhihu.com/login/phone_num\" return [scrapy.FormRequest( url=post_url, formdata=post_data, headers=self.headers, callback=self.check_login )] def check_login(self, response): \"\"\" 验证服务器的返回数据判断登录是否成功 :param response: :return: \"\"\" text_json = json.loads(response.text) if 'msg' in text_json and text_json['msg'] == '登陆成功': # 从继承的Spider类中拿的内容，恢复到正确执行 for url in self.start_urls: yield scrapy.Request(url, dont_filter=True, headers=self.headers) 首先对 scrapy.Spider 类中的 start_requests(self) 进行重载，改变首先要处理的页面为登录页面。得到登录页面后，获得xsrf，并下载验证码，通过 scrapy.FormRequest构造登录数据，通过check_login回调函数判断登录是否成功。在代码的最后一行转回正常的登录流程。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——理解Session和Cookie机制","slug":"scrapy-session-cookie","date":"2017-05-09T06:18:54.000Z","updated":"2017-05-09T06:43:11.000Z","comments":true,"path":"2017/05/09/scrapy-session-cookie/","link":"","permalink":"http://yoursite.com/2017/05/09/scrapy-session-cookie/","excerpt":"Cookie 机制Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。","text":"Cookie 机制Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。IETF RFC 2965 HTTP State Management Mechanism 是通用cookie规范。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies 。 具体来说cookie机制采用的是在客户端保持状态的方案。它是在用户端的会话状态的存贮机制，他需要用户打开客户端的cookie支持。cookie的作用就是为了解决HTTP协议无状态的缺陷所作的努力。 正统的cookie分发是通过扩展HTTP协议来实现的，服务器通过在HTTP的响应头中加上一行特殊的指示以提示浏览器按照指示生成相应的cookie。然而纯粹的客户端脚本如JavaScript也可以生成cookie。而cookie的使用是由浏览器按照一定的原则在后台自动发送给服务器的。浏览器检查所有存储的cookie，如果某个cookie所声明的作用范围大于等于将要请求的资源所在的位置，则把该cookie附在请求资源的HTTP请求头上发送给服务器。 cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。若不设置过期时间，则表示这个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式。 而session机制采用的是一种在服务器端保持状态的解决方案。同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的。而session提供了方便管理全局变量的方式 。 session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，这个值也可能设置为由get来返回给服务器。 就安全性来说：当你访问一个使用session 的站点，同时在自己机子上建立一个cookie，建立在服务器端的session机制更安全些，因为它不会任意读取客户存储的信息。 Session 机制session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。 当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个session标识（称为session id），如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来使用（检索不到，会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个session id将被在本次响应中返回给客户端保存。 保存这个session id的方式可以采用cookie，这样在交互过程中浏览器可以自动的按照规则把这个标识发挥给服务器。一般这个cookie的名字都是类似于SEEESIONID。但cookie可以被人为的禁止，则必须有其他机制以便在cookie被禁止时仍然能够把session id传递回服务器。 经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路径的后面。还有一种技术叫做表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把session id传递回服务器。 Cookie与Session都能够进行会话跟踪，但是完成的原理不太一样。普通状况下二者均能够满足需求，但有时分不能够运用Cookie，有时分不能够运用Session。 两者比较存取方式不同Cookie中只能保管ASCII字符串，假如需求存取Unicode字符或者二进制数据，需求先进行编码。Cookie中也不能直接存取Java对象。若要存储略微复杂的信息，运用Cookie是比拟艰难的。 而Session中能够存取任何类型的数据，包括而不限于String、Integer、List、Map等。Session中也能够直接保管Java Bean乃至任何Java类，对象等，运用起来十分便当。能够把Session看做是一个Java容器类。 隐私策略不同Cookie存储在客户端阅读器中，对客户端是可见的，客户端的一些程序可能会窥探、复制以至修正Cookie中的内容。而Session存储在服务器上，对客户端是透明的，不存在敏感信息泄露的风险。 假如选用Cookie，比较好的方法是，敏感的信息如账号密码等尽量不要写到Cookie中。最好是像Google、Baidu那样将Cookie信息加密，提交到服务器后再进行解密，保证Cookie中的信息只要本人能读得懂。而假如选择Session就省事多了，反正是放在服务器上，Session里任何隐私都能够有效的保护。 服务器压力不同Session是保管在服务器端的，每个用户都会产生一个Session。假如并发访问的用户十分多，会产生十分多的Session，耗费大量的内存。因而像Google、Baidu、Sina这样并发访问量极高的网站，是不太可能运用Session来追踪客户会话的。 而Cookie保管在客户端，不占用服务器资源。假如并发阅读的用户十分多，Cookie是很好的选择。关于Google、Baidu、Sina来说，Cookie或许是唯一的选择。 浏览器支持不同Cookie是需要客户端浏览器支持的。假如客户端禁用了Cookie，或者不支持Cookie，则会话跟踪会失效。关于WAP上的应用，常规的Cookie就派不上用场了。 假如客户端浏览器不支持Cookie，需要运用Session以及URL地址重写。需要注意的是一切的用到Session程序的URL都要进行URL地址重写，否则Session会话跟踪还会失效。关于WAP应用来说，Session+URL地址重写或许是它唯一的选择。 假如客户端支持Cookie，则Cookie既能够设为本浏览器窗口以及子窗口内有效（把过期时间设为–1），也能够设为一切阅读器窗口内有效（把过期时间设为某个大于0的整数）。但Session只能在本阅读器窗口以及其子窗口内有效。假如两个浏览器窗口互不相干，它们将运用两个不同的Session。（IE8下不同窗口Session相干） 跨域支持不同Cookie支持跨域名访问，例如将domain属性设置为“.biaodianfu.com”，则以“.biaodianfu.com”为后缀的一切域名均能够访问该Cookie。跨域名Cookie如今被普遍用在网络中，例如Google、Baidu、Sina等。而Session则不会支持跨域名访问。Session仅在他所在的域名内有效。 综述仅运用Cookie或者仅运用Session可能完成不了理想的效果。这时应该尝试一下同时运用Cookie与Session。Cookie与Session的搭配运用在实践项目中会完成很多意想不到的效果。 Python Django 中实现两种机制Cookie 设置以下是Cookie设置的详细流程： 客户端发起一个请求连接（如HTTP GET） 服务器在http响应头上加上Set-Cookie，里面存放字符串的键值对 客户端随后的http请求头加上Cookie首部，它包含了之前服务器响应中设置cookie的信息。 根据这个Cookie首部的信息，服务器便能“记住”当前用户的信息。 下面就来看看Python中如何设置Cookie： 12345678910111213141516171819202122from BaseHTTPServer import HTTPServerfrom SimpleHTTPServer import SimpleHTTPRequestHandlerimport Cookieclass MyRequestHandler(SimpleHTTPRequestHandler): def do_GET(self): content = \"&lt;html&gt;&lt;body&gt;Path is: %s&lt;/body&gt;&lt;/html&gt;\" % self.path self.send_response(200) self.send_header('Content-type', 'text/html') self.send_header('Content-length', str(len(content))) cookie = Cookie.SimpleCookie() cookie['id'] = 'some_value_42' self.wfile.write(cookie.output()) self.wfile.write('\\r\\n') self.end_headers() self.wfile.write(content)server = HTTPServer(('', 59900), MyRequestHandler)server.serve_forever() 查看服务器端的http响应头，会发现以下字段： 1Set-Cookie: id=some_value_42 在Django中，可以用如下的方式获取或设置Cookie： 12345678def test_cookie(request): if 'id' in request.COOKIES: cookie_id = request.COOKIES['id'] return HttpResponse('Got cookie with id=%s' % cookie_id) else: resp = HttpResponse('No id cookie! Sending cookie to client') resp.set_cookie('id', 'some_value_99') return resp Django通过一系列的包装使得封装Cookie的操作变得更加简单，那么它在其中是怎么实现cookie的读取的呢，下面来窥探原理： 1234def _get_cookies(self): if not hasattr(self, '_cookies'): self._cookies = http.parse_cookie(self.environ.get('HTTP_COOKIE', '')) return self._cookies 可以看出，获取cookie的操作用了Lazy initialization（延迟加载）的技术，因为如果客户端不需要用到cookie，这个过程只会浪费不必要的操作。 再来看parse_cookie的实现： 12345678910111213141516def parse_cookie(cookie): if cookie == '': return &#123;&#125; if not isinstance(cookie, Cookie.BaseCookie): try: c = SimpleCookie() c.load(cookie, ignore_parse_errors=True) except Cookie.CookieError: # 无效cookie return &#123;&#125; else: c = cookie cookiedict = &#123;&#125; for key in c.keys(): cookiedict[key] = c.get(key).value return cookiedict 它负责解析Cookie并把结果集成到一个dict（字典）对象中，并返回字典。而设置cookie的操作则会被WSGIHandler执行。 注：Django的底层实现了WSGI的接口（如WSGIRequest，WSGIServer等）。 Session 应用下面看一个简单的session应用例子： 1234567def test_count_session(request): if 'count' in request.session: request.session['count'] += 1 return HttpResponse('new count=%s' % request.session['count']) else: request.session['count'] = 1 return HttpResponse('No count in session. Setting to 1') 它用session实现了一个计数器，当每一个请求到来时，就为计数器加一，把新的结果更新到session中。 查看http的响应头，会得到类似下面的信息。 1234Set-Cookie:sessionid=a92d67e44a9b92d7dafca67e507985c0; expires=Thu, 07-Jul-2011 04:16:28 GMT; Max-Age=1209600; Path=/ 里面包含了session_id以及过期时间等信息。 那么服务器端是如何保存session的呢？ 在django中，默认会把session保存在setting指定的数据库中，除此之外，也可以通过指定session engine，使session保存在文件(file)，内存(cache)中。 如果保存在数据库中，django会在数据库中创建一个如下的session表。 12345CREATE TABLE &quot;django_session&quot; ( &quot;session_key&quot; varchar(40) NOT NULL PRIMARY KEY, &quot;session_data&quot; text NOT NULL, &quot;expire_date&quot; datetime NOT NULL); session_key是放置在cookie中的id，它是唯一的，而session_data则存放序列化后的session数据字符串。 通过session_key可以在数据库中取得这条session的信息： 12345from django.contrib.sessions.models import Session#...sess = Session.objects.get(pk='a92d67e44a9b92d7dafca67e507985c0')print(sess.session_data)print(sess.get_decoded()) 输出： 12ZmEyNDVhNTBhMTk2ZmRjNzVlYzQ4NTFjZDk2Y2UwODc3YmVjNWVjZjqAAn1xAVUFY291bnRxAksGcy4=&#123;'count': 6&#125; 回看第一个例子，我们是通过request.session来获取session的，为什么请求对象会附带一个session对象呢，这其中做了什么呢？ 这就引出了下面要说的django里的中间件技术 Session middleware。 关于中间件，&lt;&lt;the Django Book&gt;&gt;是这样解释的： Django的中间件框架，是django处理请求和响应的一套钩子函数的集合。 我们看传统的django视图模式一般是这样的：http请求-&gt;view-&gt;http响应，而加入中间件框架后，则变为：http请求-&gt;中间件处理-&gt;app-&gt;中间件处理-&gt;http响应。而在django中这两个处理分别对应process_request和process_response函数，这两个钩子函数将会在特定的时候被触发。 直接看SessionMiddleware可能更清晰一些： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class SessionMiddleware(object): def __init__(self): engine = import_module(settings.SESSION_ENGINE) self.SessionStore = engine.SessionStore def process_request(self, request): session_key = request.COOKIES.get(settings.SESSION_COOKIE_NAME) request.session = self.SessionStore(session_key) def process_response(self, request, response): \"\"\" If request.session was modified, or if the configuration is to save the session every time, save the changes and set a session cookie or delete the session cookie if the session has been emptied. \"\"\" try: accessed = request.session.accessed modified = request.session.modified empty = request.session.is_empty() except AttributeError: pass else: # First check if we need to delete this cookie. # The session should be deleted only if the session is entirely empty if settings.SESSION_COOKIE_NAME in request.COOKIES and empty: response.delete_cookie(settings.SESSION_COOKIE_NAME, domain=settings.SESSION_COOKIE_DOMAIN) else: if accessed: patch_vary_headers(response, ('Cookie',)) if (modified or settings.SESSION_SAVE_EVERY_REQUEST) and not empty: if request.session.get_expire_at_browser_close(): max_age = None expires = None else: max_age = request.session.get_expiry_age() expires_time = time.time() + max_age expires = cookie_date(expires_time) # Save the session data and refresh the client cookie. # Skip session save for 500 responses, refs #3881. if response.status_code != 500: try: request.session.save() except UpdateError: # The user is now logged out; redirecting to same # page will result in a redirect to the login page # if required. return redirect(request.path) response.set_cookie(settings.SESSION_COOKIE_NAME, request.session.session_key, max_age=max_age, expires=expires, domain=settings.SESSION_COOKIE_DOMAIN, path=settings.SESSION_COOKIE_PATH, secure=settings.SESSION_COOKIE_SECURE or None, httponly=settings.SESSION_COOKIE_HTTPONLY or None) return response 在请求到来后，SessionMiddleware的process_request在请求取出session_key，并把一个新的session对象赋给request.session，而在返回响应时，process_response则判断session是否被修改或过期，来更新session的信息。 Django 用户认证中的 Session在django中，用下面的方法来验证用户是否登录是常见的事情。 123456def test_user(request): user_str = str(request.user) if request.user.is_authenticated(): return HttpResponse('%s is logged in' % user_str) else: return HttpResponse('%s is not logged in' % user_str) 其实request.user的实现也借助到了session。 在这个例子中，成功登录后，session表会保存类似下面的信息，里面记录了用户的id，以后进行验证时，便会到这个表中获取用户的信息。 1&#123;'_auth_user_id': 1, '_auth_user_backend': 'django.contrib.auth.backends.ModelBackend'&#125; 跟上面提到的Session中间件相似，用户验证也有一个中间件：AuthenticationMiddleware，在process_request中，通过request.class.user = LazyUser()在request设置了一个全局的可缓存的用户对象。 1234567891011class LazyUser(object): def __get__(self, request, obj_type=None): if not hasattr(request, '_cached_user'): from django.contrib.auth import get_user request._cached_user = get_user(request) return request._cached_userclass AuthenticationMiddleware(object): def process_request(self, request): request.__class__.user = LazyUser() return None 在get_user里，会在检查session中是否存放了当前用户对应的user_id，如果有，则通过id在model查找相应的用户返回，否则返回一个匿名的用户对象(AnonymousUser)。 12345678910def get_user(request): from django.contrib.auth.models import AnonymousUser try: user_id = request.session[SESSION_KEY] backend_path = request.session[BACKEND_SESSION_KEY] backend = load_backend(backend_path) user = backend.get_user(user_id) or AnonymousUser() except KeyError: user = AnonymousUser() return user Django中的Session实现Django使用的Session默认都继承于SessionBase类里，这个类实现了一些session操作方法，以及hash，decode，encode等方法。 123456789101112class SessionBase(object): \"\"\" Base class for all Session classes. \"\"\" TEST_COOKIE_NAME = 'testcookie' TEST_COOKIE_VALUE = 'worked' def __init__(self, session_key=None): self._session_key = session_key self.accessed = False self.modified = False self.serializer = import_string(settings.SESSION_SERIALIZER) 说的更直白一些，其实django中的session就是一个模拟dict的对象，并实现了一系列的hash和序列化方法，默认持久化在数据库中（有时候也可能由于为了提高性能，用redis之类的内存数据库来缓存session）。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Django，Python","slug":"Scrapy，Django，Python","permalink":"http://yoursite.com/tags/Scrapy，Django，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Loaders机制介绍与实例","slug":"scrapy-item-loader","date":"2017-05-08T06:18:54.000Z","updated":"2017-05-08T07:02:35.000Z","comments":true,"path":"2017/05/08/scrapy-item-loader/","link":"","permalink":"http://yoursite.com/2017/05/08/scrapy-item-loader/","excerpt":"Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。 Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。","text":"Item Loaders 提供了一种便捷的方式填充抓取到的 Items 。虽然 Items 可以使用自带的类字典形式 API 填充，但是 Items Loaders 提供了更便捷的 API，可以分析原始数据并对 Item 进行赋值。 从另一方面来说，Items 提供保存抓取数据的容器，而 Item Loaders 提供的是填充容器的机制。 Item Loaders 提供的是一种灵活，高效的机制，可以更方便的被 spider 或 source format （HTML，XML，etc）扩展，并 override 更易于维护的、不同的内容分析规则。 介绍使用 Item Loaders 来填充 Items要使用 Item Loader, 你必须先将它实例化。你可以使用类似字典的对象(例如: Item or dict)来进行实例化，或者不使用对象也可以，当不用对象进行实例化的时候，Item 会自动使用 ItemLoader.default\\_item_class 属性中指定的 Item 类在 Item Loader constructor 中实例化。 然后，你开始收集数值到 Item Loader 时，通常使用 Selectors。你可以在同一个 item field 里面添加多个数 值；Item Loader 将知道如何用合适的处理函数来“添加”这些数值。 下面是在 Spider 中典型的 Item Loader 的用法，使用 Items chapter 中声明的 Product item ： 1234567891011from scrapy.contrib.loader import ItemLoader from myproject.items import Productdef parse(self, response): l = ItemLoader(item=Product(), response=response) l.add_xpath('name', '//div[@class=\"product_name\"]') l.add_xpath('name', '//div[@class=\"product_title\"]') l.add_xpath('price', '//p[@id=\"price\"]') l.add_css('stock', 'p#stock]') l.add_value('last_updated', 'today') # you can also use literal values return l.load_item() 快速查看这些代码之后，我们可以看到发现 name 字段被从页面中两个不同的 XPath 位置提取： //div[@class=&quot;product_name&quot;] //div[@class=&quot;product_title&quot;] 换言之,数据通过用 add_xpath() 的方法，把从两个不同的 XPath 位置提取的数据收集起来。这是将在以后分配给 name 字段中的数据? 之后，类似的请求被用于 price 和 stock 字段 （后者使用 CSS selector 和 add_css() 方法）， 最后使用不同的方法 add_value() 对 last_update 填充文本值( today )。 最终，当所有数据被收集起来之后，调用 ItemLoader.load_item() 方法，实际上填充并且返回了之前通过调用 add_xpath()，add_css() ，add_value() 所提取和收集到的数据的 Item。 输入和输出处理器Item Loader 在每个（Item）字段中都包含了一个输入处理器和一个输出处理器。输入处理器收到数据时立刻提取数据 （通过 add_xpath()， add_css() 或者 add_value()方法）之后输入处理器的结果被收集起来并且保存在ItemLoader内。收集到所有的数据后，调用 ItemLoader.load_item() 方法来填充，并得到填充后的 Item 对象。这是当输出处理器被和之前收集到的数据（和用输入处理器处理的）被调用。输出处理器的结果是被分配到Item的最终值。 让我们看一个例子来说明如何输入和输出处理器被一个特定的字段调用（同样适用于其他field）： 123456l = ItemLoader(Product(), some_selector)l.add_xpath('name', xpath1) # (1)l.add_xpath('name', xpath2) # (2)l.add_css('name', css) # (3)l.add_value('name', 'test') # (4)return l.load_item() # (5) 发生了这些事情: 从 xpath1 提取出的数据,传递给 输入处理器 的 name 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡ 从 xpath2 提取出来的数据,传递给(1)中使用的相同的 输入处理器 .输入处理器的结果被附加到在(1)中收集的数据(如果有的话) ｡ 和之前相似，只不过这里的数据是通过 css CSS selector抽取，之后传输到在(1)和(2)使用 的input processor 中。最终输入处理器的结果被附加到在(1)和(2)中收集的数据之后 (如果存在数据的话)。 这里的处理方式也和之前相似，但是此处的值是通过add_value直接赋予的， 而不是利用XPath表达式或CSS selector获取。得到的值仍然是被传送到输入处理器。 在这里例程中，因为得到的值并非可迭代，所以在传输到输入处理器之前需要将其 转化为可迭代的单个元素，这才是它所接受的形式。 在之前步骤中所收集到的数据被传送到 output processor 的 name field中。 输出处理器的结果就是赋到item中 name field的值。 理解： 就是在使用Item Loader 时候，会有一个输入处理器，一个输出处理器，首先是收集好同一个字段的结果，传入到输入处理器当中，然后收集完后，会传递给输出处理器进行处理。输出处理器的处理结果，就是填充到item的结果。 需要注意的是：输入处理器的返回值会是内部收集的，然后被传递给输出处理器，来填充fields。 Scrapy 内部的处理器Scrapy内部，已经有一些设置好的内置处理器 Identity这是最简单的一个处理器，实际上就是什么都不做，传入多少个字段，就存储多少个字段，以list形式。&gt;&gt;&gt; from scrapy.contrib.loader.processor import Identity \\&gt;&gt;&gt; proc = Identity() \\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;] TakeFirst从接受到的list中返回第一个非null/非空的值，&gt;&gt;&gt; from scrapy.contrib.loader.processor import TakeFirst \\&gt;&gt;&gt; proc = TakeFirst() \\&gt;&gt;&gt; proc([&#39;&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) &#39;one&#39; Join返回用分隔符（separator）作为间隔的连接形成的字符串。若不传入separator，则默认使用’ ‘（空格）。&gt;&gt;&gt; from scrapy.contrib.loader.processor import Join \\&gt;&gt;&gt; proc = Join() \\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) u&#39;one two three&#39; \\&gt;&gt;&gt; proc = Join(&#39;&lt;br&gt;&#39;) \\&gt;&gt;&gt; proc([&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]) u&#39;one&lt;br&gt;two&lt;br&gt;three&#39; 另外还有Compose以及MapCompose，这里不一一详述。 声明 Item Loaders声明ItemLoaders 和声明Item类似，使用Class语法，例子： 12345678910111213from scrapy.contrib.loader import ItemLoaderfrom scrapy.contrib.loader.processor import TakeFirst, MapCompose, Joinclass ProductLoader(ItemLoader): default_output_processor = TakeFirst() name_in = MapCompose(unicode.title) name_out = Join() price_in = MapCompose(unicode.strip) # ... 上述代码中: 输出处理器，被声明为 _in 前缀，而输出处理器被声明为 _out 前缀。 设置默认处理器 ItemLoader.default_input_processor and ItemLoader.default_output_processor 声明输入、输出处理器输入、输出可以被如上方那样被声明，这也是最正常的方式。另外，我们也可以在另外的一个地方去声明输入和输出处理器：在item Field，元数据中。 1234567891011121314151617import scrapyfrom scrapy.contrib.loader.processor import Join, MapCompose, TakeFirstfrom w3lib.html import remove_tagsdef filter_price(value): if value.isdigit(): return valueclass Product(scrapy.Item): name = scrapy.Field( input_processor=MapCompose(remove_tags), output_processor=Join(), ) price = scrapy.Field( input_processor=MapCompose(remove_tags, filter_price), output_processor=TakeFirst(), ) 输出和输出处理器的优先级如下： Item Loader field 指定的field_in 和 field_out（最优先） Field 元数据中(input_processor 和 output_processor key) item loader 默认。 ItemLoader.default_input_processor() andItemLoader.default_output_processor() (least precedence) 最后，给出Item Loader的官方说明API： ItemLoader objects 实例通过 Item loader 加载 Item首先在 jobbole.py 中引入 from scrapy.loader import ItemLoader 代码如下： 1234567891011121314item_loader = ItemLoader(item=JobBoleArticleItem(), response=response)item_loader.add_css(\"title\", \".entry-header h1::text\")item_loader.add_value(\"url\", response.url)item_loader.add_value(\"url_object_id\", get_md5(response.url))item_loader.add_css(\"create_date\", \"p.entry-meta-hide-on-mobile::text\")front_image_url = response.meta.get(\"front_image_url\", \"\") # 文章封面图item_loader.add_value(\"front_image_url\", [front_image_url])item_loader.add_css(\"praise_nums\", \".vote-post-up h10::text\")item_loader.add_css(\"comment_nums\", \"a[href='#article-comment'] span::text\")item_loader.add_css(\"fav_nums\", \".bookmark-btn::text\")item_loader.add_css(\"tags\", \"p.entry-meta-hide-on-mobile a::text\")item_loader.add_css(\"content\", \"div.entry\")article_item = item_loader.load_item() 其中第一行中 JobBoleArticleItem() 为在 items.py 中声明的实例，response 为返回的响应。这属于固定写法。add_css()中第一个值为 items.py 中定义的值，第二个值为css选择器规则，类似的方法还有 add_xpath()，根据场景进行选择。 同理，add_value()为添加确定值的方法。这里通过值传递附给 front_image_url 再通过add_value的方法，加入到最终的item中。 最后通过调用 load_item() 方法对结果进行解析，所有的结果都是一个list并保存到 article_item 中。 断点调试结果如图： 发现获取到的所有值都是一个list，这样很不方便，但使得代码可读性更高，可维护性更强。 通过 items.py 处理数据在 items.py 中引入 from scrapy.loader.processors import MapCompose ，然后可以在定义 scrapy.Field() 时可以加入处理函数（可以使匿名函数），例如： 在 MapCompose() 中可以加入多个函数，在 jobbole.py 中断点调试结果如图： 在title的结果后面出现了我们想要的后缀。 另外，可以看到，结果都是 list，我们每次都需要提取第一个值。Scrapy给我们提供了 TakeFirst 方法。 同样引入 from scrapy.loader.processors import MapCompose,TakeFirst ，修改代码如下： 1234title = scrapy.Field( input_processor = MapCompose(lambda x:x+\"-jobbole\", add_jobbole), output_processor = TakeFirst() ) 即可以得到第一个值。由于每一个结果都是取第一个值，每个值全部调用这个方法重复代码过多，可以通过自定义Item loader重载的方法解决。引入 from scrapy.loader import ItemLoader ，这个类提供了以下方法： 123456class ItemLoader(object): default_item_class = Item default_input_processor = Identity() default_output_processor = Identity() default_selector_class = Selector 我们自定义的Item loader需要继承这个类： 12class ArticleItemLoader(ItemLoader): default_output_processor = TakeFirst() 然后在 jobbole.py 文件中，把 item_loader = ItemLoader(item=JobBoleArticleItem(), response=response) 中的 ItemLoader 变为 ArticleItemLoader，即： 1item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response) 这样得到的结果就是一个str而不是list了。 不过在上图是可以看到，它的tags也取了第一个值，但实际上它的值是三个，不满足我们的需要。引入Join方法 from scrapy.loader.processors import MapCompose, TakeFirst, Join，同时不使用自定义的item loader即可。 123tags = scrapy.Field( output_processor=Join(','), ) 和前面一样，有时候tags会有 评论 的不符合要求的tags，还需要自定义函数把相应的字段去掉。 12345def remove_comment(value): if '评论' in value: return '' else: return value 在处理图片时，使用pipelines需要传递的是一个列表，这里经过处理后，变成了str。可以通过一个默认函数不让默认的TakeFirst处理即可。 12def return_value(value): return value 调用方法是： 123front_image_url = scrapy.Field( output_processor=MapCompose(return_value), ) 最后，我们在之前还用正则表达式来清洗点赞数，收藏数，评论数这些数据，在item loader中我们也可以用函数处理： 123456789101112131415161718def get_nums(value): match_re = re.match(\".*?(\\d+).*?\", value) if match_re: nums = int(match_re.group(1)) else: nums = 0 return numspraise_nums = scrapy.Field( input_processor=MapCompose(get_nums), ) comment_nums = scrapy.Field( input_processor=MapCompose(get_nums), ) fav_nums = scrapy.Field( input_processor=MapCompose(get_nums), ) 调试结果中str就变成int类型了：","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——保存item到json文件","slug":"scrapy-item-json","date":"2017-05-07T06:18:54.000Z","updated":"2017-05-07T05:05:09.000Z","comments":true,"path":"2017/05/07/scrapy-item-json/","link":"","permalink":"http://yoursite.com/2017/05/07/scrapy-item-json/","excerpt":"在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。","text":"在Scrapy中，所有item数据都会通过pipelines进行处理，想要保存为json格式文件，只需要在piplines中进行相应的处理即可。 使用系统 exporter 导出为 JSON 文件12345678910111213141516from scrapy.exporters import JsonItemExporterclass JsonExporterPipeline(object): # 调用Scrapy提供的json exporter导出json文件 def __init__(self): self.file = open('article.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding=\"utf-8\", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item 自定义 Pipeline 导出为 JSON 文件123456789101112class JsonWithEncodingPipeline(object): # 自定义json文件的导出 def __init__(self): self.file = codecs.open('article.json', 'w', encoding=\"utf-8\") def process_item(self, item, spider): lines = json.dumps(dict(item), ensure_ascii=False) + \"\\n\" self.file.write(lines) return item def spider_closed(self, spider): self.file.close() 函数说明codecs ：避免打开文件时出现编码错误。json.dumps ：dict转成strjson.loads ：str转成dictensure_ascii=False ：避免处理英文以外语言时出错return item ：交给下一个pipeline处理","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——通过Pipeline保存数据到MySQL","slug":"scrapy-item-mysql","date":"2017-05-07T06:18:54.000Z","updated":"2017-05-07T11:36:20.000Z","comments":true,"path":"2017/05/07/scrapy-item-mysql/","link":"","permalink":"http://yoursite.com/2017/05/07/scrapy-item-mysql/","excerpt":"将数据保存到MySQL数据库，需要用到 mysqlclient 模块，需要在我们的虚拟环境中用 pip 进行安装。","text":"将数据保存到MySQL数据库，需要用到 mysqlclient 模块，需要在我们的虚拟环境中用 pip 进行安装。 设计数据表需要根据之前Item来设计我们的数据表 jobbole_article ，数据库取名为 article_spider。 123456789101112class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field() front_image_path = scrapy.Field() praise_nums = scrapy.Field() comment_nums = scrapy.Field() fav_nums = scrapy.Field() tags = scrapy.Field() content = scrapy.Field() 初步设计的数据表如下，在后面使用时还会进行必要的改动： 采用同步机制写入MySQL首先在 pipelines.py 中引入数据库连接模块 import MySQLdb ，然后完善 MysqlPipeline 类的代码： 1234567891011121314class MysqlPipeline(object): # 采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('127.0.0.1', 'root', '12', 'article_spider', charset='utf8', use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = \"\"\" insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) \"\"\" self.cursor.execute(insert_sql, (item[\"title\"], item[\"url\"], item[\"create_date\"], item[\"fav_nums\"])) self.conn.commit() __init__ 方法是对数据进行初始化，定义连接信息如host，数据库用户名、密码、数据库名称、数据库编码在 process_item 方法中进行插入数据操作，格式都是固定的。 最后在 settings.py 中把 MysqlPipeline() 加入到 ITEM_PIPELINES 的配置中。 采用异步机制写入MySQL在上面的同步机制写入数据库中，我们把连接信息 MySQLdb.connect(&#39;127.0.0.1&#39;, &#39;root&#39;, &#39;12&#39;, &#39;article_spider&#39;, charset=&#39;utf8&#39;, use_unicode=True) 直接定义在函数中，如果不经常改动的话，可以把相关信息放到 settings.py 中进行调用。 1234MYSQL_HOST = '127.0.0.1'MYSQL_DBNAME = 'article_spider'MYSQL_USER = 'root'MYSQL_PASSWORD = '12' 在 pipelines.py 中新建 MysqlTwistedPipeline ，写入如下代码： 12345class MysqlTwistedPipeline(object): @classmethod def from_settings(cls, settings): host = settings['MYSQL_HOST'] pass 在 from_settings 这个类方法中，我们获取到了settings配置中的 MYSQL_HOST ，这个方法在Scrapy初始化的时候就会被调用，会将Scrapy的settings对象传递进来，我们在这里进行断点调试，查看是否获取到了这个对象： 发现在settings的attributes这个字典中，确实有我们定义的各种属性。 我们的异步操作需要引入twisted，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940from twisted.enterprise import adbapiimport MySQLdbimport MySQLdb.cursorsclass MysqlTwistedPipeline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparams = dict( host=settings['MYSQL_HOST'], db=settings['MYSQL_DBNAME'], user=settings['MYSQL_USER'], passwd=settings['MYSQL_PASSWORD'], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool(\"MySQLdb\", **dbparams) return cls(dbpool) def process_item(self, item, spider): # 使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error) # 处理异常 def handle_error(self, failure): # 处理异步插入异常 print(failure) def do_insert(self, cursor, item): # 执行具体的插入 insert_sql = \"\"\" insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) \"\"\" cursor.execute(insert_sql, (item[\"title\"], item[\"url\"], item[\"create_date\"], item[\"fav_nums\"])) 我们在使用时，绝大部分代码无须变动，只要修改 do_insert 方法中的插入内容，以及自己的信息即可。 在数据量不大时，用同步插入即可。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Feed Exports","slug":"scrapy-feed-exports","date":"2017-05-06T06:18:54.000Z","updated":"2017-05-06T14:46:00.000Z","comments":true,"path":"2017/05/06/scrapy-feed-exports/","link":"","permalink":"http://yoursite.com/2017/05/06/scrapy-feed-exports/","excerpt":"实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。 Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。","text":"实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”（通常叫做”输出 feed”），来供其他系统使用。 Scrapy 自带了 Feed 输出，并且支持多种序列化格式（serialization format）及存储方式（storage backends）。 序列化方式（serialization format）feed 输出使用到了 Item exporters 。其自带支持的类型有: JSON JSON lines CSV XML 也可以通过 FEED_EXPORTERS 设置扩展支持的属性。 在 exporters.py 中可以看到所有的 Item exporters： 下表对主要的 Item exporters进行简要的介绍： 类型 FEED_FORMAT 使用的 exporter JSON json JsonItemExporter JSON lines jsonlines JsonLinesItemExporter CSV csv CsvItemExporter XML xml XmlItemExporter Pickle pickle PickleItemExporter Marshal marshal MarshalItemExporter 存储（Storages）使用 feed 输出时您可以通过使用 URI（通过 FEED_URI 设置）来定义存储端。feed 输出支持 URI 方式支持的多种存储后端类型。 自带支持的存储后端有： 本地文件系统 FTP S3（需要 boto） 标准输出 有些存储后端会因所需的外部库未安装而不可用。例如，S3 只有在 boto 库安装的情况下才可使用。 存储 URI 参数存储 URI 也包含参数。当 feed 被创建时这些参数可以被覆盖： %(time)s - 当 feed 被创建时被 timestamp 覆盖 %(name)s - 被 spider 的名字覆盖 其他命名的参数会被 spider 同名的属性所覆盖。例如， 当 feed 被创建时，%(site_id)s 将会被 spider.site_id 属性所覆盖。 下面用一些例子来说明: 存储在 FTP，每个 spider 一个目录: ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json 存储在 S3，每一个 spider 一个目录: s3://mybucket/scraping/feeds/%(name)s/%(time)s.json 存储后端（Storage backends）本地文件系统将 feed 存储在本地系统。 URI scheme: file URI 样例: file:///tmp/export.csv 需要的外部依赖库: none 注意: (只有)存储在本地文件系统时，您可以指定一个绝对路径 /tmp/export.csv 并忽略协议(scheme)。不过这 仅仅只能在 Unix 系统中工作。 FTP将 feed 存储在 FTP 服务器。 URI scheme: ftp URI 样例: ftp://user:pass@ftp.example.com/path/to/export.csv 需要的外部依赖库: none S3将 feed 存储在 Amazon S3 。 URI scheme: s3 URI 样例: s3://mybucket/path/to/export.csv s3://aws_key:aws_secret@mybucket/path/to/export.csv 需要的外部依赖库: boto 您可以通过在 URI 中传递 user/pass 来完成 AWS 认证，或者也可以通过下列的设置来完成: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY 标准输出feed 输出到 Scrapy 进程的标准输出。 URI scheme: stdout URI 样例: stdout 需要的外部依赖库: none 设定（Settings）这些是配置 feed 输出的设定: FEED_URI (必须) FEED_FORMAT FEED_STORAGES FEED_EXPORTERS FEED_STORE_EMPTY FEED_URIDefault: None 输出 feed 的 URI。支持的 URI 协议请参见存储后端。 为了启用 feed 输出，该设定是必须的。 FEED_FORMAT输出 feed 的序列化格式。可用的值请参见序列化方式（Serialization formats）。 FEED_STORE_EMPTYDefault: False 是否输出空 feed（没有 item 的 feed）。 FEED_STORAGESDefault: {} 包含项目支持的额外 feed 存储端的字典。 字典的键（key）是 URI 协议（scheme），值是存储类（storage class）的路径。 FEED_STORAGES_BASEDefault: 1234567&#123;'': 'scrapy.contrib.feedexport.FileFeedStorage', 'file': 'scrapy.contrib.feedexport.FileFeedStorage', 'stdout': 'scrapy.contrib.feedexport.StdoutFeedStorage', 's3': 'scrapy.contrib.feedexport.S3FeedStorage', 'ftp': 'scrapy.contrib.feedexport.FTPFeedStorage',&#125; 包含 Scrapy 内置支持的 feed 存储端的字典。 FEED_EXPORTERSDefault: {} 包含项目支持的额外输出器（exporter）的字典。 该字典的键（key）是 URI 协议（scheme），值是 Item 输出器（exp orter）类的路径。 FEED_EXPORTERS_BASEDefault: 1234567FEED_EXPORTERS_BASE = &#123; 'json': 'scrapy.contrib.exporter.JsonItemExporter', 'jsonlines': 'scrapy.contrib.exporter.JsonLinesItemExporter', 'csv': 'scrapy.contrib.exporter.CsvItemExporter', 'xml': 'scrapy.contrib.exporter.XmlItemExporter', 'marshal': 'scrapy.contrib.exporter.MarshalItemExporter',&#125; 包含 Scrapy 内置支持的 feed 输出器（exporter）的字典。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Item Pipeline","slug":"scrapy-item-pipeline","date":"2017-05-06T06:18:54.000Z","updated":"2017-05-06T13:35:25.000Z","comments":true,"path":"2017/05/06/scrapy-item-pipeline/","link":"","permalink":"http://yoursite.com/2017/05/06/scrapy-item-pipeline/","excerpt":"当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。 每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。","text":"当 Item 在 Spider 中被收集之后，它将会被传递到 Item Pipeline，一些组件会按照一定的顺序执行对 Item 的处理。 每个 item pipeline 组件（有时称之为“Item Pipeline”）是实现了简单方法的 Python 类。他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过 pipeline，或是被丢弃而不再进行处理。 以下是 item pipeline 的一些典型应用： 清理 HTML 数据 验证爬取的数据（检查 item 包含某些字段） 查重（并丢弃） 将爬取结果保存到数据库中 编写自定义的 Pipeline定义一个Python类，然后实现方法 process_item(self, item, spider) 即可，返回一个字典或Item，或者抛出 DropItem 异常丢弃这个Item。 除此之外，还可以实现以下几个方法： open_spider(self, spider) ：当spider被开启时，这个方法被调用 close_spider(self, spider) ：当spider被关闭时，这个方法被调用 from_crawler(cls, crawler) ： 可访问核心组件比如配置和信号，并注册钩子函数到Scrapy中 Item Pipeline示例价格验证让我们来看一下以下这个假设的 pipeline，它为那些不含税（price_excludes_vat 属性）的item调整了price属性，同时丢弃了那些没有价格item： 12345678910111213from scrapy.exceptions import DropItemclass PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item['price']: if item['price_excludes_vat']: item['price'] = item['price'] * self.vat_factor return item else: raise DropItem(\"Missing price in %s\" % item) 将item写入Json文件下面的这个Pipeline将所有的item写入到一个单独的json文件，，每行包含一个序列化 为 JSON 格式的 item: 1234567891011import jsonclass JsonWriterPipeline(object): def __init__(self): self.file = open('items.jl', 'wb') def process_item(self, item, spider): line = json.dumps(dict(item)) + \"\\n\" self.file.write(line) return item JsonWriterPipeline 的目的只是为了介绍怎样编写 item pipeline，如果你想要将所有爬取的 item 都保存到同 一个 JSON 文件， 你需要使用 Feed exports 。 将item存储到MongoDB中这个例子使用pymongo来演示怎样讲item保存到MongoDB中。 MongoDB的地址和数据库名在配置 settings.py 中指定，这个例子主要是向你展示怎样使用from_crawler()方法，以及如何清理资源。 123456789101112131415161718192021222324252627import pymongoclass MongoPipeline(object): collection_name = 'scrapy_items' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'items') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert(dict(item)) return item 去重一个用于去重的过滤器，丢弃那些已经被处理过的 item。让我们假设我们的 item 有一个唯一的 id，但是我们 sp ider 返回的多个 item 中包含有相同的 id: 12345678910111213from scrapy.exceptions import DropItemclass DuplicatesPipeline(object): def __init__(self): self.ids_seen = set() def process_item(self, item, spider): if item['id'] in self.ids_seen: raise DropItem(\"Duplicate item found: %s\" % item) else: self.ids_seen.add(item['id']) return item 启用一个 Item Pipeline 组件为了启用一个 Item Pipeline 组件，你必须将它的类添加到 ITEM_PIPELINES 配置，就像下面这个例子: 1234ITEM_PIPELINES = &#123; 'myproject.pipelines.PricePipeline': 300, 'myproject.pipelines.JsonWriterPipeline': 800, &#125; 分配给每个类的整型值，确定了他们运行的顺序，item 按数字从低到高的顺序，通过 pipeline，通常将这些数字 定义在 0-1000 范围内。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Selectors","slug":"scrapy-selectors","date":"2017-05-05T06:18:54.000Z","updated":"2017-05-05T12:51:02.000Z","comments":true,"path":"2017/05/05/scrapy-selectors/","link":"","permalink":"http://yoursite.com/2017/05/05/scrapy-selectors/","excerpt":"当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： BeautifulSoup 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 lxml 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。 Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。","text":"当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： BeautifulSoup 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 lxml 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。 Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。 XPath 是一门用来在 XML 文件中选择节点的语言，也可以用在 HTML 上。 CSS 是一门将 HTML 文档样式化的语言。选择器由它定义，并与特定的 HTML 元素的样式相关连。 Scrapy 选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。 本文解释了选择器如何工作，并描述了相应的 API。不同于 lxml API 的臃肿，该 API 短小而简洁。这是因为 lxml 库除了用来选择标记化文档外，还可以用到许多任务上。 使用选择器构造选择器Scrapy selectors是 Selector 类的实例，通过传入 text 或 TextResponse 来创建，它自动根据传入的类型选择解析规则（XML or HTML）： 12&gt;&gt;&gt; from scrapy.selector import Selector &gt;&gt;&gt; from scrapy.http import HtmlResponse 以文字构造（都以 xpath 和 css 两种方法解析字段内容，加深理解）： 12345&gt;&gt;&gt; body = &apos;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&apos;&gt;&gt;&gt; Selector(text=body).xpath(&quot;//span/text()&quot;).extract()[&apos;good&apos;]&gt;&gt;&gt; Selector(text=body).css(&quot;html body span::text&quot;).extract()[&apos;good&apos;] 以 response 构造： 12345&gt;&gt;&gt; response = HtmlResponse(url=&apos;http://example.com&apos;, body=body, encoding=&apos;utf-8&apos;)&gt;&gt;&gt; Selector(response=response).xpath(&apos;//span/text()&apos;).extract()[&apos;good&apos;]&gt;&gt;&gt; Selector(response=response).css(&quot;html body span::text&quot;).extract()[&apos;good&apos;] response 对象以 .selector 属性提供了一个 selector ， 可以随时使用该快捷方法: 1234&gt;&gt;&gt; response.selector.xpath(&apos;//span/text()&apos;).extract()[&apos;good&apos;]&gt;&gt;&gt; response.selector.css(&quot;html body span::text&quot;).extract()[&apos;good&apos;] 使用选择器我们将使用 Scrapy shell （提供交互测试）和位于 Scrapy 文档服务器的一个样例页面，来解释如何使用选择器： http://doc.scrapy.org/en/latest/_static/selectors-sample1.html 该页面源码如下： 123456789101112131415&lt;html&gt; &lt;head&gt; &lt;base href='http://example.com/' /&gt; &lt;title&gt;Example website&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id='images'&gt; &lt;a href='image1.html'&gt;Name: My image 1 &lt;br /&gt;&lt;img src='image1_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image2.html'&gt;Name: My image 2 &lt;br /&gt;&lt;img src='image2_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image3.html'&gt;Name: My image 3 &lt;br /&gt;&lt;img src='image3_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image4.html'&gt;Name: My image 4 &lt;br /&gt;&lt;img src='image4_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image5.html'&gt;Name: My image 5 &lt;br /&gt;&lt;img src='image5_thumb.jpg' /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 首先，打开 scrapy shell， 1scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html 当 shell 载入后，您将获得名为 response 的 shell 变量，其为响应的 response，并且在其 response.selector 属性上绑定了一个 selector。 因为我们处理的是 HTML，选择器将自动使用 HTML 语法分析。 那么，通过查看该页面的源码，我们构建一个 XPath 来选择 title 标签内的文字: 12&gt;&gt;&gt; response.selector.xpath(&quot;//title/text()&quot;)&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;] 由于在 response 中使用 XPath、CSS 查询十分普遍，因此，Scrapy 提供了两个实用的快捷方式：response.xpath() 及 response.css() ： 12345&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;)&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]&gt;&gt;&gt; response.css(&quot;title::text&quot;)&gt;&gt;&gt; [&lt;Selector xpath=&apos;descendant-or-self::title/text()&apos; data=&apos;Example website&apos;&gt;] 现在我们将得到根 URL（base URL）和一些图片链接: 1234567891011121314151617&gt;&gt;&gt; response.xpath('//base/@href').extract() ['http://example.com/']&gt;&gt;&gt; response.css('base::attr(href)').extract() ['http://example.com/']&gt;&gt;&gt; response.xpath('//a[contains(@href, \"image\")]/@href').extract() ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']&gt;&gt;&gt; response.css('a[href*=image]::attr(href)').extract() ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']&gt;&gt;&gt; response.xpath('//a[contains(@href, \"image\")]/img/@src').extract() ['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg']&gt;&gt;&gt; response.css('a[href*=image] img::attr(src)').extract() ['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg'] 嵌套选择器选择器方法（ .xpath() or .css() ）返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子： 123456789101112131415161718&gt;&gt;&gt; links = response.xpath(&quot;//a[contains(@href,&apos;image&apos;)]&quot;)&gt;&gt;&gt; links.extract()[&apos;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;]&gt;&gt;&gt; for index, link in enumerate(links):...: args = (index, link.xpath(&apos;@href&apos;).extract(), link.xpath(&apos;img/@src&apos;).extract())...: print(&apos;Link number %d points to url %s and image %s&apos; % args)...:Link number 0 points to url [&apos;image1.html&apos;] and image [&apos;image1_thumb.jpg&apos;]Link number 1 points to url [&apos;image2.html&apos;] and image [&apos;image2_thumb.jpg&apos;]Link number 2 points to url [&apos;image3.html&apos;] and image [&apos;image3_thumb.jpg&apos;]Link number 3 points to url [&apos;image4.html&apos;] and image [&apos;image4_thumb.jpg&apos;]Link number 4 points to url [&apos;image5.html&apos;] and image [&apos;image5_thumb.jpg&apos;] 结合正则表达式使用选择器Selector 也有一个 .re() 方法，用来通过正则表达式来提取数据。然而，不同于使用 .xpath() 或者 .css() 方法，.re()方法返回 unicode 字符串的列表。所以你无法构造嵌套式的 .re() 调用。 下面是一个例子，从上面的 html 源码中提取图像名字： 12&gt;&gt;&gt; response.xpath(&quot;//a[contains(@href, &apos;image&apos;)]/text()&quot;).re(r&apos;Name:\\s*(.*)&apos;)[&apos;My image 1 &apos;, &apos;My image 2 &apos;, &apos;My image 3 &apos;, &apos;My image 4 &apos;, &apos;My image 5 &apos;] 使用相对 XPaths记住如果你使用嵌套的选择器，并使用起始为 / 的 XPath，那么该 XPath 将对文档使用绝对路径，而且对于你调用的 Selector 不是相对路径。 比如，假设你想提取在 &lt;div&gt; 元素中的所有 &lt;p&gt; 元素。首先，你将先得到所有的 &lt;div&gt; 元素： 1&gt;&gt;&gt; divs = response.xpath(&quot;//div&quot;) 开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 &lt;div&gt; 元素内部提取所有的 &lt;p&gt; 元素： 12&gt;&gt;&gt; for p in divs.xpath(&apos;//p&apos;): # this is wrong - gets all &lt;p&gt; from the whole document ... print p.extract() 下面是比较合适的处理方法(注意 .//p XPath 的点前缀)： 12&gt;&gt;&gt; for p in divs.xpath(&apos;.//p&apos;): # extracts all &lt;p&gt; inside ... print p.extract() 另一种常见的情况将是提取所有直系 &lt;p&gt; 的结果： 12&gt;&gt;&gt; for p in divs.xpath(&apos;p&apos;): ... print p.extract() 使用 EXSLT 扩展因建于 lxml 之上，Scrapy 选择器也支持一些 EXSLT 扩展，可以在 XPath 表达式中使用这些预先制定的命名空间： 前缀 命名空间 用途 re http://exslt.org/regular-expressions 正则表达式 set http://exslt.org/sets 集合操作 正则表达式例如在XPath的 starts-with() 或 contains() 无法满足需求时， test() 函数可以非常有用。 例如在列表中选择有”class”元素且结尾为一个数字的链接： 1234567891011121314151617&gt;&gt;&gt; from scrapy import Selector&gt;&gt;&gt; doc = &quot;&quot;&quot;... &lt;div&gt;... &lt;ul&gt;... &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;... &lt;/ul&gt;... &lt;/div&gt;... &quot;&quot;&quot;&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)&gt;&gt;&gt; sel.xpath(&quot;//li//@href&quot;).extract()[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link3.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]&gt;&gt;&gt; sel.xpath(&quot;//li[re:test(@class, &apos;item-\\d$&apos;)]//@href&quot;).extract()[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;] 注意：C语言库 libxslt 不原生支持EXSLT正则表达式，因此 lxml 在实现时使用了Python re 模块的钩子。 因此，在 XPath 表达式中使用 regexp 函数可能会牺牲少量的性能。 集合操作集合操作可以方便地用于在提取文字元素前从文档树中去除一些部分。 例如使用 itemscopes 组和对应的 itemprops 来提取微数据（来自 http://schema.org/Product 的样本内容）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&gt;&gt;&gt; doc = &quot;&quot;&quot;... &lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;... &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;... ![](kenmore-microwave-17in.jpg)... &lt;div itemprop=&quot;aggregateRating&quot;... itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;... Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5... based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews... &lt;/div&gt;...... &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;... &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;... &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock... &lt;/div&gt;...... Product description:... &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.... Has six preset cooking categories and convenience features like... Add-A-Minute and Child Lock.&lt;/span&gt;...... Customer reviews:...... &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;... &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -... by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,... &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011... &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;... &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;... &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/... &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars... &lt;/div&gt;... &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace... it. &lt;/span&gt;... &lt;/div&gt;...... &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;... &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -... by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,... &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011... &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;... &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;... &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/... &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars... &lt;/div&gt;... &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and... fits in my apartment.&lt;/span&gt;... &lt;/div&gt;... ...... &lt;/div&gt;... &quot;&quot;&quot;&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)&gt;&gt;&gt; for scope in sel.xpath(&apos;//div[@itemscope]&apos;):... print &quot;current scope:&quot;, scope.xpath(&apos;@itemtype&apos;).extract()... props = scope.xpath(&apos;&apos;&apos;... set:difference(./descendant::*/@itemprop,... .//*[@itemscope]/*/@itemprop)&apos;&apos;&apos;)... print &quot; properties:&quot;, props.extract()... printcurrent scope: [u&apos;http://schema.org/Product&apos;] properties: [u&apos;name&apos;, u&apos;aggregateRating&apos;, u&apos;offers&apos;, u&apos;description&apos;, u&apos;review&apos;, u&apos;review&apos;]current scope: [u&apos;http://schema.org/AggregateRating&apos;] properties: [u&apos;ratingValue&apos;, u&apos;reviewCount&apos;]current scope: [u&apos;http://schema.org/Offer&apos;] properties: [u&apos;price&apos;, u&apos;availability&apos;]current scope: [u&apos;http://schema.org/Review&apos;] properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]current scope: [u&apos;http://schema.org/Rating&apos;] properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]current scope: [u&apos;http://schema.org/Review&apos;] properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]current scope: [u&apos;http://schema.org/Rating&apos;] properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;] 这里我们先迭代 itemscope 元素，对于每一个元素，我们寻找所有 itemprops 元素，并排除那些在另一个元素内部的元素 itemscope 。 内置选择器参考Selector 实例1class scrapy.selector.Selector(response=None, text=None, type=None) 一个实例Selector是一个包装器响应来选择其内容的某些部分。 response是一个HtmlResponse或一个XmlResponse将被用于选择和提取的数据对象。 text是一个unicode字符串或utf-8编码的文本，当一个 response不可用时。使用text和response一起是未定义的行为。 type定义选择器类型，它可以是&quot;html&quot;，&quot;xml&quot;或None（默认）。 如果type是None，选择器将根据response类型（见下文）自动选择最佳类型，或者默认&quot;html&quot;情况下与选项一起使用text。 如果type是None和response传递，选择器类型从响应类型推断如下： &quot;html&quot;对于HtmlResponse类型 &quot;xml&quot;对于XmlResponse类型 &quot;html&quot;为任何其他 否则，如果type设置，选择器类型将被强制，并且不会发生检测。 xpath（查询）查找与xpath匹配的节点query，并将结果作为 SelectorList实例将所有元素展平。列表元素也实现Selector接口。 query 是一个包含要应用的XPATH查询的字符串。 注意 为了方便起见，这种方法可以称为 response.xpath() css（查询）应用给定的CSS选择器并返回一个SelectorList实例。 query 是一个包含要应用的CSS选择器的字符串。 在后台，CSS查询使用cssselect库和run .xpath()方法转换为XPath查询 。 注意 为了方便起见，该方法可以称为 response.css() extract（）序列化并返回匹配的节点作为unicode字符串列表。编码内容的百分比未引用。 re（regex）应用给定的正则表达式并返回一个包含匹配项的unicode字符串的列表。 regex可以是编译的正则表达式或将被编译为正则表达式的字符串 re.compile(regex) 注意 注意，re()和re_first()解码HTML实体（除\\&lt;和\\&amp;）。 register_namespace（prefix，uri）注册在此使用的给定命名空间Selector。如果不注册命名空间，则无法从非标准命名空间中选择或提取数据。参见下面的例子。 remove_namespaces（）删除所有命名空间，允许使用无命名空间的xpaths遍历文档。参见下面的例子。 nonzero（）返回True如果有选择或任何实际的内容False 除外。换句话说，a的布尔值Selector由它选择的内容给出。 SelectorList对象class scrapy.selector.SelectorList 本SelectorList类是内置的一个子list 类，它提供了几个方法。 xpath（查询）调用.xpath()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。 query 是同一个参数 Selector.xpath() css（查询）调用.css()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。 query 是同一个参数 Selector.css() extract（）调用.extract()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。 re（）调用.re()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。 nonzero（）如果列表不为空，则返回True，否则返回False。 HTML响应的选择器示例这里有几个Selector例子来说明几个概念。在所有情况下，我们假设已经Selector实例化了一个HtmlResponse对象，如下： 1sel = Selector(html_response) &lt;h1&gt;从HTML响应主体中选择所有元素，返回Selector对象列表 （即SelectorList对象）： 1sel.xpath(&quot;//h1&quot;) &lt;h1&gt;从HTML响应正文中提取所有元素的文本，返回unicode字符串 12sel.xpath(&quot;//h1&quot;).extract() # this includes the h1 tagsel.xpath(&quot;//h1/text()&quot;).extract() # this excludes the h1 tag 迭代所有&lt;p&gt;标签并打印其类属性： 12for node in sel.xpath(&quot;//p&quot;): print node.xpath(&quot;@class&quot;).extract() XML响应的选择器示例这里有几个例子来说明几个概念。在这两种情况下，我们假设已经Selector实例化了一个 XmlResponse对象，像这样： 1sel = Selector(xml_response) 从XML响应主体中选择所有元素，返回Selector对象列表（即SelectorList对象）： 1sel.xpath(&quot;//product&quot;) 从需要注册命名空间的Google Base XML Feed中提取所有价格： 12sel.register_namespace(&quot;g&quot;, &quot;http://base.google.com/ns/1.0&quot;)sel.xpath(&quot;//g:price&quot;).extract() 删除名称空间当处理抓取项目时，通常很方便地完全删除命名空间，只需处理元素名称，编写更简单/方便的XPath。你可以使用的 Selector.remove_namespaces()方法。 让我们展示一个例子，用GitHub博客atom feed来说明这一点。 首先，我们打开shell和我们想要抓取的url： 1$ scrapy shell https://github.com/blog.atom 一旦在shell中，我们可以尝试选择所有对象，并看到它不工作（因为Atom XML命名空间模糊了这些节点）： 12&gt;&gt;&gt; response.xpath(&quot;//link&quot;)[] 但是一旦我们调用该Selector.remove_namespaces()方法，所有节点都可以直接通过他们的名字访问： 12345&gt;&gt;&gt; response.selector.remove_namespaces()&gt;&gt;&gt; response.xpath(&quot;//link&quot;)[&lt;Selector xpath=&apos;//link&apos; data=u&apos;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&apos;&gt;, &lt;Selector xpath=&apos;//link&apos; data=u&apos;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&apos;&gt;, ... 如果你想知道为什么默认情况下不调用命名空间删除过程，而不是手动调用它，这是因为两个原因，按照相关性的顺序： 删除命名空间需要迭代和修改文档中的所有节点，这对于Scrapy爬取的所有文档来说是一个相当昂贵的操作 可能有一些情况下，实际上需要使用命名空间，以防某些元素名称在命名空间之间冲突。这些情况非常罕见。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——items设计","slug":"scrapy-items-design","date":"2017-05-05T06:18:54.000Z","updated":"2017-05-06T10:36:18.000Z","comments":true,"path":"2017/05/05/scrapy-items-design/","link":"","permalink":"http://yoursite.com/2017/05/05/scrapy-items-design/","excerpt":"Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 scrapy.Field() 。","text":"Scrapy的item是进行数据保存不可缺少的步骤，通过它进行数据的整理并通过Pipelines进行数据的数据库保存，图片下载等，它只有一种类型 scrapy.Field() 。 定义 items由于需要添加一个封面图，对上面的爬虫添加一个 front_image_url 字段对 parse 函数进行修改： 1234567891011121314def parse(self, response): \"\"\" 1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析 2. 获取下一页的url并交给scrapy进行下载 :param response: :return: \"\"\" # 解析列表页中的所有文章url并交给解析函数进行具体字段的解析 post_nodes = response.css(\"#archive .floated-thumb .post-thumb a\") for post_node in post_nodes: image_url = post_node.css(\"img::attr(src)\").extract_first(\"\") post_url = post_node.css(\"::attr(href)\").extract_first(\"\") yield Request(url=parse.urljoin(response.url, post_url), meta=&#123;\"front_image_url\": image_url&#125;, callback=self.parse_detail) 其中的 meta 字段是传递值的方法。在调试时返回的 response 中会出现 meta 的内容，它是一个字典，故在传递时可以直接通过 response.meta[&#39;front_image_url&#39;] 进行引用（也可以使用get的方法，附默认值防止出现异常）： 在 items.py 文件中，定义一个item并声明其字段： 123456789101112class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field() front_image_path = scrapy.Field() praise_nums = scrapy.Field() comment_nums = scrapy.Field() fav_nums = scrapy.Field() tags = scrapy.Field() content = scrapy.Field() 在 jobbole.py 中添加 from ArticleSpider.items import JobBoleArticleItem 对item进行引用，然后在 parse_detail 中进行初始化 article_item = JobBoleArticleItem() ，之后将获取到的字段内容存入初始化的item中，最终代码如下： 123456789101112131415161718192021222324252627282930313233343536def parse_detail(self, response): article_item = JobBoleArticleItem() # 通过css选择器提取字段 front_image_url = response.meta.get(\"front_image_url\", \"\") # 文章封面图 title = response.css(\".entry-header h1::text\").extract()[0] create_date = response.css(\"p.entry-meta-hide-on-mobile::text\").extract()[0].strip().replace(\"·\", \"\").strip() praise_nums = int(response.css(\".vote-post-up h10::text\").extract()[0]) fav_nums = response.css(\".bookmark-btn::text\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css(\"a[href='#article-comment'] span::text\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css(\"div.entry\").extract()[0] tag_list = response.css(\"p.entry-meta-hide-on-mobile a::text\").extract() tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")] tags = \",\".join(tag_list) article_item[\"title\"] = title article_item[\"url\"] = response.url article_item[\"create_date\"] = create_date article_item[\"front_image_url\"] = front_image_url article_item[\"praise_nums\"] = praise_nums article_item[\"comment_nums\"] = comment_nums article_item[\"fav_nums\"] = fav_nums article_item[\"content\"] = content article_item[\"tags\"] = tags yield article_item 其中 yield article_item 会自动提交到 settings 中的 ITEM_PIPELINES 进行处理。此时在 pipelines.py 中设置断点调试，可以看到 article_item 中的值已经传递到这里了。 自定义 Pipelines在 settings.py 中有一个ITEM_PIPELINES的选项，把它的注释去掉增加下载图片的代码： 123456789# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; 'ArticleSpider.pipelines.ArticlespiderPipeline': 300, 'scrapy.pipelines.images.ImagesPipeline': 1,&#125;IMAGES_URLS_FIELD = \"front_image_url\"project_dir = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_dir, 'images') 上面的代码启用了下载图片piplines，并定义了存储地址及想要存储的图片地址。在settings.py同级目录下建立文件夹 images 用来保存图片。当运行爬虫时，图片就会自动下载图片并保存到本地。如果想要得到存储的图片路径的话，需要自定义pipelines。 首先，在 pipeines.py 中引入 from scrapy.pipelines.images import ImagesPipeline ， 然后自定义一个pipeline对ImagesPipeline进行重载： 123class ArticleImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): pass 进行断点调试，查看results中的信息： 调试结果中，results是一个list，第一个值是一个bool值表示图片是否获取成功，第二个值是一个字典，保存了图片路径，图片地址等信息。 最终自定义的pipeline代码如下： 123456class ArticleImagePipeline(ImagesPipeline): def item_completed(self, results, item, info): for ok, value in results: image_file_path = value[\"path\"] item[\"front_image_path\"] = image_file_path return item 上面代码得到image_path保存到 item[&quot;front_image_path&quot;] 中并返回，这时会根据pipelines的顺序进行下一个pipelines进行处理。通过断点调试可以得到想要的结果。 完善 items 获取对之前定义的items中的 url_object_id 字段，需要对url进行md5处理，因此在 items.py 同级目录下新建一个名为 utils 的 python package，新建 common.py ，代码如下： 123456789import hashlibdef get_md5(url): if isinstance(url, str): url = url.encode(\"utf-8\") m = hashlib.md5() m.update(url) return m.hexdigest() 然后在 jobbole.py 下引入 from ArticleSpider.utils.common import get_md5 ，在item内容填充时加上 article_item[&quot;url_object_id&quot;] = get_md5(response.url) 即可。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——编写Spider爬取伯乐在线所有文章","slug":"scrapy-jobbole-spider","date":"2017-04-26T06:18:54.000Z","updated":"2017-04-26T07:58:33.000Z","comments":true,"path":"2017/04/26/scrapy-jobbole-spider/","link":"","permalink":"http://yoursite.com/2017/04/26/scrapy-jobbole-spider/","excerpt":"仍然是以 http://blog.jobbole.com/all-posts/ 页面为例","text":"仍然是以 http://blog.jobbole.com/all-posts/ 页面为例 提取文章列表页首页使用CSS选择器获取页面中的文章url列表： 1post_urls = response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract() 123456789101112131415161718192021&gt;&gt;&gt; response.css(&quot;#archive .floated-thumb .post-thumb a::attr(href)&quot;).extract()&gt;&gt;&gt;[&apos;http://blog.jobbole.com/111005/&apos;, &apos;http://blog.jobbole.com/108468/&apos;, &apos;http://blog.jobbole.com/110975/&apos;, &apos;http://blog.jobbole.com/110986/&apos;, &apos;http://blog.jobbole.com/110957/&apos;, &apos;http://blog.jobbole.com/110976/&apos;, &apos;http://blog.jobbole.com/110923/&apos;, &apos;http://blog.jobbole.com/110962/&apos;, &apos;http://blog.jobbole.com/110958/&apos;, &apos;http://blog.jobbole.com/110140/&apos;, &apos;http://blog.jobbole.com/110939/&apos;, &apos;http://blog.jobbole.com/110941/&apos;, &apos;http://blog.jobbole.com/110931/&apos;, &apos;http://blog.jobbole.com/110934/&apos;, &apos;http://blog.jobbole.com/110929/&apos;, &apos;http://blog.jobbole.com/110835/&apos;, &apos;http://blog.jobbole.com/110906/&apos;, &apos;http://blog.jobbole.com/110916/&apos;, &apos;http://blog.jobbole.com/110913/&apos;, &apos;http://blog.jobbole.com/110903/&apos;] 先在Spider头部引入from scrapy.http import Request，使用Request进行对文章url列表获取函数的调用 123456789101112def parse(self, response): \"\"\" 1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析 2. 获取下一页的url并交给scrapy进行下载 :param response: :return: \"\"\" # 解析列表页中的所有文章url并交给解析函数进行具体字段的解析 post_urls = response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract() for post_url in post_urls: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 其中，最后 yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 是对每个url调用parse_detail方法进行字段解析，这里url的参数是带有完整域名的格式，如果不是完整域名，则需要对域名进行拼接成完成域名进行解析。首先要引入 from urllib import parse ，通过parse自带的 parse.urljoin() 进行拼接，代码为： yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 。 循环获取下一个列表页 每一个列表页都有“下一页”链接，我们通过CSS选择器来获取下一页的链接，然后交给parse函数进行循环解析。 1234# 提取下一页并交给scrapy进行下载next_url = response.css(\".next.page-numbers::attr(href)\").extract_first()if next_url: yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) 其中，extract_first()方法与extract()[0]用法相同，都是提取第一个字符串元素。 解析函数如下： 12345678910111213141516171819202122def parse_detail(self, response): # 通过css选择器提取字段 title = response.css(\".entry-header h1::text\").extract()[0] create_date = response.css(\"p.entry-meta-hide-on-mobile::text\").extract()[0].strip().replace(\"·\", \"\").strip() praise_nums = int(response.css(\".vote-post-up h10::text\").extract()[0]) fav_nums = response.css(\".bookmark-btn::text\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css(\"a[href='#article-comment'] span::text\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css(\"div.entry\").extract()[0] tag_list = response.css(\"p.entry-meta-hide-on-mobile a::text\").extract() tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")] tags = \",\".join(tag_list) print(title, create_date, fav_nums, comment_nums, tags) 运行结果","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——CSS选择器","slug":"css-selector","date":"2017-04-25T12:18:54.000Z","updated":"2017-06-09T08:20:27.000Z","comments":true,"path":"2017/04/25/css-selector/","link":"","permalink":"http://yoursite.com/2017/04/25/css-selector/","excerpt":"CSS选择器的用法CSS选择器简介在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。","text":"CSS选择器的用法CSS选择器简介在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。 常用CSS选择器介绍 表达式 说明 * 选择所有节点 #container 选择id为container的节点 .container 选择所有class包含container的节点 li a 选取所有li下的所有a节点 ul + p 选择ul后面的第一个p元素 div#container &gt; ul 选取id为container的div的第一个ul子元素 ul ~ p 选取与ul相邻的所有p元素 a[title] 选取所有有title属性的a元素 a[href=”http://163.com“] 选取所有href属性为163的a元素 a[href*=”163”] 选取所有href属性包含163的a元素 a[href^=”http”] 选取所有href属性以http开头的a元素 a[href$=”.jpg”] 选取所有href以.jpg结尾的a元素 input[type=radio]:checked 选择选中的radio的元素 div:not(#container) 选取所有id非container的div属性 li:nth-child(3) 选取第三个li元素 tr:nth-child(2n) 第偶数个tr Scrapy中CSS选择器用法示例仍然是用Xpath用法示例中的例子来进行测试 获取标题 12&gt;&gt;&gt; response.css(&quot;.entry-header h1::text&quot;).extract()[0]&apos;2016 腾讯软件开发面试题（部分）&apos; 注意：这里获取文字内容的方法为::text，而不是text()。 获取文章发布时间 12&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile::text&quot;).extract()[0].strip().replace(&quot;·&quot;, &quot;&quot;).strip()&apos;2017/02/18&apos; 获取点赞数、收藏数、评论数 1234567891011# 点赞数&gt;&gt;&gt; response.css(&quot;.vote-post-up h10::text&quot;).extract()[0]&apos;2&apos;# 收藏数，获取之后需要用正则表达式进行清洗&gt;&gt;&gt; response.css(&quot;.bookmark-btn::text&quot;).extract()[0]&apos; 23 收藏&apos;# 评论数，获取之后需要用正则表达式进行清洗&gt;&gt;&gt; response.css(&quot;a[href=&apos;#article-comment&apos;] span::text&quot;).extract()[0]&apos; 7 评论&apos; 正则表达式清洗收藏数，评论数的逻辑如下 123456789fav_nums = response.css(\".bookmark-btn::text\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", fav_nums)if match_re: fav_nums = int(match_re.group(1)) comment_nums = response.css(\"a[href='#article-comment'] span::text\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", comment_nums)if match_re: comment_nums = int(match_re.group(1)) 获取正文1&gt;&gt;&gt; response.css(&quot;div.entry&quot;).extract()[0] 获取tags 12&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;] 然后需要对数据进行清洗 123tag_list = response.css(\"p.entry-meta-hide-on-mobile a::text()\").extract()tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")]tags = \",\".join(tag_list)","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，CSS，Python","slug":"Scrapy，CSS，Python","permalink":"http://yoursite.com/tags/Scrapy，CSS，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Spiders","slug":"scrapy-spiders","date":"2017-04-22T06:18:54.000Z","updated":"2017-04-22T11:34:40.000Z","comments":true,"path":"2017/04/22/scrapy-spiders/","link":"","permalink":"http://yoursite.com/2017/04/22/scrapy-spiders/","excerpt":"Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。","text":"Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。 对 spider 来说，爬取的流程如下： 先初始化请求URL列表，并指定下载后处理response的回调函数。初次请求URL通过 start_urls 指定，调用 start_requests() 产生 Request 对象，然后注册 parse 方法作为回调 在parse回调中解析response并返回字典, Item 对象, Request 对象或它们的迭代对象。 Request 对象还会包含回调函数，之后Scrapy下载完后会被这里注册的回调函数处理。 在回调函数里面，你通过使用选择器（同样可以使用BeautifulSoup,lxml或其他工具）解析页面内容，并生成解析后的结果Item。 最后返回的这些Item通常会被持久化到数据库中(使用Item Pipeline)或者使用Feed exports将其保存到文件中。 虽然该循环对任何类型的 spider 都（多少）适用，但 Scrapy 仍然为了不同的需求提供了多种默认 spider。 之后将讨论这些 spider。 Spider 参数Spider 可以通过接受参数来修改其功能。 spider 参数一般用来定义初始 URL 或者指定限制爬取网站的部分。 您也可以使用其来配置 spider 的任何功能。 在运行 crawl 时添加 -a 可以传递 Spider 参数： 1scrapy crawl myspider -a category=electronics Spider 在构造器（constructor）中获取参数： 123456789import scrapyclass MySpider(scrapy.Spider): name = 'myspider' def __init__(self, category=None, *args, **kwargs): super(MySpider, self).__init__(*args, **kwargs) self.start_urls = ['http://www.example.com/categories/%s' % category] # ... Spider 参数也可以通过 Scrapyd 的 schedule.json API 来传递。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Items","slug":"scrapy-items","date":"2017-04-21T06:18:54.000Z","updated":"2017-04-21T13:14:25.000Z","comments":true,"path":"2017/04/21/scrapy-items/","link":"","permalink":"http://yoursite.com/2017/04/21/scrapy-items/","excerpt":"爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。 Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。","text":"爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。 Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。 声明 ItemItem 使用简单的 class 定义语法以及 Field 对象来声明。例如: 1234567import scrapyclass ProductItem(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) Scrapy Item 的定义方式与 Django Models 很类似，但是没有 Django 那么多不同的字段类型（Field Type）。 Item 字段（Item Fields）Field 对象指明了每个字段的元数据（metadata）。例如上面例子中 last_updated 中指明了该字段的序列化函数。 您可以为每个字段指明任何类型的元数据。Field 对象对接受的值没有任何限制。也正是因为这个原因，文档也无法提供所有可用的元数据的键（key）参考列表。Field 对象中保存的每个键可以由多个组件使用，并且只有这些组件知道这个键的存在。您可以根据自己的需求，定义使用其他的 Field 键。 设置 Field 对象的主要目的就是在一个地方定义好所有的元数据。一般来说，那些依赖某个字段的组件肯定使用了特定的键（key）。您必须查看组件相关的文档，查看其用了哪些元数据键（metadata key）。 需要注意的是，用来声明 item 的 Field 对象并没有被赋值为 class 的属性。不过您可以通过 Item.fields 属性进 行访问。 与 Item 配合在API这里的操作，和 dict API 非常的相似。 创建 item12345678910&gt;&gt;&gt; import scrapy&gt;&gt;&gt; class ProductItem(scrapy.Item):... name = scrapy.Field()... price = scrapy.Field()... stock = scrapy.Field()... last_updated = scrapy.Field(serializer=str)...&gt;&gt;&gt; product = ProductItem(name='Desktop', price=1000)&gt;&gt;&gt; print(product)&#123;'name': 'Desktop', 'price': 1000&#125; 获取字段的值两种方式： 键值对 get() 123456789101112131415161718192021222324252627282930313233343536373839404142# 已设定key和value值&gt;&gt;&gt; product['name']'Desktop'&gt;&gt;&gt; product.get('name')'Desktop'# 未设定key和value值&gt;&gt;&gt; product['last_updated']Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 59, in __getitem__ return self._values[key]KeyError: 'last_updated' # 默认返回空值&gt;&gt;&gt; product.get('last_updated')# 设定返回值&gt;&gt;&gt; product.get('last_updated', 'not set value')'not set value'# 未声明的字段&gt;&gt;&gt; product['lala']Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 59, in __getitem__ return self._values[key]KeyError: 'lala'&gt;&gt;&gt; product.get('lala', 'not exist')'not exist'# 字段是否被赋值&gt;&gt;&gt; 'name' in productTrue&gt;&gt;&gt; 'last_updated' in productFalse# 字段是否被声明&gt;&gt;&gt; 'last_updated' in product.fieldsTrue&gt;&gt;&gt; 'lala' in product.fieldsFalse 设置字段的值123456789101112# 已经声明的字段&gt;&gt;&gt; product['last_updated'] = 'today'&gt;&gt;&gt; product['last_updated']'today'# 未声明的字段无法赋值&gt;&gt;&gt; product['lala'] = 'test'Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 66, in __setitem__ (self.__class__.__name__, key))KeyError: 'ProductItem does not support field: lala' 获取所有能够获取到的值可以使用 dict API 来获取所有的值: 1234&gt;&gt;&gt; product.keys()dict_keys(['last_updated', 'price', 'name'])&gt;&gt;&gt; product.items()ItemsView(&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125;) 复制 item12345678&gt;&gt;&gt; product2 = ProductItem(product)&gt;&gt;&gt; print(product2)&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125;# 推荐使用第二种方法&gt;&gt;&gt; product3 = product2.copy()&gt;&gt;&gt; print(product3)&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125; 根据 item 创建字典(dict)12&gt;&gt;&gt; dict(product)&#123;'price': 1000, 'last_updated': 'today', 'name': 'Desktop'&#125; 根据字典(dict)创建 item12345678910&gt;&gt;&gt; ProductItem(&#123;'name':'laptop pc', 'price':1500&#125;)&#123;'name': 'laptop pc', 'price': 1500&#125;&gt;&gt;&gt; ProductItem(&#123;'name':'laptop pc', 'lala':1500&#125;)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 56, in __init__ self[k] = v File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 66, in __setitem__ (self.__class__.__name__, key))KeyError: 'ProductItem does not support field: lala' 扩展 Item可以通过继承原始的 Item 来扩展 item(添加更多的字段或者修改某些字段的元数据)。 添加新的字段 123class DiscountedProductItem(Product): discount_percent = scrapy.Field(serializer=str) discount_expiration_date = scrapy.Field() 使用原字段的元数据 123class SpecificProduct(Product): name = scrapy.Field(Product.fields['name'], serializer=my_serializer)#my_serializer 指序列化的类型 上述代码，在保留了原始的元数据值的情况下，添加（或覆盖）了 name 字段的 serializer 。 存在及覆盖，不存在即添加。 Item 对象class scrapy.item.Item([arg]) 返回一个根据给定的参数可选初始化的 item 。 Item 复制了标准化的 dict API ，包括初始化函数也是一样。除此之外，唯一添加的额外属性就是 fields 。 fields 是一个包含了 item 所有声明的字段的字典，而不仅仅是获取到的字段。该字典的 key 是字段（field）的名字，值是 Item 声明中使用到的 Field 对象。 字段（Field）对象class scrapy.item.Field([arg]) Field仅仅是内置的 dict 类的一个别名（继承于 dict ），并没有提供额外的方法或属性。说白了，Field就是完完全全的Python字典，被用来基于类属性的方法支持 Item 声明语法。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Redis学习笔记(六)：数据安全与性能保障——处理系统故障","slug":"Redis-6","date":"2017-04-20T12:18:54.000Z","updated":"2017-04-20T13:46:42.000Z","comments":true,"path":"2017/04/20/Redis-6/","link":"","permalink":"http://yoursite.com/2017/04/20/Redis-6/","excerpt":"如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。","text":"如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。 验证快照文件和 AOF 文件无论时快照持久化还是AOF持久化，都提供了在遇到系统故障时进行数据回复的工具。Redis提供了两个命令行程序 redis-check-aof 和 redis-check-rdb(redis-check-dump was renamed to redis-check-rdb in redis version 3.2) ，它们可以在系统故障发生之后，检查AOF文件和快照文件的状态，并在有需要的情况下对文件进行修复。 在不给定任何参数的情况下运行这两个程序，就可以看见它们的基本使用方法： 1234$ redis-check-rdbUsage: redis-check-rdb &lt;rdb-file-name&gt;$ redis-check-dumpUsage: redis-check-dump &lt;dump.rdb&gt; 如果运行 redis-check-aof 程序时给了 --fix 参数，那么会对AOF文件进行修复。修复方法非常简单：扫描给定的 AOF 文件，寻找不正确或不完整的命令，当发现第一个出错命令的时候，程序会删除出错的命令以及位于出错命令之后的所有命令。在大多数情况下，被删除的都是 AOF 文件末尾的不完整的写命令。遗憾的是，目前没有办法修复出错的快照文件。尽管发现快照文件首个出现错误的地方是有可能的，但因为快照文件本身经过了压缩，而出现在快照文件中间的错误有可能会导致快照文件的剩余部分无法读取。因此，最好为重要的快照文件保留多个备份，并在进行数据恢复时，通过计算快照文件的 SHA1 散列值和 SHA256 散列值来对内容进行验证。 更换故障主服务器我们来看一下在拥有一个主服务器和一个从服务器的情况下，更换主服务器的具体步骤。假设A、B两台机器都运行着 Redis ，机器A为 master ，机器B为 slave 。机器A因为暂时无法修复的故障而断开了连接，因此决定将同样安装了 Redis 的机器 C 用作新的主服务器。 更换服务器的计划非常简单：首先向机器B发送一个 SAVE 命令，让它创建一个新的快照文件，接着将这个快照文件发送给机器C，并在机器 C 上面启动 Redis 。最后，让B成为机器C的从服务器。由于环境有限，就在同一台机器上用不同的端口进行测试，下面进行演示： 先进入 Redis 安装位置，再安装两个 Redis 服务并分别修改配置文件 redis.conf 中的 port 为6380和6381 12345678$ cd /usr/local$ sudo cp -r redis redis6380Password:$ sudo chmod -R 777 redis6380$ vim redis6380/redis.conf$ sudo cp -r redis redis6381$ sudo chmod -R 777 redis6381$ vim redis6381/redis.conf 启动机器A端口为6379，机器B端口为6380，并让B成为A的从服务器 123456789101112# 启动A$ cd redis$ ./src/redis-server redis.conf# 启动B$ cd redis6380$ ./src/redis-server redis.conf# 让B成为A的从服务器$ $ redis-cli -h localhost -p 6380localhost:6380&gt; SLAVEOF localhost 6379OK 停止机器A的 Redis 服务，此时只剩 Redis 从服务器B在运行 向机器B发送 SAVE 命令 12localhost:6380&gt; SAVEOK 将机器B的快照文件复制到机器C的对应目录，并启动 Redis 服务 123$ cp -f /usr/local/redis6380/dump.rdb /usr/local/redis6381$ cd /usr/local/redis6381$ ./src/redis-server redis.conf 让机器B成为机器C的从服务器 12localhost:6380&gt; SLAVEOF localhost 6381OK 测试机器B是否能从机器C同步数据 123456$ redis-cli -h localhost -p 6381localhost:6381&gt; set key new-masterOK$ redis-cli -h localhost -p 6380localhost:6380&gt; get key&apos;new-master&apos; 另一种创建新的主服务器的方法，就是将从服务器升级（turn）为主服务器，并为升级后的主服务器创建从服务器。 以上两种方法都可以让 Redis 回到之前的一个主服务器和一个从服务器的状态，而用户接下来需要做的就是更新客户端的配置，让它们去读写正确的服务器。除此之外，如果用户需要重启 Redis 的话，那么可能还需要对服务器的持久化配置进行更新。 Redis Sentinel可以监视指定的Redis主服务器及其下属的从服务器，并在主服务器下线时自动进行故障转移(failover)。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy命令行工具","slug":"scrapy-command-line-tools","date":"2017-04-20T06:18:54.000Z","updated":"2017-04-20T07:53:41.000Z","comments":true,"path":"2017/04/20/scrapy-command-line-tools/","link":"","permalink":"http://yoursite.com/2017/04/20/scrapy-command-line-tools/","excerpt":"Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。","text":"Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。 使用 scrapy 工具创建项目一般来说，使用 scrapy 工具的第一件事就是创建 Scrapy 项目： 1scrapy startproject myproject 该命令将会在 myproject 目录中创建一个 Scrapy 项目。 接下来，进入到项目目录中: 1cd myproject 这时候就可以使用 scrapy 命令来管理和控制项目了。 控制项目创建一个新的 spider： 1scrapy genspider mydomain mydomain.com Scrapy 提供了两种类型的命令。一种必须在 Scrapy 项目中运行(针对项目(Project-specific)的命令)，另外一种则不需要(全局命令)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。 123456789101112131415161718# 全局命令(不需要在项目中运行)startprojectsettingsrunspidershellfetchviewversion# 项目(Project-only)命令(必须在项目中运行)crawlchecklisteditparsegenspiderdeploybench 工具命令介绍我们可以通过运行命令来获取关于每个命令的详细内容： 1scrapy &lt;command&gt; -h 也可以查看所有的命令： 1scrapy -h 下面就对这些命令进行介绍。 startproject 语法：scrapy startproject &lt;project_name&gt; 全局命令 在 project_name 文件夹下创建一个名为 project_name 的 Scrapy 项目。 genspider 语法：scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt; 项目命令 在当前项目中创建 spider。这仅仅是创建 spider 的一种快捷方法。该方法可以使用提前定义好的模板来生成 spider。您也可以自己创建 spider 的源码文件。 例子： 12345678910111213141516171819202122232425# 查看模板$ scrapy genspider -lAvailable templates: basic crawl csvfeed xmlfeed# 编辑模板$ scrapy genspider -d basic# -*- coding: utf-8 -*-import scrapyclass $classname(scrapy.Spider): name = &quot;$name&quot; allowed_domains = [&quot;$domain&quot;] start_urls = [&apos;http://$domain/&apos;] def parse(self, response): pass # 根据模板来生成spider$ scrapy genspider -t basic example example.com Created spider &apos;example&apos; using template &apos;basic&apos; in module: tutorial.spiders.example crawl 语法：scrapy crawl myspider 项目命令 使用 spider 进行爬取。 例子： 12$ scrapy crawl myspider [ ... myspider starts crawling ... ] check 语法：scrapy check [-l] &lt;spider&gt; 项目命令 运行 contract 检查。 例子： 1234567891011121314$ scrapy check -l first_spider * parse * parse_item second_spider * parse * parse_item$ scrapy check [FAILED] first_spider:parse_item &gt;&gt;&gt; &apos;RetailPricex&apos; field is missing[FAILED] first_spider:parse &gt;&gt;&gt; Returned 92 requests, expected 0..4 list 语法：scrapy list 项目命令 列出当前项目中所有可用的 spider。每行输出一个 spider。 例子： 123$ scrapy list spider1 spider2 edit 语法：scrapy edit &lt;spider&gt; 项目命令 使用 EDITOR 中设定的编辑器编辑给定的 spider 该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者 IDE 来编写调试 spider。 fetch 语法：scrapy fetch &lt;url&gt; 全局命令 使用 Scrapy 下载器(downloader)下载给定的 URL，并将获取到的内容送到标准输出。 该命令以 spider 下载页面的方式获取页面。例如，如果 spider 有 USER_AGENT 属性修改了 User Agen t，该命令将会使用该属性。 因此，您可以使用该命令来查看 spider 如何获取某个特定页面。 该命令如果非项目中运行则会使用默认 Scrapy downloader 设定。 例子： 12345678910111213$ scrapy fetch --nolog http://www.example.com/some/page.html [ ... html content here ... ]$ scrapy fetch --nolog --headers http://www.example.com/ &#123;&apos;Accept-Ranges&apos;: [&apos;bytes&apos;],&apos;Age&apos;: [&apos;1263 &apos;], &apos;Connection&apos;: [&apos;close &apos;], &apos;Content-Length&apos;: [&apos;596&apos;], &apos;Content-Type&apos;: [&apos;text/html; charset=UTF-8&apos;], &apos;Date&apos;: [&apos;Wed, 18 Aug 2010 23:59:46 GMT&apos;], &apos;Etag&apos;: [&apos;&quot;573c1-254-48c9c87349680&quot;&apos;], &apos;Last-Modified&apos;: [&apos;Fri, 30 Jul 2010 15:30:18 GMT&apos;], &apos;Server&apos;: [&apos;Apache/2.2.3 (CentOS)&apos;]&#125; view 语法：scrapy view &lt;url&gt; 全局命令 在浏览器中打开给定的 URL，并以 Scrapy spider 获取到的形式展现。 有些时候 spider 获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查 spider 所获取到的页面，并确认这是您所期望的。 例子： 12$ scrapy view http://www.example.com/some/page.html [ ... browser starts ... ] shell 语法：scrapy shell [url] 全局命令 以给定的 URL(如果给出)或者空(没有给出 URL)启动 Scrapy shell。查看 Scrapy 终端(Scrapy shell) 获取更多信息。 例子： 12$ scrapy shell http://www.example.com/some/page.html [ ... scrapy shell starts ... ] parse 语法：scrapy parse &lt;url&gt; [options] 项目命令 获取给定的 URL 并使用相应的 spider 分析处理。如果提供 --callback 选项，则使用 spider 的该方法处理，否则使用 parse 。 settings 语法：scrapy settings [option] 全局命令 获取 Scrapy 的设定 在项目中运行时，该命令将会输出项目的设定值，否则输出 Scrapy 默认设定。 例子： 1234$ scrapy settings --get BOT_NAME scrapybot $ scrapy settings --get DOWNLOAD_DELAY 0 runspider 语法：scrapy runspider &lt;spider_file.py&gt; 全局命令 在未创建项目的情况下，运行一个编写在 Python 文件中的 spider。 例子： 12$ scrapy runspider myspider.py [ ... spider starts crawling ... ] version 语法：scrapy version [-v] 全局命令 输出 Scrapy 版本。配合 -v 运行时，该命令同时输出 Python，Twisted 以及平台的信息，方便 bug 提交。 例子： 1234567891011$ scrapy version -vScrapy : 1.3.3lxml : 3.7.3.0libxml2 : 2.9.4cssselect : 1.0.1parsel : 1.1.0w3lib : 1.17.0Twisted : 17.1.0Python : 3.5.2 (default, Oct 11 2016, 04:59:56) - [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)]pyOpenSSL : 16.2.0 (OpenSSL 1.1.0e 16 Feb 2017)Platform : Darwin-16.6.0-x86_64-i386-64bit deploy 语法：scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ] 项目命令 将项目部署到 Scrapyd 服务。 bench 语法：scrapy bench 全局命令 运行 benchmark 测试。Benchmarking。 自定义项目命令您也可以通过 COMMANDS_MODULE 来添加您自己的项目命令。您可以以 scrapy/commands 中 Scrapy commands 为例来了解如何实现您的命令。 COMMANDS_MODULE1Default: &apos;&apos; (empty string) 用于查找添加自定义 Scrapy 命令的模块。 例子： 1COMMANDS_MODULE = &apos;mybot.commands&apos;","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法示例","slug":"xpath-example","date":"2017-04-16T12:18:54.000Z","updated":"2017-06-09T08:20:10.000Z","comments":true,"path":"2017/04/16/xpath-example/","link":"","permalink":"http://yoursite.com/2017/04/16/xpath-example/","excerpt":"Scrapy 中 XPath 获取相应内容为了方便调试，在终端下输入以下命令进入Scrapy shell： 1scrapy shell &apos;http://blog.jobbole.com/110287&apos;","text":"Scrapy 中 XPath 获取相应内容为了方便调试，在终端下输入以下命令进入Scrapy shell： 1scrapy shell &apos;http://blog.jobbole.com/110287&apos; 获取标题 1234&gt;&gt;&gt; response.xpath(&quot;//div[@class=&apos;entry-header&apos;]/h1/text()&quot;).extract()[0]&apos;2016 腾讯软件开发面试题（部分）&apos;&gt;&gt;&gt; response.xpath(&apos;//*[@id=&quot;post-110287&quot;]/div[1]/h1/text()&apos;).extract()[0]&apos;2016 腾讯软件开发面试题（部分）&apos; 以上两种方法都可以得到文章标题，第一种方法通过标题class的属性得到，第二种方法通过确定id，然后通过列表切片得到标题字符串。 获得文章发布时间 1234&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0]&apos;\\r\\n\\r\\n 2017/02/18 · &apos;&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0].strip().replace(&apos;·&apos;, &apos;&apos;).strip()&apos;2017/02/18&apos; 第一条命令只能获取p标签中的内容，还需要对获取的数据用 strip() 和 replace() 方法进行清洗。 获取点赞数、收藏数、评论数 对于含有多个属性的class如：class=&quot; btn-bluet-bigger href-style vote-post-up register-user-only &quot;，若只使用其中的一个属性得到值，可以使用contains。 获取点赞数12&gt;&gt;&gt; int(response.xpath(&quot;//span[contains(@class, &apos;vote-post-up&apos;)]/h10/text()&quot;).extract()[0])2 获取收藏数、评论数12&gt;&gt;&gt; response.xpath(&quot;//span[contains(@class, &apos;bookmark-btn&apos;)]/text()&quot;).extract()[0]&apos; 23 收藏&apos; 得到的内容为’ 23 收藏’，需要使用正则表达式进行数据清洗。 1234fav_nums = response.xpath(\"//span[contains(@class, 'bookmark-btn')]/text()\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", fav_nums)if match_re: fav_nums = int(match_re.group(1)) 同样的，评论数的获取也需要正则表达式的帮忙。 1234comment_nums = response.xpath(\"//a[@href='#article-comment']/span/text()\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", comment_nums)if match_re: comment_nums = int(match_re.group(1)) 获取正文1content = response.xpath(\"//div[@class='entry']\").extract()[0] 获取tags 所有的tag都在a标签下，类似获得日期的方式，增加一个a标签路径即可。 12&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/a/text()&quot;).extract()[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;] 现在需要对数据进行清洗，去除评论标签。 123tag_list = response.xpath(\"//p[@class='entry-meta-hide-on-mobile']/a/text()\").extract()tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")]tags = \",\".join(tag_list) 总结最后我们构造的spider文件如下： 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import scrapyimport reclass JobboleSpider(scrapy.Spider): name = \"jobbole\" allowed_domains = [\"blog.jobbole.com\"] start_urls = ['http://blog.jobbole.com/110287'] def parse(self, response): title = response.xpath(\"//div[@class='entry-header']/h1/text()\").extract()[0] create_date = response.xpath(\"//p[@class='entry-meta-hide-on-mobile']/text()\").extract()[0].strip().replace(\"·\", \"\").strip() praise_nums = int(response.xpath(\"//span[contains(@class, 'vote-post-up')]/h10/text()\").extract()[0]) fav_nums = response.xpath(\"//span[contains(@class, 'bookmark-btn')]/text()\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", fav_nums) if match_re: fav_nums = int(match_re.group(1)) comment_nums = response.xpath(\"//a[@href='#article-comment']/span/text()\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", comment_nums) if match_re: comment_nums = int(match_re.group(1)) content = response.xpath(\"//div[@class='entry']\").extract()[0] tag_list = response.xpath(\"//p[@class='entry-meta-hide-on-mobile']/a/text()\").extract() tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")] tags = \",\".join(tag_list)","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，XPath，Python","slug":"Scrapy，XPath，Python","permalink":"http://yoursite.com/tags/Scrapy，XPath，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy简单入门","slug":"scrapy-simple-intro","date":"2017-04-16T06:18:54.000Z","updated":"2017-04-16T07:13:21.000Z","comments":true,"path":"2017/04/16/scrapy-simple-intro/","link":"","permalink":"http://yoursite.com/2017/04/16/scrapy-simple-intro/","excerpt":"创建项目开始爬取前，首先需要创建一个新的Scrapy项目 1scrapy startproject tutorial","text":"创建项目开始爬取前，首先需要创建一个新的Scrapy项目 1scrapy startproject tutorial 该命令将会创建包含下列内容的 tutorial 目录: 12345678910tutorial/ scrapy.cfg tutorial/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... 这些文件分别是: scrapy.cfg：项目的配置文件 tutorial/：该项目的 python 模块，之后我们将在此加入代码。 tutorial/items.py：项目中的 item 文件。 tutorial/pipelines.py：项目中的 pipelines 文件。 tutorial/settings.py：项目的设置文件。 tutorial/spiders/：放置 spider 代码的目录。 定义 ItemItem 是保存爬取到的数据的容器；其使用方法和 python 字典类似， 并且提供了额外保护机制来避免拼写错误导 致的未定义字段错误。 类似在 ORM 中做的一样，您可以通过创建一个scrapy.Item类， 并且定义类型为scrapy.Field的类属性来定义一个 Item。 (如果不了解 ORM, 不用担心，您会发现这个步骤非常简单) 首先根据需要从 dmoz.org 获取到的数据对 item 进行建模。 我们需要从 dmoz 中获取名字，url，以及网站的描 述。 对此，在 item 中定义相应的字段。编辑tutorial目录中的items.py文件: 123456import scrapyclass DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() 可能一开始这有些复杂，但是通过定义 item， 我们可以很方便的使用 Scrapy 的其他方法，而这些方法需要知道我们的 item 的定义。 编写第一个爬虫Spider 是用户编写用于从单个网站(或者一些网站)爬取数据的类。 其包含了一个用于下载的初始 URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方 法。 为了创建一个 Spider，我们必须继承scrapy.Spider类， 且定义以下三个属性: name : 用于区别 Spider。 该名字必须是唯一的，您不可以为不同的 Spider 设定相同的名字。 start_urls : 包含了 Spider 在启动时进行爬取的 url 列表。 因此，第一个被获取到的页面将是其中之一。 后续的 URL 则从初始的 URL 获取到的数据中提取。 parse() : spider 的一个方法。 被调用时，每个初始 URL 完成下载后生成的Response对象将会作为 唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成 item)以及生成需 要进一步处理的 URL 的Request对象。 以下为我们的第一个 Spider 代码，保存在tutorial/spiders目录下的dmoz_spider.py文件中: 1234567891011121314import scrapyclass DmozSpider(scrapy.Spider): name = \"dmoz\" allow_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\" ] def parse(self, response): filename = response.url.split(\"/\")[-2] with open(filename, 'wb') as f: f.write(response.body) 其中，allow_domains是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页。 爬取进入项目的根目录，执行以下命令启动spider 1scrapy crawl dmoz crawl dmoz启动用于爬取dmoz.org的 spider，可以得到如下输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152532017-04-15 21:51:39 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)2017-04-15 21:51:39 [scrapy.utils.log] INFO: Overridden settings: &#123;&apos;BOT_NAME&apos;: &apos;tutorial&apos;, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;]&#125;2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled extensions:[&apos;scrapy.extensions.corestats.CoreStats&apos;, &apos;scrapy.extensions.logstats.LogStats&apos;, &apos;scrapy.extensions.telnet.TelnetConsole&apos;]2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;, &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;, &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;, &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;, &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;, &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;, &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;, &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;, &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;, &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;, &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled item pipelines:[]2017-04-15 21:51:39 [scrapy.core.engine] INFO: Spider opened2017-04-15 21:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2017-04-15 21:51:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;: HTTP status code is not handled or not allowed2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt;: HTTP status code is not handled or not allowed2017-04-15 21:51:41 [scrapy.core.engine] INFO: Closing spider (finished)2017-04-15 21:51:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:&#123;&apos;downloader/request_bytes&apos;: 734, &apos;downloader/request_count&apos;: 3, &apos;downloader/request_method_count/GET&apos;: 3, &apos;downloader/response_bytes&apos;: 3525, &apos;downloader/response_count&apos;: 3, &apos;downloader/response_status_count/403&apos;: 3, &apos;finish_reason&apos;: &apos;finished&apos;, &apos;finish_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 41, 968931), &apos;log_count/DEBUG&apos;: 4, &apos;log_count/INFO&apos;: 9, &apos;response_received_count&apos;: 3, &apos;scheduler/dequeued&apos;: 2, &apos;scheduler/dequeued/memory&apos;: 2, &apos;scheduler/enqueued&apos;: 2, &apos;scheduler/enqueued/memory&apos;: 2, &apos;start_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 39, 764494)&#125;2017-04-15 21:51:41 [scrapy.core.engine] INFO: Spider closed (finished) 查看包含[dmoz]的输出，可以看到输出的 log 中包含定义在start_urls的初始 URL，并且与 spider 中是一 一对应的。在 log 中可以看到其没有指向其他页面( (referer:None) )。 除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含 url 所对应的内容的文件被创建 了: Book，Resources 。 发生了什么？Scrapy 为 Spider 的start_urls属性中的每个 URL 创建了scrapy.Request对象，并将parse方法作为回调函数(callback)赋值给了Request。 Request对象经过调度，执行生成scrapy.http.Response对象并送回给spider parse()方法。 提取 ItemSelectors 选择器简介从网页中提取数据有很多方法。Scrapy 使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors。关于 selector 和其他提取机制的信息请参考Selector文档。 关于Xpath的简单使用方法，可以查看之前的一篇博客Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法 为了配合 XPath，Scrapy 除了提供了Selector之外，还提供了方法来避免每次从 response 中提取数据时生成 selector 的麻烦。 Selector 有四个基本的方法： xpath()：传入 xpath 表达式，返回该表达式所对应的所有节点的 selector list 列表 。 css()：传入 CSS 表达式，返回该表达式所对应的所有节点的 selector list 列表。 extract()：序列化该节点为 unicode 字符串并返回 list。 re()：根据传入的正则表达式对数据进行提取，返回 unicode 字符串 list 列表。 在 Shell 中尝试 Selector 选择器为了介绍 Selector 的使用方法，接下来我们将要使用内置的 Scrapy shell。Scrapy Shell 需要我们预装好 IPython(一个扩展的 Python 终端)。 我们需要进入项目的根目录，执行下列命令来启动 shell: 1scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot; Shell 的输出类似于： 12345678910111213141516172017-04-15 22:04:22 [scrapy.core.engine] INFO: Spider opened2017-04-15 22:04:23 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)2017-04-15 22:04:24 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)[s] Available Scrapy objects:[s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s] crawler &lt;scrapy.crawler.Crawler object at 0x109728ac8&gt;[s] item &#123;&#125;[s] request &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;[s] response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;[s] settings &lt;scrapy.settings.Settings object at 0x10a2a0a58&gt;[s] spider &lt;DefaultSpider &apos;default&apos; at 0x10a4dc3c8&gt;[s] Useful shortcuts:[s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)[s] fetch(req) Fetch a scrapy.Request and update local objects[s] shelp() Shell help (print this help)[s] view(response) View response in a browser&gt;&gt;&gt; 当 shell 载入后，我们将得到一个包含 response 数据的本地 response 变量。输入 response.body 将输出 resp onse 的包体，输出 response.headers 可以看到 response 的包头。 更为重要的是，当输入 response.selector 时， 我们将获取到一个可以用于查询返回数据的 selector(选择器)， 以及映射到 response.selector.xpath() 、response.selector.css() 的 快捷方法(shortcut): response.xpat h() 和 response.css() 。 下面就来试试： 12345678&gt;&gt;&gt; response.xpath(&apos;//title&apos;)[&lt;Selector xpath=&apos;//title&apos; data=&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;&gt;]&gt;&gt;&gt; response.xpath(&apos;//title&apos;).extract()[&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;]&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;)[&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;DMOZ&apos;&gt;]&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;).extract()[&apos;DMOZ&apos;] 提取数据现在，我们来尝试从这些页面中提取些有用的数据。 我们可以在终端中输入 response.body 来观察 HTML 源码并确定合适的 XPath 表达式。不过，这任务非常无聊且不易。您可以考虑使用 Firefox 的 Firebug 扩展来使得工作更为轻松。 在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 元素中。 我们可以通过这段代码选择该页面中网站列表里所有 元素: 1response.xpath(&apos;//ul/li&apos;) 网站的描述： 1response.xpath(&apos;//ul/li/text()&apos;).extract() 网站的标题： 1response.xpath(&apos;//ul/li/a/text()&apos;).extract() 以及网站的链接： 1response.xpath(&apos;//ul/li/a/@href&apos;).extract() 之前提到过，每个 .xpath() 调用返回 selector 组成的 list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性: 12345for response in response.xpath('//ul/li'): title = response.xpath('a/text()').extract() link = response.xpath('a/@href').extract() desc = response.xpath('text()').extract() print(title, link, desc) 在我们的 spider 中加入如下代码： 123456789101112131415import scrapyclass DmozSpider(scrapy.Spider): name = \"dmoz\" allow_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"] def parse(self, response): for response in response.xpath('//ul/li'): title = response.xpath('a/text()').extract() link = response.xpath('a/@href').extract() desc = response.xpath('text()').extract() print(title, link, desc) 现在尝试再次爬取 dmoz.org，您将看到爬取到的网站信息被成功输出: 1scrapy crawl dmoz 使用 ItemItem 对象是自定义的 python 字典。您可以使用标准的字典语法来获取到其每个字段的值。(字段就是我们之前用 Field 赋值的属性): 1234&gt;&gt;&gt; item = DmozItem() &gt;&gt;&gt; item[&apos;title&apos;] = &apos;Example title&apos; &gt;&gt;&gt; item[&apos;title&apos;] &apos;Example title&apos; 一般来说，Spider 将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是: 1234567891011121314151617181920import scrapyfrom tutorial.items import DmozItemclass DmozSpider(scrapy.Spider): name = \"dmoz\" allow_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\" ] def parse(self, response): for response in response.xpath('//ul/li'): item = DmozItem() item['title'] = response.xpath('a/text()').extract() item['link'] = response.xpath('a/@href').extract() item['desc'] = response.xpath('text()').extract() yield item 现在对 dmoz.org 进行爬取将会产生 DmozItem 对象: 1[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author webs &apos;link&apos;: [u&apos;http://gnosis.cx/TPiP/&apos;], &apos;title&apos;: [u&apos;Text Processing in Python&apos;]&#125; [dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applic &apos;link&apos;: [u&apos;http://www.informit.com/store/product.aspx?isbn=0130211192&apos;], &apos;title&apos;: [u&apos;XML Processing with Python&apos;]&#125; 保存爬取到的数据最简单存储爬取的数据的方式是使用 Feed exports : 1scrapy crawl dmoz -o items.json 该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。 在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的 item 做更多更为复杂的 操作，您可以编写 Item Pipeline 。 类似于我们在创建项目时对 Item 做的，用于您编写自己的 tutorial/pipelines.py 也被创建。 不过如果您仅仅想要保存 item，您不需要实现任何的 pipeline。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"python3+Django配置mysql连接","slug":"django-py3-mysql","date":"2017-04-14T07:18:54.000Z","updated":"2017-04-14T07:24:58.000Z","comments":true,"path":"2017/04/14/django-py3-mysql/","link":"","permalink":"http://yoursite.com/2017/04/14/django-py3-mysql/","excerpt":"","text":"之前一直使用的是Django1.9和Python2.7，现在使用Python3和Django1.10，发现Mysql-Python一直无法安装 这是因为mysql官网上的版本只支持Python3.4的数据库驱动，所以Python3.5是安装不上相应的驱动的，可以使用pymysql。在虚拟环境下pip install pymysql就可以了，然后在项目目录下的__init__.py文件中添加 12import pymysqlpymysql.install_as_mysqldb() 就可以代替Django默认使用的MySQLdb了。","categories":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/categories/Django/"}],"tags":[{"name":"Django，Mysql，Python","slug":"Django，Mysql，Python","permalink":"http://yoursite.com/tags/Django，Mysql，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Python字符编码","slug":"python-str-encode","date":"2017-04-12T12:18:54.000Z","updated":"2017-06-09T08:21:36.000Z","comments":true,"path":"2017/04/12/python-str-encode/","link":"","permalink":"http://yoursite.com/2017/04/12/python-str-encode/","excerpt":"Python字符编码字符编码是计算机编程中不可回避的问题，不管你用 Python2 还是 Python3，亦或是 C++, Java 等，我都觉得非常有必要理清计算机中的字符编码概念。","text":"Python字符编码字符编码是计算机编程中不可回避的问题，不管你用 Python2 还是 Python3，亦或是 C++, Java 等，我都觉得非常有必要理清计算机中的字符编码概念。 基本概念 字符（Character） 在电脑和电信领域中，字符是一个信息单位，它是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。比如，一个汉字，一个英文字母，一个标点符号等都是一个字符。 字符集（Character set） 字符集是字符的集合。字符集的种类较多，每个字符集包含的字符个数也不同。比如，常见的字符集有 ASCII 字符集、GB2312 字符集、Unicode 字符集等，其中，ASCII 字符集共有 128 个字符，包含可显示字符（比如英文大小写字符、阿拉伯数字）和控制字符（比如空格键、回车键）；GB2312 字符集是中国国家标准的简体中文字符集，包含简化汉字、一般符号、数字等；Unicode 字符集则包含了世界各国语言中使用到的所有字符。 字符编码（Character encoding） 字符编码，是指对于字符集中的字符，将其编码为特定的二进制数，以便计算机处理。常见的字符编码有 ASCII 编码，UTF-8 编码，GBK 编码等。一般而言，字符集和字符编码往往被认为是同义的概念，比如，对于字符集 ASCII，它除了有「字符的集合」这层含义外，同时也包含了「编码」的含义，也就是说，ASCII 既表示了字符集也表示了对应的字符编码。 下面我们用一个表格做下总结： 概念 描述 实例 字符 一个信息单位，各种文字和符号的总称 ‘中’, ‘a’, ‘1’, ‘$’, ‘￥’, … 字符集 字符的集合 ASCII 字符集, GB2312 字符集, Unicode 字符集 字符编码 将字符集中的字符，编码为特定的二进制数 ASCII 编码，GB2312 编码，Unicode 编码 字节 计算机中存储数据的单元，一个 8 位（bit）的二进制数 0x01, 0x45, … 常见字符编码简介常见的字符编码有 ASCII 编码，GBK 编码，Unicode 编码和 UTF-8 编码等等。这里，我们主要介绍 ASCII、Unicode 和 UTF-8。 ASCII 计算机是在美国诞生的，人家用的是英语，而在英语的世界里，不过就是英文字母，数字和一些普通符号的组合而已。 在 20 世纪 60 年代，美国制定了一套字符编码方案，规定了英文字母，数字和一些普通符号跟二进制的转换关系，被称为 ASCII (American Standard Code for Information Interchange，美国信息互换标准编码) 码。 比如，大写英文字母 A 的二进制表示是 01000001（十进制 65），小写英文字母 a 的二进制表示是 01100001 （十进制 97），空格 SPACE 的二进制表示是 00100000（十进制 32）。 Unicode ASCII 码只规定了 128 个字符的编码，这在美国是够用的。可是，计算机后来传到了欧洲，亚洲，乃至世界各地，而世界各国的语言几乎是完全不一样的，用 ASCII 码来表示其他语言是远远不够的，所以，不同的国家和地区又制定了自己的编码方案，比如中国大陆的 GB2312 编码 和 GBK 编码等，日本的 Shift_JIS 编码等等。 虽然各个国家和地区可以制定自己的编码方案，但不同国家和地区的计算机在数据传输的过程中就会出现各种各样的乱码（mojibake），这无疑是个灾难。 怎么办？想法也很简单，就是将全世界所有的语言统一成一套编码方案，这套编码方案就叫 Unicode，它为每种语言的每个字符设定了独一无二的二进制编码，这样就可以跨语言，跨平台进行文本处理了，是不是很棒！ Unicode 1.0 版诞生于 1991 年 10 月，至今它仍在不断增修，每个新版本都会加入更多新的字符，目前最新的版本为 2016 年 6 月 21 日公布的 9.0.0。 Unicode 标准使用十六进制数字，而且在数字前面加上前缀 U+，比如，大写字母「A」的 unicode 编码为 U+0041，汉字「严」的 unicode 编码为 U+4E25。更多的符号对应表，可以查询 unicode.org，或者专门的汉字对应表。 UTF-8 Unicode 看起来已经很完美了，实现了大一统。但是，Unicode 却存在一个很大的问题：资源浪费。 为什么这么说呢？原来，Unicode 为了能表示世界各国所有文字，一开始用两个字节，后来发现两个字节不够用，又用了四个字节。比如，汉字「严」的 unicode 编码是十六进制数 4E25，转换成二进制有十五位，即 100111000100101，因此至少需要两个字节才能表示这个汉字，但是对于其他的字符，就可能需要三个或四个字节，甚至更多。 这时，问题就来了，如果以前的 ASCII 字符集也用这种方式来表示，那岂不是很浪费存储空间。比如，大写字母「A」的二进制编码为 01000001，它只需要一个字节就够了，如果 unicode 统一使用三个字节或四个字节来表示字符，那「A」的二进制编码的前面几个字节就都是 0，这是很浪费存储空间的。 为了解决这个问题，在 Unicode 的基础上，人们实现了 UTF-16, UTF-32 和 UTF-8。下面只说一下 UTF-8。 UTF-8 (8-bit Unicode Transformation Format) 是一种针对 Unicode 的可变长度字符编码，它使用一到四个字节来表示字符，例如，ASCII 字符继续使用一个字节编码，阿拉伯文、希腊文等使用两个字节编码，常用汉字使用三个字节编码，等等。 因此，我们说，UTF-8 是 Unicode 的实现方式之一，其他实现方式还包括 UTF-16（字符用两个或四个字节表示）和 UTF-32（字符用四个字节表示）。 Python的默认编码Python2 的默认编码是 ascii，Python3 的默认编码是 utf-8，可以通过下面的方式获取： Python2 123456Python 2.7.12 (default, Oct 11 2016, 05:20:59)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()&apos;ascii&apos; Python3 123456Python 3.5.2 (default, Oct 11 2016, 04:59:56)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()&apos;utf-8&apos; Python3编码问题Python3 最重要的一项改进之一就是解决了 Python2 中字符串与字符编码遗留下来的这个大坑。 Python2 字符串设计上的一些缺陷： 使用 ASCII 码作为默认编码方式，对中文处理很不友好。 把字符串的牵强地分为 unicode 和 str 两种类型，误导开发者 首先，Python3 把系统默认编码设置为 UTF-8，然后，文本字符和二进制数据区分得更清晰，分别用 str 和 bytes 表示。文本字符全部用 str 类型表示，str 能表示 Unicode 字符集中所有字符，而二进制字节数据用一种全新的数据类型，用 bytes 来表示。 str12345678910&gt;&gt;&gt; a = \"a\"&gt;&gt;&gt; a'a'&gt;&gt;&gt; type(a)&lt;class 'str'&gt;&gt;&gt;&gt; b = \"禅\"&gt;&gt;&gt; b'禅'&gt;&gt;&gt; type(b)&lt;class 'str'&gt; bytesPython3 中，在字符引号前加‘b’，明确表示这是一个 bytes 类型的对象，实际上它就是一组二进制字节序列组成的数据，bytes 类型可以是 ASCII范围内的字符和其它十六进制形式的字符数据，但不能用中文等非ASCII字符表示。 12345678910111213&gt;&gt;&gt; c = b'a'&gt;&gt;&gt; cb'a'&gt;&gt;&gt; type(c)&lt;class 'bytes'&gt;&gt;&gt;&gt; d = b'\\xe7\\xa6\\x85'&gt;&gt;&gt; db'\\xe7\\xa6\\x85'&gt;&gt;&gt; type(d)&lt;class 'bytes'&gt;&gt;&gt;&gt; e = b'禅' File \"&lt;stdin&gt;\", line 1SyntaxError: bytes can only contain ASCII literal characters. bytes 类型提供的操作和 str 一样，支持分片、索引、基本数值运算等操作。但是 str 与 bytes 类型的数据不能执行 + 操作，尽管在py2中是可行的。 123456789101112&gt;&gt;&gt; b'a'+b'c'b'ac'&gt;&gt;&gt; b'a'*2b'aa'&gt;&gt;&gt; b\"abcdef\\xd6\"[1:]b'bcdef\\xd6'&gt;&gt;&gt; b\"abcdef\\xd6\"[-1]214&gt;&gt;&gt; b\"a\" + \"b\"Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: can't concat bytes to str python2 与 python3 字节与字符的对应关系 python2 python3 表现 转换 作用 str bytes 字节 encode 存储 unicode str 字符 decode 显示 encode与decodestr 与 bytes 之间的转换可以用 encode 和 decode 方法。 encode 负责字符到字节的编码转换。默认使用 UTF-8 编码转换。 12345&gt;&gt;&gt; s = \"Python之禅\"&gt;&gt;&gt; s.encode()b'Python\\xe4\\xb9\\x8b\\xe7\\xa6\\x85'&gt;&gt;&gt; s.encode('gbk')b'Python\\xd6\\xae\\xec\\xf8' decode 负责字节到字符的解码转换，通用使用 UTF-8 编码格式进行转换。 1234&gt;&gt;&gt; b&apos;Python\\xe4\\xb9\\x8b\\xe7\\xa6\\x85&apos;.decode()&apos;Python之禅&apos;&gt;&gt;&gt; b&apos;Python\\xd6\\xae\\xec\\xf8&apos;.decode(&quot;gbk&quot;)&apos;Python之禅&apos;","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Unicode，Python","slug":"Scrapy，Unicode，Python","permalink":"http://yoursite.com/tags/Scrapy，Unicode，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy安装及调试","slug":"scrapy-install","date":"2017-04-11T12:18:54.000Z","updated":"2017-04-11T07:48:50.000Z","comments":true,"path":"2017/04/11/scrapy-install/","link":"","permalink":"http://yoursite.com/2017/04/11/scrapy-install/","excerpt":"环境搭建","text":"环境搭建 12$ mkvirtualenv article_spider$ pip install -i https://pypi.douban.com/simple/ scrapy 创建项目12345678910111213$ workon article_spider$ scrapy startproject ArticleSpiderNew Scrapy project &apos;ArticleSpider&apos;, using template directory &apos;/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/templates/project&apos;, created in: /Users/lawtech/PycharmProjects/ArticleSpiderYou can start your first spider with: cd ArticleSpider scrapy genspider example example.com $ cd ArticleSpider$ scrapy genspider jobbole blog.jobbole.comCreated spider &apos;jobbole&apos; using template &apos;basic&apos; in module: ArticleSpider.spiders.jobbole 项目目录介绍首先先要回答一个问题。 问：把网站装进爬虫里，总共分几步？答案很简单，四步： 新建项目 (Project)：新建一个新的爬虫项目 明确目标（Items）：明确你想要抓取的目标 制作爬虫（Spider）：制作爬虫开始爬取网页 存储内容（Pipeline）：设计管道存储爬取内容 创建好AticleSpider项目之后，可以看到将会创建一个AticleSpider文件夹，目录结构如下： 12345678910ArticleSpider/ scrapy.cfg ArticleSpider/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py jobbole.py 下面来简单介绍一下各个文件的作用： scrapy.cfg：项目的配置文件 ArticleSpider/：项目的 Python 模块，将会从这里引用代码 ArticleSpider/items.py：项目的 items 文件 ArticleSpider/pipelines.py：项目的 pipelines 文件 ArticleSpider/settings.py：项目的设置文件 ArticleSpider/spiders/：存储爬虫的目录 pycharm 调试 scrapy 执行流程在项目根目录下新建 main.py 文件，添加如下代码 123456789from scrapy.cmdline import executeimport sysimport os# __file__ 表示当前py文件sys.path.append(os.path.dirname(os.path.abspath(__file__)))# 将实际命令拆分execute([\"scrapy\", \"crawl\", \"spiders文件夹下的py文件名称\"])","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Xpath教程","slug":"xpath-usage","date":"2017-04-11T12:18:54.000Z","updated":"2017-06-09T08:20:49.000Z","comments":true,"path":"2017/04/11/xpath-usage/","link":"","permalink":"http://yoursite.com/2017/04/11/xpath-usage/","excerpt":"XPath的用法XPath简介 XPath 使用路径表达式在 XML / HTML 文档中进行导航 XPath 包含一个标准函数库 XPath 是 XSLT 中的主要元素 XPath 是一个 W3C 标准","text":"XPath的用法XPath简介 XPath 使用路径表达式在 XML / HTML 文档中进行导航 XPath 包含一个标准函数库 XPath 是 XSLT 中的主要元素 XPath 是一个 W3C 标准 XPath术语节点（Node）在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。XML 文档是被作为节点树来对待的。树的根被称为文档节点或者根节点。 请看下面这个 XML 文档： 123456789101112&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang=\"en\"&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 上面的XML文档中的节点例子： 123&lt;bookstore&gt; （文档节点）&lt;author&gt;J K. Rowling&lt;/author&gt; （元素节点）lang=\"en\" （属性节点） 基本值（或称原子值，Atomic value）基本值是无父或无子的节点。 基本值的例子： 12J K. Rowling\"en\" 项目（Item）项目是基本值或者节点。 节点关系参考示例： 12345678910&lt;bookstore&gt;&lt;book&gt; &lt;title&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 父节点（Parent） 每个元素以及属性都有一个父节点。 在上面的例子中，book 元素是 title、author、year 以及 price 元素的父节点 子节点（Children） 元素节点可有零个、一个或多个子节点。 在上面的例子中，title、author、year 以及 price 元素都是 book 元素的子节点 同胞节点（Sibling） 拥有相同的父节点的节点 在上面的例子中，title、author、year 以及 price 元素都是同胞节点 先辈节点（Ancestor） 某节点的父节点、父节点的父节点，等等。 在上面的例子中，title 元素的先辈节点是 book 元素和 bookstore 元素 后代节点（Descendant） 某个节点的子节点，子节点的子节点，等等。 在上面的例子中，bookstore 的后代节点是 book、title、author、year 以及 price 元素 XPath语法参考示例： 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Harry Potter&lt;/title&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Learning XML&lt;/title&gt; &lt;price&gt;39.95&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 选取节点XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。下面列出了最有用的路径表达式： 表达式 描述 nodename 选取此节点的所有子节点 / 从根节点选取 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 . 选取当前节点 .. 选取当前节点的父节点。 @ 选取属性。 实例： 路径表达式 结果 bookstore 选取bookstore元素的所有子节点 /bookstore 选取根元素bookstore 注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ bookstore/book 选取属于bookstore的子元素的所有book元素 //book 选取所有book元素，而不考虑它们的位置 bookstore//book 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置 //@lang 选取名为 lang 的所有属性 谓语（Predicates）谓语用来查找某个特定的节点或者包含某个指定的值的节点。 谓语被嵌在方括号中。 实例 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 路径表达式 结果 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素 /bookstore/book[postion()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素 //title[@lang=’eng’] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性 /bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00 /bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00 选取未知节点XPath 通配符可用来选取未知的 XML 元素 通配符 描述 * 匹配任何元素节点 @* 匹配任何属性节点 node() 匹配任何节点 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 /bookstore/* 选取 bookstore 元素的所有子元素 //* 选取文档中的所有元素 //title[@*] 选取所有带有属性的 title 元素 选取若干路径通过在路径表达式中使用“|”运算符，您可以选取若干个路径。 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 //book/title &#124; //book/price 选取 book 元素的所有 title 和 price 元素 //title &#124; //price 选取文档中的所有 title 和 price 元素 /bookstore/book/title &#124; //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，XPath，Python","slug":"Scrapy，XPath，Python","permalink":"http://yoursite.com/tags/Scrapy，XPath，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Python正则表达式","slug":"regex","date":"2017-04-11T12:18:54.000Z","updated":"2017-06-09T08:21:21.000Z","comments":true,"path":"2017/04/11/regex/","link":"","permalink":"http://yoursite.com/2017/04/11/regex/","excerpt":"Python正则表达式简介正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。 re 模块使 Python 语言拥有全部的正则表达式功能。","text":"Python正则表达式简介正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。 re 模块使 Python 语言拥有全部的正则表达式功能。 compile 函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。 re 模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串做为它们的第一个参数。 正则表达式模式（常用）模式字符串使用特殊的语法来表示一个正则表达式：字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。多数字母和数字前加一个反斜杠时会拥有不同的含义。标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。反斜杠本身需要使用反斜杠转义。由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’/t’，等价于’//t’)匹配相应的特殊字符。 下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。 模式 描述 ^ 匹配字符串的开头 $ 匹配字符串的末尾 . 匹配任意字符，除了换行符”\\n”，当re.DOTALL标记被指定时，可以匹配包含换行符的任意字符 […] 用来表示一组字符，单独列出：[amk]匹配’a’，’m’或’k’ [^…] 不在[]中的字符： [ ^abc ]匹配除了a,b,c之外的字符 re* 匹配0个或多个的表达式 re+ 匹配1个或多个的表达式 re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 re{ n,} 精确匹配n个前面表达式 re{n,m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a &#124; b 匹配a或b (re) 匹配括号内的表达式，也表示一个组 (?imx) 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域 (?-imx) 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域 \\w 匹配字母数字及下划线 \\W 匹配非字母数字及下划线 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f]. \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0-9]. \\D 匹配任意非数字 \\A 匹配字符串开始 \\Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串 \\z 匹配字符串结束 \\G 匹配最后匹配完成的位置 \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’ \\B 匹配非单词边界。’er\\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’ \\n,\\t,等 匹配一个换行符。匹配一个制表符。等 \\1…\\9 匹配第n个分组的子表达式。 \\10 匹配第n个分组的子表达式，如果它经匹配。否则指的是八进制字符码的表达式 正则表达式实例字符匹配 实例 描述 python 匹配”python” 字符类 实例 描述 [Pp]ython 匹配 “Python” 或 “python” rub[ye] 匹配 “ruby” 或 “rube” [lawtech] 匹配中括号内的任意一个字母 [0-9] 匹配任何数字。类似于 [0123456789] [a-z] 匹配任何小写字母 [A-Z] 匹配任何大写字母 [a-zA-Z0-9] 匹配任何字母及数字 [^lawtech] 除了lawtech字母以外的所有字符 [^0-9] 匹配除了数字外的字符 特殊字符类 实例 描述 . 匹配除 “\\n” 之外的任何单个字符，要匹配包括 ‘\\n’ 在内的任何字符，请使用像’[.\\n]’ 的模式 \\d 匹配一个数字字符，等价于 [0-9] \\D 匹配一个非数字字符，等价于 [ ^0-9 ] \\s 匹配任何空白字符，包括空格、制表符、换页符等等，等价于[\\f\\n\\r\\t\\v] \\S 匹配任何非空白字符。等价于 [ ^\\f\\n\\r\\t\\v ] \\w 匹配包括下划线的任何单词字符，等价于[A-Za-z0-9_] \\W 匹配任何非单词字符等价于 [ ^A-Za-z0-9_ ] 正则表达式修饰符 - 可选标志正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志： 修饰符 描述 全拼 re.I 使匹配对大小写不敏感 IGNORECASE re.L 做本地化识别（locale-aware）匹配 LOCALE re.M 多行匹配，影响 ^ 和 $ MULTILINE re.S 使 . 匹配包括换行在内的所有字符 DOTALL re.U 根据Unicode字符集解析字符。这个标志影响 \\w, \\W, \\b, \\B. UNICODE re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 VERBOSE re模块能够处理正则表达式的操作生成正则表达式对象 生成正则表达式对象 compile(pattern, flags=0)构建一个正则表达式，返回该正则表达式对象 12import repattern = re.compile('re') 进行匹配 match() 确定正则表达式是否匹配字符串的开头 search() 扫描字符串以查找匹配 findall() 找到所有正则表达式匹配的子字符串，并把它们作为一个列表返回 finditer() 找到所有正则表达式匹配的子字符串，并把它们以迭代器的形式返回 group() 返回通过正则表达式匹配到的字符串 start() 返回成功匹配开始位置 end() 返回成功匹配结束位置 span() 返回包含成功匹配开始和结束位置的元组 re.match函数re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回None。 函数语法： 1re.match(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 匹配成功re.match方法返回一个匹配的对象（match object），否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组 groups() 返回一个包含所有小组字符串的元组，从1到所含的小组号 实例1： 12345# -*- coding: UTF-8 -*- import reprint(re.match('www', 'www.lawtech0902.com').span()) # 在起始位置匹配print(re.match('com', 'www.lawtech0902.com')) # 不在起始位置匹配 运行结果： 12(0, 3)None 实例2： 12345678910111213# -*- coding: UTF-8 -*- import reline = \"Cats are smarter than dogs\"matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)if matchObj: print(\"matchObj.group() : \", matchObj.group()) print(\"matchObj.group(1) : \", matchObj.group(1)) print(\"matchObj.group(2) : \", matchObj.group(2))else: print(\"No match!!\") 运行结果： 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter re.search方法re.search 扫描整个字符串并返回第一个成功的匹配。 函数语法： 1re.search(pattern, string, flags=0) 函数参数说明与re.match方法相同。 匹配成功re.search方法返回一个匹配的对象，否则返回None。 实例1： 12345# -*- coding: UTF-8 -*- import reprint(re.search('www', 'www.lawtech0902.com').span()) # 在起始位置匹配print(re.search('com', 'www.lawtech0902.com').span()) # 不在起始位置匹配 运行结果： 12(0, 3)(16, 19) 实例2： 12345678910111213# -*- coding: UTF-8 -*- import reline = \"Cats are smarter than dogs\"searchObj = re.search( r'(.*) are (.*?) .*', line, re.M|re.I)if searchObj: print(\"searchObj.group() : \", searchObj.group()) print(\"searchObj.group(1) : \", searchObj.group(1)) print(\"searchObj.group(2) : \", searchObj.group(2))else: print(\"Nothing found!!\") 运行结果： 123searchObj.group() : Cats are smarter than dogssearchObj.group(1) : CatssearchObj.group(2) : smarter re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 实例： 12345678910111213141516# -*- coding: UTF-8 -*- import reline = \"Cats are smarter than dogs\"matchObj = re.match( r'dogs', line, re.M|re.I)if matchObj: print(\"match --&gt; matchObj.group() : \", matchObj.group())else: print(\"No match!!\")matchObj = re.search( r'dogs', line, re.M|re.I)if matchObj: print(\"search --&gt; matchObj.group() : \", matchObj.group())else: print(\"No match!!\") 运行结果： 12No match!!search --&gt; matchObj.group() : dogs 检索和替换Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。 语法： 1re.sub(pattern, repl, string, count=0, flags=0) 参数： pattern : 正则中的模式字符串。 repl : 替换的字符串，也可为一个函数。 string : 要被查找替换的原始字符串。 count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 实例： 12345678910111213# -*- coding: UTF-8 -*-import rephone = \"2004-959-559 # 这是一个国外电话号码\"# 删除字符串中的 Python注释 num = re.sub(r'#.*$', \"\", phone)print(\"电话号码是: \", num)# 删除非数字(-)的字符串 num = re.sub(r'\\D', \"\", phone)print(\"电话号码是 : \", num) 运行结果： 12电话号码是: 2004-959-559电话号码是 : 2004959559 repl参数是一个函数以下实例中将字符串中的匹配的数字乘于 2： 1234567891011# -*- coding: UTF-8 -*-import re# 将匹配的数字乘于 2def double(matched): value = int(matched.group('value')) return str(value * 2)s = 'A23G4HFD567'print(re.sub('(?P&lt;value&gt;\\d+)', double, s)) 运行结果为： 1A46G8HFD1134","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Regular Expression，Python","slug":"Scrapy，Regular-Expression，Python","permalink":"http://yoursite.com/tags/Scrapy，Regular-Expression，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——爬虫基础知识回顾","slug":"scrapy-project-basic","date":"2017-04-11T12:18:54.000Z","updated":"2017-04-11T07:49:05.000Z","comments":true,"path":"2017/04/11/scrapy-project-basic/","link":"","permalink":"http://yoursite.com/2017/04/11/scrapy-project-basic/","excerpt":"基础知识技术选型：scrapy vs requests + beautifulsoup","text":"基础知识技术选型：scrapy vs requests + beautifulsoup requests和beautifulsoup都是库，scrapy是框架 scrapy框架中可以加入requests和beautifulsoup scrapy基于twisted，性能是最大的优势 scrapy方便扩展，提供了很多内置的功能 scrapy内置的css和xpath selector非常方便，beautifulsoup最大的缺点就是慢 网页分类 静态网页 动态网页 webservice（restapi） 爬虫能做什么爬虫的作用 搜索引擎——百度、谷歌、垂直领域搜索引擎 推荐引擎——今日头条 机器学习的数据样本 数据分析、舆情分析等 正则表达式详见另一篇博客 网站url结构 深度优先遍历和广度优先遍历123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# _*_ coding: utf-8 _*_\"\"\"__author__ = 'lawtech'__date__ = '2017/4/10 下午1:41'\"\"\"\"\"\"深度优先遍历和广度优先遍历\"\"\"class Graph(object): def __init__(self, *args, **kwargs): self.node_neighbors = &#123;&#125; self.visited = &#123;&#125; def add_nodes(self, nodelist): for node in nodelist: self.add_node(node) def add_node(self, node): if node not in self.nodes(): self.node_neighbors[node] = [] def add_edge(self, edge): u, v = edge if (v not in self.node_neighbors[u]) and (u not in self.node_neighbors[v]): self.node_neighbors[u].append(v) if u != v: self.node_neighbors[v].append(u) def nodes(self): return self.node_neighbors.keys() def depth_first_search(self, root=None): \"\"\" 队列 :param root: :return: \"\"\" order = [] def dfs(node): self.visited[node] = True order.append(node) for n in self.node_neighbors[node]: if n not in self.visited: dfs(n) if root: dfs(root) for node in self.nodes(): if node not in self.visited: dfs(node) print(order) return order def breadth_first_search(self, root=None): \"\"\" 递归 :param root: :return: \"\"\" queue = [] order = [] def bfs(): while len(queue) &gt; 0: node = queue.pop(0) self.visited[node] = True for n in self.node_neighbors[node]: if (n not in self.visited) and (n not in queue): queue.append(n) order.append(n) if root: queue.append(root) order.append(root) bfs() for node in self.nodes(): if node not in self.visited: queue.append(node) order.append(node) bfs() print(order) return orderif __name__ == '__main__': g = Graph() g.add_nodes(nodelist=[i + 1 for i in range(8)]) g.add_edge((1, 2)) g.add_edge((1, 3)) g.add_edge((2, 4)) g.add_edge((2, 5)) g.add_edge((4, 8)) g.add_edge((5, 8)) g.add_edge((3, 6)) g.add_edge((3, 7)) g.add_edge((6, 7)) print(\"nodes:\", g.nodes()) g.breadth_first_search(1) g.depth_first_search(1) 运行结果： ​ (‘nodes:’, [1, 2, 3, 4, 5, 6, 7, 8]) ​ [1, 2, 3, 4, 5, 6, 7, 8] ​ [1, 2, 4, 8, 5, 3, 6, 7] 爬虫去重策略 将访问过的url保存到数据库中 将访问过的url保存到set中，只需要O(1)的代价就可以查询url url经过md5等方法哈希后保存到set中 用bitmap方法，将访问过的url通过hash函数映射到某一位 bloomfilter方法对bitmap进行改进，多重hash函数降低冲突 Python字符串编码详见另一篇博客","categories":[{"name":"Basic knowledge","slug":"Basic-knowledge","permalink":"http://yoursite.com/categories/Basic-knowledge/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Redis学习笔记(五)：数据安全与性能保障——复制","slug":"Redis-5","date":"2017-04-10T12:18:54.000Z","updated":"2017-04-12T06:16:29.000Z","comments":true,"path":"2017/04/10/Redis-5/","link":"","permalink":"http://yoursite.com/2017/04/10/Redis-5/","excerpt":"复制（replication），可以让其他服务器拥有一个不断更新的数据副本，从而使得拥有数据副本的服务器可以用于处理客户端发送的读请求。对于高负载应用来说，复制是不可或缺的一个特性。","text":"复制（replication），可以让其他服务器拥有一个不断更新的数据副本，从而使得拥有数据副本的服务器可以用于处理客户端发送的读请求。对于高负载应用来说，复制是不可或缺的一个特性。 关系型数据库通常会使用一个主服务器（master）向多个从服务器（slave）发送更新，并使用从服务器来处理所有读请求。Redis也采用了同样的方法来实现自己的复制特性，并将其用作扩展性能的一种手段。 复制相关配置选项当从服务器连接主服务器时，主服务器会执行BGSAVE操作，为了正确地使用复制特性，用户需要保证服务器已经正确地设置了dir选项和dirname选项。 配置slaveof host port选项即可连接主服务器。 下面将演示怎么实现一个简单的复制系统。我们在一台机器上起两个Redis实例，监听不同的端口，其中一个作为主库，另外一个作为从库。首先不加任何参数来启动一个Redis实例作为主数据库： 主库默认监听6379端口。 接着新建一个终端，加上slaveof参数启动另一个Redis实例作为从库，并且监听6380端口： 从控制台输出中可以看到，从库已经连接到主库：126.0.0.1:6379了，我们可以分别在主库和从库中使用info replication命令看一看当前实例在复制系统中的相关信息 现在可以测试一下主从库的数据同步了： 12345678$ redis-cli -p 6379127.0.0.1:6379&gt; set test-replicate lawtechOK$ redis-cli -p 6380127.0.0.1:6380&gt; get test-replicate&quot;lawtech&quot;127.0.0.1:6380&gt; set x y(error) READONLY You can&apos;t write against a read only slave. 可以看到，在主库中添加的数据确实同步到了从库中。但是，我们在向从库中写入数据时报错了，这是因为在默认情况下，从库是只读的。我们可以在从库的配置文件中加上如下的配置项允许从库写数据： 1slave-read-only no 但是，因为从库中修改的数据不会被同步到任何其他数据库，并且一旦主库修改了数据，从库的数据就会因为自动同步被覆盖，所以一般情况下，不建议将从库设置为可写。 相同的道理，配置多台从库也使用相同的方法，都在从库的配置文件中加上slaveof参数即可。 此外，我们可以在客户端使用命令 1SLAVEOF 新主库地址 新主库端口 来修改当前数据库的主库，如果当前数据库已经是其他库的从库， 则当前数据库会停止和原来的数据库的同步而和新的数据库同步。 最后，从数据库还可以通过运行命令： 1SLAVEOF NO ONE 来停止接受来自其他数据库的同步而升级成为主库。 Redis复制的启动过程从服务器连接主服务器时，主服务器会创建一个快照文件并将其发送至从服务器，但这只是主从复制执行过程的其中一步，下表列举出复制过程主从服务器执行的所有操作： 步骤 主服务器操作 从服务器操作 1 （ 等待命令进入） 连接(或者重连接)主服务器，发送SYNC命令 2 开始执行BGSAVE，并使用缓冲区记录BGSAVE之后执行的所有写命令 根据配置选项来决定时继续使用现在的数据来处理客户端命令，还是向发送请求的客户端返回错误 3 BGSAVE执行完毕，向从服务器发送快照文件，并在发送期间继续使用缓冲区记录被执行的写命令 丢弃所有旧的数据，开始载入主服务器发来的快照文件 4 快照文件发送完毕，开始向从服务器发送存储在缓冲区里面的写命令 完成对快照文件的解释操作，像往常一样开始接受命令请求 5 缓冲区存储的写命令发送完毕；从现在开始，没执行一个写命令，就像从服务器发送相同的写命令 执行主服务器发来的所有存储在缓冲区里面的写命令；从现在开始，接收并执行主服务器传来的每个写命令 由上述步骤可以看出，有必要给Redis主服务器留30%~45%的内存用于执行BGSAVE命令和创建记录写命令的缓冲区。另外，从服务器还有一点需要注意的是，从服务器在进行同步时，会清空自己的所有数据，因为第3步中，从服务器会丢弃所有旧数据。 警告：Redis不支持主主复制（master-master replication） 当多个从服务器尝试连接同一个主服务器的时候，就会出现下表所示的两种情况中的其中一种： 当有新的从服务器连接主服务器时 主服务器的操作 上述步骤3尚未执行 所有从服务器都会接收相同的快照文件和相同的缓冲区写命令 上述步骤3正在执行或者已经执行 当主服务器与较早进行连接的从服务器执行完复制所需的5个步骤之后，主服务器会与新连接的从服务器执行一次新的步骤1至步骤5 由此可以看出多个从服务器的同步对网络的开销挺大的，有可能会影响到主服务器接收写命令，甚至是与主服务器位于同一网络中的其他硬件。 主从链创建多个从服务器可能造成网络不可用，此时可以使用另外一个解决方案，从服务器拥有自己的从服务器，并由此形成主从链（master/slave chaining）。 从服务器对从服务器进行复制在操作上和从服务器对主服务器进行复制的唯一区别在于。如果从服务器X拥有从服务器Y，那么当从服务器X在执行启动过程表中步骤4时，X将断开与Y的连接，导致Y需要重新连接并重新同步（resync）。 当读请求的重要性明显高于写请求的重要性，并且读请求的数量需求远远超出一台Redis服务器可以处理的范围时，用户就需要添加新的从服务器来处理读请求，随着负载不断上升，主服务器可能会无法快速地更新所有从服务器。 为了缓解这个问题，可以创建一个由Redis主/从节点(master/slave node)组成的中间层来分担主服务器的复制工作，如下图所示： 上面这个示例中，树的中层有3个帮助开展复制工作的服务器，底层有9个从服务器。其中，只有3台从服务器和主服务器通信，其他都向从服务器同步数据，从而降低了系统的负载。 检验硬盘写入为了将数据保存在多台机器中，用户首先需要为主服务器设置多个从服务器，然后对每个从服务器设置appendonly yes选项和appendfsync everysec选项（如有需要，也可以对主服务器这样设置），但这只是第一步：因为用户还需要等待主服务器发送的写命令到达从服务器，并且在执行后续操作前，检查数据是否已经被写入了硬盘中。 整个操作分两个环节： 验证主服务器是否已经将写数据发送至从服务器：用户需要在向主服务器写入真正的数据之后，再向主服务器写入一个唯一的虚构值（unique dummy value），然后通过检查虚构值是否存在于从服务器来判断数据是否已经到达从服务器。 判断数据是否已经被保存到硬盘中：检查INFO命令的输出结果中aof_pending_bio_fsync属性的值是否为0，如果是，则数据已经被保存到了硬盘中。 在向主服务器写入数据后，用户可以将主服务器和从服务器的连接作为参数调用下面的代码来自动进行上述操作： 1234567891011121314151617181920212223242526272829# _*_ coding: utf-8 _*_import uuidimport timedef wait_for_sync(mconn, sconn): identifier = str(uuid.uuid4()) # 将令牌添加至主服务器 mconn.zadd('sync:wait', identifier, time.time()) # 如果有必要的话，等待从服务器完成同步 while not sconn.info()['master_link_status'] != 'up': time.sleep(.001) # 等待从服务器接收数据更新 while not sconn.zscore('sync:wait', identifier): time.sleep(.001) # 最多只等待1秒 deadline = time.time() + 1.01 # 检查数据更新是否已经被同步到了硬盘 while time.time() &lt; deadline: if sconn.info()['aof_pending_bio_fsync'] == 0: break time.sleep(.001) # 清理刚刚创建的新令牌以及之前可能留下的旧令牌 mconn.zrem('sync:wait', identifier) mconn.zremrangebyscore('sync:wait', 0, time.time() - 900) 为了确保操作可以正确执行，wait_for_sync()函数会首先确认从服务器已经连接上主服务器，然后检查自己添加到等待同步有序集合（sync wait ZSET）里面的值是否已经存在于从服务器，在发现值存在后，等待从服务器将缓冲区的所有数据写入硬盘里。最后，确认数据已经被保存到硬盘之后，函数会执行一些清理操作。 通过同时使用复制和AOF持久化，用户可以增强Redis对于系统崩溃的抵抗能力。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Redis学习笔记(四)：数据安全与性能保障——持久化","slug":"Redis-4","date":"2017-04-09T15:12:54.000Z","updated":"2017-04-09T15:12:58.000Z","comments":true,"path":"2017/04/09/Redis-4/","link":"","permalink":"http://yoursite.com/2017/04/09/Redis-4/","excerpt":"什么是持久化？持久化（Persistence），即把数据（如内存中的对象）保存到可永久保存的存储设备中（如磁盘）。持久化的主要应用是将内存中的对象存储在数据库中，或者存储在磁盘文件中、XML数据文件中等等。","text":"什么是持久化？持久化（Persistence），即把数据（如内存中的对象）保存到可永久保存的存储设备中（如磁盘）。持久化的主要应用是将内存中的对象存储在数据库中，或者存储在磁盘文件中、XML数据文件中等等。 持久化是将程序数据在持久状态和瞬时状态间转换的机制。 JDBC就是一种持久化机制。文件IO也是一种持久化机制。 我们这样理解：在一定周期内保持不变就是持久化，持久化是针对时间来说的。数据库中的数据就是持久化了的数据，只要你不去删除或修改。 持久化选项Redis提供了两种不同的持久化方法来将数据存储到硬盘中，保证数据在Redis重启后仍然存在： RDB持久化：在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot），也就是将存在于某一时刻的所有数据都写入硬盘里面，所以也叫作快照持久化。 AOF持久化：全称是 append-only file（只追加文件）， 它记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 这两种持久化的方式既可以单独使用，也可以同时使用，具体选择哪种方式需要根据用户的数据及应用进行选择。 下面的代码示例展示了Redis对于两种持久化选项提供的配置选项 1234567891011121314# 快照持久化选项save 60 1000 # 60秒内有1000次写入操作的时候执行快照的创建stop-writes-on-bgsave-error no # 创建快照失败的时候是否仍然继续执行写命令rdbcompression yes # 是否对快照文件进行压缩dbfilename dump.rdb # 如何命名硬盘上的快照文件dir ./ # 快照所保存的位置# AOF持久化选项appendonly no # 是否使用AOF持久化appendfsync everysec # 多久才将写入的内容同步到硬盘no-appendfsync-on-rewrite no # 在对AOF进行压缩(compaction)的时候能否执行同步操作auto-aof-rewrite-percentage 100 # 多久执行一次AOF压缩auto-aof-rewrite-min-size 64mb # 多久执行一次AOF压缩dir ./ # AOF所保存的位置 快照持久化（RDB）创建快照的办法 客户端通过向Redis发送BGSAVE命令来创建快照。 如果平台支持（除了Windows），那么Redis会调用fork来创建一个子进程，然后子进程负责将快照写到硬盘中，而父进程则继续处理命令请求。 使用场景： 如果用户使用了save设置，例如：save 60 1000 ,那么从Redis最近一次创建快照之后开始计算，当“60秒之内有1000次写入操作”这个条件满足的时候，Redis就会自动触发BGSAVE命令。 如果用户使用了多个save设置，那么当任意一个save配置满足条件的时候，Redis都会触发一次BGSAVE命令。 客户端通过向Redis发SAVE命令来创建快照。 接收到SAVE命令的Redis服务器在快照创建完毕之前将不再响应任何其他命令的请求。SAVE命令并不常用，我们通常只在没有足够的内存去执行BGSAVE命令的时候才会使用SAVE命令，或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令。 使用场景： 当Redis通过SHUTDOWN命令接收到关闭服务器的请求时，或者接收到标准的TERM信号时，会执行一次SAVE命令，阻塞所有的客户端，不再执行客户端发送的任何命令，并且在执行完SAVE命令之后关闭服务器。 优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次快照文件，并且在每个月的每一天，也备份一个快照文件。 这样的话，即使遇到问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心。 RDB 可以最大化 Redis 的性能：父进程在保存快照文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 缺点 如果在新的快照文件创建好之前，Redis、系统、硬件三者中任意一个发生崩溃，那么Redis将丢失最近一次创建快照之后写入的所有数据。如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork()可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOF持久化AOF持久化会将被执行的写命令写到AOF文件的末尾，以此来记录数据发生的变化。这样，我们在恢复数据的时候，只需要从头到尾的执行一下AOF文件即可恢复数据。 我们可以通过appendonly yes命令来打开AOF持久化选项 appendfsync同步频率下表展示了appendfsync选项对AOF文件的同步频率的影响 选项 同步频率 always 每个Redis写命令都要同步写入硬盘。这样做会严重降低Redis的速度 everysec 每秒执行一次同步，显示地将多个写命令同步到硬盘 no 让操作系统来决定应该何时进行同步 always的方式固然可以对没一条数据进行很好的保存，但是这种同步策略需要对硬盘进行大量的写操作，所以Redis处理命令的速度会受到硬盘性能的限制。 普通的硬盘每秒钟只能处理大约200个写命令，使用固态硬盘SSD每秒可以处理几万个写命令，但是每次只写一个命令，这种只能怪不断地写入很少量的数据的做法有可能引发严重的写入放大问题，这种情况下降严重影响固态硬盘的使用寿命。 everysec的方式，Redis以每秒一次的频率大队AOF文件进行同步。这样的话既可以兼顾数据安全也可以兼顾写入性能。 Redis以每秒同步一次AOF文件的性能和不使用任何持久化特性时的性能相差无几，使用每秒更新一次 的方式，可以保证，即使出现故障，丢失的数据也在一秒之内产生的数据。 no的方式，Redis将不对AOF文件执行任何显示的同步操作，而是由操作系统来决定应该何时对AOF文件进行同步。 这个命令一般不会对Redis的性能造成多大的影响，但是当系统出现故障的时候使用这种选项的Redis服务器丢失不定数量的数据。 另外，当用户的硬盘处理写入操作的速度不够快的话，那么缓冲区被等待写入硬盘的数据填满时，Redis的写入操作将被阻塞，并导致Redis处理命令请求的速度变慢，因为这个原因，一般不推荐使用这个选项。 重写/压缩AOF文件随着数据量的增大，AOF的文件可能会很大，这样在每次进行数据恢复的时候就会进行很长的时间，为了解决日益增大的AOF文件，用户可以向Redis发送BGREWRITEAOF命令，这个命令会通过移除AOF文件中的冗余命令来重写AOF文件，是AOF文件的体积变得尽可能的小。 BGREWRITEAOF的工作原理和BGSAVE的原理很像：Redis会创建一个子进程，然后由子进程负责对AOF文件的重写操作。 因为AOF文件重写的时候会创建子进程，所以快照持久化因为创建子进程而导致的性能和内存占用问题同样会出现在AOF文件重写的时候。 跟快照持久化通过save选项来自动执行BGSAVE一样，AOF通过设置auto-aof-rewrite-percentage和auto-aof-rewrite-min-size选项来自动执行BGREWRITEAOF。 如下配置 12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 表示当前AOF的文件体积大于64MB，并且AOF文件的体积比上一次重写之后的体积变大了至少一倍（100%）的时候，Redis将执行重写BGREWRITEAOF命令。 如果AOF重写执行的过于频繁的话，可以将auto-aof-rewrite-percentage选项的值设置为100以上，这种最偶发就可以让Redis在AOF文件的体积变得更大之后才执行重写操作，不过，这也使得在进行数据恢复的时候执行的时间变得更加长一些。 优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 如何选择RDB和AOF？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 RDB 和 AOF 之间的相互作用BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE 。 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户，BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"MxOnline项目学习总结","slug":"MxOnline-summary","date":"2017-04-07T13:18:54.000Z","updated":"2017-04-07T13:26:06.000Z","comments":true,"path":"2017/04/07/MxOnline-summary/","link":"","permalink":"http://yoursite.com/2017/04/07/MxOnline-summary/","excerpt":"拖拖拉拉地学完了imooc的”强力django+杀手级xadmin打造上线标准的在线教育平台”课程，记录一下每一章学习的内容概要。","text":"拖拖拉拉地学完了imooc的”强力django+杀手级xadmin打造上线标准的在线教育平台”课程，记录一下每一章学习的内容概要。 MxOnline项目学习总结第一、二章 课程介绍 开发环境搭建 第三章 django基础回顾 settings.py 设置 urls.py 配置 models.py 设计 views.py 编码 templates 模板编码 第四章 数据库设计 users app model 设计 organization app model 设计 course app model 设计 operation app model 设计 第五章 后台管理系统开发 django admin 介绍 xadmin 安装和 model 注册 xadmin 全局配置 第六章 登录、注册、找回密码 登录（ session 和 cookie 机制） 注册（ form 表单提交、图片验证码，发送邮件 ） 找回密码（邮件发送） 第七章 课程机构功能实现 机构列表（分页，筛选、排序） 机构详情页（收藏，富文本展示） 咨询提交（ modelform 验证和保存） 第八章 课程功能实现 课程列表（分页、排序） 课程详情页（收藏，章节展示、资源展示、评论） 第九章 讲师功能实现 讲师列表（分页、排序） 讲师详情（收藏） 第十章 个人中心功能实现 用户信息修改（修改密码、头像、邮箱、基本信息） 导航栏全局搜索功能 我的课程 我的收藏（删除收藏） 我的消息 第十一章 全局功能实现 全局404和500页面配置 首页开发 点击数和收藏数修改和退出功能 第十二章 常见 web 攻击 sql 注入攻击 xss 攻击 csrf 攻击 第十三章 xadmin 进阶开发 userprofile 注册和设置 xadmin 常见功能设置 inlinemodel 注册、proxy 代理注册 django ueditor 富文本编辑器继承 excel 导入插件集成","categories":[{"name":"Django, Python","slug":"Django-Python","permalink":"http://yoursite.com/categories/Django-Python/"}],"tags":[{"name":"Django, Python","slug":"Django-Python","permalink":"http://yoursite.com/tags/Django-Python/"}]},{"title":"Redis学习笔记(三)：Redis命令补充","slug":"Redis-3","date":"2017-04-06T12:18:54.000Z","updated":"2017-04-06T13:14:54.000Z","comments":true,"path":"2017/04/06/Redis-3/","link":"","permalink":"http://yoursite.com/2017/04/06/Redis-3/","excerpt":"在之前的学习笔记中，还有许多Redis的命令没有涉及，这一篇主要用来简要地补充，当然，详细的命令还得参考Redis的官方命令文档。","text":"在之前的学习笔记中，还有许多Redis的命令没有涉及，这一篇主要用来简要地补充，当然，详细的命令还得参考Redis的官方命令文档。 键值相关命令下表展示了Redis提供的一些键值(KEY-VALUE)相关的常用命令及其redis-py API 命令 用例 描述 redis-py API KEYS KEYS pattern 查找所有符合给定模式pattern(正则表达式)的key keys(pattern=’*’) EXISTS EXISTS key 检查给定key是否存在 exists(name) EXPIRE EXPIRE key seconds 为给定key设置生存时间，当key过期时(生存时间为0)，它会被自动删除 expire(name, time) MOVE MOVE key db 将当前数据库的key移动到给定的数据库db当中 move(name, db) PERSIST PERSIST key 移除给定key的生存时间，将这个key从『易失的』(带生存时间key)转换成『持久的』(一个不带生存时间、永不过期的key) persist(name) RANDOMKEY RANDOMKEY 从当前数据库返回一个随机的key randomkey() RENAME RENAME key newkey 将key重命名为newkey，如果key与newkey相同，将返回一个错误。如果newkey已经存在，则值将被覆盖 rename(src, dst) TYPE TYPE key 返回key所存储的value的数据结构类型，它可以返回string, list, set, zset和hash等不同的类型 type(name) TTL TTL key 返回key剩余的过期时间(单位：秒) ttl(name) 下面这个交互示例展示了Redis中关于键的过期时间相关的命令的使用方法 12345678910111213&gt;&gt;&gt; r.set('key', 'value')True&gt;&gt;&gt; r.get('key')b'value'&gt;&gt;&gt; r.expire('key', 2)True&gt;&gt;&gt; time.sleep(2)&gt;&gt;&gt; r.get('key')&gt;&gt;&gt; r.set('key', 'value2')True&gt;&gt;&gt; r.expire('key', 100); r.ttl('key')True100 发布与订阅发布订阅(pub/sub)是一种消息通信模式，主要的目的是解耦消息发布者和消息订阅者之间的耦合，这点和设计模式中的观察者模式比较相似。pub/sub不仅仅解决发布者和订阅者之间代码级别耦合也解决两者在物理部署上的耦合。Redis作为一个pub/sub的server，在订阅者和发布者之间起到了消息路由的功能。订阅者可以通过subscribe和psubscribe命令向 redis server订阅自己感兴趣的消息类型，redis将消息类型称为通道(channel)。当发布者通过publish命令向 redis server发送二进制字符串消息(binary string message)时，订阅该消息类型的全部client都会收到此消息。这里消息的传递是多对多的，一个client可以订阅多个channel，也可以向多个channel发送消息。 下表展示了Redis提供的发布与订阅命令及其redis-py API 命令 用例 描述 redis-py API SUBSCRIBE SUBSCRIBE channel [channel …] 订阅给定的频道 subscribe(args, *kwargs) UNSUBSCRIBE UNSUBSCRIBE [channel [channel …]] 退订给定的频道，如果没有给定任何频道，则退订所有频道 unsubscribe(*args) PUBLISH PUBLISH channel message 将信息message发送到指定的频道channel publish(channel, message) PSUBSCRIBE PSUBSCRIBE pattern [pattern …] 订阅与给定模式相关的频道 psubscribe(args, *kwargs) PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern …]] 退订给定的模式，如果没有给定，则退订所有模式 PUNSUBSCRIBE [pattern [pattern …]] PUBSUB PUBSUB subcommand [argument [argument …]] PUBSUB命令是一个introspection命令，允许检查Pub/Sub子系统的状态，它由单独记录的子命令组成 pubsub(**kwargs) 考虑到PUBLISH命令和SUBSCRIBE命令在Python客户端的实现方式，一个比较简单的延时发布与订阅的方法，就是像如下代码那样用辅助线程(helper thread)来执行PUBLISH命令 1234567891011121314151617181920212223242526import redisimport timeimport threadingpool = redis.ConnectionPool(host='localhost', port=6379, db=0)r = redis.StrictRedis(connection_pool=pool)def publisher(n): time.sleep(1) for i in range(n): r.publish('channel', i)def run_pubsub(): threading.Thread(target=publisher, args=(3,)).start() pubsub = r.pubsub() pubsub.subscribe(['channel']) count=0 for item in pubsub.listen(): print(item) count += 1 if count == 4: pubsub.unsubscribe() if count == 5: break publisher函数在刚开始执行时会先休眠，让订阅者有足够的时间来连接服务器并监听消息。在发布消息之后进行短暂的休眠，让消息可以一条接一条地出现。 run_pubsub函数启动发送者线程，让它发送三条消息。随后创建发布与订阅对象，并让它订阅给定的频道。通过遍历函数pubsub.listen()的执行结果来监听订阅消息。在接收到一条订阅反馈消息和三条发布者发送的消息之后，执行退订操作，停止监听新消息。客户端在接收到退订反馈消息之后，就不再接收消息。 实际运行函数并观察它们的行为123456&gt;&gt;&gt; run_pubsub()&#123;'type': 'subscribe', 'channel': b'channel', 'data': 1, 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'0', 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'1', 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'2', 'pattern': None&#125;&#123;'type': 'unsubscribe', 'channel': b'channel', 'data': 0, 'pattern': None&#125; 以上这些结构就是我们在遍历pubsub.listen()函数时得到的元素。 在刚开始订阅一个频道的时候，客户端会接收到一条关于被订阅频道的反馈消息。在退订频道时，客户端会接收到一条反馈消息，告知被退订的是哪一个频道，以及客户端目前仍在订阅的频道数量。 其他命令排序Redis中负责执行排序操作的SORT命令可以根据字符串、列表、集合、有序集合、散列这5中键里面存储的数据，对列表、集合以及有序集合进行排序，可以将SORT命令看作是SQL语言中的order by子句。 下表展示了SORT命令的定义及其redis-py API 命令 用例 描述 redis-py API SORT SORT key [BY pattern][LIMIT offset count] [GET pattern][ASC\\ DESC] [ALPHA] destination 返回或存储key的list、set或sorted set中的元素。默认是按照数值类型排序的，并且按照两个元素的双精度浮点数类型值进行比较 sort(name, start=None, num=None, by=None, get=None, desc=False, alpha=False, store=None, groups=False) 下面展示了SORT命令的一些简单的用法123456789101112131415161718&gt;&gt;&gt; r.rpush('sort-input', 23, 15, 110, 7)4&gt;&gt;&gt; r.sort('sort-input')[b'7', b'15', b'23', b'110']&gt;&gt;&gt; r.sort('sort-input', alpha=True)[b'110', b'15', b'23', b'7']&gt;&gt;&gt; r.hset('d-7', 'field', 5)1&gt;&gt;&gt; r.hset('d-15', 'field', 1)1&gt;&gt;&gt; r.hset('d-23', 'field', 9)1&gt;&gt;&gt; r.hset('d-110', 'field', 3)1&gt;&gt;&gt; r.sort('sort-input', by='d-*-&gt;field')[b'15', b'110', b'7', b'23']&gt;&gt;&gt; r.sort('sort-input', by='d-*-&gt;field', get='d-*-&gt;field')[b'1', b'3', b'5', b'9'] SORT命令不仅可以对列表进行排序，还可以对集合进行排序，然后返回一个列表形式的排序结果。上述代码除了展示如何使用alpha关键字(根据元素字母表顺序，默认根据大小)参数对元素进行字符串排序之外，还展示了如何基于外部数据对元素进行排序，以及如何获取并返回外部数据。 尽管SORT是Redis中唯一一个可以同时处理3种不同类型的数据的命令，但是事务同样可以让我们在一连串不间断执行的命令里面操作不同类型的数据。 基本的Redis事务Redis中的事务(transaction)是一组命令的集合。MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部执行，要么全不执行。 事务的应用非常普遍，比如银行汇款过程中A向B汇款，系统先从A的账户中将钱划走，然后向B的账户中增加相应的金额。这两个步骤必须属于同一个事务，要么全部执行，要么全不执行。 Redis的基本事务(basic transaction)需要用到MULTI和EXEC命令。在Redis中，被MULTI和EXEC命令包围的所有命令会一个接一个地执行，直到所有命令都执行完毕为止。当一个事务执行完毕后，才会处理其他客户端的命令。 Redis中执行事务的步骤：首先需要执行MULTI命令，然后输入我们想要在事务里面执行的命令，最后再执行EXEC命令。MULTI命令用于开启一个事务，它总是返回OK 。MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。另一方面，通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务。EXEC命令的回复是一个数组，数组中的每个元素都是执行事务中的命令所产生的回复。其中，回复元素的先后顺序和命令发送的先后顺序一致。当客户端处于事务状态时，所有传入的命令都会返回一个内容为QUEUED的状态回复status reply，这些被入队的命令将在EXEC命令被调用时执行。 从语义上来说，Redis事务在Python客户端中是由管道(pipeline)实现的：对连接对象调用pipeline()方法将创建一个事务，在一切正常的情况下，客户端会自动地调用MULTI命令包裹用户输入的多个命令。此外，为了减少Redis与客户端之间的通信往返次数，提升执行多个命令的性能，Python的Redis客户端会存储起事务包含的多个命令，然后在事务执行时一次性将所有命令都发送给Redis。 要展示事务执行的结果，最简单的方法就是将事务放到线程里面执行，下面这个交互示例展示了在没有使用事务的情况下，执行并行(parallel)自增操作的结果1234567891011121314151617&gt;&gt;&gt; import redis&gt;&gt;&gt; import threading&gt;&gt;&gt; import time&gt;&gt;&gt; r = redis.StrictRedis(host='localhost', port=6379, db=0)&gt;&gt;&gt; def notrans():... print(r.incr('notrans:'))... time.sleep(.1)... r.incr('notrans:', -1)...&gt;&gt;&gt; if 1:... for i in range(3):... threading.Thread(target=notrans).start()... time.sleep(.5)...213 上述代码启动了3个线程来执行没有被事务包裹的自增、休眠和自减操作，正因为没有使用事务，所以三个线程都可以在执行自减操作前，对notrans:计数器执行自增操作。 下面这个交互示例就展示了如何使用事务处理命令的并行执行问题 123456789101112131415&gt;&gt;&gt; def trans():... pipeline = r.pipeline()... pipeline.incr('trans:')... time.sleep(.1)... pipeline.incr('trans:', -1)... print(pipeline.execute()[0])...&gt;&gt;&gt; if 1:... for i in range(3):... threading.Thread(target=trans).start()... time.sleep(.5)...111 首先在trans函数中创建一个事务型(transactional)管道对象，然后先把针对’tans:’计数器的自增操作放入队列，等待100ms后再将针对’tans:’计数器的自减操作放入队列，最后执行被事务包裹的命令，并打印自增操作的执行结果。最终在执行结果中可以看到，尽管自增和自减操作之间有一段延迟时间，但通过使用事务，各个线程都可以在不被其他线程打断的情况下，执行各自队列里面的命令。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Redis学习笔记(二)：Redis命令及其Python API","slug":"Redis-2","date":"2017-03-27T08:18:54.000Z","updated":"2017-03-30T12:17:22.000Z","comments":true,"path":"2017/03/27/Redis-2/","link":"","permalink":"http://yoursite.com/2017/03/27/Redis-2/","excerpt":"redis-py简介安装在之前的学习笔记(一)中已经安装过redis-py，我的Python版本是3.5.2","text":"redis-py简介安装在之前的学习笔记(一)中已经安装过redis-py，我的Python版本是3.5.21$ pip3 install redis 快速开始123456&gt;&gt;&gt; import redis&gt;&gt;&gt; r = redis.StrictRedis(host='localhost', port=6379, db=0)&gt;&gt;&gt; r.set('key', 'value')True&gt;&gt;&gt; r.get('key')b'value' API参考Redis的官方命令文档很好地解释了每个命令的详细信息。 redis-py公开了实现这些命令的两个客户端类。 第一，StrictRedis类试图遵守官方命令语法， 但是有些一些例外： SELECT: 没有实现，考虑到线程安全的原因。 DEL: 由于del是python语法关键字，所用delete来代替。 CONFIG GET|SET: 分开用 config_get or config_set来代替 MULTI/EXEC: 事务作为Pipeline类的其中一部分的实现。Pipeline默认保证了MULTI,EXEC声明。但是你可以指定transaction=False来禁用这一行为。 SUBSCRIBE/LISTEN:PubSub作为一个独立的类来实现发布订阅机制。 SCAN/SSCAN/HSCAN/ZSCAN:每个命令都对应一个等价的迭代器方法scan_iter/sscan_iter/hscan_iter/zscan_iter methods for this behavior。 第二，Redis类是StrictRedis的子类，提供redis-py版本向后的兼容性。 关于StrictRedis与Redis的区别：(官方推荐使用StrictRedis.) 以下几个方法在StrictRedis和Redis类中的参数顺序不同。 LREM: 在Redis类中是这样的：lrem(self, name, value, num=0)在StrictRedis类中是这样的：lrem(self, name, count, value) ZADD: 在Redis类中是这样的：zadd(‘my-key’, ‘name1’, 1.1, ‘name2’, 2.2, name3=3.3, name4=4.4)在StrictRedis中是这样的：zadd(‘my-key’, 1.1, ‘name1’, 2.2, ‘name2’, name3=3.3, name4=4.4) SETEX: 在Redis类中是这样的：setex(self, name, value, time)而在StrictRedis中是这样的：setex(self, name, time, value) 连接池 redis-py使用connection pool来管理对一个redis server的所有连接，避免每次建立、释放连接的开销。默认情况下，每个Redis实例都会依次创建并维护一个自己的连接池。我们可以直接建立一个连接池，然后传递给Redis或StrictRedis连接命令作为参数，这样就可以实现多个Redis实例共享一个连接池，以实现客户端分片，或者对连接的管理方式进行更高精度的控制。 12&gt;&gt;&gt; pool = redis.ConnectionPool(host='localhost', port=6379, db=0)&gt;&gt;&gt; r = redis.StrictRedis(connection_pool=pool) 我们也可以创建自己的Connection子类，用于控制异步框架中的套接字行为，要使用自己的连接实例化客户端类，需要创建一个连接池，将类传递给connection_class参数。 1&gt;&gt;&gt; pool = redis.ConnectionPool(connection_class=YourConnectionClass,your_arg='...', ...) 释放连接回到连接池：可以使用Redis类的reset()方法，或者使用with上下文管理语法。 解析器：解析器控制如何解析Redis-server的响应内容，redis-py提供两种方式的解析器类支持PythonParser和HiredisParser(需要单独安装)。它优先选用HiredisParser,如果不存在，则选用PythonParser. Hiredis是redis核心团队开发的一个高性能c库，能够提高10x的解析速度。 响应回调：The client class使用一系列的callbacks来完成响应到对应python类型的映射。这些响应回调，定义在 Redis client class中的RESPONSE_CALLBACKS字典中。你可以使用set_response_callback 方法来添加自定义回调类。这个方法接受两个参数：一个命令名字，一个回调类。回调类接受至少一个参数：响应内容，关键字参数作为命令调用时的参数。 线程安全性Redis客户端实例可以安全地在线程之间共享。 在内部，连接实例只在命令执行期间从连接池检索，并在执行后直接返回到池中。 命令执行过程从不修改客户端实例上的状态。 但是，有一个警告：Redis SELECT命令。 SELECT命令允许您切换连接正在使用的数据库。 该数据库保持选中，直到选择另一个或连接关闭为止。 这会创建一个问题，因为可以将连接返回到连接到不同数据库的池。 因此，redis-py不会在客户端实例上实现SELECT命令。 如果在同一应用程序中使用多个Redis数据库，则应为每个数据库创建一个单独的客户机实例（也可能是单独的连接池）。 在线程之间传递PubSub或Pipeline对象是不安全的。 Redis命令及其对应redis-py API由于Redis官方命令文档很好地解释了每个命令的详细信息，所以我这里只对最常用的Redis命令进行整理，并给出其redis-py API。 字符串下表展示了对Redis字符串执行自增和自减操作的命令及其redis-py API。 命令 用例 描述 redis-py API INCR INCR key-name 将键存储的值加1 incr(name, amount=1) DECR DECR key-name 将键存储的值减1 decr(name, amount=1) INCRBY INCRBY key-name amount 将键存储的值加整数amount incr(name, amount=1) DECRBY DECRBY key-name amount 将键存储的值减整数amount decr(name, amount=1) INCRBYFLOAT INCRBYFLOAT key-name amount 将键存储的值加浮点数amount incrbyfloat(name, amount=1.0) 在redis-py内部，使用了INCRBY和DECRBY命令来实现incr()和decr()方法，并且第二个参数amount是可选的，默认为1。 下面这个交互示例展示了Redis的INCR和DECR操作12345678910111213&gt;&gt;&gt; r.get('key')&gt;&gt;&gt; r.incr('key')1&gt;&gt;&gt; r.incr('key', 15)16&gt;&gt;&gt; r.get('key')b'16'&gt;&gt;&gt; r.decr('key', 5)11&gt;&gt;&gt; r.set('key', 13)True&gt;&gt;&gt; r.incr('key')14 当用户将一个值存储到Redis字符串中时，如果这个值可以被解释(interpet)为十进制整数或者浮点数，那么Redis会允许用户对这个字符串执行各种INCR和DECR操作。如果用户对一个不存在的键或者一个保存了空串的键执行自增或自减操作，Redis会自动将这个键的值当作是0来处理。若非上述情况，则Redis将会返回一个错误。 除了自增和自减操作，Redis还可以对字节串进行读取和写入的操作。 下表展示了Redis用来处理字符串子串和二进制位的命令及其redis-py API。 命令 用例 描述 redis-py API APPEND APPEND key-name value 将值value追加到给定键key-name当前存储的值的末尾 append(key, value) GETRANGE GETRANGE key-name start end 获取一个偏移量从start到end的子串，包含start和end getrange(key, start, end) SETRANGE SETRANGE key-name offset value 将从start开始的子串设置为给定值 setrange(name, offset, value) GETBIT GETBIT key-name offset 将字节串看作是二进制位串，并返回位串中偏移量为offset的二进制位的值 getbit(name, offset) SETBIT SETBIT key-name offset value 将字节串看作是二进制位串，并将位串中偏移量为offset的二进制位的值设为value setbit(name, offset, value) BITCOUNT BITCOUNT key-name [start end] 统计字符串被设置为1的bit数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行 bitcount(key, start=None, end=None) BITOP BITOP operation dest-key key-name [key-name …] 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 bitop(operation, dest, *keys) 在执行SETRANGE或者SETBIT命令时，如果offset比当前key对应string还要长，那这个string后面就补空字节(null)以达到offset。使用GETRANGE时超出字符串末尾的数据会被认为是空字符串，而使用GETBIT时超出字符串末尾的二进制位会被视为是0。 下面这个交互示例展示了Redis的子串操作和二进制位操作123456789101112131415161718192021222324&gt;&gt;&gt; r.append('new-string-key', 'hello ')6&gt;&gt;&gt; r.append('new-string-key', 'world!')12&gt;&gt;&gt; r.substr('new-string-key', 3, 7)b'lo wo'&gt;&gt;&gt; r.getrange('new-string-key', 3, 7)b'lo wo'&gt;&gt;&gt; r.setrange('new-string-key', 0, 'H')12&gt;&gt;&gt; r.get('new-string-key')b'Hello world!'&gt;&gt;&gt; r.setrange('new-string-key', 11, ', how are you?')25&gt;&gt;&gt; r.get('new-string-key')b'Hello world, how are you?'&gt;&gt;&gt; r.setbit('another-key', 2, 1)0&gt;&gt;&gt; r.setbit('another-key', 7, 1)0&gt;&gt;&gt; r.getbit('another-key', 1)0&gt;&gt;&gt; r.get('another-key')b'!' Redis现在的GETRANGE命令是由以前的SUBSTR命令改名而来，所以现在redis-py中两者仍然都可以使用，但是最好还是使用getrange()方法来获取子串。 列表下表展示了一些之前介绍过的常用列表命令 命令 用例 描述 redis-py API RPUSH RPUSH key value [value …] 向存于key的列表的尾部插入所有指定的值 rpush(name, *values) LPUSH LPUSH key value [value …] 将所有指定的值插入到存于key的列表的头部 lpush(name, *values) RPOP RPOP key 移除并返回key对应的list的最后一个元素 rpop(name) LPOP LPOP key 移除并返回key对应的list的第一个元素 lpop(name) LINDEX LINDEX key index 返回列表索引位置的元素 lindex(name, index) LRANGE LRANGE key start stop 返回存储在key的列表里指定范围内的元素 lrange(name, start, end) LTRIM LTRIM key start stop 修剪(trim)一个已存在的list，这样list就会只包含指定范围的指定元素 ltrim(name, start, end) 下面这个交互示例展示了Redis列表的推入和弹出操作12345678910111213141516171819202122&gt;&gt;&gt; r.rpush('list-key', 'last')1&gt;&gt;&gt; r.lpush('list-key', 'first')2&gt;&gt;&gt; r.rpush('list-key', 'new last')3&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'first', b'last', b'new last']&gt;&gt;&gt; r.lpop('list-key')b'first'&gt;&gt;&gt; r.lpop('list-key')b'last'&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'new last']&gt;&gt;&gt; r.rpush('list-key', 'a', 'b', 'c')4&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'new last', b'a', b'b', b'c']&gt;&gt;&gt; r.ltrim('list-key', 2, -1)True&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'b', b'c'] 还有几个列表命令能将元素从一个列表移动到另一个列表，或者阻塞(block)执行命令的客户端直到有其他客户端给列表添加元素为止。 下表列出了这些阻塞弹出命令以及列表之间移动元素的命令 命令 用例 描述 redis-py API BLPOP BLPOP key [key …] timeout 弹出第一个非空列表的头元素，或在timeout秒内阻塞并等待可弹出的元素出现 blpop(keys, timeout=0) BRPOP BRPOP key [key …] timeout 弹出第一个非空列表的末尾元素，或在timeout秒内阻塞并等待可弹出的元素出现 brpop(keys, timeout=0) RPOPLPUSH RPOPLPUSH source destination 原子性地返回并移除存储在source的列表的最后一个元素(列表尾部元素)， 并把该元素放入存储在destination的列表的第一个元素位置(列表头部) rpoplpush(src, dst) BRPOPLPUSH BRPOPLPUSH source destination timeout BRPOPLPUSH 是 RPOPLPUSH 的阻塞版本。 当 source 包含元素的时候，这个命令表现得跟 RPOPLPUSH 一模一样。 当 source 是空的时候，Redis将会阻塞这个连接，直到另一个客户端 push 元素进入或者达到 timeout 时限。 brpoplpush(src, dst, timeout=0) 注：原子性是指命令正在都区或者修改数据的时候，其他客户端不能读取或修改相同的数据。 下面这个交互示例展示了Redis列表的阻塞弹出命令以及元素移动命令1234567891011121314151617181920&gt;&gt;&gt; r.rpush('list', 'item1')1&gt;&gt;&gt; r.rpush('list', 'item2')2&gt;&gt;&gt; r.rpush('list2', 'item3')1&gt;&gt;&gt; r.brpoplpush('list2', 'list', 1)b'item3'&gt;&gt;&gt; r.brpoplpush('list2', 'list', 1)&gt;&gt;&gt; r.lrange('list', 0, -1)[b'item3', b'item1', b'item2']&gt;&gt;&gt; r.brpoplpush('list', 'list2', 1)b'item2'&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list', b'item3')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list', b'item1')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list2', b'item2')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1) 对于阻塞弹出命令和弹出并推入命令，最常见的用例就是消息传递(messaging)和任务队列(task queue)。 集合下表展示了一部分最常用的集合命令 命令 用例 描述 redis-py API SADD SADD key member [member …] 添加一个或多个指定的member元素到key集合中 sadd(name, *values) SREM SREM key member [member …] 在key集合中移除指定的元素 srem(name, *values) SISMEMBER SISMEMBER key member 返回成员member是否是存储的集合key的成员 sismember(name, value) SCARD SCARD key 返回集合包含元素的数量 scard(name) SMEMBERS SMEMBERS key 返回key集合所有的元素 smembers(name) SRANDMEMBER SRANDMEMBER key [count] 仅提供key参数,那么随机返回key集合中的一个元素，返回含有 count 个不同的元素的数组，对count分情况处理 srandmember(name, number=None) SPOP SPOP key [count] 从key对应集合中返回并删除一个或多个元素 spop(name) SMOVE SMOVE source destination member 将member从source集合移动到destination集合中 smove(src, dst, value) 下面这个交互示例展示了这些常用的集合命令12345678910111213141516&gt;&gt;&gt; r.sadd('set-key', 'a', 'b', 'c')3&gt;&gt;&gt; r.srem('set-key', 'c', 'd')1&gt;&gt;&gt; r.srem('set-key', 'c', 'd')0&gt;&gt;&gt; r.scard('set-key')2&gt;&gt;&gt; r.smembers('set-key')&#123;b'b', b'a'&#125;&gt;&gt;&gt; r.smove('set-key', 'set-key2', 'a')True&gt;&gt;&gt; r.smove('set-key', 'set-key2', 'c')False&gt;&gt;&gt; r.smembers('set-key2')&#123;b'a'&#125; 但是集合真正厉害的地方在于组合和关联多个集合，下表展示了相关的Redis命令 命令 用例 描述 redis-py API SDIFF SDIFF key [key …] 返回一个集合与给定集合的差集的元素 sdiff(keys, *args) SDIFFSTORE SDIFFSTORE destination key [key …] 类似于 SDIFF，不同之处在于该命令不返回结果集，而是将结果存放在destination集合中，如果destination已经存在, 则将其覆盖重写 sdiffstore(dest, keys, *args) SINTER SINTER key [key …] 返回指定所有的集合的成员的交集 sinter(keys, *args) SINTERSTORE SINTERSTORE destination key [key …] 与SINTER命令类似，但是它并不是直接返回结果集，而是将结果保存在 destination集合中，如果destination集合存在, 则会被重写 sinterstore(dest, keys, *args) SUNION SUNION key [key …] 返回给定的多个集合的并集中的所有成员 sunion(keys, *args) SUNIONSTORE SUNIONSTORE destination key [key …] 类似于SUNION命令，不同的是它并不返回结果集，而是将结果存储在destination集合中，如果destination已经存在，则将其覆盖. sunionstore(dest, keys, *args) 这些命令分别是并集运算、交集运算和差集运算这三个基本集合操作的“返回结果”版本和“存储结果”版本，下面这个交互示例展示了这些命令的基本使用12345678910&gt;&gt;&gt; r.sadd('skey1', 'a', 'b', 'c', 'd')4&gt;&gt;&gt; r.sadd('skey2', 'c', 'd', 'e', 'f')4&gt;&gt;&gt; r.sdiff('skey1', 'skey2')&#123;b'b', b'a'&#125;&gt;&gt;&gt; r.sinter('skey1', 'skey2')&#123;b'c', b'd'&#125;&gt;&gt;&gt; r.sunion('skey1', 'skey2')&#123;b'd', b'a', b'f', b'e', b'c', b'b'&#125; 和Python的集合相比，Redis的集合除了可以被多个客户端远程地进行访问之外，其他的语义和功能基本都是相同的。 散列首先介绍一些常用的添加和删除键值对的Redis散列命令 命令 用例 描述 redis-py API HMGET HMGET key field [field …] 返回key指定的散列中指定字段的值 hmget(name, keys, *args) HMSET HMSET key field value [field value …] 设置key指定的散列中指定字段的值，该命令将重写所有在散列中存在的字段，如果key指定的散列不存在，会创建一个新的散列并与key关联 hmset(name, mapping) HDEL HDEL key field [field …] 从key指定的散列中移除指定的域，在散列中不存在的域将被忽略，如果key指定的散列不存在，它将被认为是一个空的散列，该命令将返回0 hdel(name, *keys) HLEN HLEN key 返回key指定的散列包含的字段的数量 hlen(name) 其中，HDEL命令已经介绍过了，而HLEN以及用于一次读取或设置多个键的HMGET和HMSET则是新出现的命令。它们既可以给用户带来方便，又可以通过减少命令的调用次数以及客户端与Redis之间的通信往返次数来提升Redis的性能。 下面这个交互示例展示了这些命令的使用方法12345678&gt;&gt;&gt; r.hmset('hash-key', &#123;'k1':'v1','k2':'v2','k3':'v3'&#125;)True&gt;&gt;&gt; r.hmget('hash-key', ['k2', 'k3'])[b'v2', b'v3']&gt;&gt;&gt; r.hlen('hash-key')3&gt;&gt;&gt; r.hdel('hash-key', 'k1', 'k3')2 之前的学习笔记(一)介绍的HGET命令和HSET命令分别是HMGET和HMSET命令的单参数版本。因为HDEL已经可以同时删除多个键值对了，所以Redis没有实现HMDEL命令。 下表列出了散列的其他几个批量操作命令，以及一些和字符串操作类似的散列命令。 命令 用例 描述 redis-py API HEXISTS HEXISTS key field 检查给定键是否存在于散列中 hexists(name, key) HKEYS HKEYS key 返回散列包含的所有键 hkeys(name) HVALS HVALS key 返回散列包含的所有值 hvals(name) HGETALL HGETALL key 返回散列包含的所有键值对 hgetall(name) HINCRBY HINCRBY key field increment 将键存储的值加上整数increment hincrby(name, key, amount=1) HINCRBYFLOAT HINCRBYFLOAT key field increment 将键存储的值加上浮点数increment hincrbyfloat(name, key, amount=1.0) 下面这个交互示例展示了这些命令的使用方法12345678910&gt;&gt;&gt; r.hmset('hash-key2', &#123;'short':'hello', 'long':1000*1&#125;)True&gt;&gt;&gt; r.hkeys('hash-key2')[b'short', b'long']&gt;&gt;&gt; r.hexists('hash-key2', 'num')False&gt;&gt;&gt; r.hincrby('hash-key2', 'num')1&gt;&gt;&gt; r.hexists('hash-key2', 'num')True 在对散列进行处理时，如果键值对的值的体积非常大，那么用户可以先用HKEYS获取散列的所有键，然后只获取必要的值，这样可以有效地减少需要传输的数据量，避免服务器阻塞。 有序集合下表展示了一些常用的有序集合命令，大部分在第一章都有介绍 命令 用例 描述 redis-py API ZADD ZADD key score member [score member …] 将带有给定分值的成员添加到有序集合中 zadd(name, args, *kwargs) ZREM ZREM key member [member …] 移除给定的成员，并返回被移除成员的数量 zrem(name, *values) ZCARD ZCARD key 返回有序集合包含的成员数量 zcard(name) ZINCRBY ZINCRBY key increment member 将member成员的分值加上increment zincrby(name, value, amount=1) ZCOUNT ZCOUNT key min max 返回分值介于min和max之间的成员数量 zcount(name, min, max) ZRANK ZRANK key member 返回成员member在有序集合中的排名 zrank(name, value) ZSCORE ZSCORE key member 返回成员member的分值 zscore(name, value) ZRANGE ZRANGE key start stop [WITHSCORES] 返回排名介于start和stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回 zrange(name, start, end, desc=False, withscores=False, score_cast_func=) 下面这个交互示例展示了Redis中的一些常用的有序集合命令12345678910111213141516&gt;&gt;&gt; r.zadd('zset-key', 3, 'a', 2, 'b', 1, 'c')3&gt;&gt;&gt; r.zcard('zset-key')3&gt;&gt;&gt; r.zincrby('zset-key', 'c', 3)4.0&gt;&gt;&gt; r.zscore('zset-key', 'b')2.0&gt;&gt;&gt; r.zrank('zset-key', 'c')2&gt;&gt;&gt; r.zcount('zset-key', 0, 3)2&gt;&gt;&gt; r.zrem('zset-key', 'b')1&gt;&gt;&gt; r.zrange('zset-key', 0, -1, withscores=True)[(b'a', 3.0), (b'c', 4.0)] 其中在Python客户端用StrictRedis客户端类执行ZADD命令需要先输入分值，再输入成员，这也是Redis的标准，而Redis客户端类则截然相反。 下表展示了另外一下非常有用的有序集合命令 命令 用例 描述 redis-py API ZREVRANK ZREVRANK key member 返回有序集合里成员member的排名，成员按照分值从大到小排列 zrevrank(name, value) ZREVRANGE ZREVRANGE key start stop [WITHSCORES] 返回有序集合给定排名范围内的成员，成员按照分值从大到小排列 zrevrange(name, start, end, withscores=False, score_cast_func=) ZRANGEBYSCORE ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 返回有序集合中指定分数区间内的成员 zrangebyscore(name, min, max, start=None, num=None, withscores=False, score_cast_func=) ZREVRANGEBYSCORE ZREVRANGEBYSCORE key max min [WITHSCORES][LIMIT offset count] 返回有序集合中指定分数区间内的成员，分数由高到低排序。 zrevrangebyscore(name, max, min, start=None, num=None, withscores=False, score_cast_func=) ZREMRANGEBYRANK ZREMRANGEBYRANK key start stop 移除有序集key中，指定排名(rank)区间内的所有成员 zremrangebyrank(name, min, max) ZREMRANGEBYSCORE ZREMRANGEBYSCORE key min max 移除有序集key中，所有score值介于min和max之间(包括等于min或max)的成员 zremrangebyscore(name, min, max) ZINTERSTORE ZINTERSTORE destination numkeys key [key …] [WEIGHTS weight] [SUM MIN MAX] 计算给定的numkeys个有序集合的交集，并且把结果放到destination中 zinterstore(dest, keys, aggregate=None) ZUNIONSTORE ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight] [SUM MIN MAX] 计算给定的numkeys个有序集合的并集，并且把结果放到destination中。 zunionstore(dest, keys, aggregate=None) 其中有几个是没有介绍过的新命令，除了使用逆序来处理有序集合之外，ZREV*命令的工作方式和相对应的非逆序命令的工作方式完全一样(逆序就是指元素按照分值从大到小地排列)。 下面这个交互示例展示了ZINTERSTORE和ZUNIONSTORE命令的用法123456789101112131415161718&gt;&gt;&gt; r.zadd('zset-1', 1, 'a', 2, 'b', 3, 'c')3&gt;&gt;&gt; r.zadd('zset-2', 4, 'b', 1, 'c', 0, 'd')3&gt;&gt;&gt; r.zinterstore('zset-i', ['zset-1', 'zset-2'])2&gt;&gt;&gt; r.zrange('zset-i', 0, -1, withscores=True)[(b'c', 4.0), (b'b', 6.0)]&gt;&gt;&gt; r.zunionstore('zset-u', ['zset-1', 'zset-2'], aggregate='min')4&gt;&gt;&gt; r.zrange('zset-u', 0, -1, withscores=True)[(b'd', 0.0), (b'a', 1.0), (b'c', 1.0), (b'b', 2.0)]&gt;&gt;&gt; r.sadd('set-1', 'a', 'd')2&gt;&gt;&gt; r.zunionstore('zset-u2', ['zset-1', 'zset-2', 'set-1'])4&gt;&gt;&gt; r.zrange('zset-u2', 0, -1, withscores=True)[(b'd', 1.0), (b'a', 2.0), (b'c', 4.0), (b'b', 6.0)] 用户可以在执行交并运算时传入不同的聚合函数，共有sum、min、max三种可选；用户还可以把集合作为输入传给ZINTERSTORE和ZUNIONSTORE，命令会将集合看作是成员分值全为1的有序集合来处理。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Redis学习笔记(一)：初识Redis","slug":"Redis-1","date":"2017-03-25T12:18:54.000Z","updated":"2017-03-27T09:12:22.000Z","comments":true,"path":"2017/03/25/Redis-1/","link":"","permalink":"http://yoursite.com/2017/03/25/Redis-1/","excerpt":"Redis简介Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。","text":"Redis简介Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。 Redis与memcached及其他类型数据库对比Redis经常被拿来与memcached进行比较，两者都可用于存储键值映射，性能也相差无几，但是Redis能够自动以两种不同的方式将数据写入硬盘，而且Redis除了能存储普通的字符串键，还能存储其他4种数据结构，使得Redis可以用于解决更为广泛的问题，并且即可以作为主数据库使用，又可以作为其他存储系统的辅助数据库。 下表展示了Redis与memcached，MySQL以及MongoDB的特性与功能。 名称 类型 数据存储选项 查询类型 附加功能 Redis 使用内存存储的非关系数据库 字符串、列表、集合、散列表、有序集合 每种数据类型专属的命令，以及批量操作和不完全的事务支持 发布与订阅，主从复制，持久化，脚本 memcached 使用内存存储的键值缓存 键值之间的映射 创建、读取、删除、更新等命令 多线程服务器，用于提升性能 MySQL 关系数据库 每个数据库可以包含多个表，每个表可以包含多个行；可以处理多个表的视图；支持空间和第三方扩展 SELECT、INSERT、UPDATE、DELETE、函数、存储过程 支持ACID性质(需要使用InnoDB)，主从复制，主主复制 MongoDB 使用硬盘存储(on-disk)的非关系文档存储 每个数据库可以包含多个表，每个表可以包含多个无schema的BSON文档 创建、读取、更新、删除、条件查询等命令 支持map-reduce操作，主从复制，分片，空间索引 Redis安装(mac)首先下载用于安装Rudix的引导脚本，并安装Rudix12$ curl -O http://rudix.google.code.com/hg/Ports/rudix/rudix.Py$ sudo python rudix.py install rudix 然后使用命令Rudix安装Redis，若能成功启动Redis服务器则安装成功12$ sudo rudix install redis$ redis-server 最后用pip为Python安装Redis客户端库1$ sudo pip install redis Redis数据结构简介Redis可以存储键与5种不同数据结构类型之间的映射，分别是STRING(字符串)、LIST(列表)、SET(集合)、HASH(散列)、ZSET(有序集合)。有一部分命令对于这5种数据结构是通用的，如DEL、TYPE、RENAME等；但也有一部分命令只能对特定的一种或者两种结构使用。 下表从结构存储的值及读写能力对比了Redis的5种数据结构。 结构类型 结构存储的值 结构的读写能力 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作；对整数和浮点数进行自增或自减操作 LIST 一个链表，链表上的每个节点都包含了一个字符串 从链表两端推入或弹出元素；根据偏移量对链表进行修剪(trim)；读取单个或多个元素；根据值查找或移除元素 SET 包含字符串的无序收集器，并且被包含的每个字符串互不相同 添加、获取、移除单个元素；检查一个元素是否存在于集合中；计算交集、并集、差集；从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对；获取所有键值对 ZSET 字符串成员(member)与浮点数分值(score)之间的有序映射，元素的排列顺序由分值的大小决定 添加、获取、删除单个元素；根据分值范围(range)或者成员来获取元素 Redis字符串下表展示了3种简单的字符串命令： 命令 行为 GET 获取存储在给定键中的值 SET 设置存储在给定键中的值 DEL 删除存储在给定键中的值(该命令可用于所有类型) SET、GET、DEL的使用示例：12345678910$ redis-cli127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; get hello\"world\"127.0.0.1:6379&gt; del hello(integer) 1127.0.0.1:6379&gt; get hello(nil)127.0.0.1:6379&gt; SET命令在执行成功时返回OK，Python客户端会将这个OK转换为True；DEL命令在执行成功时将会返回被成功删除的值的数量；GET命令在尝试得到不存在的值时，将会返回一个nil，Python客户端会将这个nil转换为None。 Redis列表下表展示了6种简单的列表命令： 命令 行为 LPUSH(RPUSH) 将给定值推入列表的左端(右端) LPOP(RPOP) 从列表的左端(右端)弹出一个值，并返回被弹出的值 LINDEX 获取列表在给定位置上的单个值 LRANGE 获取列表在给定范围上的所有值 RPUSH、LRANGE、LINDEX、LPOP的使用示例12345678910111213141516171819$ redis-cli127.0.0.1:6379&gt; rpush list-key item(integer) 1127.0.0.1:6379&gt; rpush list-key item2(integer) 2127.0.0.1:6379&gt; rpush list-key item(integer) 3127.0.0.1:6379&gt; lrange list-key 0 -11) \"item\"2) \"item2\"3) \"item\"127.0.0.1:6379&gt; lindex list-key 1\"item2\"127.0.0.1:6379&gt; lpop list-key \"item\"127.0.0.1:6379&gt; lrange list-key 0 -11) \"item2\"2) \"item\"127.0.0.1:6379&gt; RPUSH和LPUSH命令在执行成功后会返回当前列表的长度；列表索引范围从0开始，到-1结束，可以取出列表包含的所有元素；使用LINDEX可以从列表中取出单个元素。 Redis集合Redis的集合和列表都可以存储多个字符串，它们的不同之处在于，列表可以存储多个相同的字符串，而集合则通过散列表来保证自己存储的每个字符串都是不同的(这些散列表只有键)。 下表展示了6种简单的集合命令： 命令 行为 SADD 将给定元素添加到集合 SMEMBERS 返回集合包含的所有元素 SISMEMBER 检查给定元素是否存在于集合中 SREM 如果给定的元素存在于集合中，那么移除这个元素 SADD、SMEMBERS、SISMEMBER、SREM的使用示例12345678910111213141516171819202122232425$ redis-cli127.0.0.1:6379&gt; sadd set-key item(integer) 1127.0.0.1:6379&gt; sadd set-key item2(integer) 1127.0.0.1:6379&gt; sadd set-key item3(integer) 1127.0.0.1:6379&gt; sadd set-key item(integer) 0127.0.0.1:6379&gt; smembers set-key1) \"item2\"2) \"item3\"3) \"item\"127.0.0.1:6379&gt; sismember set-key item4(integer) 0127.0.0.1:6379&gt; sismember set-key item(integer) 1127.0.0.1:6379&gt; srem set-key item2(integer) 1127.0.0.1:6379&gt; srem set-key item2(integer) 0127.0.0.1:6379&gt; smembers set-key1) \"item3\"2) \"item\"127.0.0.1:6379&gt; SADD命令返回1表示成功添加到集合中，返回0表示该元素已存在于集合中；SMEMBERS命令获取到的元素组成的序列将会被Python客户端转换为Python集合；Python客户端会返回一个布尔值来表示SISMEMBER命令的检查结果；SREM命令会返回被移除元素的数量。 Redis散列Redis的散列就像一个微型Redis，它可以存储多个键值对之间的映射。和字符串一样，散列存储的值既可以是字符串也可以是数值。可以将散列看做文档数据库里面的文档，还可以看做是关系数据库里面的行，因为散列、文档和行都允许用户同时访问或修改一个或多个域(field)。 下表展示了4种简单的列表命令： 命令 行为 HSET 在散列里面关联给定的键值对 HGET 获取指定散列键的值 HGETALL 获取散列包含的所有键值对 HDEL 如果给定键存在于散列里面，那么移除这个键 HSET、HGET、HGETALL、HDEL的使用示例12345678910111213141516171819202122$ redis-cli127.0.0.1:6379&gt; hset hash-key sub-key1 value1(integer) 1127.0.0.1:6379&gt; hset hash-key sub-key2 value2(integer) 1127.0.0.1:6379&gt; hset hash-key sub-key1 value1(integer) 0127.0.0.1:6379&gt; hgetall hash-key1) \"sub-key1\"2) \"value1\"3) \"sub-key2\"4) \"value2\"127.0.0.1:6379&gt; hdel hash-key sub-key2(integer) 1127.0.0.1:6379&gt; hdel hash-key sub-key2(integer) 0127.0.0.1:6379&gt; hget hash-key sub-key1\"value1\"127.0.0.1:6379&gt; hgetall hash-key1) \"sub-key1\"2) \"value1\"127.0.0.1:6379&gt; HSET返回一个值来表示给定的键是否已经存在于散列里面；Python客户端会把HGETALL命令获取的整个散列转换为一个Python字典；HDEL命令执行后会返回一个值来表示给定的键在移除之前是否存在于散列里面。 Redis有序集合有序集合和散列一样，都用于存储键值对：其中有序集合的每个键称为成员（member），都是独一无二的，而有序集合的每个值称为分值（score），都必须是浮点数。有序集合是Redis里面唯一既可以根据成员访问元素（这一点和散列一样），又可以根据分值以及分值的排列顺序来访问元素的结构。 下表展示了4种简单的有序集合命令： 命令 行为 ZADD 将一个带有给定分值的成员添加到有序集合里面 ZRANGE 根据元素在有序排列中所处的位置，从有序集合里获取多个元素 ZRANGEBYSCORE 获取有序集合在给定分值范围内的所有元素 ZREM 如果给定成员存在于有序集合，那么移除这个成员 ZADD、ZRANGE、ZRANGEBYSCORE、ZREM的使用示例1234567891011121314151617181920212223$ redis-cli127.0.0.1:6379&gt; zadd zset-key 728 member1(integer) 1127.0.0.1:6379&gt; zadd zset-key 982 member0(integer) 1127.0.0.1:6379&gt; zadd zset-key 982 member0(integer) 0127.0.0.1:6379&gt; zrange zset-key 0 -1 withscores1) \"member1\"2) \"728\"3) \"member0\"4) \"982\"127.0.0.1:6379&gt; zrangebyscore zset-key 0 800 withscores1) \"member1\"2) \"728\"127.0.0.1:6379&gt; zrem zset-key member1(integer) 1127.0.0.1:6379&gt; zrem zset-key member1(integer) 0127.0.0.1:6379&gt; zrange zset-key 0 -1 withscores1) \"member0\"2) \"982\"127.0.0.1:6379&gt; 在尝试向有序集合添加元素的时候，ZADD命令会返回新添加元素的数量；ZRANGE命令获取有序集合包含的所有元素，这些元素会按照分值进行排序，Python客户端会将这些分值转换成浮点数；ZRANGEBYSCORE命令也可以根据分值来获取有序集合的其中一部分元素；ZREM命令在移除有序集合元素的时候，命令会返回被移除元素的数量。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]}]}