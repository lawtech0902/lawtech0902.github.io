{"meta":{"title":"LawTech's Blog","subtitle":"不破不立","description":"破邮python爱好者。","author":"LawTech.","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-03-26T06:40:18.000Z","updated":"2017-03-26T06:40:18.000Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":""},{"title":"关于","date":"2017-03-26T08:09:29.000Z","updated":"2017-03-26T08:36:08.000Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"About个人简介 lawtech0902— Python爱好者 学生党一枚~ 本科：南邮信息安全 硕士：南邮软件工程 Email：584563542@qq.com 关于这个博客 写着玩儿~ 其他： 皇马球迷，我罗加油；"},{"title":"分类","date":"2017-03-26T05:59:19.000Z","updated":"2017-03-26T06:00:37.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-03-26T05:59:01.000Z","updated":"2017-03-26T06:00:05.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Selectors","slug":"scrapy-selectors","date":"2017-05-05T06:18:54.000Z","updated":"2017-05-05T12:51:02.000Z","comments":true,"path":"2017/05/05/scrapy-selectors/","link":"","permalink":"http://yoursite.com/2017/05/05/scrapy-selectors/","excerpt":"当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： BeautifulSoup 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 lxml 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。 Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。","text":"当抓取网页时，你做的最常见的任务是从 HTML 源码中提取数据。现有的一些库可以达到这个目的： BeautifulSoup 是在程序员间非常流行的网页分析库，它基于 HTML 代码的结构来构造一个 Python 对象，对不良标记的处理也非常合理，但它有一个缺点：慢。 lxml 是一个基于 ElementTree（不是 Python 标准库的一部分）的 python 化的 XML 解析库（也可以解析 HTML）。 Scrapy 提取数据有自己的一套机制。它们被称作选择器（seletors），因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML 文件中的某个部分。 XPath 是一门用来在 XML 文件中选择节点的语言，也可以用在 HTML 上。 CSS 是一门将 HTML 文档样式化的语言。选择器由它定义，并与特定的 HTML 元素的样式相关连。 Scrapy 选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。 本文解释了选择器如何工作，并描述了相应的 API。不同于 lxml API 的臃肿，该 API 短小而简洁。这是因为 lxml 库除了用来选择标记化文档外，还可以用到许多任务上。 使用选择器构造选择器Scrapy selectors是 Selector 类的实例，通过传入 text 或 TextResponse 来创建，它自动根据传入的类型选择解析规则（XML or HTML）： 12&gt;&gt;&gt; from scrapy.selector import Selector &gt;&gt;&gt; from scrapy.http import HtmlResponse 以文字构造（都以 xpath 和 css 两种方法解析字段内容，加深理解）： 12345&gt;&gt;&gt; body = &apos;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&apos;&gt;&gt;&gt; Selector(text=body).xpath(&quot;//span/text()&quot;).extract()[&apos;good&apos;]&gt;&gt;&gt; Selector(text=body).css(&quot;html body span::text&quot;).extract()[&apos;good&apos;] 以 response 构造： 12345&gt;&gt;&gt; response = HtmlResponse(url=&apos;http://example.com&apos;, body=body, encoding=&apos;utf-8&apos;)&gt;&gt;&gt; Selector(response=response).xpath(&apos;//span/text()&apos;).extract()[&apos;good&apos;]&gt;&gt;&gt; Selector(response=response).css(&quot;html body span::text&quot;).extract()[&apos;good&apos;] response 对象以 .selector 属性提供了一个 selector ， 可以随时使用该快捷方法: 1234&gt;&gt;&gt; response.selector.xpath(&apos;//span/text()&apos;).extract()[&apos;good&apos;]&gt;&gt;&gt; response.selector.css(&quot;html body span::text&quot;).extract()[&apos;good&apos;] 使用选择器我们将使用 Scrapy shell （提供交互测试）和位于 Scrapy 文档服务器的一个样例页面，来解释如何使用选择器： http://doc.scrapy.org/en/latest/_static/selectors-sample1.html 该页面源码如下： 123456789101112131415&lt;html&gt; &lt;head&gt; &lt;base href='http://example.com/' /&gt; &lt;title&gt;Example website&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id='images'&gt; &lt;a href='image1.html'&gt;Name: My image 1 &lt;br /&gt;&lt;img src='image1_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image2.html'&gt;Name: My image 2 &lt;br /&gt;&lt;img src='image2_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image3.html'&gt;Name: My image 3 &lt;br /&gt;&lt;img src='image3_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image4.html'&gt;Name: My image 4 &lt;br /&gt;&lt;img src='image4_thumb.jpg' /&gt;&lt;/a&gt; &lt;a href='image5.html'&gt;Name: My image 5 &lt;br /&gt;&lt;img src='image5_thumb.jpg' /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 首先，打开 scrapy shell， 1scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html 当 shell 载入后，您将获得名为 response 的 shell 变量，其为响应的 response，并且在其 response.selector 属性上绑定了一个 selector。 因为我们处理的是 HTML，选择器将自动使用 HTML 语法分析。 那么，通过查看该页面的源码，我们构建一个 XPath 来选择 title 标签内的文字: 12&gt;&gt;&gt; response.selector.xpath(&quot;//title/text()&quot;)&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;] 由于在 response 中使用 XPath、CSS 查询十分普遍，因此，Scrapy 提供了两个实用的快捷方式：response.xpath() 及 response.css() ： 12345&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;)&gt;&gt;&gt; [&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;Example website&apos;&gt;]&gt;&gt;&gt; response.css(&quot;title::text&quot;)&gt;&gt;&gt; [&lt;Selector xpath=&apos;descendant-or-self::title/text()&apos; data=&apos;Example website&apos;&gt;] 现在我们将得到根 URL（base URL）和一些图片链接: 1234567891011121314151617&gt;&gt;&gt; response.xpath('//base/@href').extract() ['http://example.com/']&gt;&gt;&gt; response.css('base::attr(href)').extract() ['http://example.com/']&gt;&gt;&gt; response.xpath('//a[contains(@href, \"image\")]/@href').extract() ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']&gt;&gt;&gt; response.css('a[href*=image]::attr(href)').extract() ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']&gt;&gt;&gt; response.xpath('//a[contains(@href, \"image\")]/img/@src').extract() ['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg']&gt;&gt;&gt; response.css('a[href*=image] img::attr(src)').extract() ['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg'] 嵌套选择器选择器方法（ .xpath() or .css() ）返回相同类型的选择器列表，因此你也可以对这些选择器调用选择器方法。下面是一个例子： 123456789101112131415161718&gt;&gt;&gt; links = response.xpath(&quot;//a[contains(@href,&apos;image&apos;)]&quot;)&gt;&gt;&gt; links.extract()[&apos;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;, &apos;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;]&gt;&gt;&gt; for index, link in enumerate(links):...: args = (index, link.xpath(&apos;@href&apos;).extract(), link.xpath(&apos;img/@src&apos;).extract())...: print(&apos;Link number %d points to url %s and image %s&apos; % args)...:Link number 0 points to url [&apos;image1.html&apos;] and image [&apos;image1_thumb.jpg&apos;]Link number 1 points to url [&apos;image2.html&apos;] and image [&apos;image2_thumb.jpg&apos;]Link number 2 points to url [&apos;image3.html&apos;] and image [&apos;image3_thumb.jpg&apos;]Link number 3 points to url [&apos;image4.html&apos;] and image [&apos;image4_thumb.jpg&apos;]Link number 4 points to url [&apos;image5.html&apos;] and image [&apos;image5_thumb.jpg&apos;] 结合正则表达式使用选择器Selector 也有一个 .re() 方法，用来通过正则表达式来提取数据。然而，不同于使用 .xpath() 或者 .css() 方法，.re()方法返回 unicode 字符串的列表。所以你无法构造嵌套式的 .re() 调用。 下面是一个例子，从上面的 html 源码中提取图像名字： 12&gt;&gt;&gt; response.xpath(&quot;//a[contains(@href, &apos;image&apos;)]/text()&quot;).re(r&apos;Name:\\s*(.*)&apos;)[&apos;My image 1 &apos;, &apos;My image 2 &apos;, &apos;My image 3 &apos;, &apos;My image 4 &apos;, &apos;My image 5 &apos;] 使用相对 XPaths记住如果你使用嵌套的选择器，并使用起始为 / 的 XPath，那么该 XPath 将对文档使用绝对路径，而且对于你调用的 Selector 不是相对路径。 比如，假设你想提取在 &lt;div&gt; 元素中的所有 &lt;p&gt; 元素。首先，你将先得到所有的 &lt;div&gt; 元素： 1&gt;&gt;&gt; divs = response.xpath(&quot;//div&quot;) 开始时，你可能会尝试使用下面的错误的方法，因为它其实是从整篇文档中，而不仅仅是从那些 &lt;div&gt; 元素内部提取所有的 &lt;p&gt; 元素： 12&gt;&gt;&gt; for p in divs.xpath(&apos;//p&apos;): # this is wrong - gets all &lt;p&gt; from the whole document ... print p.extract() 下面是比较合适的处理方法(注意 .//p XPath 的点前缀)： 12&gt;&gt;&gt; for p in divs.xpath(&apos;.//p&apos;): # extracts all &lt;p&gt; inside ... print p.extract() 另一种常见的情况将是提取所有直系 &lt;p&gt; 的结果： 12&gt;&gt;&gt; for p in divs.xpath(&apos;p&apos;): ... print p.extract() 使用 EXSLT 扩展因建于 lxml 之上，Scrapy 选择器也支持一些 EXSLT 扩展，可以在 XPath 表达式中使用这些预先制定的命名空间： 前缀 命名空间 用途 re http://exslt.org/regular-expressions 正则表达式 set http://exslt.org/sets 集合操作 正则表达式例如在XPath的 starts-with() 或 contains() 无法满足需求时， test() 函数可以非常有用。 例如在列表中选择有”class”元素且结尾为一个数字的链接： 1234567891011121314151617&gt;&gt;&gt; from scrapy import Selector&gt;&gt;&gt; doc = &quot;&quot;&quot;... &lt;div&gt;... &lt;ul&gt;... &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;... &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;... &lt;/ul&gt;... &lt;/div&gt;... &quot;&quot;&quot;&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)&gt;&gt;&gt; sel.xpath(&quot;//li//@href&quot;).extract()[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link3.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;]&gt;&gt;&gt; sel.xpath(&quot;//li[re:test(@class, &apos;item-\\d$&apos;)]//@href&quot;).extract()[&apos;link1.html&apos;, &apos;link2.html&apos;, &apos;link4.html&apos;, &apos;link5.html&apos;] 注意：C语言库 libxslt 不原生支持EXSLT正则表达式，因此 lxml 在实现时使用了Python re 模块的钩子。 因此，在 XPath 表达式中使用 regexp 函数可能会牺牲少量的性能。 集合操作集合操作可以方便地用于在提取文字元素前从文档树中去除一些部分。 例如使用 itemscopes 组和对应的 itemprops 来提取微数据（来自 http://schema.org/Product 的样本内容）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&gt;&gt;&gt; doc = &quot;&quot;&quot;... &lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;... &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;... ![](kenmore-microwave-17in.jpg)... &lt;div itemprop=&quot;aggregateRating&quot;... itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;... Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5... based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews... &lt;/div&gt;...... &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;... &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;... &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock... &lt;/div&gt;...... Product description:... &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.... Has six preset cooking categories and convenience features like... Add-A-Minute and Child Lock.&lt;/span&gt;...... Customer reviews:...... &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;... &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -... by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,... &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011... &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;... &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;... &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/... &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars... &lt;/div&gt;... &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace... it. &lt;/span&gt;... &lt;/div&gt;...... &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;... &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -... by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,... &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011... &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;... &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;... &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/... &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars... &lt;/div&gt;... &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and... fits in my apartment.&lt;/span&gt;... &lt;/div&gt;... ...... &lt;/div&gt;... &quot;&quot;&quot;&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)&gt;&gt;&gt; for scope in sel.xpath(&apos;//div[@itemscope]&apos;):... print &quot;current scope:&quot;, scope.xpath(&apos;@itemtype&apos;).extract()... props = scope.xpath(&apos;&apos;&apos;... set:difference(./descendant::*/@itemprop,... .//*[@itemscope]/*/@itemprop)&apos;&apos;&apos;)... print &quot; properties:&quot;, props.extract()... printcurrent scope: [u&apos;http://schema.org/Product&apos;] properties: [u&apos;name&apos;, u&apos;aggregateRating&apos;, u&apos;offers&apos;, u&apos;description&apos;, u&apos;review&apos;, u&apos;review&apos;]current scope: [u&apos;http://schema.org/AggregateRating&apos;] properties: [u&apos;ratingValue&apos;, u&apos;reviewCount&apos;]current scope: [u&apos;http://schema.org/Offer&apos;] properties: [u&apos;price&apos;, u&apos;availability&apos;]current scope: [u&apos;http://schema.org/Review&apos;] properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]current scope: [u&apos;http://schema.org/Rating&apos;] properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;]current scope: [u&apos;http://schema.org/Review&apos;] properties: [u&apos;name&apos;, u&apos;author&apos;, u&apos;datePublished&apos;, u&apos;reviewRating&apos;, u&apos;description&apos;]current scope: [u&apos;http://schema.org/Rating&apos;] properties: [u&apos;worstRating&apos;, u&apos;ratingValue&apos;, u&apos;bestRating&apos;] 这里我们先迭代 itemscope 元素，对于每一个元素，我们寻找所有 itemprops 元素，并排除那些在另一个元素内部的元素 itemscope 。 内置选择器参考Selector 实例1class scrapy.selector.Selector(response=None, text=None, type=None) 一个实例Selector是一个包装器响应来选择其内容的某些部分。 response是一个HtmlResponse或一个XmlResponse将被用于选择和提取的数据对象。 text是一个unicode字符串或utf-8编码的文本，当一个 response不可用时。使用text和response一起是未定义的行为。 type定义选择器类型，它可以是&quot;html&quot;，&quot;xml&quot;或None（默认）。 如果type是None，选择器将根据response类型（见下文）自动选择最佳类型，或者默认&quot;html&quot;情况下与选项一起使用text。 如果type是None和response传递，选择器类型从响应类型推断如下： &quot;html&quot;对于HtmlResponse类型 &quot;xml&quot;对于XmlResponse类型 &quot;html&quot;为任何其他 否则，如果type设置，选择器类型将被强制，并且不会发生检测。 xpath（查询）查找与xpath匹配的节点query，并将结果作为 SelectorList实例将所有元素展平。列表元素也实现Selector接口。 query 是一个包含要应用的XPATH查询的字符串。 注意 为了方便起见，这种方法可以称为 response.xpath() css（查询）应用给定的CSS选择器并返回一个SelectorList实例。 query 是一个包含要应用的CSS选择器的字符串。 在后台，CSS查询使用cssselect库和run .xpath()方法转换为XPath查询 。 注意 为了方便起见，该方法可以称为 response.css() extract（）序列化并返回匹配的节点作为unicode字符串列表。编码内容的百分比未引用。 re（regex）应用给定的正则表达式并返回一个包含匹配项的unicode字符串的列表。 regex可以是编译的正则表达式或将被编译为正则表达式的字符串 re.compile(regex) 注意 注意，re()和re_first()解码HTML实体（除\\&lt;和\\&amp;）。 register_namespace（prefix，uri）注册在此使用的给定命名空间Selector。如果不注册命名空间，则无法从非标准命名空间中选择或提取数据。参见下面的例子。 remove_namespaces（）删除所有命名空间，允许使用无命名空间的xpaths遍历文档。参见下面的例子。 nonzero（）返回True如果有选择或任何实际的内容False 除外。换句话说，a的布尔值Selector由它选择的内容给出。 SelectorList对象class scrapy.selector.SelectorList 本SelectorList类是内置的一个子list 类，它提供了几个方法。 xpath（查询）调用.xpath()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。 query 是同一个参数 Selector.xpath() css（查询）调用.css()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。 query 是同一个参数 Selector.css() extract（）调用.extract()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。 re（）调用.re()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。 nonzero（）如果列表不为空，则返回True，否则返回False。 HTML响应的选择器示例这里有几个Selector例子来说明几个概念。在所有情况下，我们假设已经Selector实例化了一个HtmlResponse对象，如下： 1sel = Selector(html_response) &lt;h1&gt;从HTML响应主体中选择所有元素，返回Selector对象列表 （即SelectorList对象）： 1sel.xpath(\"//h1\") &lt;h1&gt;从HTML响应正文中提取所有元素的文本，返回unicode字符串 12sel.xpath(\"//h1\").extract() # this includes the h1 tagsel.xpath(\"//h1/text()\").extract() # this excludes the h1 tag 迭代所有&lt;p&gt;标签并打印其类属性： 12for node in sel.xpath(\"//p\"): print node.xpath(\"@class\").extract() XML响应的选择器示例这里有几个例子来说明几个概念。在这两种情况下，我们假设已经Selector实例化了一个 XmlResponse对象，像这样： 1sel = Selector(xml_response) 从XML响应主体中选择所有元素，返回Selector对象列表（即SelectorList对象）： 1sel.xpath(\"//product\") 从需要注册命名空间的Google Base XML Feed中提取所有价格： 12sel.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")sel.xpath(\"//g:price\").extract() 删除名称空间当处理抓取项目时，通常很方便地完全删除命名空间，只需处理元素名称，编写更简单/方便的XPath。你可以使用的 Selector.remove_namespaces()方法。 让我们展示一个例子，用GitHub博客atom feed来说明这一点。 首先，我们打开shell和我们想要抓取的url： 1$ scrapy shell https://github.com/blog.atom 一旦在shell中，我们可以尝试选择所有对象，并看到它不工作（因为Atom XML命名空间模糊了这些节点）： 12&gt;&gt;&gt; response.xpath(\"//link\")[] 但是一旦我们调用该Selector.remove_namespaces()方法，所有节点都可以直接通过他们的名字访问： 12345&gt;&gt;&gt; response.selector.remove_namespaces()&gt;&gt;&gt; response.xpath(\"//link\")[&lt;Selector xpath='//link' data=u'&lt;link xmlns=\"http://www.w3.org/2005/Atom'&gt;, &lt;Selector xpath='//link' data=u'&lt;link xmlns=\"http://www.w3.org/2005/Atom'&gt;, ... 如果你想知道为什么默认情况下不调用命名空间删除过程，而不是手动调用它，这是因为两个原因，按照相关性的顺序： 删除命名空间需要迭代和修改文档中的所有节点，这对于Scrapy爬取的所有文档来说是一个相当昂贵的操作 可能有一些情况下，实际上需要使用命名空间，以防某些元素名称在命名空间之间冲突。这些情况非常罕见。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——编写Spider爬取伯乐在线所有文章","slug":"scrapy-jobbole-spider","date":"2017-04-26T06:18:54.000Z","updated":"2017-04-26T07:58:33.000Z","comments":true,"path":"2017/04/26/scrapy-jobbole-spider/","link":"","permalink":"http://yoursite.com/2017/04/26/scrapy-jobbole-spider/","excerpt":"仍然是以 http://blog.jobbole.com/all-posts/ 页面为例","text":"仍然是以 http://blog.jobbole.com/all-posts/ 页面为例 提取文章列表页首页使用CSS选择器获取页面中的文章url列表： 1post_urls = response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract() 123456789101112131415161718192021&gt;&gt;&gt; response.css(&quot;#archive .floated-thumb .post-thumb a::attr(href)&quot;).extract()&gt;&gt;&gt;[&apos;http://blog.jobbole.com/111005/&apos;, &apos;http://blog.jobbole.com/108468/&apos;, &apos;http://blog.jobbole.com/110975/&apos;, &apos;http://blog.jobbole.com/110986/&apos;, &apos;http://blog.jobbole.com/110957/&apos;, &apos;http://blog.jobbole.com/110976/&apos;, &apos;http://blog.jobbole.com/110923/&apos;, &apos;http://blog.jobbole.com/110962/&apos;, &apos;http://blog.jobbole.com/110958/&apos;, &apos;http://blog.jobbole.com/110140/&apos;, &apos;http://blog.jobbole.com/110939/&apos;, &apos;http://blog.jobbole.com/110941/&apos;, &apos;http://blog.jobbole.com/110931/&apos;, &apos;http://blog.jobbole.com/110934/&apos;, &apos;http://blog.jobbole.com/110929/&apos;, &apos;http://blog.jobbole.com/110835/&apos;, &apos;http://blog.jobbole.com/110906/&apos;, &apos;http://blog.jobbole.com/110916/&apos;, &apos;http://blog.jobbole.com/110913/&apos;, &apos;http://blog.jobbole.com/110903/&apos;] 先在Spider头部引入from scrapy.http import Request，使用Request进行对文章url列表获取函数的调用 123456789101112def parse(self, response): \"\"\" 1. 获取文章列表页中的文章url并交给解析函数进行具体字段的解析 2. 获取下一页的url并交给scrapy进行下载 :param response: :return: \"\"\" # 解析列表页中的所有文章url并交给解析函数进行具体字段的解析 post_urls = response.css(\"#archive .floated-thumb .post-thumb a::attr(href)\").extract() for post_url in post_urls: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 其中，最后 yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 是对每个url调用parse_detail方法进行字段解析，这里url的参数是带有完整域名的格式，如果不是完整域名，则需要对域名进行拼接成完成域名进行解析。首先要引入 from urllib import parse ，通过parse自带的 parse.urljoin() 进行拼接，代码为： yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail) 。 循环获取下一个列表页 每一个列表页都有“下一页”链接，我们通过CSS选择器来获取下一页的链接，然后交给parse函数进行循环解析。 1234# 提取下一页并交给scrapy进行下载next_url = response.css(\".next.page-numbers::attr(href)\").extract_first()if next_url: yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) 其中，extract_first()方法与extract()[0]用法相同，都是提取第一个字符串元素。 解析函数如下： 12345678910111213141516171819202122def parse_detail(self, response): # 通过css选择器提取字段 title = response.css(\".entry-header h1::text\").extract()[0] create_date = response.css(\"p.entry-meta-hide-on-mobile::text\").extract()[0].strip().replace(\"·\", \"\").strip() praise_nums = int(response.css(\".vote-post-up h10::text\").extract()[0]) fav_nums = response.css(\".bookmark-btn::text\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css(\"a[href='#article-comment'] span::text\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css(\"div.entry\").extract()[0] tag_list = response.css(\"p.entry-meta-hide-on-mobile a::text\").extract() tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")] tags = \",\".join(tag_list) print(title, create_date, fav_nums, comment_nums, tags) 运行结果","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——CSS选择器","slug":"css-selector","date":"2017-04-25T12:18:54.000Z","updated":"2017-04-25T13:13:49.000Z","comments":true,"path":"2017/04/25/css-selector/","link":"","permalink":"http://yoursite.com/2017/04/25/css-selector/","excerpt":"CSS选择器的用法CSS选择器简介在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。","text":"CSS选择器的用法CSS选择器简介在 CSS 中，选择器是一种模式，用于选择需要添加样式的元素。 常用CSS选择器介绍 表达式 说明 * 选择所有节点 #container 选择id为container的节点 .container 选择所有class包含container的节点 li a 选取所有li下的所有a节点 ul + p 选择ul后面的第一个p元素 div#container &gt; ul 选取id为container的div的第一个ul子元素 ul ~ p 选取与ul相邻的所有p元素 a[title] 选取所有有title属性的a元素 a[href=”http://163.com“] 选取所有href属性为163的a元素 a[href*=”163”] 选取所有href属性包含163的a元素 a[href^=”http”] 选取所有href属性以http开头的a元素 a[href$=”.jpg”] 选取所有href以.jpg结尾的a元素 input[type=radio]:checked 选择选中的radio的元素 div:not(#container) 选取所有id非container的div属性 li:nth-child(3) 选取第三个li元素 tr:nth-child(2n) 第偶数个tr Scrapy中CSS选择器用法示例仍然是用Xpath用法示例中的例子来进行测试 获取标题 12&gt;&gt;&gt; response.css(&quot;.entry-header h1::text&quot;).extract()[0]&apos;2016 腾讯软件开发面试题（部分）&apos; 注意：这里获取文字内容的方法为::text，而不是text()。 获取文章发布时间 12&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile::text&quot;).extract()[0].strip().replace(&quot;·&quot;, &quot;&quot;).strip()&apos;2017/02/18&apos; 获取点赞数、收藏数、评论数 1234567891011# 点赞数&gt;&gt;&gt; response.css(&quot;.vote-post-up h10::text&quot;).extract()[0]&apos;2&apos;# 收藏数，获取之后需要用正则表达式进行清洗&gt;&gt;&gt; response.css(&quot;.bookmark-btn::text&quot;).extract()[0]&apos; 23 收藏&apos;# 评论数，获取之后需要用正则表达式进行清洗&gt;&gt;&gt; response.css(&quot;a[href=&apos;#article-comment&apos;] span::text&quot;).extract()[0]&apos; 7 评论&apos; 正则表达式清洗收藏数，评论数的逻辑如下 123456789fav_nums = response.css(\".bookmark-btn::text\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", fav_nums)if match_re: fav_nums = int(match_re.group(1)) comment_nums = response.css(\"a[href='#article-comment'] span::text\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", comment_nums)if match_re: comment_nums = int(match_re.group(1)) 获取正文1&gt;&gt;&gt; response.css(&quot;div.entry&quot;).extract()[0] 获取tags 12&gt;&gt;&gt; response.css(&quot;p.entry-meta-hide-on-mobile a::text&quot;).extract()[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;] 然后需要对数据进行清洗 123tag_list = response.css(\"p.entry-meta-hide-on-mobile a::text()\").extract()tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")]tags = \",\".join(tag_list)","categories":[{"name":"CSS","slug":"CSS","permalink":"http://yoursite.com/categories/CSS/"}],"tags":[{"name":"Scrapy，CSS，Python","slug":"Scrapy，CSS，Python","permalink":"http://yoursite.com/tags/Scrapy，CSS，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Spiders","slug":"scrapy-spiders","date":"2017-04-22T06:18:54.000Z","updated":"2017-04-22T11:34:40.000Z","comments":true,"path":"2017/04/22/scrapy-spiders/","link":"","permalink":"http://yoursite.com/2017/04/22/scrapy-spiders/","excerpt":"Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。","text":"Spider 类定义了如何爬取某个（或某些）网站。包括了爬取的动作(例如：是否跟进链接)以及如何从网页的内容中提取结构化数据（爬取 item）。换句话说，Spider 就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。 对 spider 来说，爬取的流程如下： 先初始化请求URL列表，并指定下载后处理response的回调函数。初次请求URL通过 start_urls 指定，调用 start_requests() 产生 Request 对象，然后注册 parse 方法作为回调 在parse回调中解析response并返回字典, Item 对象, Request 对象或它们的迭代对象。 Request 对象还会包含回调函数，之后Scrapy下载完后会被这里注册的回调函数处理。 在回调函数里面，你通过使用选择器（同样可以使用BeautifulSoup,lxml或其他工具）解析页面内容，并生成解析后的结果Item。 最后返回的这些Item通常会被持久化到数据库中(使用Item Pipeline)或者使用Feed exports将其保存到文件中。 虽然该循环对任何类型的 spider 都（多少）适用，但 Scrapy 仍然为了不同的需求提供了多种默认 spider。 之后将讨论这些 spider。 Spider 参数Spider 可以通过接受参数来修改其功能。 spider 参数一般用来定义初始 URL 或者指定限制爬取网站的部分。 您也可以使用其来配置 spider 的任何功能。 在运行 crawl 时添加 -a 可以传递 Spider 参数： 1scrapy crawl myspider -a category=electronics Spider 在构造器（constructor）中获取参数： 123456789import scrapyclass MySpider(scrapy.Spider): name = 'myspider' def __init__(self, category=None, *args, **kwargs): super(MySpider, self).__init__(*args, **kwargs) self.start_urls = ['http://www.example.com/categories/%s' % category] # ... Spider 参数也可以通过 Scrapyd 的 schedule.json API 来传递。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy Items","slug":"scrapy-items","date":"2017-04-21T06:18:54.000Z","updated":"2017-04-21T13:14:25.000Z","comments":true,"path":"2017/04/21/scrapy-items/","link":"","permalink":"http://yoursite.com/2017/04/21/scrapy-items/","excerpt":"爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。 Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。","text":"爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy 提供 Item 类来满足这样的需求。 Item 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的API以及用于声明可用字段的简单语法。 声明 ItemItem 使用简单的 class 定义语法以及 Field 对象来声明。例如: 1234567import scrapyclass ProductItem(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) Scrapy Item 的定义方式与 Django Models 很类似，但是没有 Django 那么多不同的字段类型（Field Type）。 Item 字段（Item Fields）Field 对象指明了每个字段的元数据（metadata）。例如上面例子中 last_updated 中指明了该字段的序列化函数。 您可以为每个字段指明任何类型的元数据。Field 对象对接受的值没有任何限制。也正是因为这个原因，文档也无法提供所有可用的元数据的键（key）参考列表。Field 对象中保存的每个键可以由多个组件使用，并且只有这些组件知道这个键的存在。您可以根据自己的需求，定义使用其他的 Field 键。 设置 Field 对象的主要目的就是在一个地方定义好所有的元数据。一般来说，那些依赖某个字段的组件肯定使用了特定的键（key）。您必须查看组件相关的文档，查看其用了哪些元数据键（metadata key）。 需要注意的是，用来声明 item 的 Field 对象并没有被赋值为 class 的属性。不过您可以通过 Item.fields 属性进 行访问。 与 Item 配合在API这里的操作，和 dict API 非常的相似。 创建 item12345678910&gt;&gt;&gt; import scrapy&gt;&gt;&gt; class ProductItem(scrapy.Item):... name = scrapy.Field()... price = scrapy.Field()... stock = scrapy.Field()... last_updated = scrapy.Field(serializer=str)...&gt;&gt;&gt; product = ProductItem(name='Desktop', price=1000)&gt;&gt;&gt; print(product)&#123;'name': 'Desktop', 'price': 1000&#125; 获取字段的值两种方式： 键值对 get() 123456789101112131415161718192021222324252627282930313233343536373839404142# 已设定key和value值&gt;&gt;&gt; product['name']'Desktop'&gt;&gt;&gt; product.get('name')'Desktop'# 未设定key和value值&gt;&gt;&gt; product['last_updated']Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 59, in __getitem__ return self._values[key]KeyError: 'last_updated' # 默认返回空值&gt;&gt;&gt; product.get('last_updated')# 设定返回值&gt;&gt;&gt; product.get('last_updated', 'not set value')'not set value'# 未声明的字段&gt;&gt;&gt; product['lala']Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 59, in __getitem__ return self._values[key]KeyError: 'lala'&gt;&gt;&gt; product.get('lala', 'not exist')'not exist'# 字段是否被赋值&gt;&gt;&gt; 'name' in productTrue&gt;&gt;&gt; 'last_updated' in productFalse# 字段是否被声明&gt;&gt;&gt; 'last_updated' in product.fieldsTrue&gt;&gt;&gt; 'lala' in product.fieldsFalse 设置字段的值123456789101112# 已经声明的字段&gt;&gt;&gt; product['last_updated'] = 'today'&gt;&gt;&gt; product['last_updated']'today'# 未声明的字段无法赋值&gt;&gt;&gt; product['lala'] = 'test'Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 66, in __setitem__ (self.__class__.__name__, key))KeyError: 'ProductItem does not support field: lala' 获取所有能够获取到的值可以使用 dict API 来获取所有的值: 1234&gt;&gt;&gt; product.keys()dict_keys(['last_updated', 'price', 'name'])&gt;&gt;&gt; product.items()ItemsView(&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125;) 复制 item12345678&gt;&gt;&gt; product2 = ProductItem(product)&gt;&gt;&gt; print(product2)&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125;# 推荐使用第二种方法&gt;&gt;&gt; product3 = product2.copy()&gt;&gt;&gt; print(product3)&#123;'last_updated': 'today', 'name': 'Desktop', 'price': 1000&#125; 根据 item 创建字典(dict)12&gt;&gt;&gt; dict(product)&#123;'price': 1000, 'last_updated': 'today', 'name': 'Desktop'&#125; 根据字典(dict)创建 item12345678910&gt;&gt;&gt; ProductItem(&#123;'name':'laptop pc', 'price':1500&#125;)&#123;'name': 'laptop pc', 'price': 1500&#125;&gt;&gt;&gt; ProductItem(&#123;'name':'laptop pc', 'lala':1500&#125;)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 56, in __init__ self[k] = v File \"/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/item.py\", line 66, in __setitem__ (self.__class__.__name__, key))KeyError: 'ProductItem does not support field: lala' 扩展 Item可以通过继承原始的 Item 来扩展 item(添加更多的字段或者修改某些字段的元数据)。 添加新的字段 123class DiscountedProductItem(Product): discount_percent = scrapy.Field(serializer=str) discount_expiration_date = scrapy.Field() 使用原字段的元数据 123class SpecificProduct(Product): name = scrapy.Field(Product.fields['name'], serializer=my_serializer)#my_serializer 指序列化的类型 上述代码，在保留了原始的元数据值的情况下，添加（或覆盖）了 name 字段的 serializer 。 存在及覆盖，不存在即添加。 Item 对象class scrapy.item.Item([arg]) 返回一个根据给定的参数可选初始化的 item 。 Item 复制了标准化的 dict API ，包括初始化函数也是一样。除此之外，唯一添加的额外属性就是 fields 。 fields 是一个包含了 item 所有声明的字段的字典，而不仅仅是获取到的字段。该字典的 key 是字段（field）的名字，值是 Item 声明中使用到的 Field 对象。 字段（Field）对象class scrapy.item.Field([arg]) Field仅仅是内置的 dict 类的一个别名（继承于 dict ），并没有提供额外的方法或属性。说白了，Field就是完完全全的Python字典，被用来基于类属性的方法支持 Item 声明语法。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Redis学习笔记(六)：数据安全与性能保障——处理系统故障","slug":"Redis-6","date":"2017-04-20T12:18:54.000Z","updated":"2017-04-20T13:46:42.000Z","comments":true,"path":"2017/04/20/Redis-6/","link":"","permalink":"http://yoursite.com/2017/04/20/Redis-6/","excerpt":"如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。","text":"如果我们决定要将Redis用作应用程序唯一的数据存储手段的话，那么就必须确保Redis不会丢失任何数据。跟提供了ACID（原子性atomicity，一致性consistency，隔离性isolation，耐久性durability，如果一个数据库想要实现可靠的数据事务，那么它就必须保证 ACID 性质）保证的传统关系数据库不同，在使用Redis为后端构建应用程序的时候，我们需要多做一些工作才能保证数据的一致性。 验证快照文件和 AOF 文件无论时快照持久化还是AOF持久化，都提供了在遇到系统故障时进行数据回复的工具。Redis提供了两个命令行程序 redis-check-aof 和 redis-check-rdb(redis-check-dump was renamed to redis-check-rdb in redis version 3.2) ，它们可以在系统故障发生之后，检查AOF文件和快照文件的状态，并在有需要的情况下对文件进行修复。 在不给定任何参数的情况下运行这两个程序，就可以看见它们的基本使用方法： 1234$ redis-check-rdbUsage: redis-check-rdb &lt;rdb-file-name&gt;$ redis-check-dumpUsage: redis-check-dump &lt;dump.rdb&gt; 如果运行 redis-check-aof 程序时给了 --fix 参数，那么会对AOF文件进行修复。修复方法非常简单：扫描给定的 AOF 文件，寻找不正确或不完整的命令，当发现第一个出错命令的时候，程序会删除出错的命令以及位于出错命令之后的所有命令。在大多数情况下，被删除的都是 AOF 文件末尾的不完整的写命令。遗憾的是，目前没有办法修复出错的快照文件。尽管发现快照文件首个出现错误的地方是有可能的，但因为快照文件本身经过了压缩，而出现在快照文件中间的错误有可能会导致快照文件的剩余部分无法读取。因此，最好为重要的快照文件保留多个备份，并在进行数据恢复时，通过计算快照文件的 SHA1 散列值和 SHA256 散列值来对内容进行验证。 更换故障主服务器我们来看一下在拥有一个主服务器和一个从服务器的情况下，更换主服务器的具体步骤。假设A、B两台机器都运行着 Redis ，机器A为 master ，机器B为 slave 。机器A因为暂时无法修复的故障而断开了连接，因此决定将同样安装了 Redis 的机器 C 用作新的主服务器。 更换服务器的计划非常简单：首先向机器B发送一个 SAVE 命令，让它创建一个新的快照文件，接着将这个快照文件发送给机器C，并在机器 C 上面启动 Redis 。最后，让B成为机器C的从服务器。由于环境有限，就在同一台机器上用不同的端口进行测试，下面进行演示： 先进入 Redis 安装位置，再安装两个 Redis 服务并分别修改配置文件 redis.conf 中的 port 为6380和6381 12345678$ cd /usr/local$ sudo cp -r redis redis6380Password:$ sudo chmod -R 777 redis6380$ vim redis6380/redis.conf$ sudo cp -r redis redis6381$ sudo chmod -R 777 redis6381$ vim redis6381/redis.conf 启动机器A端口为6379，机器B端口为6380，并让B成为A的从服务器 123456789101112# 启动A$ cd redis$ ./src/redis-server redis.conf# 启动B$ cd redis6380$ ./src/redis-server redis.conf# 让B成为A的从服务器$ $ redis-cli -h localhost -p 6380localhost:6380&gt; SLAVEOF localhost 6379OK 停止机器A的 Redis 服务，此时只剩 Redis 从服务器B在运行 向机器B发送 SAVE 命令 12localhost:6380&gt; SAVEOK 将机器B的快照文件复制到机器C的对应目录，并启动 Redis 服务 123$ cp -f /usr/local/redis6380/dump.rdb /usr/local/redis6381$ cd /usr/local/redis6381$ ./src/redis-server redis.conf 让机器B成为机器C的从服务器 12localhost:6380&gt; SLAVEOF localhost 6381OK 测试机器B是否能从机器C同步数据 123456$ redis-cli -h localhost -p 6381localhost:6381&gt; set key new-masterOK$ redis-cli -h localhost -p 6380localhost:6380&gt; get key&apos;new-master&apos; 另一种创建新的主服务器的方法，就是将从服务器升级（turn）为主服务器，并为升级后的主服务器创建从服务器。 以上两种方法都可以让 Redis 回到之前的一个主服务器和一个从服务器的状态，而用户接下来需要做的就是更新客户端的配置，让它们去读写正确的服务器。除此之外，如果用户需要重启 Redis 的话，那么可能还需要对服务器的持久化配置进行更新。 Redis Sentinel可以监视指定的Redis主服务器及其下属的从服务器，并在主服务器下线时自动进行故障转移(failover)。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy命令行工具","slug":"scrapy-command-line-tools","date":"2017-04-20T06:18:54.000Z","updated":"2017-04-20T07:53:41.000Z","comments":true,"path":"2017/04/20/scrapy-command-line-tools/","link":"","permalink":"http://yoursite.com/2017/04/20/scrapy-command-line-tools/","excerpt":"Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。","text":"Scrapy 是通过 scrapy 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区 分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。 Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。 使用 scrapy 工具创建项目一般来说，使用 scrapy 工具的第一件事就是创建 Scrapy 项目： 1scrapy startproject myproject 该命令将会在 myproject 目录中创建一个 Scrapy 项目。 接下来，进入到项目目录中: 1cd myproject 这时候就可以使用 scrapy 命令来管理和控制项目了。 控制项目创建一个新的 spider： 1scrapy genspider mydomain mydomain.com Scrapy 提供了两种类型的命令。一种必须在 Scrapy 项目中运行(针对项目(Project-specific)的命令)，另外一种则不需要(全局命令)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。 123456789101112131415161718# 全局命令(不需要在项目中运行)startprojectsettingsrunspidershellfetchviewversion# 项目(Project-only)命令(必须在项目中运行)crawlchecklisteditparsegenspiderdeploybench 工具命令介绍我们可以通过运行命令来获取关于每个命令的详细内容： 1scrapy &lt;command&gt; -h 也可以查看所有的命令： 1scrapy -h 下面就对这些命令进行介绍。 startproject 语法：scrapy startproject &lt;project_name&gt; 全局命令 在 project_name 文件夹下创建一个名为 project_name 的 Scrapy 项目。 genspider 语法：scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt; 项目命令 在当前项目中创建 spider。这仅仅是创建 spider 的一种快捷方法。该方法可以使用提前定义好的模板来生成 spider。您也可以自己创建 spider 的源码文件。 例子： 12345678910111213141516171819202122232425# 查看模板$ scrapy genspider -lAvailable templates: basic crawl csvfeed xmlfeed# 编辑模板$ scrapy genspider -d basic# -*- coding: utf-8 -*-import scrapyclass $classname(scrapy.Spider): name = &quot;$name&quot; allowed_domains = [&quot;$domain&quot;] start_urls = [&apos;http://$domain/&apos;] def parse(self, response): pass # 根据模板来生成spider$ scrapy genspider -t basic example example.com Created spider &apos;example&apos; using template &apos;basic&apos; in module: tutorial.spiders.example crawl 语法：scrapy crawl myspider 项目命令 使用 spider 进行爬取。 例子： 12$ scrapy crawl myspider [ ... myspider starts crawling ... ] check 语法：scrapy check [-l] &lt;spider&gt; 项目命令 运行 contract 检查。 例子： 1234567891011121314$ scrapy check -l first_spider * parse * parse_item second_spider * parse * parse_item$ scrapy check [FAILED] first_spider:parse_item &gt;&gt;&gt; &apos;RetailPricex&apos; field is missing[FAILED] first_spider:parse &gt;&gt;&gt; Returned 92 requests, expected 0..4 list 语法：scrapy list 项目命令 列出当前项目中所有可用的 spider。每行输出一个 spider。 例子： 123$ scrapy list spider1 spider2 edit 语法：scrapy edit &lt;spider&gt; 项目命令 使用 EDITOR 中设定的编辑器编辑给定的 spider 该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者 IDE 来编写调试 spider。 fetch 语法：scrapy fetch &lt;url&gt; 全局命令 使用 Scrapy 下载器(downloader)下载给定的 URL，并将获取到的内容送到标准输出。 该命令以 spider 下载页面的方式获取页面。例如，如果 spider 有 USER_AGENT 属性修改了 User Agen t，该命令将会使用该属性。 因此，您可以使用该命令来查看 spider 如何获取某个特定页面。 该命令如果非项目中运行则会使用默认 Scrapy downloader 设定。 例子： 12345678910111213$ scrapy fetch --nolog http://www.example.com/some/page.html [ ... html content here ... ]$ scrapy fetch --nolog --headers http://www.example.com/ &#123;&apos;Accept-Ranges&apos;: [&apos;bytes&apos;],&apos;Age&apos;: [&apos;1263 &apos;], &apos;Connection&apos;: [&apos;close &apos;], &apos;Content-Length&apos;: [&apos;596&apos;], &apos;Content-Type&apos;: [&apos;text/html; charset=UTF-8&apos;], &apos;Date&apos;: [&apos;Wed, 18 Aug 2010 23:59:46 GMT&apos;], &apos;Etag&apos;: [&apos;&quot;573c1-254-48c9c87349680&quot;&apos;], &apos;Last-Modified&apos;: [&apos;Fri, 30 Jul 2010 15:30:18 GMT&apos;], &apos;Server&apos;: [&apos;Apache/2.2.3 (CentOS)&apos;]&#125; view 语法：scrapy view &lt;url&gt; 全局命令 在浏览器中打开给定的 URL，并以 Scrapy spider 获取到的形式展现。 有些时候 spider 获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查 spider 所获取到的页面，并确认这是您所期望的。 例子： 12$ scrapy view http://www.example.com/some/page.html [ ... browser starts ... ] shell 语法：scrapy shell [url] 全局命令 以给定的 URL(如果给出)或者空(没有给出 URL)启动 Scrapy shell。查看 Scrapy 终端(Scrapy shell) 获取更多信息。 例子： 12$ scrapy shell http://www.example.com/some/page.html [ ... scrapy shell starts ... ] parse 语法：scrapy parse &lt;url&gt; [options] 项目命令 获取给定的 URL 并使用相应的 spider 分析处理。如果提供 --callback 选项，则使用 spider 的该方法处理，否则使用 parse 。 settings 语法：scrapy settings [option] 全局命令 获取 Scrapy 的设定 在项目中运行时，该命令将会输出项目的设定值，否则输出 Scrapy 默认设定。 例子： 1234$ scrapy settings --get BOT_NAME scrapybot $ scrapy settings --get DOWNLOAD_DELAY 0 runspider 语法：scrapy runspider &lt;spider_file.py&gt; 全局命令 在未创建项目的情况下，运行一个编写在 Python 文件中的 spider。 例子： 12$ scrapy runspider myspider.py [ ... spider starts crawling ... ] version 语法：scrapy version [-v] 全局命令 输出 Scrapy 版本。配合 -v 运行时，该命令同时输出 Python，Twisted 以及平台的信息，方便 bug 提交。 例子： 1234567891011$ scrapy version -vScrapy : 1.3.3lxml : 3.7.3.0libxml2 : 2.9.4cssselect : 1.0.1parsel : 1.1.0w3lib : 1.17.0Twisted : 17.1.0Python : 3.5.2 (default, Oct 11 2016, 04:59:56) - [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)]pyOpenSSL : 16.2.0 (OpenSSL 1.1.0e 16 Feb 2017)Platform : Darwin-16.6.0-x86_64-i386-64bit deploy 语法：scrapy deploy [ &lt;target:project&gt; | -l &lt;target&gt; | -L ] 项目命令 将项目部署到 Scrapyd 服务。 bench 语法：scrapy bench 全局命令 运行 benchmark 测试。Benchmarking。 自定义项目命令您也可以通过 COMMANDS_MODULE 来添加您自己的项目命令。您可以以 scrapy/commands 中 Scrapy commands 为例来了解如何实现您的命令。 COMMANDS_MODULE1Default: &apos;&apos; (empty string) 用于查找添加自定义 Scrapy 命令的模块。 例子： 1COMMANDS_MODULE = &apos;mybot.commands&apos;","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法示例","slug":"xpath-example","date":"2017-04-16T12:18:54.000Z","updated":"2017-04-16T10:26:54.000Z","comments":true,"path":"2017/04/16/xpath-example/","link":"","permalink":"http://yoursite.com/2017/04/16/xpath-example/","excerpt":"Scrapy 中 XPath 获取相应内容为了方便调试，在终端下输入以下命令进入Scrapy shell： 1scrapy shell &apos;http://blog.jobbole.com/110287&apos;","text":"Scrapy 中 XPath 获取相应内容为了方便调试，在终端下输入以下命令进入Scrapy shell： 1scrapy shell &apos;http://blog.jobbole.com/110287&apos; 获取标题 1234&gt;&gt;&gt; response.xpath(&quot;//div[@class=&apos;entry-header&apos;]/h1/text()&quot;).extract()[0]&apos;2016 腾讯软件开发面试题（部分）&apos;&gt;&gt;&gt; response.xpath(&apos;//*[@id=&quot;post-110287&quot;]/div[1]/h1/text()&apos;).extract()[0]&apos;2016 腾讯软件开发面试题（部分）&apos; 以上两种方法都可以得到文章标题，第一种方法通过标题class的属性得到，第二种方法通过确定id，然后通过列表切片得到标题字符串。 获得文章发布时间 1234&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0]&apos;\\r\\n\\r\\n 2017/02/18 · &apos;&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/text()&quot;).extract()[0].strip().replace(&apos;·&apos;, &apos;&apos;).strip()&apos;2017/02/18&apos; 第一条命令只能获取p标签中的内容，还需要对获取的数据用 strip() 和 replace() 方法进行清洗。 获取点赞数、收藏数、评论数 对于含有多个属性的class如：class=&quot; btn-bluet-bigger href-style vote-post-up register-user-only &quot;，若只使用其中的一个属性得到值，可以使用contains。 获取点赞数12&gt;&gt;&gt; int(response.xpath(&quot;//span[contains(@class, &apos;vote-post-up&apos;)]/h10/text()&quot;).extract()[0])2 获取收藏数、评论数12&gt;&gt;&gt; response.xpath(&quot;//span[contains(@class, &apos;bookmark-btn&apos;)]/text()&quot;).extract()[0]&apos; 23 收藏&apos; 得到的内容为’ 23 收藏’，需要使用正则表达式进行数据清洗。 1234fav_nums = response.xpath(\"//span[contains(@class, 'bookmark-btn')]/text()\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", fav_nums)if match_re: fav_nums = int(match_re.group(1)) 同样的，评论数的获取也需要正则表达式的帮忙。 1234comment_nums = response.xpath(\"//a[@href='#article-comment']/span/text()\").extract()[0]match_re = re.match(\".*?(\\d+).*?\", comment_nums)if match_re: comment_nums = int(match_re.group(1)) 获取正文1content = response.xpath(\"//div[@class='entry']\").extract()[0] 获取tags 所有的tag都在a标签下，类似获得日期的方式，增加一个a标签路径即可。 12&gt;&gt;&gt; response.xpath(&quot;//p[@class=&apos;entry-meta-hide-on-mobile&apos;]/a/text()&quot;).extract()[&apos;职场&apos;, &apos; 7 评论 &apos;, &apos;面试&apos;] 现在需要对数据进行清洗，去除评论标签。 123tag_list = response.xpath(\"//p[@class='entry-meta-hide-on-mobile']/a/text()\").extract()tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")]tags = \",\".join(tag_list) 总结最后我们构造的spider文件如下： 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import scrapyimport reclass JobboleSpider(scrapy.Spider): name = \"jobbole\" allowed_domains = [\"blog.jobbole.com\"] start_urls = ['http://blog.jobbole.com/110287'] def parse(self, response): title = response.xpath(\"//div[@class='entry-header']/h1/text()\").extract()[0] create_date = response.xpath(\"//p[@class='entry-meta-hide-on-mobile']/text()\").extract()[0].strip().replace(\"·\", \"\").strip() praise_nums = int(response.xpath(\"//span[contains(@class, 'vote-post-up')]/h10/text()\").extract()[0]) fav_nums = response.xpath(\"//span[contains(@class, 'bookmark-btn')]/text()\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", fav_nums) if match_re: fav_nums = int(match_re.group(1)) comment_nums = response.xpath(\"//a[@href='#article-comment']/span/text()\").extract()[0] match_re = re.match(\".*?(\\d+).*?\", comment_nums) if match_re: comment_nums = int(match_re.group(1)) content = response.xpath(\"//div[@class='entry']\").extract()[0] tag_list = response.xpath(\"//p[@class='entry-meta-hide-on-mobile']/a/text()\").extract() tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")] tags = \",\".join(tag_list)","categories":[{"name":"XPath","slug":"XPath","permalink":"http://yoursite.com/categories/XPath/"}],"tags":[{"name":"Scrapy，XPath，Python","slug":"Scrapy，XPath，Python","permalink":"http://yoursite.com/tags/Scrapy，XPath，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy简单入门","slug":"scrapy-simple-intro","date":"2017-04-16T06:18:54.000Z","updated":"2017-04-16T07:13:21.000Z","comments":true,"path":"2017/04/16/scrapy-simple-intro/","link":"","permalink":"http://yoursite.com/2017/04/16/scrapy-simple-intro/","excerpt":"创建项目开始爬取前，首先需要创建一个新的Scrapy项目 1scrapy startproject tutorial","text":"创建项目开始爬取前，首先需要创建一个新的Scrapy项目 1scrapy startproject tutorial 该命令将会创建包含下列内容的 tutorial 目录: 12345678910tutorial/ scrapy.cfg tutorial/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... 这些文件分别是: scrapy.cfg：项目的配置文件 tutorial/：该项目的 python 模块，之后我们将在此加入代码。 tutorial/items.py：项目中的 item 文件。 tutorial/pipelines.py：项目中的 pipelines 文件。 tutorial/settings.py：项目的设置文件。 tutorial/spiders/：放置 spider 代码的目录。 定义 ItemItem 是保存爬取到的数据的容器；其使用方法和 python 字典类似， 并且提供了额外保护机制来避免拼写错误导 致的未定义字段错误。 类似在 ORM 中做的一样，您可以通过创建一个scrapy.Item类， 并且定义类型为scrapy.Field的类属性来定义一个 Item。 (如果不了解 ORM, 不用担心，您会发现这个步骤非常简单) 首先根据需要从 dmoz.org 获取到的数据对 item 进行建模。 我们需要从 dmoz 中获取名字，url，以及网站的描 述。 对此，在 item 中定义相应的字段。编辑tutorial目录中的items.py文件: 123456import scrapyclass DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() 可能一开始这有些复杂，但是通过定义 item， 我们可以很方便的使用 Scrapy 的其他方法，而这些方法需要知道我们的 item 的定义。 编写第一个爬虫Spider 是用户编写用于从单个网站(或者一些网站)爬取数据的类。 其包含了一个用于下载的初始 URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方 法。 为了创建一个 Spider，我们必须继承scrapy.Spider类， 且定义以下三个属性: name : 用于区别 Spider。 该名字必须是唯一的，您不可以为不同的 Spider 设定相同的名字。 start_urls : 包含了 Spider 在启动时进行爬取的 url 列表。 因此，第一个被获取到的页面将是其中之一。 后续的 URL 则从初始的 URL 获取到的数据中提取。 parse() : spider 的一个方法。 被调用时，每个初始 URL 完成下载后生成的Response对象将会作为 唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成 item)以及生成需 要进一步处理的 URL 的Request对象。 以下为我们的第一个 Spider 代码，保存在tutorial/spiders目录下的dmoz_spider.py文件中: 1234567891011121314import scrapyclass DmozSpider(scrapy.Spider): name = \"dmoz\" allow_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\" ] def parse(self, response): filename = response.url.split(\"/\")[-2] with open(filename, 'wb') as f: f.write(response.body) 其中，allow_domains是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页。 爬取进入项目的根目录，执行以下命令启动spider 1scrapy crawl dmoz crawl dmoz启动用于爬取dmoz.org的 spider，可以得到如下输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152532017-04-15 21:51:39 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)2017-04-15 21:51:39 [scrapy.utils.log] INFO: Overridden settings: &#123;&apos;BOT_NAME&apos;: &apos;tutorial&apos;, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;]&#125;2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled extensions:[&apos;scrapy.extensions.corestats.CoreStats&apos;, &apos;scrapy.extensions.logstats.LogStats&apos;, &apos;scrapy.extensions.telnet.TelnetConsole&apos;]2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;, &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;, &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;, &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;, &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;, &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;, &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;, &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;, &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;, &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;, &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]2017-04-15 21:51:39 [scrapy.middleware] INFO: Enabled item pipelines:[]2017-04-15 21:51:39 [scrapy.core.engine] INFO: Spider opened2017-04-15 21:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2017-04-15 21:51:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;: HTTP status code is not handled or not allowed2017-04-15 21:51:41 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)2017-04-15 21:51:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt;: HTTP status code is not handled or not allowed2017-04-15 21:51:41 [scrapy.core.engine] INFO: Closing spider (finished)2017-04-15 21:51:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:&#123;&apos;downloader/request_bytes&apos;: 734, &apos;downloader/request_count&apos;: 3, &apos;downloader/request_method_count/GET&apos;: 3, &apos;downloader/response_bytes&apos;: 3525, &apos;downloader/response_count&apos;: 3, &apos;downloader/response_status_count/403&apos;: 3, &apos;finish_reason&apos;: &apos;finished&apos;, &apos;finish_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 41, 968931), &apos;log_count/DEBUG&apos;: 4, &apos;log_count/INFO&apos;: 9, &apos;response_received_count&apos;: 3, &apos;scheduler/dequeued&apos;: 2, &apos;scheduler/dequeued/memory&apos;: 2, &apos;scheduler/enqueued&apos;: 2, &apos;scheduler/enqueued/memory&apos;: 2, &apos;start_time&apos;: datetime.datetime(2017, 4, 15, 13, 51, 39, 764494)&#125;2017-04-15 21:51:41 [scrapy.core.engine] INFO: Spider closed (finished) 查看包含[dmoz]的输出，可以看到输出的 log 中包含定义在start_urls的初始 URL，并且与 spider 中是一 一对应的。在 log 中可以看到其没有指向其他页面( (referer:None) )。 除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含 url 所对应的内容的文件被创建 了: Book，Resources 。 发生了什么？Scrapy 为 Spider 的start_urls属性中的每个 URL 创建了scrapy.Request对象，并将parse方法作为回调函数(callback)赋值给了Request。 Request对象经过调度，执行生成scrapy.http.Response对象并送回给spider parse()方法。 提取 ItemSelectors 选择器简介从网页中提取数据有很多方法。Scrapy 使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors。关于 selector 和其他提取机制的信息请参考Selector文档。 关于Xpath的简单使用方法，可以查看之前的一篇博客Python分布式爬虫打造搜索引擎项目学习笔记——Xpath用法 为了配合 XPath，Scrapy 除了提供了Selector之外，还提供了方法来避免每次从 response 中提取数据时生成 selector 的麻烦。 Selector 有四个基本的方法： xpath()：传入 xpath 表达式，返回该表达式所对应的所有节点的 selector list 列表 。 css()：传入 CSS 表达式，返回该表达式所对应的所有节点的 selector list 列表。 extract()：序列化该节点为 unicode 字符串并返回 list。 re()：根据传入的正则表达式对数据进行提取，返回 unicode 字符串 list 列表。 在 Shell 中尝试 Selector 选择器为了介绍 Selector 的使用方法，接下来我们将要使用内置的 Scrapy shell。Scrapy Shell 需要我们预装好 IPython(一个扩展的 Python 终端)。 我们需要进入项目的根目录，执行下列命令来启动 shell: 1scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot; Shell 的输出类似于： 12345678910111213141516172017-04-15 22:04:22 [scrapy.core.engine] INFO: Spider opened2017-04-15 22:04:23 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)2017-04-15 22:04:24 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)[s] Available Scrapy objects:[s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s] crawler &lt;scrapy.crawler.Crawler object at 0x109728ac8&gt;[s] item &#123;&#125;[s] request &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;[s] response &lt;403 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;[s] settings &lt;scrapy.settings.Settings object at 0x10a2a0a58&gt;[s] spider &lt;DefaultSpider &apos;default&apos; at 0x10a4dc3c8&gt;[s] Useful shortcuts:[s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)[s] fetch(req) Fetch a scrapy.Request and update local objects[s] shelp() Shell help (print this help)[s] view(response) View response in a browser&gt;&gt;&gt; 当 shell 载入后，我们将得到一个包含 response 数据的本地 response 变量。输入 response.body 将输出 resp onse 的包体，输出 response.headers 可以看到 response 的包头。 更为重要的是，当输入 response.selector 时， 我们将获取到一个可以用于查询返回数据的 selector(选择器)， 以及映射到 response.selector.xpath() 、response.selector.css() 的 快捷方法(shortcut): response.xpat h() 和 response.css() 。 下面就来试试： 12345678&gt;&gt;&gt; response.xpath(&apos;//title&apos;)[&lt;Selector xpath=&apos;//title&apos; data=&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;&gt;]&gt;&gt;&gt; response.xpath(&apos;//title&apos;).extract()[&apos;&lt;title&gt;DMOZ&lt;/title&gt;&apos;]&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;)[&lt;Selector xpath=&apos;//title/text()&apos; data=&apos;DMOZ&apos;&gt;]&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;).extract()[&apos;DMOZ&apos;] 提取数据现在，我们来尝试从这些页面中提取些有用的数据。 我们可以在终端中输入 response.body 来观察 HTML 源码并确定合适的 XPath 表达式。不过，这任务非常无聊且不易。您可以考虑使用 Firefox 的 Firebug 扩展来使得工作更为轻松。 在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 元素中。 我们可以通过这段代码选择该页面中网站列表里所有 元素: 1response.xpath(&apos;//ul/li&apos;) 网站的描述： 1response.xpath(&apos;//ul/li/text()&apos;).extract() 网站的标题： 1response.xpath(&apos;//ul/li/a/text()&apos;).extract() 以及网站的链接： 1response.xpath(&apos;//ul/li/a/@href&apos;).extract() 之前提到过，每个 .xpath() 调用返回 selector 组成的 list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性: 12345for response in response.xpath('//ul/li'): title = response.xpath('a/text()').extract() link = response.xpath('a/@href').extract() desc = response.xpath('text()').extract() print(title, link, desc) 在我们的 spider 中加入如下代码： 123456789101112131415import scrapyclass DmozSpider(scrapy.Spider): name = \"dmoz\" allow_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"] def parse(self, response): for response in response.xpath('//ul/li'): title = response.xpath('a/text()').extract() link = response.xpath('a/@href').extract() desc = response.xpath('text()').extract() print(title, link, desc) 现在尝试再次爬取 dmoz.org，您将看到爬取到的网站信息被成功输出: 1scrapy crawl dmoz 使用 ItemItem 对象是自定义的 python 字典。您可以使用标准的字典语法来获取到其每个字段的值。(字段就是我们之前用 Field 赋值的属性): 1234&gt;&gt;&gt; item = DmozItem() &gt;&gt;&gt; item[&apos;title&apos;] = &apos;Example title&apos; &gt;&gt;&gt; item[&apos;title&apos;] &apos;Example title&apos; 一般来说，Spider 将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是: 1234567891011121314151617181920import scrapyfrom tutorial.items import DmozItemclass DmozSpider(scrapy.Spider): name = \"dmoz\" allow_domains = [\"dmoz.org\"] start_urls = [ \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\", \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\" ] def parse(self, response): for response in response.xpath('//ul/li'): item = DmozItem() item['title'] = response.xpath('a/text()').extract() item['link'] = response.xpath('a/@href').extract() item['desc'] = response.xpath('text()').extract() yield item 现在对 dmoz.org 进行爬取将会产生 DmozItem 对象: 1[dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author webs &apos;link&apos;: [u&apos;http://gnosis.cx/TPiP/&apos;], &apos;title&apos;: [u&apos;Text Processing in Python&apos;]&#125; [dmoz] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; &#123;&apos;desc&apos;: [u&apos; - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applic &apos;link&apos;: [u&apos;http://www.informit.com/store/product.aspx?isbn=0130211192&apos;], &apos;title&apos;: [u&apos;XML Processing with Python&apos;]&#125; 保存爬取到的数据最简单存储爬取的数据的方式是使用 Feed exports : 1scrapy crawl dmoz -o items.json 该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。 在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。 如果需要对爬取到的 item 做更多更为复杂的 操作，您可以编写 Item Pipeline 。 类似于我们在创建项目时对 Item 做的，用于您编写自己的 tutorial/pipelines.py 也被创建。 不过如果您仅仅想要保存 item，您不需要实现任何的 pipeline。","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"python3+Django配置mysql连接","slug":"django-py3-mysql","date":"2017-04-14T07:18:54.000Z","updated":"2017-04-14T07:24:58.000Z","comments":true,"path":"2017/04/14/django-py3-mysql/","link":"","permalink":"http://yoursite.com/2017/04/14/django-py3-mysql/","excerpt":"","text":"之前一直使用的是Django1.9和Python2.7，现在使用Python3和Django1.10，发现Mysql-Python一直无法安装 这是因为mysql官网上的版本只支持Python3.4的数据库驱动，所以Python3.5是安装不上相应的驱动的，可以使用pymysql。在虚拟环境下pip install pymysql就可以了，然后在项目目录下的__init__.py文件中添加 12import pymysqlpymysql.install_as_mysqldb() 就可以代替Django默认使用的MySQLdb了。","categories":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/categories/Django/"}],"tags":[{"name":"Django，Mysql，Python","slug":"Django，Mysql，Python","permalink":"http://yoursite.com/tags/Django，Mysql，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Python字符编码","slug":"python-str-encode","date":"2017-04-12T12:18:54.000Z","updated":"2017-04-12T07:05:23.000Z","comments":true,"path":"2017/04/12/python-str-encode/","link":"","permalink":"http://yoursite.com/2017/04/12/python-str-encode/","excerpt":"Python字符编码字符编码是计算机编程中不可回避的问题，不管你用 Python2 还是 Python3，亦或是 C++, Java 等，我都觉得非常有必要理清计算机中的字符编码概念。","text":"Python字符编码字符编码是计算机编程中不可回避的问题，不管你用 Python2 还是 Python3，亦或是 C++, Java 等，我都觉得非常有必要理清计算机中的字符编码概念。 基本概念 字符（Character） 在电脑和电信领域中，字符是一个信息单位，它是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。比如，一个汉字，一个英文字母，一个标点符号等都是一个字符。 字符集（Character set） 字符集是字符的集合。字符集的种类较多，每个字符集包含的字符个数也不同。比如，常见的字符集有 ASCII 字符集、GB2312 字符集、Unicode 字符集等，其中，ASCII 字符集共有 128 个字符，包含可显示字符（比如英文大小写字符、阿拉伯数字）和控制字符（比如空格键、回车键）；GB2312 字符集是中国国家标准的简体中文字符集，包含简化汉字、一般符号、数字等；Unicode 字符集则包含了世界各国语言中使用到的所有字符。 字符编码（Character encoding） 字符编码，是指对于字符集中的字符，将其编码为特定的二进制数，以便计算机处理。常见的字符编码有 ASCII 编码，UTF-8 编码，GBK 编码等。一般而言，字符集和字符编码往往被认为是同义的概念，比如，对于字符集 ASCII，它除了有「字符的集合」这层含义外，同时也包含了「编码」的含义，也就是说，ASCII 既表示了字符集也表示了对应的字符编码。 下面我们用一个表格做下总结： 概念 描述 实例 字符 一个信息单位，各种文字和符号的总称 ‘中’, ‘a’, ‘1’, ‘$’, ‘￥’, … 字符集 字符的集合 ASCII 字符集, GB2312 字符集, Unicode 字符集 字符编码 将字符集中的字符，编码为特定的二进制数 ASCII 编码，GB2312 编码，Unicode 编码 字节 计算机中存储数据的单元，一个 8 位（bit）的二进制数 0x01, 0x45, … 常见字符编码简介常见的字符编码有 ASCII 编码，GBK 编码，Unicode 编码和 UTF-8 编码等等。这里，我们主要介绍 ASCII、Unicode 和 UTF-8。 ASCII 计算机是在美国诞生的，人家用的是英语，而在英语的世界里，不过就是英文字母，数字和一些普通符号的组合而已。 在 20 世纪 60 年代，美国制定了一套字符编码方案，规定了英文字母，数字和一些普通符号跟二进制的转换关系，被称为 ASCII (American Standard Code for Information Interchange，美国信息互换标准编码) 码。 比如，大写英文字母 A 的二进制表示是 01000001（十进制 65），小写英文字母 a 的二进制表示是 01100001 （十进制 97），空格 SPACE 的二进制表示是 00100000（十进制 32）。 Unicode ASCII 码只规定了 128 个字符的编码，这在美国是够用的。可是，计算机后来传到了欧洲，亚洲，乃至世界各地，而世界各国的语言几乎是完全不一样的，用 ASCII 码来表示其他语言是远远不够的，所以，不同的国家和地区又制定了自己的编码方案，比如中国大陆的 GB2312 编码 和 GBK 编码等，日本的 Shift_JIS 编码等等。 虽然各个国家和地区可以制定自己的编码方案，但不同国家和地区的计算机在数据传输的过程中就会出现各种各样的乱码（mojibake），这无疑是个灾难。 怎么办？想法也很简单，就是将全世界所有的语言统一成一套编码方案，这套编码方案就叫 Unicode，它为每种语言的每个字符设定了独一无二的二进制编码，这样就可以跨语言，跨平台进行文本处理了，是不是很棒！ Unicode 1.0 版诞生于 1991 年 10 月，至今它仍在不断增修，每个新版本都会加入更多新的字符，目前最新的版本为 2016 年 6 月 21 日公布的 9.0.0。 Unicode 标准使用十六进制数字，而且在数字前面加上前缀 U+，比如，大写字母「A」的 unicode 编码为 U+0041，汉字「严」的 unicode 编码为 U+4E25。更多的符号对应表，可以查询 unicode.org，或者专门的汉字对应表。 UTF-8 Unicode 看起来已经很完美了，实现了大一统。但是，Unicode 却存在一个很大的问题：资源浪费。 为什么这么说呢？原来，Unicode 为了能表示世界各国所有文字，一开始用两个字节，后来发现两个字节不够用，又用了四个字节。比如，汉字「严」的 unicode 编码是十六进制数 4E25，转换成二进制有十五位，即 100111000100101，因此至少需要两个字节才能表示这个汉字，但是对于其他的字符，就可能需要三个或四个字节，甚至更多。 这时，问题就来了，如果以前的 ASCII 字符集也用这种方式来表示，那岂不是很浪费存储空间。比如，大写字母「A」的二进制编码为 01000001，它只需要一个字节就够了，如果 unicode 统一使用三个字节或四个字节来表示字符，那「A」的二进制编码的前面几个字节就都是 0，这是很浪费存储空间的。 为了解决这个问题，在 Unicode 的基础上，人们实现了 UTF-16, UTF-32 和 UTF-8。下面只说一下 UTF-8。 UTF-8 (8-bit Unicode Transformation Format) 是一种针对 Unicode 的可变长度字符编码，它使用一到四个字节来表示字符，例如，ASCII 字符继续使用一个字节编码，阿拉伯文、希腊文等使用两个字节编码，常用汉字使用三个字节编码，等等。 因此，我们说，UTF-8 是 Unicode 的实现方式之一，其他实现方式还包括 UTF-16（字符用两个或四个字节表示）和 UTF-32（字符用四个字节表示）。 Python的默认编码Python2 的默认编码是 ascii，Python3 的默认编码是 utf-8，可以通过下面的方式获取： Python2 123456Python 2.7.12 (default, Oct 11 2016, 05:20:59)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()&apos;ascii&apos; Python3 123456Python 3.5.2 (default, Oct 11 2016, 04:59:56)[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()&apos;utf-8&apos; Python3编码问题Python3 最重要的一项改进之一就是解决了 Python2 中字符串与字符编码遗留下来的这个大坑。 Python2 字符串设计上的一些缺陷： 使用 ASCII 码作为默认编码方式，对中文处理很不友好。 把字符串的牵强地分为 unicode 和 str 两种类型，误导开发者 首先，Python3 把系统默认编码设置为 UTF-8，然后，文本字符和二进制数据区分得更清晰，分别用 str 和 bytes 表示。文本字符全部用 str 类型表示，str 能表示 Unicode 字符集中所有字符，而二进制字节数据用一种全新的数据类型，用 bytes 来表示。 str12345678910&gt;&gt;&gt; a = \"a\"&gt;&gt;&gt; a'a'&gt;&gt;&gt; type(a)&lt;class 'str'&gt;&gt;&gt;&gt; b = \"禅\"&gt;&gt;&gt; b'禅'&gt;&gt;&gt; type(b)&lt;class 'str'&gt; bytesPython3 中，在字符引号前加‘b’，明确表示这是一个 bytes 类型的对象，实际上它就是一组二进制字节序列组成的数据，bytes 类型可以是 ASCII范围内的字符和其它十六进制形式的字符数据，但不能用中文等非ASCII字符表示。 12345678910111213&gt;&gt;&gt; c = b'a'&gt;&gt;&gt; cb'a'&gt;&gt;&gt; type(c)&lt;class 'bytes'&gt;&gt;&gt;&gt; d = b'\\xe7\\xa6\\x85'&gt;&gt;&gt; db'\\xe7\\xa6\\x85'&gt;&gt;&gt; type(d)&lt;class 'bytes'&gt;&gt;&gt;&gt; e = b'禅' File \"&lt;stdin&gt;\", line 1SyntaxError: bytes can only contain ASCII literal characters. bytes 类型提供的操作和 str 一样，支持分片、索引、基本数值运算等操作。但是 str 与 bytes 类型的数据不能执行 + 操作，尽管在py2中是可行的。 123456789101112&gt;&gt;&gt; b'a'+b'c'b'ac'&gt;&gt;&gt; b'a'*2b'aa'&gt;&gt;&gt; b\"abcdef\\xd6\"[1:]b'bcdef\\xd6'&gt;&gt;&gt; b\"abcdef\\xd6\"[-1]214&gt;&gt;&gt; b\"a\" + \"b\"Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: can't concat bytes to str python2 与 python3 字节与字符的对应关系 python2 python3 表现 转换 作用 str bytes 字节 encode 存储 unicode str 字符 decode 显示 encode与decodestr 与 bytes 之间的转换可以用 encode 和 decode 方法。 encode 负责字符到字节的编码转换。默认使用 UTF-8 编码转换。 12345&gt;&gt;&gt; s = \"Python之禅\"&gt;&gt;&gt; s.encode()b'Python\\xe4\\xb9\\x8b\\xe7\\xa6\\x85'&gt;&gt;&gt; s.encode('gbk')b'Python\\xd6\\xae\\xec\\xf8' decode 负责字节到字符的解码转换，通用使用 UTF-8 编码格式进行转换。 1234&gt;&gt;&gt; b&apos;Python\\xe4\\xb9\\x8b\\xe7\\xa6\\x85&apos;.decode()&apos;Python之禅&apos;&gt;&gt;&gt; b&apos;Python\\xd6\\xae\\xec\\xf8&apos;.decode(&quot;gbk&quot;)&apos;Python之禅&apos;","categories":[{"name":"Unicode","slug":"Unicode","permalink":"http://yoursite.com/categories/Unicode/"}],"tags":[{"name":"Scrapy，Unicode，Python","slug":"Scrapy，Unicode，Python","permalink":"http://yoursite.com/tags/Scrapy，Unicode，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Python正则表达式","slug":"regex","date":"2017-04-11T12:18:54.000Z","updated":"2017-04-11T12:10:21.000Z","comments":true,"path":"2017/04/11/regex/","link":"","permalink":"http://yoursite.com/2017/04/11/regex/","excerpt":"Python正则表达式简介正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。 re 模块使 Python 语言拥有全部的正则表达式功能。","text":"Python正则表达式简介正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 Python 自1.5版本起增加了re 模块，它提供 Perl 风格的正则表达式模式。 re 模块使 Python 语言拥有全部的正则表达式功能。 compile 函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。 re 模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串做为它们的第一个参数。 正则表达式模式（常用）模式字符串使用特殊的语法来表示一个正则表达式：字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。多数字母和数字前加一个反斜杠时会拥有不同的含义。标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。反斜杠本身需要使用反斜杠转义。由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’/t’，等价于’//t’)匹配相应的特殊字符。 下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。 模式 描述 ^ 匹配字符串的开头 $ 匹配字符串的末尾 . 匹配任意字符，除了换行符”\\n”，当re.DOTALL标记被指定时，可以匹配包含换行符的任意字符 […] 用来表示一组字符，单独列出：[amk]匹配’a’，’m’或’k’ [^…] 不在[]中的字符： [ ^abc ]匹配除了a,b,c之外的字符 re* 匹配0个或多个的表达式 re+ 匹配1个或多个的表达式 re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 re{ n,} 精确匹配n个前面表达式 re{n,m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a &#124; b 匹配a或b (re) 匹配括号内的表达式，也表示一个组 (?imx) 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域 (?-imx) 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域 \\w 匹配字母数字及下划线 \\W 匹配非字母数字及下划线 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f]. \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0-9]. \\D 匹配任意非数字 \\A 匹配字符串开始 \\Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串 \\z 匹配字符串结束 \\G 匹配最后匹配完成的位置 \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’ \\B 匹配非单词边界。’er\\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’ \\n,\\t,等 匹配一个换行符。匹配一个制表符。等 \\1…\\9 匹配第n个分组的子表达式。 \\10 匹配第n个分组的子表达式，如果它经匹配。否则指的是八进制字符码的表达式 正则表达式实例字符匹配 实例 描述 python 匹配”python” 字符类 实例 描述 [Pp]ython 匹配 “Python” 或 “python” rub[ye] 匹配 “ruby” 或 “rube” [lawtech] 匹配中括号内的任意一个字母 [0-9] 匹配任何数字。类似于 [0123456789] [a-z] 匹配任何小写字母 [A-Z] 匹配任何大写字母 [a-zA-Z0-9] 匹配任何字母及数字 [^lawtech] 除了lawtech字母以外的所有字符 [^0-9] 匹配除了数字外的字符 特殊字符类 实例 描述 . 匹配除 “\\n” 之外的任何单个字符，要匹配包括 ‘\\n’ 在内的任何字符，请使用像’[.\\n]’ 的模式 \\d 匹配一个数字字符，等价于 [0-9] \\D 匹配一个非数字字符，等价于 [ ^0-9 ] \\s 匹配任何空白字符，包括空格、制表符、换页符等等，等价于[\\f\\n\\r\\t\\v] \\S 匹配任何非空白字符。等价于 [ ^\\f\\n\\r\\t\\v ] \\w 匹配包括下划线的任何单词字符，等价于[A-Za-z0-9_] \\W 匹配任何非单词字符等价于 [ ^A-Za-z0-9_ ] 正则表达式修饰符 - 可选标志正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志： 修饰符 描述 全拼 re.I 使匹配对大小写不敏感 IGNORECASE re.L 做本地化识别（locale-aware）匹配 LOCALE re.M 多行匹配，影响 ^ 和 $ MULTILINE re.S 使 . 匹配包括换行在内的所有字符 DOTALL re.U 根据Unicode字符集解析字符。这个标志影响 \\w, \\W, \\b, \\B. UNICODE re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 VERBOSE re模块能够处理正则表达式的操作生成正则表达式对象 生成正则表达式对象 compile(pattern, flags=0)构建一个正则表达式，返回该正则表达式对象 12import repattern = re.compile('re') 进行匹配 match() 确定正则表达式是否匹配字符串的开头 search() 扫描字符串以查找匹配 findall() 找到所有正则表达式匹配的子字符串，并把它们作为一个列表返回 finditer() 找到所有正则表达式匹配的子字符串，并把它们以迭代器的形式返回 group() 返回通过正则表达式匹配到的字符串 start() 返回成功匹配开始位置 end() 返回成功匹配结束位置 span() 返回包含成功匹配开始和结束位置的元组 re.match函数re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回None。 函数语法： 1re.match(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 匹配成功re.match方法返回一个匹配的对象（match object），否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组 groups() 返回一个包含所有小组字符串的元组，从1到所含的小组号 实例1： 12345# -*- coding: UTF-8 -*- import reprint(re.match('www', 'www.lawtech0902.com').span()) # 在起始位置匹配print(re.match('com', 'www.lawtech0902.com')) # 不在起始位置匹配 运行结果： 12(0, 3)None 实例2： 12345678910111213# -*- coding: UTF-8 -*- import reline = \"Cats are smarter than dogs\"matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)if matchObj: print(\"matchObj.group() : \", matchObj.group()) print(\"matchObj.group(1) : \", matchObj.group(1)) print(\"matchObj.group(2) : \", matchObj.group(2))else: print(\"No match!!\") 运行结果： 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter re.search方法re.search 扫描整个字符串并返回第一个成功的匹配。 函数语法： 1re.search(pattern, string, flags=0) 函数参数说明与re.match方法相同。 匹配成功re.search方法返回一个匹配的对象，否则返回None。 实例1： 12345# -*- coding: UTF-8 -*- import reprint(re.search('www', 'www.lawtech0902.com').span()) # 在起始位置匹配print(re.search('com', 'www.lawtech0902.com').span()) # 不在起始位置匹配 运行结果： 12(0, 3)(16, 19) 实例2： 12345678910111213# -*- coding: UTF-8 -*- import reline = \"Cats are smarter than dogs\"searchObj = re.search( r'(.*) are (.*?) .*', line, re.M|re.I)if searchObj: print(\"searchObj.group() : \", searchObj.group()) print(\"searchObj.group(1) : \", searchObj.group(1)) print(\"searchObj.group(2) : \", searchObj.group(2))else: print(\"Nothing found!!\") 运行结果： 123searchObj.group() : Cats are smarter than dogssearchObj.group(1) : CatssearchObj.group(2) : smarter re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 实例： 12345678910111213141516# -*- coding: UTF-8 -*- import reline = \"Cats are smarter than dogs\"matchObj = re.match( r'dogs', line, re.M|re.I)if matchObj: print(\"match --&gt; matchObj.group() : \", matchObj.group())else: print(\"No match!!\")matchObj = re.search( r'dogs', line, re.M|re.I)if matchObj: print(\"search --&gt; matchObj.group() : \", matchObj.group())else: print(\"No match!!\") 运行结果： 12No match!!search --&gt; matchObj.group() : dogs 检索和替换Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。 语法： 1re.sub(pattern, repl, string, count=0, flags=0) 参数： pattern : 正则中的模式字符串。 repl : 替换的字符串，也可为一个函数。 string : 要被查找替换的原始字符串。 count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 实例： 12345678910111213# -*- coding: UTF-8 -*-import rephone = \"2004-959-559 # 这是一个国外电话号码\"# 删除字符串中的 Python注释 num = re.sub(r'#.*$', \"\", phone)print(\"电话号码是: \", num)# 删除非数字(-)的字符串 num = re.sub(r'\\D', \"\", phone)print(\"电话号码是 : \", num) 运行结果： 12电话号码是: 2004-959-559电话号码是 : 2004959559 repl参数是一个函数以下实例中将字符串中的匹配的数字乘于 2： 1234567891011# -*- coding: UTF-8 -*-import re# 将匹配的数字乘于 2def double(matched): value = int(matched.group('value')) return str(value * 2)s = 'A23G4HFD567'print(re.sub('(?P&lt;value&gt;\\d+)', double, s)) 运行结果为： 1A46G8HFD1134","categories":[{"name":"Regular Expression","slug":"Regular-Expression","permalink":"http://yoursite.com/categories/Regular-Expression/"}],"tags":[{"name":"Scrapy，Regular Expression，Python","slug":"Scrapy，Regular-Expression，Python","permalink":"http://yoursite.com/tags/Scrapy，Regular-Expression，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Scrapy安装及调试","slug":"scrapy-install","date":"2017-04-11T12:18:54.000Z","updated":"2017-04-11T07:48:50.000Z","comments":true,"path":"2017/04/11/scrapy-install/","link":"","permalink":"http://yoursite.com/2017/04/11/scrapy-install/","excerpt":"环境搭建","text":"环境搭建 12$ mkvirtualenv article_spider$ pip install -i https://pypi.douban.com/simple/ scrapy 创建项目12345678910111213$ workon article_spider$ scrapy startproject ArticleSpiderNew Scrapy project &apos;ArticleSpider&apos;, using template directory &apos;/Users/lawtech/myvirtualenvs/article_spider/lib/python3.5/site-packages/scrapy/templates/project&apos;, created in: /Users/lawtech/PycharmProjects/ArticleSpiderYou can start your first spider with: cd ArticleSpider scrapy genspider example example.com $ cd ArticleSpider$ scrapy genspider jobbole blog.jobbole.comCreated spider &apos;jobbole&apos; using template &apos;basic&apos; in module: ArticleSpider.spiders.jobbole 项目目录介绍首先先要回答一个问题。 问：把网站装进爬虫里，总共分几步？答案很简单，四步： 新建项目 (Project)：新建一个新的爬虫项目 明确目标（Items）：明确你想要抓取的目标 制作爬虫（Spider）：制作爬虫开始爬取网页 存储内容（Pipeline）：设计管道存储爬取内容 创建好AticleSpider项目之后，可以看到将会创建一个AticleSpider文件夹，目录结构如下： 12345678910ArticleSpider/ scrapy.cfg ArticleSpider/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py jobbole.py 下面来简单介绍一下各个文件的作用： scrapy.cfg：项目的配置文件 ArticleSpider/：项目的 Python 模块，将会从这里引用代码 ArticleSpider/items.py：项目的 items 文件 ArticleSpider/pipelines.py：项目的 pipelines 文件 ArticleSpider/settings.py：项目的设置文件 ArticleSpider/spiders/：存储爬虫的目录 pycharm 调试 scrapy 执行流程在项目根目录下新建 main.py 文件，添加如下代码 123456789from scrapy.cmdline import executeimport sysimport os# __file__ 表示当前py文件sys.path.append(os.path.dirname(os.path.abspath(__file__)))# 将实际命令拆分execute([\"scrapy\", \"crawl\", \"spiders文件夹下的py文件名称\"])","categories":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://yoursite.com/categories/Scrapy/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——Xpath教程","slug":"xpath-usage","date":"2017-04-11T12:18:54.000Z","updated":"2017-04-16T08:41:39.000Z","comments":true,"path":"2017/04/11/xpath-usage/","link":"","permalink":"http://yoursite.com/2017/04/11/xpath-usage/","excerpt":"XPath的用法XPath简介 XPath 使用路径表达式在 XML / HTML 文档中进行导航 XPath 包含一个标准函数库 XPath 是 XSLT 中的主要元素 XPath 是一个 W3C 标准","text":"XPath的用法XPath简介 XPath 使用路径表达式在 XML / HTML 文档中进行导航 XPath 包含一个标准函数库 XPath 是 XSLT 中的主要元素 XPath 是一个 W3C 标准 XPath术语节点（Node）在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。XML 文档是被作为节点树来对待的。树的根被称为文档节点或者根节点。 请看下面这个 XML 文档： 123456789101112&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang=\"en\"&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 上面的XML文档中的节点例子： 123&lt;bookstore&gt; （文档节点）&lt;author&gt;J K. Rowling&lt;/author&gt; （元素节点）lang=\"en\" （属性节点） 基本值（或称原子值，Atomic value）基本值是无父或无子的节点。 基本值的例子： 12J K. Rowling\"en\" 项目（Item）项目是基本值或者节点。 节点关系参考示例： 12345678910&lt;bookstore&gt;&lt;book&gt; &lt;title&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 父节点（Parent） 每个元素以及属性都有一个父节点。 在上面的例子中，book 元素是 title、author、year 以及 price 元素的父节点 子节点（Children） 元素节点可有零个、一个或多个子节点。 在上面的例子中，title、author、year 以及 price 元素都是 book 元素的子节点 同胞节点（Sibling） 拥有相同的父节点的节点 在上面的例子中，title、author、year 以及 price 元素都是同胞节点 先辈节点（Ancestor） 某节点的父节点、父节点的父节点，等等。 在上面的例子中，title 元素的先辈节点是 book 元素和 bookstore 元素 后代节点（Descendant） 某个节点的子节点，子节点的子节点，等等。 在上面的例子中，bookstore 的后代节点是 book、title、author、year 以及 price 元素 XPath语法参考示例： 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Harry Potter&lt;/title&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Learning XML&lt;/title&gt; &lt;price&gt;39.95&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 选取节点XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。下面列出了最有用的路径表达式： 表达式 描述 nodename 选取此节点的所有子节点 / 从根节点选取 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 . 选取当前节点 .. 选取当前节点的父节点。 @ 选取属性。 实例： 路径表达式 结果 bookstore 选取bookstore元素的所有子节点 /bookstore 选取根元素bookstore 注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ bookstore/book 选取属于bookstore的子元素的所有book元素 //book 选取所有book元素，而不考虑它们的位置 bookstore//book 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置 //@lang 选取名为 lang 的所有属性 谓语（Predicates）谓语用来查找某个特定的节点或者包含某个指定的值的节点。 谓语被嵌在方括号中。 实例 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 路径表达式 结果 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素 /bookstore/book[postion()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素 //title[@lang=’eng’] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性 /bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00 /bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00 选取未知节点XPath 通配符可用来选取未知的 XML 元素 通配符 描述 * 匹配任何元素节点 @* 匹配任何属性节点 node() 匹配任何节点 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 /bookstore/* 选取 bookstore 元素的所有子元素 //* 选取文档中的所有元素 //title[@*] 选取所有带有属性的 title 元素 选取若干路径通过在路径表达式中使用“|”运算符，您可以选取若干个路径。 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 //book/title &#124; //book/price 选取 book 元素的所有 title 和 price 元素 //title &#124; //price 选取文档中的所有 title 和 price 元素 /bookstore/book/title &#124; //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素","categories":[{"name":"XPath","slug":"XPath","permalink":"http://yoursite.com/categories/XPath/"}],"tags":[{"name":"Scrapy，XPath，Python","slug":"Scrapy，XPath，Python","permalink":"http://yoursite.com/tags/Scrapy，XPath，Python/"}]},{"title":"Python分布式爬虫打造搜索引擎项目学习笔记——爬虫基础知识回顾","slug":"scrapy-project-basic","date":"2017-04-11T12:18:54.000Z","updated":"2017-04-11T07:49:05.000Z","comments":true,"path":"2017/04/11/scrapy-project-basic/","link":"","permalink":"http://yoursite.com/2017/04/11/scrapy-project-basic/","excerpt":"基础知识技术选型：scrapy vs requests + beautifulsoup","text":"基础知识技术选型：scrapy vs requests + beautifulsoup requests和beautifulsoup都是库，scrapy是框架 scrapy框架中可以加入requests和beautifulsoup scrapy基于twisted，性能是最大的优势 scrapy方便扩展，提供了很多内置的功能 scrapy内置的css和xpath selector非常方便，beautifulsoup最大的缺点就是慢 网页分类 静态网页 动态网页 webservice（restapi） 爬虫能做什么爬虫的作用 搜索引擎——百度、谷歌、垂直领域搜索引擎 推荐引擎——今日头条 机器学习的数据样本 数据分析、舆情分析等 正则表达式详见另一篇博客 网站url结构 深度优先遍历和广度优先遍历123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# _*_ coding: utf-8 _*_\"\"\"__author__ = 'lawtech'__date__ = '2017/4/10 下午1:41'\"\"\"\"\"\"深度优先遍历和广度优先遍历\"\"\"class Graph(object): def __init__(self, *args, **kwargs): self.node_neighbors = &#123;&#125; self.visited = &#123;&#125; def add_nodes(self, nodelist): for node in nodelist: self.add_node(node) def add_node(self, node): if node not in self.nodes(): self.node_neighbors[node] = [] def add_edge(self, edge): u, v = edge if (v not in self.node_neighbors[u]) and (u not in self.node_neighbors[v]): self.node_neighbors[u].append(v) if u != v: self.node_neighbors[v].append(u) def nodes(self): return self.node_neighbors.keys() def depth_first_search(self, root=None): \"\"\" 队列 :param root: :return: \"\"\" order = [] def dfs(node): self.visited[node] = True order.append(node) for n in self.node_neighbors[node]: if n not in self.visited: dfs(n) if root: dfs(root) for node in self.nodes(): if node not in self.visited: dfs(node) print(order) return order def breadth_first_search(self, root=None): \"\"\" 递归 :param root: :return: \"\"\" queue = [] order = [] def bfs(): while len(queue) &gt; 0: node = queue.pop(0) self.visited[node] = True for n in self.node_neighbors[node]: if (n not in self.visited) and (n not in queue): queue.append(n) order.append(n) if root: queue.append(root) order.append(root) bfs() for node in self.nodes(): if node not in self.visited: queue.append(node) order.append(node) bfs() print(order) return orderif __name__ == '__main__': g = Graph() g.add_nodes(nodelist=[i + 1 for i in range(8)]) g.add_edge((1, 2)) g.add_edge((1, 3)) g.add_edge((2, 4)) g.add_edge((2, 5)) g.add_edge((4, 8)) g.add_edge((5, 8)) g.add_edge((3, 6)) g.add_edge((3, 7)) g.add_edge((6, 7)) print(\"nodes:\", g.nodes()) g.breadth_first_search(1) g.depth_first_search(1) 运行结果： ​ (‘nodes:’, [1, 2, 3, 4, 5, 6, 7, 8]) ​ [1, 2, 3, 4, 5, 6, 7, 8] ​ [1, 2, 4, 8, 5, 3, 6, 7] 爬虫去重策略 将访问过的url保存到数据库中 将访问过的url保存到set中，只需要O(1)的代价就可以查询url url经过md5等方法哈希后保存到set中 用bitmap方法，将访问过的url通过hash函数映射到某一位 bloomfilter方法对bitmap进行改进，多重hash函数降低冲突 Python字符串编码详见另一篇博客","categories":[{"name":"Basic knowledge","slug":"Basic-knowledge","permalink":"http://yoursite.com/categories/Basic-knowledge/"}],"tags":[{"name":"Scrapy，Python","slug":"Scrapy，Python","permalink":"http://yoursite.com/tags/Scrapy，Python/"}]},{"title":"Redis学习笔记(五)：数据安全与性能保障——复制","slug":"Redis-5","date":"2017-04-10T12:18:54.000Z","updated":"2017-04-12T06:16:29.000Z","comments":true,"path":"2017/04/10/Redis-5/","link":"","permalink":"http://yoursite.com/2017/04/10/Redis-5/","excerpt":"复制（replication），可以让其他服务器拥有一个不断更新的数据副本，从而使得拥有数据副本的服务器可以用于处理客户端发送的读请求。对于高负载应用来说，复制是不可或缺的一个特性。","text":"复制（replication），可以让其他服务器拥有一个不断更新的数据副本，从而使得拥有数据副本的服务器可以用于处理客户端发送的读请求。对于高负载应用来说，复制是不可或缺的一个特性。 关系型数据库通常会使用一个主服务器（master）向多个从服务器（slave）发送更新，并使用从服务器来处理所有读请求。Redis也采用了同样的方法来实现自己的复制特性，并将其用作扩展性能的一种手段。 复制相关配置选项当从服务器连接主服务器时，主服务器会执行BGSAVE操作，为了正确地使用复制特性，用户需要保证服务器已经正确地设置了dir选项和dirname选项。 配置slaveof host port选项即可连接主服务器。 下面将演示怎么实现一个简单的复制系统。我们在一台机器上起两个Redis实例，监听不同的端口，其中一个作为主库，另外一个作为从库。首先不加任何参数来启动一个Redis实例作为主数据库： 主库默认监听6379端口。 接着新建一个终端，加上slaveof参数启动另一个Redis实例作为从库，并且监听6380端口： 从控制台输出中可以看到，从库已经连接到主库：126.0.0.1:6379了，我们可以分别在主库和从库中使用info replication命令看一看当前实例在复制系统中的相关信息 现在可以测试一下主从库的数据同步了： 12345678$ redis-cli -p 6379127.0.0.1:6379&gt; set test-replicate lawtechOK$ redis-cli -p 6380127.0.0.1:6380&gt; get test-replicate&quot;lawtech&quot;127.0.0.1:6380&gt; set x y(error) READONLY You can&apos;t write against a read only slave. 可以看到，在主库中添加的数据确实同步到了从库中。但是，我们在向从库中写入数据时报错了，这是因为在默认情况下，从库是只读的。我们可以在从库的配置文件中加上如下的配置项允许从库写数据： 1slave-read-only no 但是，因为从库中修改的数据不会被同步到任何其他数据库，并且一旦主库修改了数据，从库的数据就会因为自动同步被覆盖，所以一般情况下，不建议将从库设置为可写。 相同的道理，配置多台从库也使用相同的方法，都在从库的配置文件中加上slaveof参数即可。 此外，我们可以在客户端使用命令 1SLAVEOF 新主库地址 新主库端口 来修改当前数据库的主库，如果当前数据库已经是其他库的从库， 则当前数据库会停止和原来的数据库的同步而和新的数据库同步。 最后，从数据库还可以通过运行命令： 1SLAVEOF NO ONE 来停止接受来自其他数据库的同步而升级成为主库。 Redis复制的启动过程从服务器连接主服务器时，主服务器会创建一个快照文件并将其发送至从服务器，但这只是主从复制执行过程的其中一步，下表列举出复制过程主从服务器执行的所有操作： 步骤 主服务器操作 从服务器操作 1 （ 等待命令进入） 连接(或者重连接)主服务器，发送SYNC命令 2 开始执行BGSAVE，并使用缓冲区记录BGSAVE之后执行的所有写命令 根据配置选项来决定时继续使用现在的数据来处理客户端命令，还是向发送请求的客户端返回错误 3 BGSAVE执行完毕，向从服务器发送快照文件，并在发送期间继续使用缓冲区记录被执行的写命令 丢弃所有旧的数据，开始载入主服务器发来的快照文件 4 快照文件发送完毕，开始向从服务器发送存储在缓冲区里面的写命令 完成对快照文件的解释操作，像往常一样开始接受命令请求 5 缓冲区存储的写命令发送完毕；从现在开始，没执行一个写命令，就像从服务器发送相同的写命令 执行主服务器发来的所有存储在缓冲区里面的写命令；从现在开始，接收并执行主服务器传来的每个写命令 由上述步骤可以看出，有必要给Redis主服务器留30%~45%的内存用于执行BGSAVE命令和创建记录写命令的缓冲区。另外，从服务器还有一点需要注意的是，从服务器在进行同步时，会清空自己的所有数据，因为第3步中，从服务器会丢弃所有旧数据。 警告：Redis不支持主主复制（master-master replication） 当多个从服务器尝试连接同一个主服务器的时候，就会出现下表所示的两种情况中的其中一种： 当有新的从服务器连接主服务器时 主服务器的操作 上述步骤3尚未执行 所有从服务器都会接收相同的快照文件和相同的缓冲区写命令 上述步骤3正在执行或者已经执行 当主服务器与较早进行连接的从服务器执行完复制所需的5个步骤之后，主服务器会与新连接的从服务器执行一次新的步骤1至步骤5 由此可以看出多个从服务器的同步对网络的开销挺大的，有可能会影响到主服务器接收写命令，甚至是与主服务器位于同一网络中的其他硬件。 主从链创建多个从服务器可能造成网络不可用，此时可以使用另外一个解决方案，从服务器拥有自己的从服务器，并由此形成主从链（master/slave chaining）。 从服务器对从服务器进行复制在操作上和从服务器对主服务器进行复制的唯一区别在于。如果从服务器X拥有从服务器Y，那么当从服务器X在执行启动过程表中步骤4时，X将断开与Y的连接，导致Y需要重新连接并重新同步（resync）。 当读请求的重要性明显高于写请求的重要性，并且读请求的数量需求远远超出一台Redis服务器可以处理的范围时，用户就需要添加新的从服务器来处理读请求，随着负载不断上升，主服务器可能会无法快速地更新所有从服务器。 为了缓解这个问题，可以创建一个由Redis主/从节点(master/slave node)组成的中间层来分担主服务器的复制工作，如下图所示： 上面这个示例中，树的中层有3个帮助开展复制工作的服务器，底层有9个从服务器。其中，只有3台从服务器和主服务器通信，其他都向从服务器同步数据，从而降低了系统的负载。 检验硬盘写入为了将数据保存在多台机器中，用户首先需要为主服务器设置多个从服务器，然后对每个从服务器设置appendonly yes选项和appendfsync everysec选项（如有需要，也可以对主服务器这样设置），但这只是第一步：因为用户还需要等待主服务器发送的写命令到达从服务器，并且在执行后续操作前，检查数据是否已经被写入了硬盘中。 整个操作分两个环节： 验证主服务器是否已经将写数据发送至从服务器：用户需要在向主服务器写入真正的数据之后，再向主服务器写入一个唯一的虚构值（unique dummy value），然后通过检查虚构值是否存在于从服务器来判断数据是否已经到达从服务器。 判断数据是否已经被保存到硬盘中：检查INFO命令的输出结果中aof_pending_bio_fsync属性的值是否为0，如果是，则数据已经被保存到了硬盘中。 在向主服务器写入数据后，用户可以将主服务器和从服务器的连接作为参数调用下面的代码来自动进行上述操作： 1234567891011121314151617181920212223242526272829# _*_ coding: utf-8 _*_import uuidimport timedef wait_for_sync(mconn, sconn): identifier = str(uuid.uuid4()) # 将令牌添加至主服务器 mconn.zadd('sync:wait', identifier, time.time()) # 如果有必要的话，等待从服务器完成同步 while not sconn.info()['master_link_status'] != 'up': time.sleep(.001) # 等待从服务器接收数据更新 while not sconn.zscore('sync:wait', identifier): time.sleep(.001) # 最多只等待1秒 deadline = time.time() + 1.01 # 检查数据更新是否已经被同步到了硬盘 while time.time() &lt; deadline: if sconn.info()['aof_pending_bio_fsync'] == 0: break time.sleep(.001) # 清理刚刚创建的新令牌以及之前可能留下的旧令牌 mconn.zrem('sync:wait', identifier) mconn.zremrangebyscore('sync:wait', 0, time.time() - 900) 为了确保操作可以正确执行，wait_for_sync()函数会首先确认从服务器已经连接上主服务器，然后检查自己添加到等待同步有序集合（sync wait ZSET）里面的值是否已经存在于从服务器，在发现值存在后，等待从服务器将缓冲区的所有数据写入硬盘里。最后，确认数据已经被保存到硬盘之后，函数会执行一些清理操作。 通过同时使用复制和AOF持久化，用户可以增强Redis对于系统崩溃的抵抗能力。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Redis学习笔记(四)：数据安全与性能保障——持久化","slug":"Redis-4","date":"2017-04-09T15:12:54.000Z","updated":"2017-04-09T15:12:58.000Z","comments":true,"path":"2017/04/09/Redis-4/","link":"","permalink":"http://yoursite.com/2017/04/09/Redis-4/","excerpt":"什么是持久化？持久化（Persistence），即把数据（如内存中的对象）保存到可永久保存的存储设备中（如磁盘）。持久化的主要应用是将内存中的对象存储在数据库中，或者存储在磁盘文件中、XML数据文件中等等。","text":"什么是持久化？持久化（Persistence），即把数据（如内存中的对象）保存到可永久保存的存储设备中（如磁盘）。持久化的主要应用是将内存中的对象存储在数据库中，或者存储在磁盘文件中、XML数据文件中等等。 持久化是将程序数据在持久状态和瞬时状态间转换的机制。 JDBC就是一种持久化机制。文件IO也是一种持久化机制。 我们这样理解：在一定周期内保持不变就是持久化，持久化是针对时间来说的。数据库中的数据就是持久化了的数据，只要你不去删除或修改。 持久化选项Redis提供了两种不同的持久化方法来将数据存储到硬盘中，保证数据在Redis重启后仍然存在： RDB持久化：在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot），也就是将存在于某一时刻的所有数据都写入硬盘里面，所以也叫作快照持久化。 AOF持久化：全称是 append-only file（只追加文件）， 它记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 这两种持久化的方式既可以单独使用，也可以同时使用，具体选择哪种方式需要根据用户的数据及应用进行选择。 下面的代码示例展示了Redis对于两种持久化选项提供的配置选项 1234567891011121314# 快照持久化选项save 60 1000 # 60秒内有1000次写入操作的时候执行快照的创建stop-writes-on-bgsave-error no # 创建快照失败的时候是否仍然继续执行写命令rdbcompression yes # 是否对快照文件进行压缩dbfilename dump.rdb # 如何命名硬盘上的快照文件dir ./ # 快照所保存的位置# AOF持久化选项appendonly no # 是否使用AOF持久化appendfsync everysec # 多久才将写入的内容同步到硬盘no-appendfsync-on-rewrite no # 在对AOF进行压缩(compaction)的时候能否执行同步操作auto-aof-rewrite-percentage 100 # 多久执行一次AOF压缩auto-aof-rewrite-min-size 64mb # 多久执行一次AOF压缩dir ./ # AOF所保存的位置 快照持久化（RDB）创建快照的办法 客户端通过向Redis发送BGSAVE命令来创建快照。 如果平台支持（除了Windows），那么Redis会调用fork来创建一个子进程，然后子进程负责将快照写到硬盘中，而父进程则继续处理命令请求。 使用场景： 如果用户使用了save设置，例如：save 60 1000 ,那么从Redis最近一次创建快照之后开始计算，当“60秒之内有1000次写入操作”这个条件满足的时候，Redis就会自动触发BGSAVE命令。 如果用户使用了多个save设置，那么当任意一个save配置满足条件的时候，Redis都会触发一次BGSAVE命令。 客户端通过向Redis发SAVE命令来创建快照。 接收到SAVE命令的Redis服务器在快照创建完毕之前将不再响应任何其他命令的请求。SAVE命令并不常用，我们通常只在没有足够的内存去执行BGSAVE命令的时候才会使用SAVE命令，或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令。 使用场景： 当Redis通过SHUTDOWN命令接收到关闭服务器的请求时，或者接收到标准的TERM信号时，会执行一次SAVE命令，阻塞所有的客户端，不再执行客户端发送的任何命令，并且在执行完SAVE命令之后关闭服务器。 优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次快照文件，并且在每个月的每一天，也备份一个快照文件。 这样的话，即使遇到问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心。 RDB 可以最大化 Redis 的性能：父进程在保存快照文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 缺点 如果在新的快照文件创建好之前，Redis、系统、硬件三者中任意一个发生崩溃，那么Redis将丢失最近一次创建快照之后写入的所有数据。如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork()可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOF持久化AOF持久化会将被执行的写命令写到AOF文件的末尾，以此来记录数据发生的变化。这样，我们在恢复数据的时候，只需要从头到尾的执行一下AOF文件即可恢复数据。 我们可以通过appendonly yes命令来打开AOF持久化选项 appendfsync同步频率下表展示了appendfsync选项对AOF文件的同步频率的影响 选项 同步频率 always 每个Redis写命令都要同步写入硬盘。这样做会严重降低Redis的速度 everysec 每秒执行一次同步，显示地将多个写命令同步到硬盘 no 让操作系统来决定应该何时进行同步 always的方式固然可以对没一条数据进行很好的保存，但是这种同步策略需要对硬盘进行大量的写操作，所以Redis处理命令的速度会受到硬盘性能的限制。 普通的硬盘每秒钟只能处理大约200个写命令，使用固态硬盘SSD每秒可以处理几万个写命令，但是每次只写一个命令，这种只能怪不断地写入很少量的数据的做法有可能引发严重的写入放大问题，这种情况下降严重影响固态硬盘的使用寿命。 everysec的方式，Redis以每秒一次的频率大队AOF文件进行同步。这样的话既可以兼顾数据安全也可以兼顾写入性能。 Redis以每秒同步一次AOF文件的性能和不使用任何持久化特性时的性能相差无几，使用每秒更新一次 的方式，可以保证，即使出现故障，丢失的数据也在一秒之内产生的数据。 no的方式，Redis将不对AOF文件执行任何显示的同步操作，而是由操作系统来决定应该何时对AOF文件进行同步。 这个命令一般不会对Redis的性能造成多大的影响，但是当系统出现故障的时候使用这种选项的Redis服务器丢失不定数量的数据。 另外，当用户的硬盘处理写入操作的速度不够快的话，那么缓冲区被等待写入硬盘的数据填满时，Redis的写入操作将被阻塞，并导致Redis处理命令请求的速度变慢，因为这个原因，一般不推荐使用这个选项。 重写/压缩AOF文件随着数据量的增大，AOF的文件可能会很大，这样在每次进行数据恢复的时候就会进行很长的时间，为了解决日益增大的AOF文件，用户可以向Redis发送BGREWRITEAOF命令，这个命令会通过移除AOF文件中的冗余命令来重写AOF文件，是AOF文件的体积变得尽可能的小。 BGREWRITEAOF的工作原理和BGSAVE的原理很像：Redis会创建一个子进程，然后由子进程负责对AOF文件的重写操作。 因为AOF文件重写的时候会创建子进程，所以快照持久化因为创建子进程而导致的性能和内存占用问题同样会出现在AOF文件重写的时候。 跟快照持久化通过save选项来自动执行BGSAVE一样，AOF通过设置auto-aof-rewrite-percentage和auto-aof-rewrite-min-size选项来自动执行BGREWRITEAOF。 如下配置 12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 表示当前AOF的文件体积大于64MB，并且AOF文件的体积比上一次重写之后的体积变大了至少一倍（100%）的时候，Redis将执行重写BGREWRITEAOF命令。 如果AOF重写执行的过于频繁的话，可以将auto-aof-rewrite-percentage选项的值设置为100以上，这种最偶发就可以让Redis在AOF文件的体积变得更大之后才执行重写操作，不过，这也使得在进行数据恢复的时候执行的时间变得更加长一些。 优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 如何选择RDB和AOF？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 RDB 和 AOF 之间的相互作用BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE 。 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户，BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"MxOnline项目学习总结","slug":"MxOnline-summary","date":"2017-04-07T13:18:54.000Z","updated":"2017-04-07T13:26:06.000Z","comments":true,"path":"2017/04/07/MxOnline-summary/","link":"","permalink":"http://yoursite.com/2017/04/07/MxOnline-summary/","excerpt":"拖拖拉拉地学完了imooc的”强力django+杀手级xadmin打造上线标准的在线教育平台”课程，记录一下每一章学习的内容概要。","text":"拖拖拉拉地学完了imooc的”强力django+杀手级xadmin打造上线标准的在线教育平台”课程，记录一下每一章学习的内容概要。 MxOnline项目学习总结第一、二章 课程介绍 开发环境搭建 第三章 django基础回顾 settings.py 设置 urls.py 配置 models.py 设计 views.py 编码 templates 模板编码 第四章 数据库设计 users app model 设计 organization app model 设计 course app model 设计 operation app model 设计 第五章 后台管理系统开发 django admin 介绍 xadmin 安装和 model 注册 xadmin 全局配置 第六章 登录、注册、找回密码 登录（ session 和 cookie 机制） 注册（ form 表单提交、图片验证码，发送邮件 ） 找回密码（邮件发送） 第七章 课程机构功能实现 机构列表（分页，筛选、排序） 机构详情页（收藏，富文本展示） 咨询提交（ modelform 验证和保存） 第八章 课程功能实现 课程列表（分页、排序） 课程详情页（收藏，章节展示、资源展示、评论） 第九章 讲师功能实现 讲师列表（分页、排序） 讲师详情（收藏） 第十章 个人中心功能实现 用户信息修改（修改密码、头像、邮箱、基本信息） 导航栏全局搜索功能 我的课程 我的收藏（删除收藏） 我的消息 第十一章 全局功能实现 全局404和500页面配置 首页开发 点击数和收藏数修改和退出功能 第十二章 常见 web 攻击 sql 注入攻击 xss 攻击 csrf 攻击 第十三章 xadmin 进阶开发 userprofile 注册和设置 xadmin 常见功能设置 inlinemodel 注册、proxy 代理注册 django ueditor 富文本编辑器继承 excel 导入插件集成","categories":[{"name":"Django, Python","slug":"Django-Python","permalink":"http://yoursite.com/categories/Django-Python/"}],"tags":[{"name":"Django, Python","slug":"Django-Python","permalink":"http://yoursite.com/tags/Django-Python/"}]},{"title":"Redis学习笔记(三)：Redis命令补充","slug":"Redis-3","date":"2017-04-06T12:18:54.000Z","updated":"2017-04-06T13:14:54.000Z","comments":true,"path":"2017/04/06/Redis-3/","link":"","permalink":"http://yoursite.com/2017/04/06/Redis-3/","excerpt":"在之前的学习笔记中，还有许多Redis的命令没有涉及，这一篇主要用来简要地补充，当然，详细的命令还得参考Redis的官方命令文档。","text":"在之前的学习笔记中，还有许多Redis的命令没有涉及，这一篇主要用来简要地补充，当然，详细的命令还得参考Redis的官方命令文档。 键值相关命令下表展示了Redis提供的一些键值(KEY-VALUE)相关的常用命令及其redis-py API 命令 用例 描述 redis-py API KEYS KEYS pattern 查找所有符合给定模式pattern(正则表达式)的key keys(pattern=’*’) EXISTS EXISTS key 检查给定key是否存在 exists(name) EXPIRE EXPIRE key seconds 为给定key设置生存时间，当key过期时(生存时间为0)，它会被自动删除 expire(name, time) MOVE MOVE key db 将当前数据库的key移动到给定的数据库db当中 move(name, db) PERSIST PERSIST key 移除给定key的生存时间，将这个key从『易失的』(带生存时间key)转换成『持久的』(一个不带生存时间、永不过期的key) persist(name) RANDOMKEY RANDOMKEY 从当前数据库返回一个随机的key randomkey() RENAME RENAME key newkey 将key重命名为newkey，如果key与newkey相同，将返回一个错误。如果newkey已经存在，则值将被覆盖 rename(src, dst) TYPE TYPE key 返回key所存储的value的数据结构类型，它可以返回string, list, set, zset和hash等不同的类型 type(name) TTL TTL key 返回key剩余的过期时间(单位：秒) ttl(name) 下面这个交互示例展示了Redis中关于键的过期时间相关的命令的使用方法 12345678910111213&gt;&gt;&gt; r.set('key', 'value')True&gt;&gt;&gt; r.get('key')b'value'&gt;&gt;&gt; r.expire('key', 2)True&gt;&gt;&gt; time.sleep(2)&gt;&gt;&gt; r.get('key')&gt;&gt;&gt; r.set('key', 'value2')True&gt;&gt;&gt; r.expire('key', 100); r.ttl('key')True100 发布与订阅发布订阅(pub/sub)是一种消息通信模式，主要的目的是解耦消息发布者和消息订阅者之间的耦合，这点和设计模式中的观察者模式比较相似。pub/sub不仅仅解决发布者和订阅者之间代码级别耦合也解决两者在物理部署上的耦合。Redis作为一个pub/sub的server，在订阅者和发布者之间起到了消息路由的功能。订阅者可以通过subscribe和psubscribe命令向 redis server订阅自己感兴趣的消息类型，redis将消息类型称为通道(channel)。当发布者通过publish命令向 redis server发送二进制字符串消息(binary string message)时，订阅该消息类型的全部client都会收到此消息。这里消息的传递是多对多的，一个client可以订阅多个channel，也可以向多个channel发送消息。 下表展示了Redis提供的发布与订阅命令及其redis-py API 命令 用例 描述 redis-py API SUBSCRIBE SUBSCRIBE channel [channel …] 订阅给定的频道 subscribe(args, *kwargs) UNSUBSCRIBE UNSUBSCRIBE [channel [channel …]] 退订给定的频道，如果没有给定任何频道，则退订所有频道 unsubscribe(*args) PUBLISH PUBLISH channel message 将信息message发送到指定的频道channel publish(channel, message) PSUBSCRIBE PSUBSCRIBE pattern [pattern …] 订阅与给定模式相关的频道 psubscribe(args, *kwargs) PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern …]] 退订给定的模式，如果没有给定，则退订所有模式 PUNSUBSCRIBE [pattern [pattern …]] PUBSUB PUBSUB subcommand [argument [argument …]] PUBSUB命令是一个introspection命令，允许检查Pub/Sub子系统的状态，它由单独记录的子命令组成 pubsub(**kwargs) 考虑到PUBLISH命令和SUBSCRIBE命令在Python客户端的实现方式，一个比较简单的延时发布与订阅的方法，就是像如下代码那样用辅助线程(helper thread)来执行PUBLISH命令 1234567891011121314151617181920212223242526import redisimport timeimport threadingpool = redis.ConnectionPool(host='localhost', port=6379, db=0)r = redis.StrictRedis(connection_pool=pool)def publisher(n): time.sleep(1) for i in range(n): r.publish('channel', i)def run_pubsub(): threading.Thread(target=publisher, args=(3,)).start() pubsub = r.pubsub() pubsub.subscribe(['channel']) count=0 for item in pubsub.listen(): print(item) count += 1 if count == 4: pubsub.unsubscribe() if count == 5: break publisher函数在刚开始执行时会先休眠，让订阅者有足够的时间来连接服务器并监听消息。在发布消息之后进行短暂的休眠，让消息可以一条接一条地出现。 run_pubsub函数启动发送者线程，让它发送三条消息。随后创建发布与订阅对象，并让它订阅给定的频道。通过遍历函数pubsub.listen()的执行结果来监听订阅消息。在接收到一条订阅反馈消息和三条发布者发送的消息之后，执行退订操作，停止监听新消息。客户端在接收到退订反馈消息之后，就不再接收消息。 实际运行函数并观察它们的行为123456&gt;&gt;&gt; run_pubsub()&#123;'type': 'subscribe', 'channel': b'channel', 'data': 1, 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'0', 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'1', 'pattern': None&#125;&#123;'type': 'message', 'channel': b'channel', 'data': b'2', 'pattern': None&#125;&#123;'type': 'unsubscribe', 'channel': b'channel', 'data': 0, 'pattern': None&#125; 以上这些结构就是我们在遍历pubsub.listen()函数时得到的元素。 在刚开始订阅一个频道的时候，客户端会接收到一条关于被订阅频道的反馈消息。在退订频道时，客户端会接收到一条反馈消息，告知被退订的是哪一个频道，以及客户端目前仍在订阅的频道数量。 其他命令排序Redis中负责执行排序操作的SORT命令可以根据字符串、列表、集合、有序集合、散列这5中键里面存储的数据，对列表、集合以及有序集合进行排序，可以将SORT命令看作是SQL语言中的order by子句。 下表展示了SORT命令的定义及其redis-py API 命令 用例 描述 redis-py API SORT SORT key [BY pattern][LIMIT offset count] [GET pattern][ASC\\ DESC] [ALPHA] destination 返回或存储key的list、set或sorted set中的元素。默认是按照数值类型排序的，并且按照两个元素的双精度浮点数类型值进行比较 sort(name, start=None, num=None, by=None, get=None, desc=False, alpha=False, store=None, groups=False) 下面展示了SORT命令的一些简单的用法123456789101112131415161718&gt;&gt;&gt; r.rpush('sort-input', 23, 15, 110, 7)4&gt;&gt;&gt; r.sort('sort-input')[b'7', b'15', b'23', b'110']&gt;&gt;&gt; r.sort('sort-input', alpha=True)[b'110', b'15', b'23', b'7']&gt;&gt;&gt; r.hset('d-7', 'field', 5)1&gt;&gt;&gt; r.hset('d-15', 'field', 1)1&gt;&gt;&gt; r.hset('d-23', 'field', 9)1&gt;&gt;&gt; r.hset('d-110', 'field', 3)1&gt;&gt;&gt; r.sort('sort-input', by='d-*-&gt;field')[b'15', b'110', b'7', b'23']&gt;&gt;&gt; r.sort('sort-input', by='d-*-&gt;field', get='d-*-&gt;field')[b'1', b'3', b'5', b'9'] SORT命令不仅可以对列表进行排序，还可以对集合进行排序，然后返回一个列表形式的排序结果。上述代码除了展示如何使用alpha关键字(根据元素字母表顺序，默认根据大小)参数对元素进行字符串排序之外，还展示了如何基于外部数据对元素进行排序，以及如何获取并返回外部数据。 尽管SORT是Redis中唯一一个可以同时处理3种不同类型的数据的命令，但是事务同样可以让我们在一连串不间断执行的命令里面操作不同类型的数据。 基本的Redis事务Redis中的事务(transaction)是一组命令的集合。MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部执行，要么全不执行。 事务的应用非常普遍，比如银行汇款过程中A向B汇款，系统先从A的账户中将钱划走，然后向B的账户中增加相应的金额。这两个步骤必须属于同一个事务，要么全部执行，要么全不执行。 Redis的基本事务(basic transaction)需要用到MULTI和EXEC命令。在Redis中，被MULTI和EXEC命令包围的所有命令会一个接一个地执行，直到所有命令都执行完毕为止。当一个事务执行完毕后，才会处理其他客户端的命令。 Redis中执行事务的步骤：首先需要执行MULTI命令，然后输入我们想要在事务里面执行的命令，最后再执行EXEC命令。MULTI命令用于开启一个事务，它总是返回OK 。MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。另一方面，通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务。EXEC命令的回复是一个数组，数组中的每个元素都是执行事务中的命令所产生的回复。其中，回复元素的先后顺序和命令发送的先后顺序一致。当客户端处于事务状态时，所有传入的命令都会返回一个内容为QUEUED的状态回复status reply，这些被入队的命令将在EXEC命令被调用时执行。 从语义上来说，Redis事务在Python客户端中是由管道(pipeline)实现的：对连接对象调用pipeline()方法将创建一个事务，在一切正常的情况下，客户端会自动地调用MULTI命令包裹用户输入的多个命令。此外，为了减少Redis与客户端之间的通信往返次数，提升执行多个命令的性能，Python的Redis客户端会存储起事务包含的多个命令，然后在事务执行时一次性将所有命令都发送给Redis。 要展示事务执行的结果，最简单的方法就是将事务放到线程里面执行，下面这个交互示例展示了在没有使用事务的情况下，执行并行(parallel)自增操作的结果1234567891011121314151617&gt;&gt;&gt; import redis&gt;&gt;&gt; import threading&gt;&gt;&gt; import time&gt;&gt;&gt; r = redis.StrictRedis(host='localhost', port=6379, db=0)&gt;&gt;&gt; def notrans():... print(r.incr('notrans:'))... time.sleep(.1)... r.incr('notrans:', -1)...&gt;&gt;&gt; if 1:... for i in range(3):... threading.Thread(target=notrans).start()... time.sleep(.5)...213 上述代码启动了3个线程来执行没有被事务包裹的自增、休眠和自减操作，正因为没有使用事务，所以三个线程都可以在执行自减操作前，对notrans:计数器执行自增操作。 下面这个交互示例就展示了如何使用事务处理命令的并行执行问题 123456789101112131415&gt;&gt;&gt; def trans():... pipeline = r.pipeline()... pipeline.incr('trans:')... time.sleep(.1)... pipeline.incr('trans:', -1)... print(pipeline.execute()[0])...&gt;&gt;&gt; if 1:... for i in range(3):... threading.Thread(target=trans).start()... time.sleep(.5)...111 首先在trans函数中创建一个事务型(transactional)管道对象，然后先把针对’tans:’计数器的自增操作放入队列，等待100ms后再将针对’tans:’计数器的自减操作放入队列，最后执行被事务包裹的命令，并打印自增操作的执行结果。最终在执行结果中可以看到，尽管自增和自减操作之间有一段延迟时间，但通过使用事务，各个线程都可以在不被其他线程打断的情况下，执行各自队列里面的命令。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Redis学习笔记(二)：Redis命令及其Python API","slug":"Redis-2","date":"2017-03-27T08:18:54.000Z","updated":"2017-03-30T12:17:22.000Z","comments":true,"path":"2017/03/27/Redis-2/","link":"","permalink":"http://yoursite.com/2017/03/27/Redis-2/","excerpt":"redis-py简介安装在之前的学习笔记(一)中已经安装过redis-py，我的Python版本是3.5.2","text":"redis-py简介安装在之前的学习笔记(一)中已经安装过redis-py，我的Python版本是3.5.21$ pip3 install redis 快速开始123456&gt;&gt;&gt; import redis&gt;&gt;&gt; r = redis.StrictRedis(host='localhost', port=6379, db=0)&gt;&gt;&gt; r.set('key', 'value')True&gt;&gt;&gt; r.get('key')b'value' API参考Redis的官方命令文档很好地解释了每个命令的详细信息。 redis-py公开了实现这些命令的两个客户端类。 第一，StrictRedis类试图遵守官方命令语法， 但是有些一些例外： SELECT: 没有实现，考虑到线程安全的原因。 DEL: 由于del是python语法关键字，所用delete来代替。 CONFIG GET|SET: 分开用 config_get or config_set来代替 MULTI/EXEC: 事务作为Pipeline类的其中一部分的实现。Pipeline默认保证了MULTI,EXEC声明。但是你可以指定transaction=False来禁用这一行为。 SUBSCRIBE/LISTEN:PubSub作为一个独立的类来实现发布订阅机制。 SCAN/SSCAN/HSCAN/ZSCAN:每个命令都对应一个等价的迭代器方法scan_iter/sscan_iter/hscan_iter/zscan_iter methods for this behavior。 第二，Redis类是StrictRedis的子类，提供redis-py版本向后的兼容性。 关于StrictRedis与Redis的区别：(官方推荐使用StrictRedis.) 以下几个方法在StrictRedis和Redis类中的参数顺序不同。 LREM: 在Redis类中是这样的：lrem(self, name, value, num=0)在StrictRedis类中是这样的：lrem(self, name, count, value) ZADD: 在Redis类中是这样的：zadd(‘my-key’, ‘name1’, 1.1, ‘name2’, 2.2, name3=3.3, name4=4.4)在StrictRedis中是这样的：zadd(‘my-key’, 1.1, ‘name1’, 2.2, ‘name2’, name3=3.3, name4=4.4) SETEX: 在Redis类中是这样的：setex(self, name, value, time)而在StrictRedis中是这样的：setex(self, name, time, value) 连接池 redis-py使用connection pool来管理对一个redis server的所有连接，避免每次建立、释放连接的开销。默认情况下，每个Redis实例都会依次创建并维护一个自己的连接池。我们可以直接建立一个连接池，然后传递给Redis或StrictRedis连接命令作为参数，这样就可以实现多个Redis实例共享一个连接池，以实现客户端分片，或者对连接的管理方式进行更高精度的控制。 12&gt;&gt;&gt; pool = redis.ConnectionPool(host='localhost', port=6379, db=0)&gt;&gt;&gt; r = redis.StrictRedis(connection_pool=pool) 我们也可以创建自己的Connection子类，用于控制异步框架中的套接字行为，要使用自己的连接实例化客户端类，需要创建一个连接池，将类传递给connection_class参数。 1&gt;&gt;&gt; pool = redis.ConnectionPool(connection_class=YourConnectionClass,your_arg='...', ...) 释放连接回到连接池：可以使用Redis类的reset()方法，或者使用with上下文管理语法。 解析器：解析器控制如何解析Redis-server的响应内容，redis-py提供两种方式的解析器类支持PythonParser和HiredisParser(需要单独安装)。它优先选用HiredisParser,如果不存在，则选用PythonParser. Hiredis是redis核心团队开发的一个高性能c库，能够提高10x的解析速度。 响应回调：The client class使用一系列的callbacks来完成响应到对应python类型的映射。这些响应回调，定义在 Redis client class中的RESPONSE_CALLBACKS字典中。你可以使用set_response_callback 方法来添加自定义回调类。这个方法接受两个参数：一个命令名字，一个回调类。回调类接受至少一个参数：响应内容，关键字参数作为命令调用时的参数。 线程安全性Redis客户端实例可以安全地在线程之间共享。 在内部，连接实例只在命令执行期间从连接池检索，并在执行后直接返回到池中。 命令执行过程从不修改客户端实例上的状态。 但是，有一个警告：Redis SELECT命令。 SELECT命令允许您切换连接正在使用的数据库。 该数据库保持选中，直到选择另一个或连接关闭为止。 这会创建一个问题，因为可以将连接返回到连接到不同数据库的池。 因此，redis-py不会在客户端实例上实现SELECT命令。 如果在同一应用程序中使用多个Redis数据库，则应为每个数据库创建一个单独的客户机实例（也可能是单独的连接池）。 在线程之间传递PubSub或Pipeline对象是不安全的。 Redis命令及其对应redis-py API由于Redis官方命令文档很好地解释了每个命令的详细信息，所以我这里只对最常用的Redis命令进行整理，并给出其redis-py API。 字符串下表展示了对Redis字符串执行自增和自减操作的命令及其redis-py API。 命令 用例 描述 redis-py API INCR INCR key-name 将键存储的值加1 incr(name, amount=1) DECR DECR key-name 将键存储的值减1 decr(name, amount=1) INCRBY INCRBY key-name amount 将键存储的值加整数amount incr(name, amount=1) DECRBY DECRBY key-name amount 将键存储的值减整数amount decr(name, amount=1) INCRBYFLOAT INCRBYFLOAT key-name amount 将键存储的值加浮点数amount incrbyfloat(name, amount=1.0) 在redis-py内部，使用了INCRBY和DECRBY命令来实现incr()和decr()方法，并且第二个参数amount是可选的，默认为1。 下面这个交互示例展示了Redis的INCR和DECR操作12345678910111213&gt;&gt;&gt; r.get('key')&gt;&gt;&gt; r.incr('key')1&gt;&gt;&gt; r.incr('key', 15)16&gt;&gt;&gt; r.get('key')b'16'&gt;&gt;&gt; r.decr('key', 5)11&gt;&gt;&gt; r.set('key', 13)True&gt;&gt;&gt; r.incr('key')14 当用户将一个值存储到Redis字符串中时，如果这个值可以被解释(interpet)为十进制整数或者浮点数，那么Redis会允许用户对这个字符串执行各种INCR和DECR操作。如果用户对一个不存在的键或者一个保存了空串的键执行自增或自减操作，Redis会自动将这个键的值当作是0来处理。若非上述情况，则Redis将会返回一个错误。 除了自增和自减操作，Redis还可以对字节串进行读取和写入的操作。 下表展示了Redis用来处理字符串子串和二进制位的命令及其redis-py API。 命令 用例 描述 redis-py API APPEND APPEND key-name value 将值value追加到给定键key-name当前存储的值的末尾 append(key, value) GETRANGE GETRANGE key-name start end 获取一个偏移量从start到end的子串，包含start和end getrange(key, start, end) SETRANGE SETRANGE key-name offset value 将从start开始的子串设置为给定值 setrange(name, offset, value) GETBIT GETBIT key-name offset 将字节串看作是二进制位串，并返回位串中偏移量为offset的二进制位的值 getbit(name, offset) SETBIT SETBIT key-name offset value 将字节串看作是二进制位串，并将位串中偏移量为offset的二进制位的值设为value setbit(name, offset, value) BITCOUNT BITCOUNT key-name [start end] 统计字符串被设置为1的bit数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行 bitcount(key, start=None, end=None) BITOP BITOP operation dest-key key-name [key-name …] 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 bitop(operation, dest, *keys) 在执行SETRANGE或者SETBIT命令时，如果offset比当前key对应string还要长，那这个string后面就补空字节(null)以达到offset。使用GETRANGE时超出字符串末尾的数据会被认为是空字符串，而使用GETBIT时超出字符串末尾的二进制位会被视为是0。 下面这个交互示例展示了Redis的子串操作和二进制位操作123456789101112131415161718192021222324&gt;&gt;&gt; r.append('new-string-key', 'hello ')6&gt;&gt;&gt; r.append('new-string-key', 'world!')12&gt;&gt;&gt; r.substr('new-string-key', 3, 7)b'lo wo'&gt;&gt;&gt; r.getrange('new-string-key', 3, 7)b'lo wo'&gt;&gt;&gt; r.setrange('new-string-key', 0, 'H')12&gt;&gt;&gt; r.get('new-string-key')b'Hello world!'&gt;&gt;&gt; r.setrange('new-string-key', 11, ', how are you?')25&gt;&gt;&gt; r.get('new-string-key')b'Hello world, how are you?'&gt;&gt;&gt; r.setbit('another-key', 2, 1)0&gt;&gt;&gt; r.setbit('another-key', 7, 1)0&gt;&gt;&gt; r.getbit('another-key', 1)0&gt;&gt;&gt; r.get('another-key')b'!' Redis现在的GETRANGE命令是由以前的SUBSTR命令改名而来，所以现在redis-py中两者仍然都可以使用，但是最好还是使用getrange()方法来获取子串。 列表下表展示了一些之前介绍过的常用列表命令 命令 用例 描述 redis-py API RPUSH RPUSH key value [value …] 向存于key的列表的尾部插入所有指定的值 rpush(name, *values) LPUSH LPUSH key value [value …] 将所有指定的值插入到存于key的列表的头部 lpush(name, *values) RPOP RPOP key 移除并返回key对应的list的最后一个元素 rpop(name) LPOP LPOP key 移除并返回key对应的list的第一个元素 lpop(name) LINDEX LINDEX key index 返回列表索引位置的元素 lindex(name, index) LRANGE LRANGE key start stop 返回存储在key的列表里指定范围内的元素 lrange(name, start, end) LTRIM LTRIM key start stop 修剪(trim)一个已存在的list，这样list就会只包含指定范围的指定元素 ltrim(name, start, end) 下面这个交互示例展示了Redis列表的推入和弹出操作12345678910111213141516171819202122&gt;&gt;&gt; r.rpush('list-key', 'last')1&gt;&gt;&gt; r.lpush('list-key', 'first')2&gt;&gt;&gt; r.rpush('list-key', 'new last')3&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'first', b'last', b'new last']&gt;&gt;&gt; r.lpop('list-key')b'first'&gt;&gt;&gt; r.lpop('list-key')b'last'&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'new last']&gt;&gt;&gt; r.rpush('list-key', 'a', 'b', 'c')4&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'new last', b'a', b'b', b'c']&gt;&gt;&gt; r.ltrim('list-key', 2, -1)True&gt;&gt;&gt; r.lrange('list-key', 0, -1)[b'b', b'c'] 还有几个列表命令能将元素从一个列表移动到另一个列表，或者阻塞(block)执行命令的客户端直到有其他客户端给列表添加元素为止。 下表列出了这些阻塞弹出命令以及列表之间移动元素的命令 命令 用例 描述 redis-py API BLPOP BLPOP key [key …] timeout 弹出第一个非空列表的头元素，或在timeout秒内阻塞并等待可弹出的元素出现 blpop(keys, timeout=0) BRPOP BRPOP key [key …] timeout 弹出第一个非空列表的末尾元素，或在timeout秒内阻塞并等待可弹出的元素出现 brpop(keys, timeout=0) RPOPLPUSH RPOPLPUSH source destination 原子性地返回并移除存储在source的列表的最后一个元素(列表尾部元素)， 并把该元素放入存储在destination的列表的第一个元素位置(列表头部) rpoplpush(src, dst) BRPOPLPUSH BRPOPLPUSH source destination timeout BRPOPLPUSH 是 RPOPLPUSH 的阻塞版本。 当 source 包含元素的时候，这个命令表现得跟 RPOPLPUSH 一模一样。 当 source 是空的时候，Redis将会阻塞这个连接，直到另一个客户端 push 元素进入或者达到 timeout 时限。 brpoplpush(src, dst, timeout=0) 注：原子性是指命令正在都区或者修改数据的时候，其他客户端不能读取或修改相同的数据。 下面这个交互示例展示了Redis列表的阻塞弹出命令以及元素移动命令1234567891011121314151617181920&gt;&gt;&gt; r.rpush('list', 'item1')1&gt;&gt;&gt; r.rpush('list', 'item2')2&gt;&gt;&gt; r.rpush('list2', 'item3')1&gt;&gt;&gt; r.brpoplpush('list2', 'list', 1)b'item3'&gt;&gt;&gt; r.brpoplpush('list2', 'list', 1)&gt;&gt;&gt; r.lrange('list', 0, -1)[b'item3', b'item1', b'item2']&gt;&gt;&gt; r.brpoplpush('list', 'list2', 1)b'item2'&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list', b'item3')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list', b'item1')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1)(b'list2', b'item2')&gt;&gt;&gt; r.blpop(['list', 'list2'], 1) 对于阻塞弹出命令和弹出并推入命令，最常见的用例就是消息传递(messaging)和任务队列(task queue)。 集合下表展示了一部分最常用的集合命令 命令 用例 描述 redis-py API SADD SADD key member [member …] 添加一个或多个指定的member元素到key集合中 sadd(name, *values) SREM SREM key member [member …] 在key集合中移除指定的元素 srem(name, *values) SISMEMBER SISMEMBER key member 返回成员member是否是存储的集合key的成员 sismember(name, value) SCARD SCARD key 返回集合包含元素的数量 scard(name) SMEMBERS SMEMBERS key 返回key集合所有的元素 smembers(name) SRANDMEMBER SRANDMEMBER key [count] 仅提供key参数,那么随机返回key集合中的一个元素，返回含有 count 个不同的元素的数组，对count分情况处理 srandmember(name, number=None) SPOP SPOP key [count] 从key对应集合中返回并删除一个或多个元素 spop(name) SMOVE SMOVE source destination member 将member从source集合移动到destination集合中 smove(src, dst, value) 下面这个交互示例展示了这些常用的集合命令12345678910111213141516&gt;&gt;&gt; r.sadd('set-key', 'a', 'b', 'c')3&gt;&gt;&gt; r.srem('set-key', 'c', 'd')1&gt;&gt;&gt; r.srem('set-key', 'c', 'd')0&gt;&gt;&gt; r.scard('set-key')2&gt;&gt;&gt; r.smembers('set-key')&#123;b'b', b'a'&#125;&gt;&gt;&gt; r.smove('set-key', 'set-key2', 'a')True&gt;&gt;&gt; r.smove('set-key', 'set-key2', 'c')False&gt;&gt;&gt; r.smembers('set-key2')&#123;b'a'&#125; 但是集合真正厉害的地方在于组合和关联多个集合，下表展示了相关的Redis命令 命令 用例 描述 redis-py API SDIFF SDIFF key [key …] 返回一个集合与给定集合的差集的元素 sdiff(keys, *args) SDIFFSTORE SDIFFSTORE destination key [key …] 类似于 SDIFF，不同之处在于该命令不返回结果集，而是将结果存放在destination集合中，如果destination已经存在, 则将其覆盖重写 sdiffstore(dest, keys, *args) SINTER SINTER key [key …] 返回指定所有的集合的成员的交集 sinter(keys, *args) SINTERSTORE SINTERSTORE destination key [key …] 与SINTER命令类似，但是它并不是直接返回结果集，而是将结果保存在 destination集合中，如果destination集合存在, 则会被重写 sinterstore(dest, keys, *args) SUNION SUNION key [key …] 返回给定的多个集合的并集中的所有成员 sunion(keys, *args) SUNIONSTORE SUNIONSTORE destination key [key …] 类似于SUNION命令，不同的是它并不返回结果集，而是将结果存储在destination集合中，如果destination已经存在，则将其覆盖. sunionstore(dest, keys, *args) 这些命令分别是并集运算、交集运算和差集运算这三个基本集合操作的“返回结果”版本和“存储结果”版本，下面这个交互示例展示了这些命令的基本使用12345678910&gt;&gt;&gt; r.sadd('skey1', 'a', 'b', 'c', 'd')4&gt;&gt;&gt; r.sadd('skey2', 'c', 'd', 'e', 'f')4&gt;&gt;&gt; r.sdiff('skey1', 'skey2')&#123;b'b', b'a'&#125;&gt;&gt;&gt; r.sinter('skey1', 'skey2')&#123;b'c', b'd'&#125;&gt;&gt;&gt; r.sunion('skey1', 'skey2')&#123;b'd', b'a', b'f', b'e', b'c', b'b'&#125; 和Python的集合相比，Redis的集合除了可以被多个客户端远程地进行访问之外，其他的语义和功能基本都是相同的。 散列首先介绍一些常用的添加和删除键值对的Redis散列命令 命令 用例 描述 redis-py API HMGET HMGET key field [field …] 返回key指定的散列中指定字段的值 hmget(name, keys, *args) HMSET HMSET key field value [field value …] 设置key指定的散列中指定字段的值，该命令将重写所有在散列中存在的字段，如果key指定的散列不存在，会创建一个新的散列并与key关联 hmset(name, mapping) HDEL HDEL key field [field …] 从key指定的散列中移除指定的域，在散列中不存在的域将被忽略，如果key指定的散列不存在，它将被认为是一个空的散列，该命令将返回0 hdel(name, *keys) HLEN HLEN key 返回key指定的散列包含的字段的数量 hlen(name) 其中，HDEL命令已经介绍过了，而HLEN以及用于一次读取或设置多个键的HMGET和HMSET则是新出现的命令。它们既可以给用户带来方便，又可以通过减少命令的调用次数以及客户端与Redis之间的通信往返次数来提升Redis的性能。 下面这个交互示例展示了这些命令的使用方法12345678&gt;&gt;&gt; r.hmset('hash-key', &#123;'k1':'v1','k2':'v2','k3':'v3'&#125;)True&gt;&gt;&gt; r.hmget('hash-key', ['k2', 'k3'])[b'v2', b'v3']&gt;&gt;&gt; r.hlen('hash-key')3&gt;&gt;&gt; r.hdel('hash-key', 'k1', 'k3')2 之前的学习笔记(一)介绍的HGET命令和HSET命令分别是HMGET和HMSET命令的单参数版本。因为HDEL已经可以同时删除多个键值对了，所以Redis没有实现HMDEL命令。 下表列出了散列的其他几个批量操作命令，以及一些和字符串操作类似的散列命令。 命令 用例 描述 redis-py API HEXISTS HEXISTS key field 检查给定键是否存在于散列中 hexists(name, key) HKEYS HKEYS key 返回散列包含的所有键 hkeys(name) HVALS HVALS key 返回散列包含的所有值 hvals(name) HGETALL HGETALL key 返回散列包含的所有键值对 hgetall(name) HINCRBY HINCRBY key field increment 将键存储的值加上整数increment hincrby(name, key, amount=1) HINCRBYFLOAT HINCRBYFLOAT key field increment 将键存储的值加上浮点数increment hincrbyfloat(name, key, amount=1.0) 下面这个交互示例展示了这些命令的使用方法12345678910&gt;&gt;&gt; r.hmset('hash-key2', &#123;'short':'hello', 'long':1000*1&#125;)True&gt;&gt;&gt; r.hkeys('hash-key2')[b'short', b'long']&gt;&gt;&gt; r.hexists('hash-key2', 'num')False&gt;&gt;&gt; r.hincrby('hash-key2', 'num')1&gt;&gt;&gt; r.hexists('hash-key2', 'num')True 在对散列进行处理时，如果键值对的值的体积非常大，那么用户可以先用HKEYS获取散列的所有键，然后只获取必要的值，这样可以有效地减少需要传输的数据量，避免服务器阻塞。 有序集合下表展示了一些常用的有序集合命令，大部分在第一章都有介绍 命令 用例 描述 redis-py API ZADD ZADD key score member [score member …] 将带有给定分值的成员添加到有序集合中 zadd(name, args, *kwargs) ZREM ZREM key member [member …] 移除给定的成员，并返回被移除成员的数量 zrem(name, *values) ZCARD ZCARD key 返回有序集合包含的成员数量 zcard(name) ZINCRBY ZINCRBY key increment member 将member成员的分值加上increment zincrby(name, value, amount=1) ZCOUNT ZCOUNT key min max 返回分值介于min和max之间的成员数量 zcount(name, min, max) ZRANK ZRANK key member 返回成员member在有序集合中的排名 zrank(name, value) ZSCORE ZSCORE key member 返回成员member的分值 zscore(name, value) ZRANGE ZRANGE key start stop [WITHSCORES] 返回排名介于start和stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回 zrange(name, start, end, desc=False, withscores=False, score_cast_func=) 下面这个交互示例展示了Redis中的一些常用的有序集合命令12345678910111213141516&gt;&gt;&gt; r.zadd('zset-key', 3, 'a', 2, 'b', 1, 'c')3&gt;&gt;&gt; r.zcard('zset-key')3&gt;&gt;&gt; r.zincrby('zset-key', 'c', 3)4.0&gt;&gt;&gt; r.zscore('zset-key', 'b')2.0&gt;&gt;&gt; r.zrank('zset-key', 'c')2&gt;&gt;&gt; r.zcount('zset-key', 0, 3)2&gt;&gt;&gt; r.zrem('zset-key', 'b')1&gt;&gt;&gt; r.zrange('zset-key', 0, -1, withscores=True)[(b'a', 3.0), (b'c', 4.0)] 其中在Python客户端用StrictRedis客户端类执行ZADD命令需要先输入分值，再输入成员，这也是Redis的标准，而Redis客户端类则截然相反。 下表展示了另外一下非常有用的有序集合命令 命令 用例 描述 redis-py API ZREVRANK ZREVRANK key member 返回有序集合里成员member的排名，成员按照分值从大到小排列 zrevrank(name, value) ZREVRANGE ZREVRANGE key start stop [WITHSCORES] 返回有序集合给定排名范围内的成员，成员按照分值从大到小排列 zrevrange(name, start, end, withscores=False, score_cast_func=) ZRANGEBYSCORE ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 返回有序集合中指定分数区间内的成员 zrangebyscore(name, min, max, start=None, num=None, withscores=False, score_cast_func=) ZREVRANGEBYSCORE ZREVRANGEBYSCORE key max min [WITHSCORES][LIMIT offset count] 返回有序集合中指定分数区间内的成员，分数由高到低排序。 zrevrangebyscore(name, max, min, start=None, num=None, withscores=False, score_cast_func=) ZREMRANGEBYRANK ZREMRANGEBYRANK key start stop 移除有序集key中，指定排名(rank)区间内的所有成员 zremrangebyrank(name, min, max) ZREMRANGEBYSCORE ZREMRANGEBYSCORE key min max 移除有序集key中，所有score值介于min和max之间(包括等于min或max)的成员 zremrangebyscore(name, min, max) ZINTERSTORE ZINTERSTORE destination numkeys key [key …] [WEIGHTS weight] [SUM MIN MAX] 计算给定的numkeys个有序集合的交集，并且把结果放到destination中 zinterstore(dest, keys, aggregate=None) ZUNIONSTORE ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight] [SUM MIN MAX] 计算给定的numkeys个有序集合的并集，并且把结果放到destination中。 zunionstore(dest, keys, aggregate=None) 其中有几个是没有介绍过的新命令，除了使用逆序来处理有序集合之外，ZREV*命令的工作方式和相对应的非逆序命令的工作方式完全一样(逆序就是指元素按照分值从大到小地排列)。 下面这个交互示例展示了ZINTERSTORE和ZUNIONSTORE命令的用法123456789101112131415161718&gt;&gt;&gt; r.zadd('zset-1', 1, 'a', 2, 'b', 3, 'c')3&gt;&gt;&gt; r.zadd('zset-2', 4, 'b', 1, 'c', 0, 'd')3&gt;&gt;&gt; r.zinterstore('zset-i', ['zset-1', 'zset-2'])2&gt;&gt;&gt; r.zrange('zset-i', 0, -1, withscores=True)[(b'c', 4.0), (b'b', 6.0)]&gt;&gt;&gt; r.zunionstore('zset-u', ['zset-1', 'zset-2'], aggregate='min')4&gt;&gt;&gt; r.zrange('zset-u', 0, -1, withscores=True)[(b'd', 0.0), (b'a', 1.0), (b'c', 1.0), (b'b', 2.0)]&gt;&gt;&gt; r.sadd('set-1', 'a', 'd')2&gt;&gt;&gt; r.zunionstore('zset-u2', ['zset-1', 'zset-2', 'set-1'])4&gt;&gt;&gt; r.zrange('zset-u2', 0, -1, withscores=True)[(b'd', 1.0), (b'a', 2.0), (b'c', 4.0), (b'b', 6.0)] 用户可以在执行交并运算时传入不同的聚合函数，共有sum、min、max三种可选；用户还可以把集合作为输入传给ZINTERSTORE和ZUNIONSTORE，命令会将集合看作是成员分值全为1的有序集合来处理。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]},{"title":"Redis学习笔记(一)：初识Redis","slug":"Redis-1","date":"2017-03-25T12:18:54.000Z","updated":"2017-03-27T09:12:22.000Z","comments":true,"path":"2017/03/25/Redis-1/","link":"","permalink":"http://yoursite.com/2017/03/25/Redis-1/","excerpt":"Redis简介Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。","text":"Redis简介Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。 Redis与memcached及其他类型数据库对比Redis经常被拿来与memcached进行比较，两者都可用于存储键值映射，性能也相差无几，但是Redis能够自动以两种不同的方式将数据写入硬盘，而且Redis除了能存储普通的字符串键，还能存储其他4种数据结构，使得Redis可以用于解决更为广泛的问题，并且即可以作为主数据库使用，又可以作为其他存储系统的辅助数据库。 下表展示了Redis与memcached，MySQL以及MongoDB的特性与功能。 名称 类型 数据存储选项 查询类型 附加功能 Redis 使用内存存储的非关系数据库 字符串、列表、集合、散列表、有序集合 每种数据类型专属的命令，以及批量操作和不完全的事务支持 发布与订阅，主从复制，持久化，脚本 memcached 使用内存存储的键值缓存 键值之间的映射 创建、读取、删除、更新等命令 多线程服务器，用于提升性能 MySQL 关系数据库 每个数据库可以包含多个表，每个表可以包含多个行；可以处理多个表的视图；支持空间和第三方扩展 SELECT、INSERT、UPDATE、DELETE、函数、存储过程 支持ACID性质(需要使用InnoDB)，主从复制，主主复制 MongoDB 使用硬盘存储(on-disk)的非关系文档存储 每个数据库可以包含多个表，每个表可以包含多个无schema的BSON文档 创建、读取、更新、删除、条件查询等命令 支持map-reduce操作，主从复制，分片，空间索引 Redis安装(mac)首先下载用于安装Rudix的引导脚本，并安装Rudix12$ curl -O http://rudix.google.code.com/hg/Ports/rudix/rudix.Py$ sudo python rudix.py install rudix 然后使用命令Rudix安装Redis，若能成功启动Redis服务器则安装成功12$ sudo rudix install redis$ redis-server 最后用pip为Python安装Redis客户端库1$ sudo pip install redis Redis数据结构简介Redis可以存储键与5种不同数据结构类型之间的映射，分别是STRING(字符串)、LIST(列表)、SET(集合)、HASH(散列)、ZSET(有序集合)。有一部分命令对于这5种数据结构是通用的，如DEL、TYPE、RENAME等；但也有一部分命令只能对特定的一种或者两种结构使用。 下表从结构存储的值及读写能力对比了Redis的5种数据结构。 结构类型 结构存储的值 结构的读写能力 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作；对整数和浮点数进行自增或自减操作 LIST 一个链表，链表上的每个节点都包含了一个字符串 从链表两端推入或弹出元素；根据偏移量对链表进行修剪(trim)；读取单个或多个元素；根据值查找或移除元素 SET 包含字符串的无序收集器，并且被包含的每个字符串互不相同 添加、获取、移除单个元素；检查一个元素是否存在于集合中；计算交集、并集、差集；从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对；获取所有键值对 ZSET 字符串成员(member)与浮点数分值(score)之间的有序映射，元素的排列顺序由分值的大小决定 添加、获取、删除单个元素；根据分值范围(range)或者成员来获取元素 Redis字符串下表展示了3种简单的字符串命令： 命令 行为 GET 获取存储在给定键中的值 SET 设置存储在给定键中的值 DEL 删除存储在给定键中的值(该命令可用于所有类型) SET、GET、DEL的使用示例：12345678910$ redis-cli127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; get hello\"world\"127.0.0.1:6379&gt; del hello(integer) 1127.0.0.1:6379&gt; get hello(nil)127.0.0.1:6379&gt; SET命令在执行成功时返回OK，Python客户端会将这个OK转换为True；DEL命令在执行成功时将会返回被成功删除的值的数量；GET命令在尝试得到不存在的值时，将会返回一个nil，Python客户端会将这个nil转换为None。 Redis列表下表展示了6种简单的列表命令： 命令 行为 LPUSH(RPUSH) 将给定值推入列表的左端(右端) LPOP(RPOP) 从列表的左端(右端)弹出一个值，并返回被弹出的值 LINDEX 获取列表在给定位置上的单个值 LRANGE 获取列表在给定范围上的所有值 RPUSH、LRANGE、LINDEX、LPOP的使用示例12345678910111213141516171819$ redis-cli127.0.0.1:6379&gt; rpush list-key item(integer) 1127.0.0.1:6379&gt; rpush list-key item2(integer) 2127.0.0.1:6379&gt; rpush list-key item(integer) 3127.0.0.1:6379&gt; lrange list-key 0 -11) \"item\"2) \"item2\"3) \"item\"127.0.0.1:6379&gt; lindex list-key 1\"item2\"127.0.0.1:6379&gt; lpop list-key \"item\"127.0.0.1:6379&gt; lrange list-key 0 -11) \"item2\"2) \"item\"127.0.0.1:6379&gt; RPUSH和LPUSH命令在执行成功后会返回当前列表的长度；列表索引范围从0开始，到-1结束，可以取出列表包含的所有元素；使用LINDEX可以从列表中取出单个元素。 Redis集合Redis的集合和列表都可以存储多个字符串，它们的不同之处在于，列表可以存储多个相同的字符串，而集合则通过散列表来保证自己存储的每个字符串都是不同的(这些散列表只有键)。 下表展示了6种简单的集合命令： 命令 行为 SADD 将给定元素添加到集合 SMEMBERS 返回集合包含的所有元素 SISMEMBER 检查给定元素是否存在于集合中 SREM 如果给定的元素存在于集合中，那么移除这个元素 SADD、SMEMBERS、SISMEMBER、SREM的使用示例12345678910111213141516171819202122232425$ redis-cli127.0.0.1:6379&gt; sadd set-key item(integer) 1127.0.0.1:6379&gt; sadd set-key item2(integer) 1127.0.0.1:6379&gt; sadd set-key item3(integer) 1127.0.0.1:6379&gt; sadd set-key item(integer) 0127.0.0.1:6379&gt; smembers set-key1) \"item2\"2) \"item3\"3) \"item\"127.0.0.1:6379&gt; sismember set-key item4(integer) 0127.0.0.1:6379&gt; sismember set-key item(integer) 1127.0.0.1:6379&gt; srem set-key item2(integer) 1127.0.0.1:6379&gt; srem set-key item2(integer) 0127.0.0.1:6379&gt; smembers set-key1) \"item3\"2) \"item\"127.0.0.1:6379&gt; SADD命令返回1表示成功添加到集合中，返回0表示该元素已存在于集合中；SMEMBERS命令获取到的元素组成的序列将会被Python客户端转换为Python集合；Python客户端会返回一个布尔值来表示SISMEMBER命令的检查结果；SREM命令会返回被移除元素的数量。 Redis散列Redis的散列就像一个微型Redis，它可以存储多个键值对之间的映射。和字符串一样，散列存储的值既可以是字符串也可以是数值。可以将散列看做文档数据库里面的文档，还可以看做是关系数据库里面的行，因为散列、文档和行都允许用户同时访问或修改一个或多个域(field)。 下表展示了4种简单的列表命令： 命令 行为 HSET 在散列里面关联给定的键值对 HGET 获取指定散列键的值 HGETALL 获取散列包含的所有键值对 HDEL 如果给定键存在于散列里面，那么移除这个键 HSET、HGET、HGETALL、HDEL的使用示例12345678910111213141516171819202122$ redis-cli127.0.0.1:6379&gt; hset hash-key sub-key1 value1(integer) 1127.0.0.1:6379&gt; hset hash-key sub-key2 value2(integer) 1127.0.0.1:6379&gt; hset hash-key sub-key1 value1(integer) 0127.0.0.1:6379&gt; hgetall hash-key1) \"sub-key1\"2) \"value1\"3) \"sub-key2\"4) \"value2\"127.0.0.1:6379&gt; hdel hash-key sub-key2(integer) 1127.0.0.1:6379&gt; hdel hash-key sub-key2(integer) 0127.0.0.1:6379&gt; hget hash-key sub-key1\"value1\"127.0.0.1:6379&gt; hgetall hash-key1) \"sub-key1\"2) \"value1\"127.0.0.1:6379&gt; HSET返回一个值来表示给定的键是否已经存在于散列里面；Python客户端会把HGETALL命令获取的整个散列转换为一个Python字典；HDEL命令执行后会返回一个值来表示给定的键在移除之前是否存在于散列里面。 Redis有序集合有序集合和散列一样，都用于存储键值对：其中有序集合的每个键称为成员（member），都是独一无二的，而有序集合的每个值称为分值（score），都必须是浮点数。有序集合是Redis里面唯一既可以根据成员访问元素（这一点和散列一样），又可以根据分值以及分值的排列顺序来访问元素的结构。 下表展示了4种简单的有序集合命令： 命令 行为 ZADD 将一个带有给定分值的成员添加到有序集合里面 ZRANGE 根据元素在有序排列中所处的位置，从有序集合里获取多个元素 ZRANGEBYSCORE 获取有序集合在给定分值范围内的所有元素 ZREM 如果给定成员存在于有序集合，那么移除这个成员 ZADD、ZRANGE、ZRANGEBYSCORE、ZREM的使用示例1234567891011121314151617181920212223$ redis-cli127.0.0.1:6379&gt; zadd zset-key 728 member1(integer) 1127.0.0.1:6379&gt; zadd zset-key 982 member0(integer) 1127.0.0.1:6379&gt; zadd zset-key 982 member0(integer) 0127.0.0.1:6379&gt; zrange zset-key 0 -1 withscores1) \"member1\"2) \"728\"3) \"member0\"4) \"982\"127.0.0.1:6379&gt; zrangebyscore zset-key 0 800 withscores1) \"member1\"2) \"728\"127.0.0.1:6379&gt; zrem zset-key member1(integer) 1127.0.0.1:6379&gt; zrem zset-key member1(integer) 0127.0.0.1:6379&gt; zrange zset-key 0 -1 withscores1) \"member0\"2) \"982\"127.0.0.1:6379&gt; 在尝试向有序集合添加元素的时候，ZADD命令会返回新添加元素的数量；ZRANGE命令获取有序集合包含的所有元素，这些元素会按照分值进行排序，Python客户端会将这些分值转换成浮点数；ZRANGEBYSCORE命令也可以根据分值来获取有序集合的其中一部分元素；ZREM命令在移除有序集合元素的时候，命令会返回被移除元素的数量。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis, Python","slug":"Redis-Python","permalink":"http://yoursite.com/tags/Redis-Python/"}]}]}